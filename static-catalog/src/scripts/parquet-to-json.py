#!/usr/bin/env pyspark

# Reads the HF-derived metadata as parquet files and writes the data to JSON.
# Code generated by llama3.3:latest running in Ollama using this query:
# >>> write a pyspark program that reads parquet files and writes them as JSON files
# Plus some addtions afterwards...

from pyspark.sql import SparkSession
from datetime import datetime
import argparse, os, pathlib, re, sys

def parquet_to_json(args):
    # Create a new Spark Session or get an existing one
    spark = SparkSession.builder.appName("ParquetToJSON").getOrCreate()

    # Read Parquet files
    df = spark.read.parquet(args.input)

    # Show the schema and a few rows of data for verification (optional)
    print("Schema:")
    df.printSchema()
    print("Data Preview:")
    df.show()

    df.registerTempTable('hf')
    df_good = spark.sql("""
        SELECT croissant FROM hf WHERE response_reason = 'OK'
        """)
    df_good.count()
    df_good.show()
    df_good.show(n=1, truncate=False)

    df_bad = spark.sql("""
        SELECT croissant FROM hf WHERE response_reason != 'OK'
        """)
    df_bad.count()
    df_bad.show()
    df_bad.show(n=1, truncate=False)

    # Specify the output path for JSON files of the good data.

    def make_directories(path: str):
        new_dir_path = pathlib.Path(path)
        try:
            new_dir_path.mkdir(parents=True)
            print(f"Directory '{new_dir_path}' created successfully.")
        except FileExistsError:
            print(f"Directory '{new_dir_path}' already exists.")
        except OSError as e:
            print(f"Error creating directory: {e}")
            sys.exit(1)

    make_directories(args.output)
    
    # Write DataFrame as JSON files
    df_good.write.json(args.output, mode="overwrite")

    # Stop the SparkSession when you're done
    spark.stop()
    
def run():
    parser = argparse.ArgumentParser(
                        prog='hf-metadata',
                        description='Reads HF metadata from Parquet and writes what we need to JSON',
                        epilog='')
    parser.add_argument('-i', '--input',
                        help="The input directory of parquet files.")
    parser.add_argument('-o', '--output',
                        help="The output directory for JSON files.")
    parser.add_argument('-v', '--verbose',
                        help="Show verbose output",
                        action='store_true')  # on/off flag
    args = parser.parse_args(sys.argv[1:])
    parquet_to_json(args)

if __name__ == "__main__":
    run()
