{"name":"Pt-Corpus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPortuguese-Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nPortuguese-Corpus is a concatenation of several portions of Brazilian Portuguese datasets found in the Hub.\\nIn a tokenized format, the dataset (uncompressed) weighs 50 GB and has approximately 4.1B tokens. This version does not have instructional content.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for tasks involving language modeling.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nPortuguese.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/Pt-Corpus.","license":"https://choosealicense.com/licenses/other/","url":"https://huggingface.co/datasets/nicholasKluge/Pt-Corpus","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","keywords":["text-generation","Portuguese","other","1M - 10M","parquet","Text","Datasets","Dask","Croissant","Polars","arxiv:2112.11446","ðŸ‡ºðŸ‡¸ Region: US","portuguese","language-modeling"]}
{"name":"Pt-Corpus-tokenized","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPortuguese-Corpus (tokenized)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository has a tokenized version (using the TeenyTinyLlama tokenizer) of the Portuguese-Corpus dataset. All sequences are 2048 tokens long. This dataset was used in \\\"TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese\\\".\\nFor more information, see the original dataset card.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nPortuguese.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/Pt-Corpus-tokenized.","license":"https://choosealicense.com/licenses/other/","url":"https://huggingface.co/datasets/nicholasKluge/Pt-Corpus-tokenized","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","keywords":["text-generation","Portuguese","other","1M - 10M","parquet","Datasets","Dask","Croissant","Polars","ðŸ‡ºðŸ‡¸ Region: US","portuguese","language-modeling"]}
{"name":"aya_dataset_pt","description":"CohereForAI Aya Dataset filtrado para portuguÃªs (PT).\\nAya Dataset Summary\\nThe Aya Dataset is a multilingual instruction fine-tuning dataset curated by an open-science community via Aya Annotation Platform from Cohere For AI. The dataset contains a total of 204k human-annotated prompt-completion pairs along with the demographics data of the annotators.\\nThis dataset can be used to train, finetune, and evaluate multilingual LLMs.\\nCurated by: Contributors of Aya Open Science Intiative.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/botbot-ai/aya_dataset_pt.","license":"https://choosealicense.com/licenses/apache-2.0/","url":"https://huggingface.co/datasets/botbot-ai/aya_dataset_pt","creator_name":"BotBot","creator_url":"https://huggingface.co/botbot-ai","keywords":["Portuguese","apache-2.0","1K - 10K","parquet","Text","Datasets","pandas","Croissant","Polars","ðŸ‡ºðŸ‡¸ Region: US","aya","portuguese","legal","chemistry"]}
{"name":"anvisa_instruct_tokenized","description":"EikESousA/anvisa_instruct_tokenized dataset hosted on Hugging Face and contributed by the HF Datasets community","license":"https://choosealicense.com/licenses/other/","url":"https://huggingface.co/datasets/EikESousA/anvisa_instruct_tokenized","creator_name":"Eike Natan Sousa Brito","creator_url":"https://huggingface.co/EikESousA","keywords":["text-generation","Portuguese","other","1M - 10M","parquet","Datasets","Dask","Croissant","Polars","ðŸ‡ºðŸ‡¸ Region: US","portuguese","language-modeling"]}
{"name":"portuguese-blogs","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nBlog-1 may include other languages in an unstructured text format without markdown. The latest one, Blog-6, is formatted in markdown and may contain less other languages text.\\nTexts are separated by the string <|endoftext|>.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUses\\n\\t\\n\\nTraining language models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nA simple text file with articles separated by <|endoftext|> between each text.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\nFirst semester of 2024.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBias, Risksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fabiovilao/portuguese-blogs.","license":"https://choosealicense.com/licenses/apache-2.0/","url":"https://huggingface.co/datasets/fabiovilao/portuguese-blogs","creator_name":"fabio vila","creator_url":"https://huggingface.co/fabiovilao","keywords":["text-generation","Portuguese","apache-2.0","100M - 1B","text","Text","Datasets","Croissant","ðŸ‡ºðŸ‡¸ Region: US","portuguese","text","blogs"]}
{"name":"ptbr-question-and-answer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPerguntas e Respostas Brasileiras\\n\\t\\n\\nEsse dataset Ã© uma compilaÃ§Ã£o das perguntas e respostas em portuguÃªs disponÃ­veis em clips/mqa.\\nFoi realizada uma limpeza e normalizaÃ§Ã£o dos dados, mantendo apenas domÃ­nios mais relevantes, removendo texto danosos e inadequados.\\nO cÃ³digo para a limpeza dos dados pode ser acessado aqui\\nO principal objetivo deste dataset Ã© ajudar modelos de linguagem natural e modelos de embedding em portuguÃªs a gerar textos e cÃ¡lculos de similaridade\\nmais precisosâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/emdemor/ptbr-question-and-answer.","license":"https://choosealicense.com/licenses/cc/","url":"https://huggingface.co/datasets/emdemor/ptbr-question-and-answer","creator_name":"Eduardo Morais","creator_url":"https://huggingface.co/emdemor","keywords":["question-answering","text2text-generation","sentence-similarity","Portuguese","cc","1M - 10M","parquet","Text","Datasets","pandas","Croissant","Polars","ðŸ‡ºðŸ‡¸ Region: US","q&a","brazil","question","embedding","brasil","portuguese","portugues","perguntas"]}
{"name":"wikipedia-PT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWikipedia-PT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Portuguese portion of the Wikipedia dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThe dataset is generally used for Language Modeling.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nPortuguese\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nAn example looks as follows:\\n{\\n 'text': 'Abril Ã© o quarto mÃªs...'\\n}\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\n\\ntext (str): Text content of the article.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits\\n\\t\\n\\nAll configurationsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TucanoBR/wikipedia-PT.","license":"https://choosealicense.com/licenses/cc-by-sa-3.0/","url":"https://huggingface.co/datasets/TucanoBR/wikipedia-PT","creator_name":"Tucano","creator_url":"https://huggingface.co/TucanoBR","keywords":["text-generation","Portuguese","cc-by-sa-3.0","1M - 10M","parquet","Text","Datasets","Dask","Croissant","Polars","ðŸ‡ºðŸ‡¸ Region: US","portuguese"]}
{"name":"Tucano-SFT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTucano-SFT\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis is the dataset used to train the \\\"Instruct\\\" versions of the Tucano series, being a concatenation of three datasets:\\n\\ncnmoro/GPT4-500k-Augmented-PTBR-Clean\\nrhaymison/orca-math-portuguese-64k\\nnicholasKluge/instruct-aira-dataset-v3\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for tasks involving the aligment of language models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nPortuguese\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TucanoBR/Tucano-SFT.","license":"https://choosealicense.com/licenses/other/","url":"https://huggingface.co/datasets/TucanoBR/Tucano-SFT","creator_name":"Tucano","creator_url":"https://huggingface.co/TucanoBR","keywords":["text-generation","Portuguese","other","100K - 1M","parquet","Text","Datasets","Dask","Croissant","Polars","arxiv:2411.07854","ðŸ‡ºðŸ‡¸ Region: US","portuguese","language-modeling","chat","conversation","instruction"]}
{"name":"oab_bench","description":"\\n\\t\\n\\t\\t\\n\\t\\tOABench: Brazilian Bar Exams Benchmark Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nOABench is a benchmark dataset designed to evaluate the performance of Large Language Models (LLMs) on Brazilian legal exams. It is based on the Unified Bar Exam of the Brazilian Bar Association (OAB), a comprehensive and challenging exam required for law graduates to practice law in Brazil. This dataset provides a rigorous and realistic testbed for LLMs in the legal domain, covering a wide range of legal topicsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/felipeoes/oab_bench.","license":"https://choosealicense.com/licenses/mit/","url":"https://huggingface.co/datasets/felipeoes/oab_bench","creator_name":"Felipe Oliveira","creator_url":"https://huggingface.co/felipeoes","keywords":["question-answering","Portuguese","mit","< 1K","parquet","Tabular","Text","Datasets","pandas","Croissant","Polars","ðŸ‡ºðŸ‡¸ Region: US","legal","law","benchmark","portuguese","brazilian"]}
{"name":"spider-test-portuguese","description":"\\n\\t\\n\\t\\t\\n\\t\\tSpider Dataset - VersÃ£o em PortuguÃªs\\n\\t\\n\\nEste repositÃ³rio contÃ©m a traduÃ§Ã£o para portuguÃªs da partiÃ§Ã£o de teste do dataset Spider, um benchmark para a tarefa de Text-to-SQL.\\n\\n\\t\\n\\t\\t\\n\\t\\tSobre esta traduÃ§Ã£o\\n\\t\\n\\nA traduÃ§Ã£o da partiÃ§Ã£o \\\"test\\\" do Spider (contendo 2.147 instÃ¢ncias) foi realizada seguindo um processo rigoroso:\\n\\nTraduÃ§Ã£o inicial: Utilizando a API do GPT-4o mini da OpenAI\\nRevisÃ£o manual: Todas as 2.147 questÃµes foram revisadas e validadas manualmente\\nCritÃ©rios de traduÃ§Ã£o:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Boakpe/spider-test-portuguese.","license":"https://choosealicense.com/licenses/cc-by-sa-4.0/","url":"https://huggingface.co/datasets/Boakpe/spider-test-portuguese","creator_name":"Breno","creator_url":"https://huggingface.co/Boakpe","keywords":["Portuguese","cc-by-sa-4.0","1K<n<10K","arxiv:1809.08887","ðŸ‡ºðŸ‡¸ Region: US","sql","portuguese","spider","text-to-sql","text2sql","nlp","spider-dataset","benchmark","question-answering","database"]}
{"name":"parlamento-pt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ParlamentoPT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe ParlamentoPT is a Portuguese language data set obtained by collecting publicly available documents containing transcriptions of debates in the Portuguese Parliament.\\nThe data was collected from the Portuguese Parliament portal in accordance with its open data policy.\\nThis dataset was collected with the purpose of creating the Albertina-PT* language model, and it serves as training data for model development. \\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PORTULAN/parlamento-pt.","license":"https://choosealicense.com/licenses/other/","url":"https://huggingface.co/datasets/PORTULAN/parlamento-pt","creator_name":"PORTULAN","creator_url":"https://huggingface.co/PORTULAN","keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation","monolingual","original","Portuguese","other","1M<n<10M","Text","arxiv:2305.06721","ðŸ‡ºðŸ‡¸ Region: US","parlamentopt","parlamento","parlamento-pt","albertina-pt*","albertina-ptpt","albertina-ptbr","fill-mask","bert","deberta","portuguese","encoder","foundation model"]}
{"name":"Pt-Corpus-Instruct","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPortuguese-Corpus Instruct\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nPortuguese-Corpus Instruct is a concatenation of several portions of Brazilian Portuguese datasets found in the Hub.\\nIn a tokenized format, the dataset (uncompressed) weighs 80 GB and has approximately 6.2B tokens. This version of the corpus (Pt-Corpus-Instruct) includes several instances of conversational and general instructional data, allowing trained models to go through preference pre-training during their initialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/Pt-Corpus-Instruct.","license":"https://choosealicense.com/licenses/other/","url":"https://huggingface.co/datasets/nicholasKluge/Pt-Corpus-Instruct","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","keywords":["text-generation","Portuguese","other","10M - 100M","parquet","Text","Datasets","Dask","Croissant","Polars","arxiv:2112.11446","ðŸ‡ºðŸ‡¸ Region: US","portuguese","language-modeling"]}
{"name":"GigaVerbo-Text-Filter","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGigaVerbo Text-Filter\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGigaVerbo Text-Filter is a dataset with 110,000 randomly selected samples from 9 subsets of GigaVerbo (i.e., specifically those that were not synthetic). This dataset was used to train the text-quality filters described in \\\"Tucano: Advancing Neural Text Generation for Portuguese\\\". To create the text embeddings, we used sentence-transformers/LaBSE. All scores were generated by GPT-4o.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TucanoBR/GigaVerbo-Text-Filter.","license":"https://choosealicense.com/licenses/apache-2.0/","url":"https://huggingface.co/datasets/TucanoBR/GigaVerbo-Text-Filter","creator_name":"Tucano","creator_url":"https://huggingface.co/TucanoBR","keywords":["text-classification","Portuguese","apache-2.0","100K - 1M","parquet","Text","Datasets","Dask","Croissant","Polars","arxiv:2411.07854","ðŸ‡ºðŸ‡¸ Region: US","portuguese","language-modeling"]}
{"name":"GigaVerbo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGigaVerbo: a 780 GB Dataset of Portuguese Text\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGigaVerbo is an extensive dataset comprising 780 GB of Portuguese text, being a concatenated version of several datasets available in Hugging Face, containing over 200 billion tokens. It encompasses various sources, including crawled websites, articles, translated conversations, and legal documents. This dataset offers a comprehensive and rich resource for various natural language processing tasksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TucanoBR/GigaVerbo.","license":"https://choosealicense.com/licenses/other/","url":"https://huggingface.co/datasets/TucanoBR/GigaVerbo","creator_name":"Tucano","creator_url":"https://huggingface.co/TucanoBR","keywords":["text-generation","Portuguese","other","100M - 1B","parquet","Tabular","Text","Datasets","Dask","Croissant","Polars","arxiv:2411.07854","ðŸ‡ºðŸ‡¸ Region: US","portuguese","language-modeling"]}
{"name":"calame-pt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCALAME-PT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContext-Aware LAnguage Modeling Evaluation for Portuguese\\n\\t\\n\\nCALAME-PT is a PT benchmark composed of small texts (contexts) and their respective last words. \\nThese contexts should, in theory, contain enough information so that a human or a model is capable of guessing its last word - without being too specific and/or too ambiguous.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tComposition\\n\\t\\n\\nCALAME-PT is composed of 2 \\\"sets\\\" of data - handwritten and generated. \\n\\nHandwritten Set: containsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NOVA-vision-language/calame-pt.","license":"https://choosealicense.com/licenses/mit/","url":"https://huggingface.co/datasets/NOVA-vision-language/calame-pt","creator_name":"NOVA Vision & Language","creator_url":"https://huggingface.co/NOVA-vision-language","keywords":["Portuguese","mit","1K - 10K","json","Text","Datasets","pandas","Croissant","Polars","ðŸ‡ºðŸ‡¸ Region: US","generation","language modeling","portuguese","lambada","zero-shot"]}
{"name":"Pt-Corpus-Instruct-tokenized-large","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPortuguese-Corpus Instruct (tokenized large)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository has a tokenized version (using the TeenyTinyLlama tokenizer) of the Portuguese-Corpus Instruct dataset. All sequences are 2048 tokens long. All sequences are 2048 tokens long. This dataset was used in \\\"TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese\\\".\\nFor more information, see the original dataset card.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nPortuguese.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/Pt-Corpus-Instruct-tokenized-large.","license":"https://choosealicense.com/licenses/other/","url":"https://huggingface.co/datasets/nicholasKluge/Pt-Corpus-Instruct-tokenized-large","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","keywords":["text-generation","Portuguese","other","1M - 10M","parquet","Datasets","Dask","Croissant","Polars","ðŸ‡ºðŸ‡¸ Region: US","portuguese","language-modeling"]}
