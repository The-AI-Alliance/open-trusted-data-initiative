const data_for_language_europe_latin = 
[
	{"name":"c4","keyword":"latin","description":"\n\t\n\t\t\n\t\tC4\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: \"https://commoncrawl.org\".\nThis is the processed version of Google's C4 dataset\nWe prepared five variants of the data: en, en.noclean, en.noblocklist, realnewslike, and multilingual (mC4).\nFor reference, these are the sizes of the variants:\n\nen: 305GB\nen.noclean: 2.3TB\nen.noblocklist: 380GB\nrealnewslike: 15GB\nmultilingual (mC4): 9.7TB (108 subsets, one per‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/c4.","url":"https://huggingface.co/datasets/allenai/c4","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"LatinYoutube","keyword":"latin","description":"This is a dataset with text/audio pairs of Classical Latin extracted from youtube videos from the channels Scorpio Martianus, LATINITIUS and Musa Pedestris\n","url":"https://huggingface.co/datasets/thiagolira/LatinYoutube","creator_name":"Thiago Ildeu Albuquerque Lira","creator_url":"https://huggingface.co/thiagolira","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["Latin","afl-3.0","1K - 10K","parquet","Audio"],"keywords_longer_than_N":true},
	{"name":"udhr-lid","keyword":"latin","description":"\n\t\n\t\t\n\t\tUDHR-LID\n\t\n\nWhy UDHR-LID?\nYou can access UDHR (Universal Declaration of Human Rights) here, but when a verse is missing, they have texts such as \"missing\" or \"?\". Also, about 1/3 of the sentences consist only of \"articles 1-30\" in different languages. We cleaned the entire dataset from XML files and selected only the paragraphs. We cleared any unrelated language texts from the data and also removed the cases that were incorrect.\nIncorrect? Look at the ckb and kmr files in the UDHR.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/udhr-lid.","url":"https://huggingface.co/datasets/cis-lmu/udhr-lid","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Tigrinya","Balkan Romani","Standard Arabic","Metlat√≥noc Mixtec"],"keywords_longer_than_N":true},
	{"name":"CC-Cat","keyword":"latin","description":"\n\t\n\t\t\n\t\tCC_Cat\n\t\n\n\nExtract from CC-WARC snapshots.\nMainly includes texts with 149 languages.\nPDF/IMAGE/AUDIO/VIDEO raw downloading link.\n\n\n\t\n\t\t\n\t\tNotice\n\t\n\n\nSince my computing resources are limited, this dataset will update by one-day of CC snapshots timestampts.\nAfter a snapshot is updated, the deduplicated version will be uploaded.\nIf you are interested in providing computing resources or have cooperation needs, please contact me.\n  carreyallthetime@gmail.com  \n      \n  \n\n","url":"https://huggingface.co/datasets/chengshidehaimianti/CC-Cat","creator_name":"zyq","creator_url":"https://huggingface.co/chengshidehaimianti","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","German","Russian"],"keywords_longer_than_N":true},
	{"name":"tapaco","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for TaPaCo Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA freely available paraphrase corpus for 73 languages extracted from the Tatoeba database. \nTatoeba is a crowdsourcing project mainly geared towards language learners. Its aim is to provide example sentences \nand translations for particular linguistic constructions and words. The paraphrase corpus is created by populating a \ngraph with Tatoeba sentences and equivalence links between sentences ‚Äúmeaning the same thing‚Äù. This‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/tapaco.","url":"https://huggingface.co/datasets/community-datasets/tapaco","creator_name":"Community Datasets","creator_url":"https://huggingface.co/community-datasets","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","text-classification","semantic-similarity-classification","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"finepdfs","keyword":"latin","description":"\n\nLiberating 3T of the finest tokens from PDFs\n\n\n\t\n\t\t\n\t\tWhat is this?\n\t\n\nAs we run out of web pages to process, the natural question has always been: what to do next? Only a few knew about a data source that everyone avoided for ages, due to its incredible extraction cost and complexity: PDFs.\nüìÑ FinePDFs is exactly that. It is the largest publicly available corpus sourced exclusively from PDFs, containing about 3 trillion tokens across 475 million documents in 1733 languages.\nCompared to HTML‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/finepdfs.","url":"https://huggingface.co/datasets/HuggingFaceFW/finepdfs","creator_name":"FineData","creator_url":"https://huggingface.co/HuggingFaceFW","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Arifama-Miniafia","Ankave","Abau","Amarasi"],"keywords_longer_than_N":true},
	{"name":"xP3x","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for xP3x\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nxP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts & datasets across 277 languages & 16 NLP tasks. It contains all of xP3 + much more! It is used for training future contenders of mT0 & BLOOMZ at project Aya @Cohere Labs üß°\n\n\nCreation: The dataset can be recreated using instructions available here together with the file in this repository named xp3x_create.py. We provide this version to save processing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/xP3x.","url":"https://huggingface.co/datasets/CohereLabs/xP3x","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"vietnamese-nom-latin-translation","keyword":"latin","description":"lunovian/vietnamese-nom-latin-translation dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/lunovian/vietnamese-nom-latin-translation","creator_name":"Nguyen Xuan An","creator_url":"https://huggingface.co/lunovian","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","Vietnamese","Latin"],"keywords_longer_than_N":true},
	{"name":"LatinSummarizer","keyword":"latin","description":"\n\t\n\t\t\n\t\tLatinSummarizer Dataset\n\t\n\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\naligned_en_la_data_raw.csv\naligned_en_la_data_cleaned.csv\naligned_en_la_data_cleaned_with_stanza.csv\nconcat_aligned_data.csv\nconcat_cleaned.csv\nlatin_wikipedia_cleaned.csv\nlatin_wikipedia_raw.csv\nlatin-literature-dataset-170M_raw_cleaned.csv\nlatin-literature-dataset-170M_raw_cleaned_chunked.csv\nElsa_aligned/\nREADME.md\n\n\n\t\n\t\t\n\t\n\t\n\t\tDetails\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\taligned_en_la_data_raw.csv\n\t\n\nThis dataset contains aligned Latin (la) - English (en)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/LatinSummarizer.","url":"https://huggingface.co/datasets/LatinNLP/LatinSummarizer","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","summarization","news-articles-summarization","document-retrieval"],"keywords_longer_than_N":true},
	{"name":"LatinSummarizer","keyword":"latin","description":"\n\t\n\t\t\n\t\tLatinSummarizer Dataset\n\t\n\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\naligned_en_la_data_raw.csv\naligned_en_la_data_cleaned.csv\naligned_en_la_data_cleaned_with_stanza.csv\nconcat_aligned_data.csv\nconcat_cleaned.csv\nlatin_wikipedia_cleaned.csv\nlatin_wikipedia_raw.csv\nlatin-literature-dataset-170M_raw_cleaned.csv\nlatin-literature-dataset-170M_raw_cleaned_chunked.csv\nElsa_aligned/\nREADME.md\n\n\n\t\n\t\t\n\t\n\t\n\t\tDetails\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\taligned_en_la_data_raw.csv\n\t\n\nThis dataset contains aligned Latin (la) - English (en)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/LatinSummarizer.","url":"https://huggingface.co/datasets/LatinNLP/LatinSummarizer","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","summarization","news-articles-summarization","document-retrieval"],"keywords_longer_than_N":true},
	{"name":"multilingual_translation_gen_binarized","keyword":"latin","description":"Youseff1987/multilingual_translation_gen_binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Youseff1987/multilingual_translation_gen_binarized","creator_name":"JOON HYOUNG JUN","creator_url":"https://huggingface.co/Youseff1987","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Korean","English","Chinese","Zulu"],"keywords_longer_than_N":true},
	{"name":"test_modern_dataset","keyword":"latin","description":"SerhiiLebediuk/test_modern_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/SerhiiLebediuk/test_modern_dataset","creator_name":"Serhii Lebediuk","creator_url":"https://huggingface.co/SerhiiLebediuk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Latin","English","Ukrainian","mit"],"keywords_longer_than_N":true},
	{"name":"TreeOfLife-200M","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for TreeOfLife-200M\n\t\n\nWith nearly 214 million images representing 952,257 taxa across the tree of life, TreeOfLife-200M is the largest and most diverse public ML-ready dataset for computer vision models in biology at release. This dataset combines images and metadata from four core biodiversity data providers: Global Biodiversity Information Facility (GBIF), Encyclopedia of Life (EOL), BIOSCAN-5M, and FathomNet to more than double the number of unique taxa covered by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imageomics/TreeOfLife-200M.","url":"https://huggingface.co/datasets/imageomics/TreeOfLife-200M","creator_name":"HDR Imageomics Institute","creator_url":"https://huggingface.co/imageomics","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","English","Latin","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"latin","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"tatoeba-tokipona","keyword":"latin","description":"NetherQuartz/tatoeba-tokipona dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/NetherQuartz/tatoeba-tokipona","creator_name":"Vladimir Larkin","creator_url":"https://huggingface.co/NetherQuartz","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","Toki Pona","English","Russian","Ukrainian"],"keywords_longer_than_N":true},
	{"name":"modern","keyword":"latin","description":"\n\n\t\n\t\t\n\t\tDataset Card for CATMuS Modern and Contemporary (McCATMuS)\n\t\n\nJoin our Discord to ask questions about the dataset: \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nHandwritten Text Recognition (HTR) has emerged as a crucial tool for converting manuscripts images into machine-readable formats, enabling researchers and scholars to analyze vast collections efficiently. Despite significant technological progress, establishing consistent ground truth across projects for HTR tasks, particularly for complex and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATMuS/modern.","url":"https://huggingface.co/datasets/CATMuS/modern","creator_name":"CATMuS: Consistent Approach to Transcribing ManuScripts","creator_url":"https://huggingface.co/CATMuS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","German","English","Italian"],"keywords_longer_than_N":true},
	{"name":"librivox-tracks","keyword":"latin","description":"A dataset of all audio files uploaded to LibriVox before 26th September 2023.\nForked from https://huggingface.co/datasets/pykeio/librivox-tracks\nChanges:\n\nUsed archive.org metadata API to annotate rows with \"duration\" column\n\n","url":"https://huggingface.co/datasets/xacer/librivox-tracks","creator_name":"xacer","creator_url":"https://huggingface.co/xacer","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","feature-extraction","Achinese","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"wiktionary-data","keyword":"latin","description":"\n\t\n\t\t\n\t\tWiktionary Data on Hugging Face Datasets\n\t\n\n\n\n\n\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\nsupports the following languages:\n\nDeutsch - German\nLatinum - Latin\n·ºôŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ - Ancient Greek\nÌïúÍµ≠Ïñ¥ - Korean\nêé†êéºêéπ- Old Persian\níÄùíÖóíÅ∫íåë(íåù) - Akkadian\nElamite\n‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç - Sanskrit, or Classical Sanskrit\n\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\nthe dataset it's getting bigger, I noticed a wave of more exciting potentials this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/paion-data/wiktionary-data.","url":"https://huggingface.co/datasets/paion-data/wiktionary-data","creator_name":"Paion Data","creator_url":"https://huggingface.co/paion-data","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","German","Latin","Ancient Greek (to 1453)","Korean"],"keywords_longer_than_N":true},
	{"name":"wiktionary-data","keyword":"latin","description":"\n\t\n\t\t\n\t\tWiktionary Data on Hugging Face Datasets\n\t\n\n\n\n\n\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\nsupports the following languages:\n\nDeutsch - German\nLatinum - Latin\n·ºôŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ - Ancient Greek\nÌïúÍµ≠Ïñ¥ - Korean\nêé†êéºêéπ- Old Persian\níÄùíÖóíÅ∫íåë(íåù) - Akkadian\nElamite\n‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç - Sanskrit, or Classical Sanskrit\n\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\nthe dataset it's getting bigger, I noticed a wave of more exciting potentials this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/paion-data/wiktionary-data.","url":"https://huggingface.co/datasets/paion-data/wiktionary-data","creator_name":"Paion Data","creator_url":"https://huggingface.co/paion-data","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","German","Latin","Ancient Greek (to 1453)","Korean"],"keywords_longer_than_N":true},
	{"name":"WillyShakes","keyword":"latin","description":"cd your-dataset-name\ncp /path/to/your/data/* .\ngit add .\ngit commit -m \"Add my dataset\"\ngit push\n","url":"https://huggingface.co/datasets/scenecoachai/WillyShakes","creator_name":"Janis Deedy","creator_url":"https://huggingface.co/scenecoachai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","Latin","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"librivox-tracks","keyword":"latin","description":"A dataset of all audio files uploaded to LibriVox before 26th September 2023.\n","url":"https://huggingface.co/datasets/pykeio/librivox-tracks","creator_name":"pyke.io","creator_url":"https://huggingface.co/pykeio","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Achinese","Afrikaans","Ancient Greek (to 1453)"],"keywords_longer_than_N":true},
	{"name":"fiNERweb","keyword":"latin","description":"fiNERweb is a multilingual named entity recognition dataset containing annotated text in multiple languages.\nEach example contains the original text, tokenized text, BIO tags, and character/token spans for entities.","url":"https://huggingface.co/datasets/whoisjones/fiNERweb","creator_name":"Jonas Golde","creator_url":"https://huggingface.co/whoisjones","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","Vietnamese","Tamil","Oriya"],"keywords_longer_than_N":true},
	{"name":"Tatoeba-Translations","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis is the latest version of Tatoeba translations as of December 2024.\nThe sentences are downloaded from the Tatoeba collection website.\nThe dataset is processed through mapping sentences.tar.bz2 using sentences_base.tar.bz2 to find source (sentence_src) and target (sentence_tgt) sentences.\nWhile lang_src and lang_tgt columns follow the mapping provided by Tatoeba, the lang_pair column merely lists the two languages in the translation pair.\n\n\t\n\t\t\n\t\n\t\n\t\tStatistics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/Tatoeba-Translations.","url":"https://huggingface.co/datasets/ymoslem/Tatoeba-Translations","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","Abkhaz","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for BibleNLP Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPartial and complete Bible translations in 833 languages, aligned by verse.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\naai, aak, aau, aaz, abt, abx, aby, acf, acr, acu, adz, aer, aey, agd, agg, agm, agn, agr, agt, agu, aia, aii, aka, ake, alp, alq, als, aly, ame, amf, amk, amm, amn, amo, amp, amr, amu, amx, anh, anv, aoi, aoj, aom, aon, apb, ape, apn, apr, apu, apw, apz, arb, are, arl, arn, arp, asm, aso, ata, atb, atd, atg, att, auc, aui, auy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bible-nlp/biblenlp-corpus.","url":"https://huggingface.co/datasets/bible-nlp/biblenlp-corpus","creator_name":"The Partnership for Applied Biblical NLP","creator_url":"https://huggingface.co/bible-nlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","no-annotation","expert-generated","translation","multilingual"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for BibleNLP Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPartial and complete Bible translations in 833 languages, aligned by verse.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\naai, aak, aau, aaz, abt, abx, aby, acf, acr, acu, adz, aer, aey, agd, agg, agm, agn, agr, agt, agu, aia, aii, aka, ake, alp, alq, als, aly, ame, amf, amk, amm, amn, amo, amp, amr, amu, amx, anh, anv, aoi, aoj, aom, aon, apb, ape, apn, apr, apu, apw, apz, arb, are, arl, arn, arp, asm, aso, ata, atb, atd, atg, att, auc, aui, auy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bible-nlp/biblenlp-corpus.","url":"https://huggingface.co/datasets/bible-nlp/biblenlp-corpus","creator_name":"The Partnership for Applied Biblical NLP","creator_url":"https://huggingface.co/bible-nlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","no-annotation","expert-generated","translation","multilingual"],"keywords_longer_than_N":true},
	{"name":"latin-jerome-intertextuality-classification","keyword":"latin","description":"\n\t\n\t\t\n\t\tLatin Intertextuality Classification Dataset\n\t\n\nThis dataset contains pairs of Latin texts for binary classification of intertextual relationships, specifically focused on detecting connections between Jerome (Hieronymus) and other classical Latin authors.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nquery_text: The query text (typically from Jerome)\ncandidate_text: The candidate text (from various classical authors)\nlabel: Binary label (1 = intertextual relationship exists, 0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/julian-schelb/latin-jerome-intertextuality-classification.","url":"https://huggingface.co/datasets/julian-schelb/latin-jerome-intertextuality-classification","creator_name":"Julian","creator_url":"https://huggingface.co/julian-schelb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multi-class-classification","Latin","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"latin-jerome-intertextuality-classification","keyword":"latin","description":"\n\t\n\t\t\n\t\tLatin Intertextuality Classification Dataset\n\t\n\nThis dataset contains pairs of Latin texts for binary classification of intertextual relationships, specifically focused on detecting connections between Jerome (Hieronymus) and other classical Latin authors.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nquery_text: The query text (typically from Jerome)\ncandidate_text: The candidate text (from various classical authors)\nlabel: Binary label (1 = intertextual relationship exists, 0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/julian-schelb/latin-jerome-intertextuality-classification.","url":"https://huggingface.co/datasets/julian-schelb/latin-jerome-intertextuality-classification","creator_name":"Julian","creator_url":"https://huggingface.co/julian-schelb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multi-class-classification","Latin","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"bible_para","keyword":"latin","description":"This is a multilingual parallel corpus created from translations of the Bible compiled by Christos Christodoulopoulos and Mark Steedman.\n\n102 languages, 5,148 bitexts\ntotal number of files: 107\ntotal number of tokens: 56.43M\ntotal number of sentence fragments: 2.84M","url":"https://huggingface.co/datasets/Helsinki-NLP/bible_para","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"CulturaY","keyword":"latin","description":"\n\t\n\t\t\n\t\tCulturaY: A Large Cleaned Multilingual Dataset of 75 Languages\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nFrom the team that brought you CulturaX, we present CulturaY, another substantial multilingual dataset of 15TB (uncompressed)/3TB (zstd-compressed) that applies the same dataset cleaning methodology to the HPLT v1.1 dataset. \nPlease note that HPLT v1.2 has also been released and is an alternative verison with different cleaning methodolgies. \nThis data was used in part to train our SOTA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Viet-Mistral/CulturaY.","url":"https://huggingface.co/datasets/Viet-Mistral/CulturaY","creator_name":"Vietnamese Mistral","creator_url":"https://huggingface.co/Viet-Mistral","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"xP3x-sample","keyword":"latin","description":"A multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual commonsense reasoning capabilities.","url":"https://huggingface.co/datasets/Muennighoff/xP3x-sample","creator_name":"Niklas Muennighoff","creator_url":"https://huggingface.co/Muennighoff","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"latin_english_parallel","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for \"latin_english_parallel\"\n\t\n\n101k translation pairs between Latin and English, split 99/1/1 as train/test/val. These have been collected roughly 66% from the Loeb Classical Library and 34% from the Vulgate translation. \nFor those that were gathered from the Loeb Classical Library, alignment was performd manually between Source and Target sequences. Additionally, the English translations were both 1. copyrighted and 2. outdated. As such, we decided to modernize and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/grosenthal/latin_english_parallel.","url":"https://huggingface.co/datasets/grosenthal/latin_english_parallel","creator_name":"Gil Rosenthal","creator_url":"https://huggingface.co/grosenthal","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Latin","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Himanis-line","keyword":"latin","description":"\n\t\n\t\t\n\t\tHimanis - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHimanis (HIstorical MANuscript Indexing for user controlled Search) is a corpus of medieval documents.\nThe historical corpus is described in the following publication:\nStutzmann, D., Moufflet, J-F., & Hamel, S. (2017). La recherche en plein texte dans les sources manuscrites m√©di√©vales‚ÄØ: enjeux et perspectives du projet HIMANIS pour l‚Äô√©dition √©lectronique. M√©di√©vales‚ÄØ: Langue, textes, histoire 73 (2017): 67‚Äë96.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/Himanis-line.","url":"https://huggingface.co/datasets/Teklia/Himanis-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Latin","French","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus-mmteb","keyword":"latin","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\nLoading example:\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"davidstap/biblenlp-corpus-mmteb\", \"eng-arb\", trust_remote_code=True)\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 28723\n    })\n    validation: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 1578\n    })‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb.","url":"https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","expert-generated","translation","multilingual","Arifama-Miniafia"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus-mmteb","keyword":"latin","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\nLoading example:\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"davidstap/biblenlp-corpus-mmteb\", \"eng-arb\", trust_remote_code=True)\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 28723\n    })\n    validation: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 1578\n    })‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb.","url":"https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","expert-generated","translation","multilingual","Arifama-Miniafia"],"keywords_longer_than_N":true},
	{"name":"ApolloMoEDataset","keyword":"latin","description":"\n\t\n\t\t\n\t\tDemocratizing Medical LLMs For Much More Languages\n\t\n\nCovering 12 Major Languages including English, Chinese, French, Hindi, Spanish, Arabic, Russian, Japanese, Korean, German, Italian, Portuguese and 38 Minor Languages So far.\n\n   üìÉ Paper ‚Ä¢ üåê Demo ‚Ä¢ ü§ó ApolloMoEDataset ‚Ä¢ ü§ó ApolloMoEBench  ‚Ä¢ ü§ó Models  ‚Ä¢üåê Apollo  ‚Ä¢ üåê ApolloMoE\n\n\n\n\n\n\n\t\n\t\t\n\t\tüåà Update\n\t\n\n\n[2024.10.15] ApolloMoE repo is publishedÔºÅüéâ\n\n\n\t\n\t\t\n\t\tLanguages Coverage\n\t\n\n12 Major Languages and 38 Minor Languages\n\n  Click to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEDataset.","url":"https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEDataset","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Arabic","English","Chinese","Korean"],"keywords_longer_than_N":true},
	{"name":"muri-it-language-split","keyword":"latin","description":"\n\t\n\t\t\n\t\tMURI-IT: Multilingual Instruction Tuning Dataset for 200 Languages via Multilingual Reverse Instructions\n\t\n\nMURI-IT is a large-scale multilingual instruction tuning dataset containing 2.2 million instruction-output pairs across 200 languages. It is designed to address the challenges of instruction tuning in low-resource languages with Multilingual Reverse Instructions (MURI), which ensures that the output is human-written, high-quality, and authentic to the cultural and linguistic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/akoksal/muri-it-language-split.","url":"https://huggingface.co/datasets/akoksal/muri-it-language-split","creator_name":"Abdullatif Koksal","creator_url":"https://huggingface.co/akoksal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","question-answering","summarization","Achinese"],"keywords_longer_than_N":true},
	{"name":"possumm-local","keyword":"latin","description":"\n\t\n\t\t\n\t\tPOSSUMM Neumes: Musical Symbols from Medieval Music Manuscripts\n\t\n\n\n‚ö†Ô∏è DATASET UNDER CONSTRUCTION - This is currently a private working repository serving as a data bucket while transitioning from GitHub storage. Structure and organization are actively being refined.\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe POSSUMM (Paleographic Object Script Sorter Using Machine Methods) Neumes dataset is a specialized collection of medieval musical symbols extracted from historical manuscripts. This‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/grackle-in-a-HEB-parking-lot/possumm-local.","url":"https://huggingface.co/datasets/grackle-in-a-HEB-parking-lot/possumm-local","creator_name":"Kyrie","creator_url":"https://huggingface.co/grackle-in-a-HEB-parking-lot","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","Latin","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"panlex-meanings","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for panlex-meanings\n\t\n\nThis is a dataset of words in several thousand languages, extracted from https://panlex.org.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset has been extracted from https://panlex.org (the 20240301 database dump) and rearranged on the per-language basis.\nEach language subset consists of expressions (words and phrases). \nEach expression is associated with some meanings (if there is more than one meaning, they are in separate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cointegrated/panlex-meanings.","url":"https://huggingface.co/datasets/cointegrated/panlex-meanings","creator_name":"David Dale","creator_url":"https://huggingface.co/cointegrated","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["translation","Afar","Western Abnaki","Abkhazian","Abaza"],"keywords_longer_than_N":true},
	{"name":"latin-summarizer-dataset","keyword":"latin","description":"\n  \n    ‚ú® LatinSummarizer Dataset ‚ú®\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\nNote: If Dataset Viewer is not available, see samples of dataset for samples from the dataset.The LatinSummarizer Dataset is a comprehensive collection of Latin texts designed to support natural language processing research for a low-resource language. It provides parallel data for various tasks, including translation (Latin-to-English) and summarization (extractive and abstractive).\nThis dataset was created for a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/latin-summarizer-dataset.","url":"https://huggingface.co/datasets/LatinNLP/latin-summarizer-dataset","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["summarization","translation","Latin","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"latin-summarizer-dataset","keyword":"latin","description":"\n  \n    ‚ú® LatinSummarizer Dataset ‚ú®\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\nNote: If Dataset Viewer is not available, see samples of dataset for samples from the dataset.The LatinSummarizer Dataset is a comprehensive collection of Latin texts designed to support natural language processing research for a low-resource language. It provides parallel data for various tasks, including translation (Latin-to-English) and summarization (extractive and abstractive).\nThis dataset was created for a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/latin-summarizer-dataset.","url":"https://huggingface.co/datasets/LatinNLP/latin-summarizer-dataset","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["summarization","translation","Latin","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"fishmt5","keyword":"latin","description":"\n\t\n\t\t\n\t\tFish Names Chinese-Latin Parallel Corpora\n\t\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nWe curated over 60,000 authoritative Chinese-Latin bilingual parallel corpora for fish names by integrating cross-source data, including Eschmeyer's Catalog of Fishes online database. Using a dual translation approach, we applied the Multilingual Text-to-Text Transfer Transformer (mT5) model to generate missing Chinese names.\nNote: The current release provides 10,000 paired data entries.\n\n\t\n\t\t\n\t\tDataset Details‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MHBS-IHB/fishmt5.","url":"https://huggingface.co/datasets/MHBS-IHB/fishmt5","creator_name":"Museum of Hydrobiological Sciences, Institute of Hydrobiology, Chinese Academy of Sciences","creator_url":"https://huggingface.co/MHBS-IHB","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Chinese","Latin","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"floras","keyword":"latin","description":"\n\t\n\t\t\n\t\tFLORAS\n\t\n\nFLORAS is a 50-language benchmark For LOng-form Recognition And Summarization of spoken language. \nThe goal of FLORAS is to create a more realistic benchmarking environment for speech recognition, translation, and summarization models. \nUnlike typical academic benchmarks like LibriSpeech and FLEURS that uses pre-segmented single-speaker read-speech, FLORAS tests the capabilities of models on raw long-form conversational audio, which can have one or many speakers.\nTo encourage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/espnet/floras.","url":"https://huggingface.co/datasets/espnet/floras","creator_name":"ESPnet","creator_url":"https://huggingface.co/espnet","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","translation","summarization","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"Latin-Audio","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVox Classica is a Latin speech corpus of ~73 hours of audio, segmented into short audio clips by sentence. Vox Classica is a large-scale, ML-ready dataset of human-read Classical Latin. It was designed to address the absence of a publicly available human-read Latin corpus large enough for model training.\n\nAlignment and curation: Kaiyuan Zhao\nLanguage: Latin (Classical)\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nThis dataset is built for training and evaluating speech processing models for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ken-Z/Latin-Audio.","url":"https://huggingface.co/datasets/Ken-Z/Latin-Audio","creator_name":"Ken Zhao","creator_url":"https://huggingface.co/Ken-Z","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Latin","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Latin-Audio","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVox Classica is a Latin speech corpus of ~73 hours of audio, segmented into short audio clips by sentence. Vox Classica is a large-scale, ML-ready dataset of human-read Classical Latin. It was designed to address the absence of a publicly available human-read Latin corpus large enough for model training.\n\nAlignment and curation: Kaiyuan Zhao\nLanguage: Latin (Classical)\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nThis dataset is built for training and evaluating speech processing models for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ken-Z/Latin-Audio.","url":"https://huggingface.co/datasets/Ken-Z/Latin-Audio","creator_name":"Ken Zhao","creator_url":"https://huggingface.co/Ken-Z","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Latin","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LatinSummarizerDataset","keyword":"latin","description":"\n\t\n\t\t\n\t\tLatinSummarizer Dataset\n\t\n\n    \n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe LatinSummarizerDataset is a structured dataset used in the GitHub Repository for Latin summarization and translation tasks. This dataset provides aligned English-Latin texts, extractive summaries, and pre-training prompts for fine-tuning models like mT5 for low-resource NLP applications.\n\n\t\n\t\t\n\t\tStructure\n\t\n\nThe dataset is divided into two main phases: \n\nPre-training Data: Includes aligned bilingual corpora, synthetic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/LatinSummarizerDataset.","url":"https://huggingface.co/datasets/LatinNLP/LatinSummarizerDataset","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","summarization","news-articles-summarization","document-retrieval"],"keywords_longer_than_N":true},
	{"name":"LatinSummarizerDataset","keyword":"latin","description":"\n\t\n\t\t\n\t\tLatinSummarizer Dataset\n\t\n\n    \n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe LatinSummarizerDataset is a structured dataset used in the GitHub Repository for Latin summarization and translation tasks. This dataset provides aligned English-Latin texts, extractive summaries, and pre-training prompts for fine-tuning models like mT5 for low-resource NLP applications.\n\n\t\n\t\t\n\t\tStructure\n\t\n\nThe dataset is divided into two main phases: \n\nPre-training Data: Includes aligned bilingual corpora, synthetic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/LatinSummarizerDataset.","url":"https://huggingface.co/datasets/LatinNLP/LatinSummarizerDataset","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","summarization","news-articles-summarization","document-retrieval"],"keywords_longer_than_N":true},
	{"name":"GlotCC-V1","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n \n\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\n\n\t\t\n\t\tUsage (Huggingface Hub -- Recommended)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1.","url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Abau","Amarasi","Abkhaz","Abkhazian"],"keywords_longer_than_N":true},
	{"name":"GlotCC-V1","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n \n\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\n\n\t\t\n\t\tUsage (Huggingface Hub -- Recommended)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1.","url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Abau","Amarasi","Abkhaz","Abkhazian"],"keywords_longer_than_N":true},
	{"name":"MonadGPT","keyword":"latin","description":"This finetuning dataset has been used to train MonadGPT, a chatGPT-like model for the early modern period. \nIt contains 10,797 excerpts of texts in English, French and Latin, mostly published in the 17th century, as well as synthetic questions generated by Mistral-Hermes.\nThe instructions use the chatML format with a unique system prompt (to help with consistency), user questions and assistant answers.\nAll the excerpts are in the public domain and so are the synthetic instructions (in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Pclanglais/MonadGPT.","url":"https://huggingface.co/datasets/Pclanglais/MonadGPT","creator_name":"Pierre-Carl Langlais","creator_url":"https://huggingface.co/Pclanglais","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["English","French","Latin","cc0-1.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"wiktionary-data","keyword":"latin","description":"\n\t\n\t\t\n\t\tWiktionary Data on Hugging Face Datasets\n\t\n\n\n\n\n\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\nsupports the following languages:\n\nDeutsch - German\nLatinum - Latin\n·ºôŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ - Ancient Greek\nÌïúÍµ≠Ïñ¥ - Korean\nêé†êéºêéπ - Old Persian\níÄùíÖóíÅ∫íåë(íåù) - Akkadian\nElamite\n‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç - Sanskrit, or Classical Sanskrit\n\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\nthe dataset it's getting bigger, I noticed a wave of more exciting potentials this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wiktionary-data.","url":"https://huggingface.co/datasets/QubitPi/wiktionary-data","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","German","Latin","Ancient Greek (to 1453)","Korean"],"keywords_longer_than_N":true},
	{"name":"wiktionary-data","keyword":"latin","description":"\n\t\n\t\t\n\t\tWiktionary Data on Hugging Face Datasets\n\t\n\n\n\n\n\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\nsupports the following languages:\n\nDeutsch - German\nLatinum - Latin\n·ºôŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ - Ancient Greek\nÌïúÍµ≠Ïñ¥ - Korean\nêé†êéºêéπ - Old Persian\níÄùíÖóíÅ∫íåë(íåù) - Akkadian\nElamite\n‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç - Sanskrit, or Classical Sanskrit\n\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\nthe dataset it's getting bigger, I noticed a wave of more exciting potentials this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wiktionary-data.","url":"https://huggingface.co/datasets/QubitPi/wiktionary-data","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","German","Latin","Ancient Greek (to 1453)","Korean"],"keywords_longer_than_N":true},
	{"name":"multiblimp","keyword":"latin","description":"\n\t\n\t\t\n\t\tMultiBLiMP\n\t\n\nMultiBLiMP is a massively Multilingual Benchmark for Linguistic Minimal Pairs. The dataset is composed of synthetic pairs generated using Universal Dependencies and UniMorph.\nThe paper can be found here.\nWe split the data set by language: each language consists of a single .tsv file. The rows contain many attributes for a particular pair, most important are the sen and wrong_sen fields, which we use for evaluating the language models.\n\n\t\n\t\t\n\t\n\t\n\t\tUsing MultiBLiMP\n\t\n\nTo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jumelet/multiblimp.","url":"https://huggingface.co/datasets/jumelet/multiblimp","creator_name":"Jaap Jumelet","creator_url":"https://huggingface.co/jumelet","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Buriat","Spanish","Sanskrit","Romanian"],"keywords_longer_than_N":true},
	{"name":"oscar-mini","keyword":"latin","description":"The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\\","url":"https://huggingface.co/datasets/nthngdy/oscar-mini","creator_name":"Nathan Godey","creator_url":"https://huggingface.co/nthngdy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"ARK-Metadata-V2","keyword":"latin","description":"\n\t\n\t\t\n\t\tTitle\n\t\n\nMetadata of the \"Alter Realkatalog\" (ARK) of Berlin State Library (SBB) Version 2 ‚Äì August 2025\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset was created with the intent to provide a single larger set of metadata from Berlin State Library for research purposes and the development of AI applications.\nThe dataset comprises descriptive metadata of 2.639.554 titles derived from the union catalogue K10plus, a database with about 200 million records from libraries across 11 German states.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SBB/ARK-Metadata-V2.","url":"https://huggingface.co/datasets/SBB/ARK-Metadata-V2","creator_name":"Staatsbibliothek zu Berlin - Preu√üischer Kulturbesitz","creator_url":"https://huggingface.co/SBB","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","German","Latin","English"],"keywords_longer_than_N":true},
	{"name":"language_tags","keyword":"latin","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nDataset listing 27,328 languages and dialects (also includes macrolanguage names).For each language, either the ISO 639 code, the Glottolog code or both are provided.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\nEnglish_Name: Language name in English (e.g. \"French\").\nNative_Name: If value is not 0, corresponds to the name of the language by native speakers (e.g. \"Fran√ßais\") which may have been found in Wikipedia's nativename field.\nGlottocode: The language tag in the Glottolog convention (e.g.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/language_tags.","url":"https://huggingface.co/datasets/lbourdois/language_tags","creator_name":"Lo√Øck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Afade","Par√° Ar√°ra","Afar","Aka-Bea","Abon"],"keywords_longer_than_N":true},
	{"name":"tatoeba","keyword":"latin","description":"This is a collection of translated sentences from Tatoeba\n359 languages, 3,403 bitexts\ntotal number of files: 750\ntotal number of tokens: 65.54M\ntotal number of sentence fragments: 8.96M","url":"https://huggingface.co/datasets/Helsinki-NLP/tatoeba","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":null,"first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"bnl_newspapers1841-1879","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for BnL Newspapers 1841-1881\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n592.192 articles from historical newspapers (1841-1881) along with metadata and the full text.\n21 newspaper titles\n24.415 newspaper issues\n99.957 scanned pages\nTranscribed using a variety of OCR engines and corrected using https://github.com/natliblux/nautilusocr (95% threshold)\nPublic Domain, CC0 (See copyright notice)\nThe newspapers used are:\n\nDer Arbeiter (1878-1881)\nL'Arlequin (1848-1848)\nL'Avenir (1868-1871)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biglam/bnl_newspapers1841-1879.","url":"https://huggingface.co/datasets/biglam/bnl_newspapers1841-1879","creator_name":"BigLAM: BigScience Libraries, Archives and Museums","creator_url":"https://huggingface.co/biglam","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"entity_cs","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for EntityCS\n\t\n\n\nRepository: https://github.com/huawei-noah/noah-research/tree/master/NLP/EntityCS  \nPaper: https://aclanthology.org/2022.findings-emnlp.499.pdf  \nPoint of Contact: Fenia Christopoulou, Chenxi Whitehouse\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nWe use the English Wikipedia and leverage entity information from Wikidata to construct an entity-based Code Switching corpus. \nTo achieve this, we make use of wikilinks in Wikipedia, i.e. links from one page to another.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huawei-noah/entity_cs.","url":"https://huggingface.co/datasets/huawei-noah/entity_cs","creator_name":"HUAWEI Noah's Ark Lab","creator_url":"https://huggingface.co/huawei-noah","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Afrikaans","Amharic","Arabic","Assamese","Azerbaijani"],"keywords_longer_than_N":true},
	{"name":"georges-1913-normalization","keyword":"latin","description":"\n\t\n\t\t\n\t\tNormalized Georges 1913\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset was created as part of the Burchard's Dekret Digital project (www.burchards-dekret-digital.de), \nfunded by the Academy of Sciences and Literature | Mainz.\nIt is based on 55,000 lemmata from Karl Georges, Ausf√ºhrliches lateinisch-deutsches Handw√∂rterbuch, Hannover 1913 (Georges 1913) \nand was developed to train models for normalization tasks in the context of medieval Latin.\nThe dataset consists of approximately 5 million‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mschonhardt/georges-1913-normalization.","url":"https://huggingface.co/datasets/mschonhardt/georges-1913-normalization","creator_name":"Michael Schonhardt","creator_url":"https://huggingface.co/mschonhardt","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","Latin","cc-by-4.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"reranker_continuous_filt_max7_train","keyword":"latin","description":"\n\t\n\t\t\n\t\tReranker training data\n\t\n\nThis data was generated using 4 steps:\n\nWe gathered queries and corresponding text data from 35 high quality datasets covering more than 95 languages.\nFor datasets which did not already have negative texts for queries, we mined hard negatives using the BAAI/bge-m3 embedding model.\nFor each query, we selected one positive and one negative text and used Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4 to rate the relatedness of each query-text pair using a token \"1\", \"2\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lightblue/reranker_continuous_filt_max7_train.","url":"https://huggingface.co/datasets/lightblue/reranker_continuous_filt_max7_train","creator_name":"Lightblue KK.","creator_url":"https://huggingface.co/lightblue","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","Spanish","German","Arabic"],"keywords_longer_than_N":true},
	{"name":"QuantumAI","keyword":"latin","description":"Groovy-123/QuantumAI dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/QuantumAI","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Everything_Instruct_Multilingual","keyword":"latin","description":"\n\t\n\t\t\n\t\tEverything Instruct (Multilingual Edition)\n\t\n\nEverything you need... all in one place üíò\n\nEverything instruct (Multilingual Edition) is a massive alpaca instruct formatted dataset consisting of a wide variety of topics meant to bring LLM's to the next level in open source AI.\nNote: This dataset is fully uncensored (No model will refuse any request trained on this dataset unless otherwise aligned)\nNote2: This version of the dataset supports the following languages:\n\nEnglish\nRussian‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rombodawg/Everything_Instruct_Multilingual.","url":"https://huggingface.co/datasets/rombodawg/Everything_Instruct_Multilingual","creator_name":"rombo dawg","creator_url":"https://huggingface.co/rombodawg","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Russian","Chinese","Korean","Urdu"],"keywords_longer_than_N":true},
	{"name":"TreeOfLife-10M-WEBP","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for TreeOfLife-10M-WEBP\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is an optimized version of the TreeOfLife-10M dataset,\ncontaining over 10 million images covering 454 thousand taxa in the tree of life.\nThis version has been processed to improve usability and reduce storage requirements while maintaining full compatibility with the original dataset structure.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis version modifies the original dataset as follows:\n\nCorrupted files were repaired.\nVery‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/birder-project/TreeOfLife-10M-WEBP.","url":"https://huggingface.co/datasets/birder-project/TreeOfLife-10M-WEBP","creator_name":"Birder","creator_url":"https://huggingface.co/birder-project","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","imageomics/TreeOfLife-10M","English","Latin"],"keywords_longer_than_N":true},
	{"name":"PleIAs-ToxicCommons","keyword":"latin","description":"\n\t\n\t\t\n\t\tPleIAs/ToxicCommons\n\t\n\nThis dataset is a refined version of the PleIAs/ToxicCommons collection, focusing on historical texts labeled for content that may be considered objectionable by modern standards (what the authors of the dataset deem \"toxic\"). \nThe cleaned dataset contains 1‚Äâ051‚Äâ027 rows, each representing a text sample with associated toxicity scores across five dimensions:\n\nRace and origin-based bias\nGender and sexuality-based bias\nReligious bias\nAbility bias\nViolence and abuse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/PleIAs-ToxicCommons.","url":"https://huggingface.co/datasets/agentlans/PleIAs-ToxicCommons","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","French","Spanish","German"],"keywords_longer_than_N":true},
	{"name":"wilhelm-vocabulary","keyword":"latin","description":"\n\t\n\t\t\n\t\tWilhelm Vocabulary\n\t\n\n[![Hugging Face dataset badge]][Hugging Face dataset URL]\n[![Vocabulary count - German]][Docker Hub URL]\n[![Vocabulary count - Latin]][Docker Hub URL]\n[![Vocabulary count - Ancient Greek]][Docker Hub URL]\n[![Docker Hub][Docker Pulls Badge]][Docker Hub URL]\n[![GitHub workflow status badge][GitHub workflow status badge]][GitHub workflow status URL]\n[![Hugging Face sync status badge]][Hugging Face sync status URL]\n[![Apache License Badge]][Apache License, Version‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary.","url":"https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","German","Latin","Ancient Greek (to 1453)","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"wilhelm-vocabulary","keyword":"latin","description":"\n\t\n\t\t\n\t\tWilhelm Vocabulary\n\t\n\n[![Hugging Face dataset badge]][Hugging Face dataset URL]\n[![Vocabulary count - German]][Docker Hub URL]\n[![Vocabulary count - Latin]][Docker Hub URL]\n[![Vocabulary count - Ancient Greek]][Docker Hub URL]\n[![Docker Hub][Docker Pulls Badge]][Docker Hub URL]\n[![GitHub workflow status badge][GitHub workflow status badge]][GitHub workflow status URL]\n[![Hugging Face sync status badge]][Hugging Face sync status URL]\n[![Apache License Badge]][Apache License, Version‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary.","url":"https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","German","Latin","Ancient Greek (to 1453)","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"multilingual_translation_gpt4o_gen","keyword":"latin","description":"Youseff1987/multilingual_translation_gpt4o_gen dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Youseff1987/multilingual_translation_gpt4o_gen","creator_name":"JOON HYOUNG JUN","creator_url":"https://huggingface.co/Youseff1987","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Korean","English","Chinese","Zulu"],"keywords_longer_than_N":true},
	{"name":"HOME-Alcar-line","keyword":"latin","description":"\n\t\n\t\t\n\t\tHOME-Alcar - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe HOME-Alcar (Aligned and Annotated Cartularies) dataset is a Medieval corpus. The 17 medieval manuscripts in this corpus are cartularies, i.e. books copying charters and legal acts, produced between the 12th and 14th centuries. \nThis dataset comes from the following publication:\nStutzmann, D., Torres Aguilar, S., & Chaffenet, P. (2021). HOME-Alcar: Aligned and Annotated Cartularies [Data set]. Zenodo.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/HOME-Alcar-line.","url":"https://huggingface.co/datasets/Teklia/HOME-Alcar-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Latin","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"latino_italiano_traduzioni_DIRETTE","keyword":"latin","description":"Dddixyy/latino_italiano_traduzioni_DIRETTE dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Dddixyy/latino_italiano_traduzioni_DIRETTE","creator_name":"Davide Brunori","creator_url":"https://huggingface.co/Dddixyy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Latin","Italian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mHumanEval-Benchmark","keyword":"latin","description":"\n\n\n\t\n\t\t\n\t\tüî∑ Accepted in NAACL Proceedings (2025) üî∑\n\t\n\n\n\n\n\n  \n    \n      \n        \n          \n        \n      \n      \n        \n          \n        \n      \n      \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n  \n\n\t\n\t\n\t\n\t\tmHumanEval\n\t\n\nThe mHumanEval benchmark is curated based on prompts from the original HumanEval üìö [Chen et al., 2021]. It includes a total of 33,456 prompts for Python, and 836,400 in total - significantly expanding from the original 164. \n\n\n\t\n\t\t\n\t\tQuick Start\n\t\n\n  Detailed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/md-nishat-008/mHumanEval-Benchmark.","url":"https://huggingface.co/datasets/md-nishat-008/mHumanEval-Benchmark","creator_name":"Nishat Raihan","creator_url":"https://huggingface.co/md-nishat-008","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Afar","Abkhaz","Avestan","Afrikaans","Akan"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus-mmteb","keyword":"latin","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\nLoading example:\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"davidstap/biblenlp-corpus-mmteb\", \"eng-arb\", trust_remote_code=True)\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 28723\n    })\n    validation: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 1578\n    })‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb.","url":"https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","expert-generated","translation","multilingual","Arifama-Miniafia"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus-mmteb","keyword":"latin","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\nLoading example:\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"davidstap/biblenlp-corpus-mmteb\", \"eng-arb\", trust_remote_code=True)\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 28723\n    })\n    validation: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 1578\n    })‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb.","url":"https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","expert-generated","translation","multilingual","Arifama-Miniafia"],"keywords_longer_than_N":true},
	{"name":"muri-it","keyword":"latin","description":"\n\t\n\t\t\n\t\tMURI-IT: Multilingual Instruction Tuning Dataset for 200 Languages via Multilingual Reverse Instructions\n\t\n\nMURI-IT is a large-scale multilingual instruction tuning dataset containing 2.2 million instruction-output pairs across 200 languages. It is designed to address the challenges of instruction tuning in low-resource languages with Multilingual Reverse Instructions (MURI), which ensures that the output is human-written, high-quality, and authentic to the cultural and linguistic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/akoksal/muri-it.","url":"https://huggingface.co/datasets/akoksal/muri-it","creator_name":"Abdullatif Koksal","creator_url":"https://huggingface.co/akoksal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","question-answering","summarization","Achinese"],"keywords_longer_than_N":true},
	{"name":"BK-Training-Dataset","keyword":"latin","description":"\n\t\n\t\t\n\t\tTitle\n\t\n\n\"Basisklassifikation\" (BK) Training Dataset for Automatic Subject Indexing: Titles and Subjects from the K10plus Library Catalogue\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis is a training dataset for automatic subject indexing containing more than 6 million titles and their corresponding subjects (classes) from the \"Basisklassifikation\" (BK). Initially introduced in the 1980s, today the Basisklassifikation constitutes the most widely used classification system for subject indexing within the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SBB/BK-Training-Dataset.","url":"https://huggingface.co/datasets/SBB/BK-Training-Dataset","creator_name":"Staatsbibliothek zu Berlin - Preu√üischer Kulturbesitz","creator_url":"https://huggingface.co/SBB","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","German","English","French","Italian"],"keywords_longer_than_N":true},
	{"name":"latin-greek-hebrew-english-dataset","keyword":"latin","description":"\n\t\n\t\t\n\t\tProverbs: Ancient Languages Set\n\t\n\nThis repository contains a collection of 2,000 short phrases translated into three ancient languages: Ancient Latin, Ancient Greek, Biblical Hebrew, and English. The phrases cover a wide variety of contexts, providing insight into the linguistic, cultural, and philosophical landscapes of these ancient civilizations.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe \"Proverbs: Ancient Languages Set\" is a resource designed to help individuals explore and understand ancient‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dddixyy/latin-greek-hebrew-english-dataset.","url":"https://huggingface.co/datasets/Dddixyy/latin-greek-hebrew-english-dataset","creator_name":"Davide Brunori","creator_url":"https://huggingface.co/Dddixyy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Latin","Hebrew","Greek","English"],"keywords_longer_than_N":true},
	{"name":"ParaNames","keyword":"latin","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bltlab/ParaNames.","url":"https://huggingface.co/datasets/bltlab/ParaNames","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","Nias","Kotava","Banjar","Angika"],"keywords_longer_than_N":true},
	{"name":"xP3x-Kongo","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for xP3x Kikongo Focus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nxP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts & datasets across 277 languages & 16 NLP tasks. It contains all of xP3 + much more! It is used for training future contenders of mT0 & BLOOMZ at project Aya @C4AI üß°\n\n\nCreation: The dataset can be recreated using instructions available here together with the file in this repository named xp3x_create.py. We provide this version to save‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Svngoku/xP3x-Kongo.","url":"https://huggingface.co/datasets/Svngoku/xP3x-Kongo","creator_name":"NIONGOLO Chrys F√©-Marty","creator_url":"https://huggingface.co/Svngoku","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","translation","expert-generated","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"wikipedia-citation-index","keyword":"latin","description":"Dataset with citation indexes as of 1 August 2024 for 47 million Wikipedia articles in 55 language versions. Research: ArXiv\n","url":"https://huggingface.co/datasets/lewoniewski/wikipedia-citation-index","creator_name":"W≈Çodzimierz Lewoniewski","creator_url":"https://huggingface.co/lewoniewski","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","Azerbaijani","Belarusian","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"vitruvius_alberti_fludd_corpus","keyword":"latin","description":"\n\t\n\t\t\n\t\n\t\n\t\tVitruvius Alberti Fludd Architecture Corpus\n\t\n\nA comprehensive collection of historical architectural texts focusing on works by Vitruvius, Leon Battista Alberti, and Robert Fludd.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statistics\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\n\nTotal Documents: 16 PDFs\nTotal Pages: 5,233\nEmpty Pages: 1,156\nPages with Errors: 0\n\n\n\t\n\t\t\n\t\n\t\n\t\tDocument Length Statistics\n\t\n\n\nAverage Pages per Document: 327.1\nMinimum Pages: 10\nMaximum Pages: 623\n\n\n\t\n\t\t\n\t\n\t\n\t\tContent Statistics (words per‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/forcemultiplier/vitruvius_alberti_fludd_corpus.","url":"https://huggingface.co/datasets/forcemultiplier/vitruvius_alberti_fludd_corpus","creator_name":"The Force Multiplier","creator_url":"https://huggingface.co/forcemultiplier","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Latin","mit"],"keywords_longer_than_N":true},
	{"name":"reranking-datasets-light","keyword":"latin","description":"\n\t\n\t\t\n\t\tüî• Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation üî•\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tReRanking Datasets : A comprehensive collection of retrieval and reranking datasets with full passage contexts, including titles, text, and metadata for in-depth research.\n\t\n\n\n    \n    \n    \n    \n    \n    \n        \n    \n    \n        \n    \n    \n        \n    \n    \n        \n    \n    \n    \n\n\n\nA curated collection of ready-to-use datasets for retrieval and reranking‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light.","url":"https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light","creator_name":"Abdelrahman Abdallah","creator_url":"https://huggingface.co/abdoelsayed","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","Arabic","German","French"],"keywords_longer_than_N":true},
	{"name":"tatoeba_mt","keyword":"latin","description":"The Tatoeba Translation Challenge is a multilingual data set of\nmachine translation benchmarks derived from user-contributed\ntranslations collected by [Tatoeba.org](https://tatoeba.org/) and\nprovided as parallel corpus from [OPUS](https://opus.nlpl.eu/). This\ndataset includes test and development data sorted by language pair. It\nincludes test sets for hundreds of language pairs and is continuously\nupdated. Please, check the version number tag to refer to the release\nthat your are using.","url":"https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":null,"first_N":5,"first_N_keywords":["text-generation","translation","no-annotation","crowdsourced","translation"],"keywords_longer_than_N":true},
	{"name":"CC-100-Latin","keyword":"latin","description":"Cicciokr/CC-100-Latin dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Cicciokr/CC-100-Latin","creator_name":"Francesco","creator_url":"https://huggingface.co/Cicciokr","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["Latin","cc0-1.0","10M - 100M","text","Text"],"keywords_longer_than_N":true},
	{"name":"latin_english_translation","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for \"latin_english_parallel\"\n\t\n\n101k translation pairs between Latin and English, split 99/1/1 as train/test/val. These have been collected roughly 66% from the Loeb Classical Library and 34% from the Vulgate translation. \nFor those that were gathered from the Loeb Classical Library, alignment was performd manually between Source and Target sequences.\nEach sample is annotated with the index and file (and therefore author/work) that the sample is from. If you find errors‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/grosenthal/latin_english_translation.","url":"https://huggingface.co/datasets/grosenthal/latin_english_translation","creator_name":"Gil Rosenthal","creator_url":"https://huggingface.co/grosenthal","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Latin","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"oscar-small","keyword":"latin","description":"The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\\","url":"https://huggingface.co/datasets/djstrong/oscar-small","creator_name":"Krzysztof Wr√≥bel","creator_url":"https://huggingface.co/djstrong","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"oscar-small","keyword":"latin","description":"The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\\","url":"https://huggingface.co/datasets/nthngdy/oscar-small","creator_name":"Nathan Godey","creator_url":"https://huggingface.co/nthngdy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"multilingual-pl-bert","keyword":"latin","description":"Attribution: Wikipedia.org\n","url":"https://huggingface.co/datasets/styletts2-community/multilingual-pl-bert","creator_name":"StyleTTS 2 Community","creator_url":"https://huggingface.co/styletts2-community","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Afrikaans","Aragonese","Arabic","Azerbaijani","Bashkir"],"keywords_longer_than_N":true},
	{"name":"mc4-sampling","keyword":"latin","description":"A sampling-enabled version of mC4, the colossal, cleaned version of Common Crawl's web crawl corpus.\n\nBased on Common Crawl dataset: \"https://commoncrawl.org\".\n\nThis is a version of the processed version of Google's mC4 dataset by AllenAI, in which sampling methods are implemented to perform on the fly.","url":"https://huggingface.co/datasets/bertin-project/mc4-sampling","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"panlex","keyword":"latin","description":"\n\t\n\t\t\n\t\tPanLex\n\t\n\nJanuary 1, 2024 version of PanLex Language Vocabulary with 24,650,274 rows covering 6,152 languages.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\nvocab: contains the text entry.  \n639-3: contains the ISO 639-3 languages tags to allow users to filter on the language(s) of their choice.\n639-3_english_name: the English language name associated to the code ISO 639-3. \nvar_code: contains a code to differentiate language variants. In practice, this is the code 639-3 + a number.  If 000, it corresponds to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/panlex.","url":"https://huggingface.co/datasets/lbourdois/panlex","creator_name":"Lo√Øck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["Ghotuo","Alumu-Tesu","Ari","Amal","Arb√´resh√´ Albanian"],"keywords_longer_than_N":true},
	{"name":"tatoeba-mt-all-in-one","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for The Tatoeba Translation Challenge | All In One\n\t\n\n~7.3M entries.\nJust more user-friendly version that combines all of the entries of original dataset in a single file:\nhttps://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt\n","url":"https://huggingface.co/datasets/0x22almostEvil/tatoeba-mt-all-in-one","creator_name":"David Glushkov","creator_url":"https://huggingface.co/0x22almostEvil","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["Helsinki-NLP","crowdsourced","translation","Helsinki-NLP/tatoeba_mt","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"ApolloMoEBench","keyword":"latin","description":"\n\t\n\t\t\n\t\tDemocratizing Medical LLMs For Much More Languages\n\t\n\nCovering 12 Major Languages including English, Chinese, French, Hindi, Spanish, Arabic, Russian, Japanese, Korean, German, Italian, Portuguese and 38 Minor Languages So far.\n\n   üìÉ Paper ‚Ä¢ üåê Demo ‚Ä¢ ü§ó ApolloMoEDataset ‚Ä¢ ü§ó ApolloMoEBench  ‚Ä¢ ü§ó Models  ‚Ä¢üåê Apollo  ‚Ä¢ üåê ApolloMoE\n\n\n\n\n\n\n\t\n\t\t\n\t\tüåà Update\n\t\n\n\n[2024.10.15] ApolloMoE repo is publishedÔºÅüéâ\n\n\n\t\n\t\t\n\t\tLanguages Coverage\n\t\n\n12 Major Languages and 38 Minor Languages\n\n  Click to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEBench.","url":"https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEBench","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Arabic","English","Chinese","Korean"],"keywords_longer_than_N":true},
	{"name":"translation_latin_to_italian","keyword":"latin","description":"Dddixyy/translation_latin_to_italian dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Dddixyy/translation_latin_to_italian","creator_name":"Davide Brunori","creator_url":"https://huggingface.co/Dddixyy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Latin","Italian","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Synthdog-Multilingual-100","keyword":"latin","description":"\n\t\n\t\t\n\t\tSynthdog Multilingual\n\t\n\n\n\nThe Synthdog dataset created for training in Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model.\nUsing the official Synthdog code, we created >1 million training samples for improving OCR capabilities in Large Vision-Language Models.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nWe provide the images for download in two .tar.gz files. Download and extract them in folders of the same name (so cat images.tar.gz.* | tar xvzf -C images; tar xvzf‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/Synthdog-Multilingual-100.","url":"https://huggingface.co/datasets/WueNLP/Synthdog-Multilingual-100","creator_name":"W√ºNLP","creator_url":"https://huggingface.co/WueNLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","multilingual","Afrikaans","Amharic","Arabic"],"keywords_longer_than_N":true},
	{"name":"Italian_latin_parallel_animals","keyword":"latin","description":"\n\t\n\t\t\n\t\tdescrizioni di animali e habitat - Synthetic Dataset\n\t\n\nThis dataset was generated using the Synthetic Dataset Generator powered by Gemini AI.\n\nTopic: descrizioni di animali e habitat\nField 1: italiano\nField 2: latino antico(traduzione)\nRows: 280\n\nGenerated on: 2025-05-27T00:07:49.042Z\n","url":"https://huggingface.co/datasets/Dddixyy/Italian_latin_parallel_animals","creator_name":"Davide Brunori","creator_url":"https://huggingface.co/Dddixyy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Italian","Latin","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"evalatin2024","keyword":"latin","description":"\n\t\n\t\t\n\t\tTartuNLP at EvaLatin 2024: Emotion Polarity Detection\n\t\n\n\n\t\n\t\t\n\t\tBibTeX entry and citation info\n\t\n\n@inproceedings{dorkin-sirts-2024-tartunlp-evalatin,\n    title = \"{T}artu{NLP} at {E}va{L}atin 2024: Emotion Polarity Detection\",\n    author = \"Dorkin, Aleksei  and\n      Sirts, Kairit\",\n    editor = \"Sprugnoli, Rachele  and\n      Passarotti, Marco\",\n    booktitle = \"Proceedings of the Third Workshop on Language Technologies for Historical and Ancient Languages (LT4HALA) @‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/adorkin/evalatin2024.","url":"https://huggingface.co/datasets/adorkin/evalatin2024","creator_name":"Aleksei Dorkin","creator_url":"https://huggingface.co/adorkin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","Latin","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"MultiQ","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for MultiQ\n\t\n\nThis is the dataset corresponding to the paper \"Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ\". \nIt is a silver standard benchmark that can be used to evaluate the basic multilingual capabilities of LLMs. It contains 200 open ended questions automatically \ntranslated into 137 typologically diverse languages. \n\nCurated by: Carolin Holtermann, Paul R√∂ttger, Timm Dill, Anne Lauscher\nLanguage(s) (NLP): 137 diverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/caro-holt/MultiQ.","url":"https://huggingface.co/datasets/caro-holt/MultiQ","creator_name":"Holtermann","creator_url":"https://huggingface.co/caro-holt","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Tagalog","Samoan","Macedonian","Gujarati"],"keywords_longer_than_N":true},
	{"name":"sentinel-beetles","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for Beetles as Sentinel Taxa: Predicting drought conditions from NEON specimen imagery\n\t\n\nThis dataset contains images of pinned carabid beetle specimens collected by the National Ecological Observatory Network (NEON) from ecological sites across the U.S., along with associated metadata and drought severity indices (Standardized Precipitation Evapotranspiration Index (SPEI)). It was developed to for the second HDR ML Challenge, and is intended to support the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imageomics/sentinel-beetles.","url":"https://huggingface.co/datasets/imageomics/sentinel-beetles","creator_name":"HDR Imageomics Institute","creator_url":"https://huggingface.co/imageomics","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-feature-extraction","English","Latin","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ToxicCommons","keyword":"latin","description":"\n\t\n\t\t\n\t\tToxic Commons\n\t\n\nToxic Commons is a release of 2 million samples of annotated, public domain, multilingual text that was used to train Celadon. \nIt is being released alongside Celadon, in order to better understand multilingual and multicultural toxicity. \nEach sample was classified across 5 axes of toxicity:\n\nRace and origin-based bias: includes racism as well as bias against someone‚Äôs country or region of origin or immigration status, especially immigrant or refugee status. \nGender‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PleIAs/ToxicCommons.","url":"https://huggingface.co/datasets/PleIAs/ToxicCommons","creator_name":"PleIAs","creator_url":"https://huggingface.co/PleIAs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","French","Spanish","German"],"keywords_longer_than_N":true},
	{"name":"Diplomatarium-Fennicum","keyword":"latin","description":"\n\t\n\t\t\n\t\tDiplomatarium Fennicum-dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset summary\n\t\n\nDataset consisting of eight data fields taken from the Diplomatarium Fennicum -database. \nDiplomatarium Fennicum -database contains medieval charters and text excerpts conserning Finland and Finns, \nand is published and maintained by the National Archives of Finland.\nThe data fields selected are central to identifying, categorizing and analyzing the texts. \nThe dataset represents only very minimally the whole database; the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kansallisarkisto/Diplomatarium-Fennicum.","url":"https://huggingface.co/datasets/Kansallisarkisto/Diplomatarium-Fennicum","creator_name":"National Archives of Finland","creator_url":"https://huggingface.co/Kansallisarkisto","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["Finnish","Swedish","Latin","German","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"multilingual_translation_sft","keyword":"latin","description":"Youseff1987/multilingual_translation_sft dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Youseff1987/multilingual_translation_sft","creator_name":"JOON HYOUNG JUN","creator_url":"https://huggingface.co/Youseff1987","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Korean","English","Chinese","Zulu"],"keywords_longer_than_N":true},
	{"name":"wikipedia_quality_wikirank","keyword":"latin","description":"Datasets with WikiRank quality score as of 1 August 2024 for 47 million Wikipedia articles in 55 language versions (also simplified version for each language in separate files).\nThe WikiRank quality score is a metric designed to assess the overall quality of a Wikipedia article. Although its specific algorithm can vary depending on the implementation, the score typically combines several key features of the Wikipedia article.\n\n\t\n\t\t\n\t\n\t\n\t\tWhy It‚Äôs Important\n\t\n\n\nEnhances Trust: For readers and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank.","url":"https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank","creator_name":"W≈Çodzimierz Lewoniewski","creator_url":"https://huggingface.co/lewoniewski","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","Azerbaijani","Belarusian","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"fineweb-2","keyword":"latin","description":"\n\t\n\t\t\n\t\tü•Ç FineWeb2\n\t\n\n\n    \n\n\n\nA sparkling update with 1000s of languages\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nThis is the second iteration of the popular üç∑ FineWeb dataset, bringing high quality pretraining data to over 1000 üó£Ô∏è languages.\nThe ü•Ç FineWeb2 dataset is fully reproducible, available under the permissive ODC-By 1.0 license and extensively validated through hundreds of ablation experiments.\nIn particular, on the set of 9 diverse languages we used to guide our processing decisions, ü•Ç FineWeb2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/fineweb-2.","url":"https://huggingface.co/datasets/HuggingFaceFW/fineweb-2","creator_name":"FineData","creator_url":"https://huggingface.co/HuggingFaceFW","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Arifama-Miniafia","Ankave","Abau","Amarasi"],"keywords_longer_than_N":true},
	{"name":"panlex-definitions","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for panlex-definitions\n\t\n\nThis is a dataset of word definitions in several hudnred languages, extracted from https://panlex.org.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset has been extracted from https://panlex.org (the 20250201 database dump) and rearranged on the per-language basis (by the language of the definition).\nEach language subset consists of definitions (short phrases).\nEach definition is associated with some meanings (if there is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cointegrated/panlex-definitions.","url":"https://huggingface.co/datasets/cointegrated/panlex-definitions","creator_name":"David Dale","creator_url":"https://huggingface.co/cointegrated","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["translation","Abkhazian","Hijazi Arabic","Afrikaans","Ainu (Japan)"],"keywords_longer_than_N":true},
	{"name":"parallel-catholic-bible-versions","keyword":"latin","description":"\n\t\n\t\t\n\t\tParallel Catholic Bible Versions\n\t\n\nAligned verses from all 73 books of Catholic Bible in three versions: the Latin Vulgate (vulgate), the Catholic Public Domain Version (cpdv), and the Douay-Rheims Challoner Revision (drc). Includes 2,450 translation commentary notes from the Latin English Study Bible by Ronald L. Conte Jr.\n\n\t\n\t\t\n\t\tSources\n\t\n\n\nLatin-English Study Bible with notes scraped by aseemsavio\nDouay-Rheims via scrollmapper\n\nText has been cleaned to remove HTML tags (e.g.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jam963/parallel-catholic-bible-versions.","url":"https://huggingface.co/datasets/jam963/parallel-catholic-bible-versions","creator_name":"Jacob Matthews","creator_url":"https://huggingface.co/jam963","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["translation","sentence-similarity","summarization","Latin","English"],"keywords_longer_than_N":true},
	{"name":"GlobalDISCO","keyword":"latin","description":"\n\t\n\t\t\n\t\tGlobalDISCO\n\t\n\nGlobalDISCO is a large-scale dataset consisting of 73k music tracks generated by state-of-the-art commercial generative music models, along with paired links to 93k reference tracks in LAION-DISCO-12M. The dataset spans 147 languages and includes musical style prompts extracted from MusicBrainz and Wikipedia. The dataset is globally balanced, representing musical styles from artists across 79 countries and five continents. It is aimed to support the research community in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/disco-eth/GlobalDISCO.","url":"https://huggingface.co/datasets/disco-eth/GlobalDISCO","creator_name":"DISCO","creator_url":"https://huggingface.co/disco-eth","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["audio-classification","English","Spanish","French","German"],"keywords_longer_than_N":true},
	{"name":"ARK-Metadata","keyword":"latin","description":"\n\t\n\t\t\n\t\tMetadata of the \"Alter Realkatalog\" (ARK) of Berlin State Library (SBB)\n\t\n\n\n\t\n\t\t\n\t\tMotivation\n\t\n\nThis dataset was created with the intent to provide a single larger set of metadata from Berlin State Library for research purposes and the development of AI applications.\nThe dataset comprises of descriptive metadata of 2.619.397 titles, which together form the \"Alte Realkatalog\" of Berlin State Libray, which may be translated to \"Old Subject Catalogue\". The data are stored in columnar‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SBB/ARK-Metadata.","url":"https://huggingface.co/datasets/SBB/ARK-Metadata","creator_name":"Staatsbibliothek zu Berlin - Preu√üischer Kulturbesitz","creator_url":"https://huggingface.co/SBB","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","German","Latin","English"],"keywords_longer_than_N":true},
	{"name":"tatoeba-bitext-mining","keyword":"latin","description":"\n  Tatoeba\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\n1,000 English-aligned sentence pairs for each language based on the Tatoeba corpus\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://github.com/facebookresearch/LASER/tree/main/data/tatoeba/v1\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"Tatoeba\"])\nevaluator = mteb.MTEB(task)\n\nmodel =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/tatoeba-bitext-mining.","url":"https://huggingface.co/datasets/mteb/tatoeba-bitext-mining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","human-annotated","multilingual","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"TreeOfLife-10M-EOL-NaturalImages","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for TreeOfLife-10M-EOL-NaturalImages\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a curated version of the TreeOfLife-10M-WEBP EOL training split, filtered to contain exclusively natural biological imagery.\nThe dataset has been systematically cleaned using the Vision Data Curation (VDC) framework to remove non-natural content while preserving high-quality biological specimens.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis version further refines the dataset from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/birder-project/TreeOfLife-10M-EOL-NaturalImages.","url":"https://huggingface.co/datasets/birder-project/TreeOfLife-10M-EOL-NaturalImages","creator_name":"Birder","creator_url":"https://huggingface.co/birder-project","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","image-feature-extraction","imageomics/TreeOfLife-10M","birder-project/TreeOfLife-10M-WEBP"],"keywords_longer_than_N":true},
	{"name":"comma-jsonl","keyword":"latin","description":"\n\t\n\t\t\n\t\tDataset Card for CoMMA JSON-L\n\t\n\nCoMMA is a large-scale corpus of digitized medieval manuscripts \ntranscribed using Handwritten Text Recognition (HTR). It \ncontains over 2.5 billion tokens from more than 23,000 manuscripts in\nLatin and Old French (801‚Äì1600 CE). Unlike most existing resources, the corpus\nprovides raw, non-normalized text.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: Thibault Cl√©rice\nFunded by: Inria, COLaF, ParamHTRs\nLanguage(s) (NLP):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/comma-project/comma-jsonl.","url":"https://huggingface.co/datasets/comma-project/comma-jsonl","creator_name":"Corpus of Multilingual Medieval Archives","creator_url":"https://huggingface.co/comma-project","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Latin","French","Italian","cc-by-4.0","1B<n<10B"],"keywords_longer_than_N":true},
	{"name":"Tridis_layout_manuscripts","keyword":"latin","description":"\n\t\n\t\t\n\t\tA Unified Dataset for Codicological Document Layout Analysis\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains a large-scale, unified dataset for Document Layout Analysis (DLA) in historical manuscripts. It was created by harmonizing three distinct public corpora‚Äîe-NDP, CATMuS, and HORAE‚Äîwhich cover a wide range of document types from the 12th to the 17th century (administrative registers, literary manuscripts, printed books, and Books of Hours).\nThe key feature of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/magistermilitum/Tridis_layout_manuscripts.","url":"https://huggingface.co/datasets/magistermilitum/Tridis_layout_manuscripts","creator_name":"Sergio Torres","creator_url":"https://huggingface.co/magistermilitum","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","French","Latin","German","mit"],"keywords_longer_than_N":true},
	{"name":"VD-Metadata","keyword":"latin","description":"\n\t\n\t\t\n\t\tMetadata of the \"Verzeichnis der im deutschen Sprachraum erschienen Drucke\"\n\t\n\n\n\t\n\t\t\n\t\tTitle\n\t\n\nMetadata of the \"Verzeichnis der im deutschen Sprachraum erschienen Drucke\"\n\n\t\n\t\t\n\t\tDescription and Motivation\n\t\n\nThis data publication was created with the intent to provide bibliographic and subject indexing metadata for research purposes and the development of AI applications. This data publication can be regarded as the German national bibliography of the period 1500‚Äì1800. It consists of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SBB/VD-Metadata.","url":"https://huggingface.co/datasets/SBB/VD-Metadata","creator_name":"Staatsbibliothek zu Berlin - Preu√üischer Kulturbesitz","creator_url":"https://huggingface.co/SBB","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","German","Latin","Greek"],"keywords_longer_than_N":true},
	{"name":"Tridis","keyword":"latin","description":"This is the first version of the dataset derived from the corpora used for TRIDIS (Tria Digita Scribunt). \nTRIDIS encompasses a series of Handwriting Text Recognition (HTR) models trained using semi-diplomatic transcriptions of medieval and early modern manuscripts.\nThe semi-diplomatic transcription approach involves resolving abbreviations found in the original manuscripts and normalizing Punctuation and Allographs.\nThe dataset contains approximately 4,000 pages of manuscripts and is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/magistermilitum/Tridis.","url":"https://huggingface.co/datasets/magistermilitum/Tridis","creator_name":"Sergio Torres","creator_url":"https://huggingface.co/magistermilitum","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","Spanish","Latin","German"],"keywords_longer_than_N":true}
]
;
