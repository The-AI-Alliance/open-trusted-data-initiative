const data_for_language_europe_german = 
[
	{"name":"ALMA-R-Preference","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/haoranxu/ALMA-R-Preference","creator_name":"Haoran Xu","creator_url":"https://huggingface.co/haoranxu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"ALMA-R-Preference\\\"\\n\\t\\n\\nThis is triplet preference data used by ALMA-R model.\\nThe triplet preference data, supporting 10 translation directions, is built upon the FLORES-200 development and test data. For each direction, we provide a source sentence along with three translations: one from GPT-4, another from ALMA-13B-LoRA, and a reference translation. For instance, in the English-German pair, our data structure is as follows:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSentences:\\n\\t\\n\\n\\nde: Originalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/haoranxu/ALMA-R-Preference."},
	{"name":"mittens","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/mittens","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMiTTenS: A Dataset for Evaluating Misgendering in Translation\\n\\t\\n\\nMisgendering is the act of referring to someone in a way that does not reflect their gender identity.  Translation systems, including foundation models capable of translation, can produce errors that result in misgendering harms. To measure the extent of such potential harms when translating into and out of English, we introduce a dataset, MiTTenS, covering 26 languages from a variety of language families and scriptsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google/mittens."},
	{"name":"Capybara-de","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maxidl/Capybara-de","creator_name":"Max Idahl","creator_url":"https://huggingface.co/maxidl","description":"German version of LDJnr/Capybara. Translated using DeepL (informal style).\\n\\n\\t\\n\\t\\t\\nlang\\n#chars\\n\\n\\n\\t\\t\\nen\\n71_102_832\\n\\n\\nde\\n81_422_005\\n\\n\\n\\t\\n\\n"},
	{"name":"math-prm-800k-de","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/maxidl/math-prm-800k-de","creator_name":"Max Idahl","creator_url":"https://huggingface.co/maxidl","description":"German version of prm800k. Translated using DeepL (informal style).\\n\\n\\t\\n\\t\\t\\nlang\\n#chars\\n\\n\\n\\t\\t\\nen\\n11_479_654\\n\\n\\nde\\n12_516_903\\n\\n\\n\\t\\n\\n"},
	{"name":"panlex","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lbourdois/panlex","creator_name":"LoÃ¯ck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPanLex\\n\\t\\n\\nJanuary 1, 2024 version of PanLex Language Vocabulary with 24,650,274 rows covering 6,152 languages.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tColumns\\n\\t\\n\\n\\nvocab: contains the text entry.  \\n639-3: contains the ISO 639-3 languages tags to allow users to filter on the language(s) of their choice.\\n639-3_english_name: the English language name associated to the code ISO 639-3. \\nvar_code: contains a code to differentiate language variants. In practice, this is the code 639-3 + a number.  If 000, itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/panlex."},
	{"name":"panlex","keyword":"swiss german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lbourdois/panlex","creator_name":"LoÃ¯ck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPanLex\\n\\t\\n\\nJanuary 1, 2024 version of PanLex Language Vocabulary with 24,650,274 rows covering 6,152 languages.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tColumns\\n\\t\\n\\n\\nvocab: contains the text entry.  \\n639-3: contains the ISO 639-3 languages tags to allow users to filter on the language(s) of their choice.\\n639-3_english_name: the English language name associated to the code ISO 639-3. \\nvar_code: contains a code to differentiate language variants. In practice, this is the code 639-3 + a number.  If 000, itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/panlex."},
	{"name":"german-court-decisions","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SH108/german-court-decisions","creator_name":"Stefan HÃ¤usler","creator_url":"https://huggingface.co/SH108","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for german-court-decisions\\n\\t\\n\\n60k judicial decisions in Germany retrieved on January 1, 2024.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nLanguage(s) (NLP): German\\nLicense: MIT\\nCopyright notice: Automated retrieval of decisions from federal and state databases in Germany is permitted for non-commercial purposes only. As a result, the use of this dataset is permitted for non-commercial purposes only.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUses\\n\\t\\n\\nPrediction of verdicts based on statement of facts.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SH108/german-court-decisions."},
	{"name":"germanrag","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DiscoResearch/germanrag","creator_name":"Disco Research","creator_url":"https://huggingface.co/DiscoResearch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGermanRAG ðŸ‡©ðŸ‡ªðŸ“œðŸ¦œ\\n\\t\\n\\nThis dataset is derived from the GermanDPR dataset and enhances it by providing fully formulated answers instead of answer spans.\\nIt can be used to finetune for retrieval augmented generation tasks (RAG) in German.\\nWe deduplicated the original contexts resulting in 2243 unique contexts and repeated the hard negatives of half of them, such that the last third of the total dataset contains only not answerable examples.\\nIn contrast to the original dataset theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DiscoResearch/germanrag."},
	{"name":"wikipedia-22-12-de-dpr","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/deutsche-telekom/wikipedia-22-12-de-dpr","creator_name":"Deutsche Telekom AG","creator_url":"https://huggingface.co/deutsche-telekom","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWikipedia 22-12 DE DPR\\n\\t\\n\\nThis is a German dataset for DPR model training.\\nDPR (Dense Passage Retrieval) is one of the most important components of RAG applications.\\nBased on this dataset, German document retrieval models can be trained.\\nThe unique feature of this data set is that it contains not only training data for questions,\\nbut also imperative questions.\\nAn imperative question is a type of question that is phrased as a command or an instruction.\\nSince there is a formal andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deutsche-telekom/wikipedia-22-12-de-dpr."},
	{"name":"MSD_manual_topics_user_base","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nuvocare/MSD_manual_topics_user_base","creator_name":"Nuvocare","creator_url":"https://huggingface.co/nuvocare","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMSD_manual_topics_user_base\\n\\t\\n\\nThis dataset has been built with the website https://www.msdmanuals.com/ provided by Merck & Co for the greater audience.\\nThe MSD manual is an essential source of knowledge for many topics related to symptoms, diseases, health and other related topics. The manual makes an extra effort to make it available both for professionals and patients by having two distinct version. \\nThe content, while being labelled the same, differs by the type of user inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nuvocare/MSD_manual_topics_user_base."},
	{"name":"klexikon_dpo","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/udkai/klexikon_dpo","creator_name":"UDK dot AI","creator_url":"https://huggingface.co/udkai","description":"Version of https://huggingface.co/datasets/dennlinger/klexikon which can be useful for Direct Preference Optimization of large language models generating sentences in simple german.\\n"},
	{"name":"synthetic_RAG_dataset_ger_de_v02","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SebastianBodza/synthetic_RAG_dataset_ger_de_v02","creator_name":"SebastianB","creator_url":"https://huggingface.co/SebastianBodza","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic German RAG Dataset\\n\\t\\n\\nThe dataset consist of questions in the 3 styles \\\"implicit\\\", \\\"search string\\\" and \\\"standard questions\\\". \\nAs well as of the additional Positive and Hard Negative document example. Additionally all intermediate data was kept.\\nBenchmarks of RAG Pipelines showed differing results for the different question styles.\\nThe Generation-Pipelone is based on vLLM and Mixtral in 4bit quant. All scripts: https://github.com/SebastianBodza/Embedding_Training\\nOneâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SebastianBodza/synthetic_RAG_dataset_ger_de_v02."},
	{"name":"ChatML-aya_dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Felladrin/ChatML-aya_dataset","creator_name":"Victor Nogueira","creator_url":"https://huggingface.co/Felladrin","description":"CohereForAI/aya_dataset in ChatML format, ready to use in HuggingFace TRL's SFT Trainer.\\nPython code used for conversion:\\nfrom datasets import load_dataset\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"Felladrin/Llama-160M-Chat-v1\\\")\\n\\ndataset = load_dataset(\\\"CohereForAI/aya_dataset\\\", split=\\\"train\\\")\\n\\ndef format(columns):\\n    messages = [\\n        {\\n            \\\"role\\\": \\\"user\\\",\\n            \\\"content\\\": columns[\\\"inputs\\\"].strip(),\\n        },\\n        {â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Felladrin/ChatML-aya_dataset."},
	{"name":"eagle","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MasahiroKaneko/eagle","creator_name":"Masahiro Kaneko","creator_url":"https://huggingface.co/MasahiroKaneko","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEagle ðŸ¦…: Ethical Dataset Given from Real Interactions\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThis repository contains the Eagle dataset, which is an ethical dataset of real interactions between humans and ChatGPT. This dataset is created to evaluate social bias, opinion bias, toxic language, and morality in Large Language Models (LLMs).\\nIf you use the Eagle dataset in your research, please cite the following:\\n@inproceedings{Eagle:arxiv:2024,\\n    title={Eagle: Ethical Dataset Given fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MasahiroKaneko/eagle."},
	{"name":"tinystories_german","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SkySyrup/tinystories_german","creator_name":"Ashley","creator_url":"https://huggingface.co/SkySyrup","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhat have you done\\n\\t\\n\\nthis dataset is a german interpretation of the roneneldan/TinyStories dataset\\nthat dataset is amazing- I wanted to make a german version to experiment with the bilinguality of tiny language models (more coming on that soon!!!) (i wrote a paper :D)\\nthis is the result of a bunch of work and months of screwing around\\nit was made with basically 0 budget; \\n\\nargos-translate contains 200k opennmt translated tinystories\\ngerman_GEMINI_async-combined contains about 180kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SkySyrup/tinystories_german."},
	{"name":"intel_orca_dpo_pairs_de","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de","creator_name":"Mayflower GmbH","creator_url":"https://huggingface.co/mayflowergmbh","description":"German translation of Intel/orca_dpo_pairs\\nUsing azureml for translation and hermeo-7b for rejected answers.\\n"},
	{"name":"MAGBIG","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/felfri/MAGBIG","creator_name":"Felix Friedrich","creator_url":"https://huggingface.co/felfri","description":"\\n\\t\\n\\t\\t\\n\\t\\tMAGBIG benchmark\\n\\t\\n\\nThis is the MAGBIG benchmark proposed in https://arxiv.org/abs/2401.16092\\nThis benchmark is intended for multilingual text-to-image models. With MAGBIG, you can generate images for a diverse set of prompts across ten different languages. These images can be evaluated for differences across languages. MAGBIG is designed to uncover and assess biases across languages such as gender, race, age, etc. This way, we can measure whether bias exists in a language, but also ifâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/felfri/MAGBIG."},
	{"name":"german-law-bgb","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wndknd/german-law-bgb","creator_name":"Lukas K","creator_url":"https://huggingface.co/wndknd","description":"The BÃ¼rgerliche Gesetzbuch divided by each paragraph for text-generation.\\n"},
	{"name":"german-function-calling","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/flozi00/german-function-calling","creator_name":"Florian Zimmermeister","creator_url":"https://huggingface.co/flozi00","description":"flozi00/german-function-calling dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Sci-Fi-Books-gutenberg","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/stevez80/Sci-Fi-Books-gutenberg","creator_name":"Steve t","creator_url":"https://huggingface.co/stevez80","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGutenberg Sci-Fi Book Dataset\\n\\t\\n\\nThis dataset contains information about science fiction books. Itâ€™s designed for training AI models, research, or any other purpose related to natural language processing.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Format\\n\\t\\n\\nThe dataset is provided in CSV format. Each record represents a book and includes the following fields:\\nID: A unique identifier for the book.\\nTitle: The title of the book.\\nAuthor: The author(s) of the book.\\nText: The text content of the book (e.g.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/stevez80/Sci-Fi-Books-gutenberg."},
	{"name":"German-PD-Newspapers","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/storytracer/German-PD-Newspapers","creator_name":"Sebastian Majstorovic","creator_url":"https://huggingface.co/storytracer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Public Domain Newspapers (German)\\n\\t\\n\\n\\n\\nThis dataset contains 13 billion words of OCR text extracted from German historical newspapers. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\nCurated by: Sebastian Majstorovic\\nLanguage(s) (NLP): German\\nLicense: Dataset: CC0, Texts: Public Domain\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: https://www.deutsche-digitale-bibliothek.de/newspaper\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright & License\\n\\t\\n\\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/storytracer/German-PD-Newspapers."},
	{"name":"llm-latent-language","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wendlerc/llm-latent-language","creator_name":"Chris Wendler","creator_url":"https://huggingface.co/wendlerc","description":"Latents computed using meta-llama/Llama-2-7b-hf, meta-llama/Llama-2-13b-hf, meta-llama/Llama-2-70b-hf\\n"},
	{"name":"MSD_instruct","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nuvocare/MSD_instruct","creator_name":"Nuvocare","creator_url":"https://huggingface.co/nuvocare","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMSD_manual_topics_user_base\\n\\t\\n\\nThis dataset has been built with the website https://www.msdmanuals.com/ provided by Merck & Co for the greater audience.\\nThe MSD manual is an essential source of knowledge for many topics related to symptoms, diseases, health and other related topics. The manual makes an extra effort to make it available both for professionals and patients by having two distinct version. \\nThe content, while being labelled the same, differs by the type of user inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nuvocare/MSD_instruct."},
	{"name":"Multi-lingual_Detection","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Manirathinam21/Multi-lingual_Detection","creator_name":"Manirathinam","creator_url":"https://huggingface.co/Manirathinam21","description":"Manirathinam21/Multi-lingual_Detection dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"SB10k","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Alienmaster/SB10k","creator_name":"Robert Geislinger","creator_url":"https://huggingface.co/Alienmaster","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tA Twitter corpus and benchmark resources for german sentiment analysis\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource\\n\\t\\n\\nThe data is a snapshot from the SB10k Dataset.\\nThe snapshot was made by Oliver Guhr.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation Information\\n\\t\\n\\nPaper\\n@inproceedings{cieliebak2017twitter,\\n  title={A twitter corpus and benchmark resources for german sentiment analysis},\\n  author={Cieliebak, Mark and Deriu, Jan Milan and Egger, Dominic and Uzdilli, Fatih},\\n  booktitle={5th International Workshop on Naturalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Alienmaster/SB10k."},
	{"name":"ehri-ner-all","keyword":"german","license":"European Union Public License 1.1","license_url":"https://choosealicense.com/licenses/eupl-1.1/","language":"en","dataset_url":"https://huggingface.co/datasets/ehri-ner/ehri-ner-all","creator_name":"EHRI-NER","creator_url":"https://huggingface.co/ehri-ner","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ehri-ner/ehri-ner-all\\n\\t\\n\\n\\nThe European Holocaust Research Infrastructure (EHRI) aims to support Holocaust research by making information about dispersed Holocaust material accessible and interconnected through its services. Creating a tool capable of detecting named entities in texts such as Holocaust testimonies or archival descriptions would make it easier to link more material with relevant identifiers in domain-specific controlled vocabularies, semanticallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ehri-ner/ehri-ner-all."},
	{"name":"iati-policy-markers","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/devinitorg/iati-policy-markers","creator_name":"Development Initiatives","creator_url":"https://huggingface.co/devinitorg","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInternational Aid Transparency Initiative (IATI) Policy Marker Dataset\\n\\t\\n\\nA multi-purpose dataset including all activity title and description text published to IATI with metadata for policy markers.\\nFor more information on IATI policy markers, see the element page on the IATI Standard Website.\\nIATI is a living data source, and this dataset was last updated on 21 August, 2024. For the code to generate an updated version of this dataset, please see my Github repository here.\\nFor anyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/devinitorg/iati-policy-markers."},
	{"name":"Publico","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hugosousa/Publico","creator_name":"Hugo Sousa","creator_url":"https://huggingface.co/hugosousa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPÃºblico\\n\\t\\n\\nThis dataset was build by translating a set of 34,157 news from PÃºblico, an European Portuguese news paper. The news have been translated using Google Translator.\\nTo now more about the data visit the Github repos used to scrape and translate the news.\\n"},
	{"name":"mewsli-x","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/izhx/mewsli-x","creator_name":"Xin Zhang","creator_url":"https://huggingface.co/izhx","description":"I generated the dataset following mewsli-x.md#getting-started\\nand converted into different parts (see process.py):\\n\\nar/de/en/es/fa/ja/pl/ro/ta/tr/uk wikinews_mentions dev and test (from wikinews_mentions-dev/test.jsonl)\\ncandidate entities of 50 languages (from candidate_set_entities.jsonl)\\nEnglish wikipedia_pairs to fine-tune models (from wikipedia_pairs-dev/train.jsonl)\\n\\nRaw data files are in raw.tar.gz, which contains:\\n[...] 535M Feb 24 22:06 candidate_set_entities.jsonl\\n[...] 9.8M Feb 24â€¦ See the full description on the dataset page: https://huggingface.co/datasets/izhx/mewsli-x."},
	{"name":"PotTS","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Alienmaster/PotTS","creator_name":"Robert Geislinger","creator_url":"https://huggingface.co/Alienmaster","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPotTS: The Potsdam Twitter Sentiment Corpus\\n\\t\\n\\nThis dataset contains the Potsdam Twitter Sentiment Corpus based on this data.\\nThe link to the original annotated dataset can be found under Links.\\nThe only difference is that the mixed sentiment is removed (32 dev/55 test/401 train).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLinks\\n\\t\\n\\nhttps://github.com/WladimirSidorenko/PotTS\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation Information\\n\\t\\n\\n@inproceedings{sidarenka-2016-potts,\\n    title = \\\"{P}ot{TS}: The {P}otsdam {T}witter Sentiment Corpus\\\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Alienmaster/PotTS."},
	{"name":"SpeakGer_sample","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/K-RLange/SpeakGer_sample","creator_name":"Kai Robin Lange","creator_url":"https://huggingface.co/K-RLange","description":"This data set contains all speeches of all German federal state parliaments as well as the Bundestag from 2022, that are not interjections from the crowd or comments by the chair of the plenary session. Some additional meta data is provided, such as the date of the speech as well as the party/parties of the speaker.\\nThis is a small test sample of the SpeakGer data set, restricted to the year 2022 and with limited meta data. For more information, please visit the official GitHub page. Whenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/K-RLange/SpeakGer_sample."},
	{"name":"MultiQ","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/caro-holt/MultiQ","creator_name":"Holtermann","creator_url":"https://huggingface.co/caro-holt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MultiQ\\n\\t\\n\\nThis is the dataset corresponding to the paper \\\"Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ\\\". \\nIt is a silver standard benchmark that can be used to evaluate the basic multilingual capabilities of LLMs. It contains 200 open ended questions automatically \\ntranslated into 137 typologically diverse languages. \\n\\nCurated by: Carolin Holtermann, Paul RÃ¶ttger, Timm Dill, Anne Lauscher\\nLanguage(s) (NLP): 137 diverseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/caro-holt/MultiQ."},
	{"name":"bhojpuri","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ankur02/bhojpuri","creator_name":"Ankur Verma","creator_url":"https://huggingface.co/ankur02","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFLEURS\\n\\t\\n\\nFleurs is the speech version of the FLoRes machine translation benchmark. \\nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \\nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\\nused and â€unit error rateâ€ (characters, signs) of all languages is averaged. Languages and results are also grouped into sevenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ankur02/bhojpuri."},
	{"name":"wikipedia_leipzig_de_2021","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Alienmaster/wikipedia_leipzig_de_2021","creator_name":"Robert Geislinger","creator_url":"https://huggingface.co/Alienmaster","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLeipzig Corpora Wikipedia 2021 German\\n\\t\\n\\nThis dataset contains different splits (between 10k and 1mio) from the german wikipedia 2021. The data were collected 2021.\\nEvery element in the dataset is labeled as \\\"neutral\\\".\\nThe source can be found here\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\n@inproceedings{goldhahn-etal-2012-building,\\n    title = \\\"Building Large Monolingual Dictionaries at the {L}eipzig Corpora Collection: From 100 to 200 Languages\\\",\\n    author = \\\"Goldhahn, Dirk  and\\n      Eckartâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Alienmaster/wikipedia_leipzig_de_2021."},
	{"name":"invoices-example","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/parsee-ai/invoices-example","creator_name":"Parsee.ai","creator_url":"https://huggingface.co/parsee-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInoices Sample Dataset\\n\\t\\n\\nThis is a sample dataset generated on app.parsee.ai for invoices. The goal was to evaluate different LLMs on this RAG task using the Parsee evaluation tools. A full study can be found here: https://github.com/parsee-ai/parsee-datasets/blob/main/datasets/invoices/parsee-loader/README.md\\nparsee-core version used: 0.1.3.11\\nThis dataset was created on the basis of 15 sample invoices (PDF files).\\nAll PDF files are publicly accessible on parsee.ai, to accessâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/parsee-ai/invoices-example."},
	{"name":"multi-hatecheck","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/multi-hatecheck","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nCombines multilingual HateCheck datasets (10 languages, including English), by Paul Roettger and colleagues (2021, 2022).\\nThe original English dataset can be found under https://github.com/Paul/hatecheck.\\nDatasets for other languages are found at:\\n\\nhttps://github.com/Paul/hatecheck-arabic\\nhttps://github.com/Paul/hatecheck-mandarin\\nhttps://github.com/Paul/hatecheck-german\\nhttps://github.com/Paul/hatecheck-french\\nhttps://github.com/Paul/hatecheck-hindiâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/multi-hatecheck."},
	{"name":"CircuitSketchTextAnnotations","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/edesaras/CircuitSketchTextAnnotations","creator_name":"Aras EdeÅŸ","creator_url":"https://huggingface.co/edesaras","description":"edesaras/CircuitSketchTextAnnotations dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"panlex-meanings","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cointegrated/panlex-meanings","creator_name":"David Dale","creator_url":"https://huggingface.co/cointegrated","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for panlex-meanings\\n\\t\\n\\nThis is a dataset of words in several thousand languages, extracted from https://panlex.org.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset has been extracted from https://panlex.org (the 20240301 database dump) and rearranged on the per-language basis.\\nEach language subset consists of expressions (words and phrases). \\nEach expression is associated with some meanings (if there is more than one meaning, they are inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cointegrated/panlex-meanings."},
	{"name":"panlex-meanings","keyword":"swiss german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cointegrated/panlex-meanings","creator_name":"David Dale","creator_url":"https://huggingface.co/cointegrated","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for panlex-meanings\\n\\t\\n\\nThis is a dataset of words in several thousand languages, extracted from https://panlex.org.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset has been extracted from https://panlex.org (the 20240301 database dump) and rearranged on the per-language basis.\\nEach language subset consists of expressions (words and phrases). \\nEach expression is associated with some meanings (if there is more than one meaning, they are inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cointegrated/panlex-meanings."},
	{"name":"co-funer","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/stefan-it/co-funer","creator_name":"Stefan Schweter","creator_url":"https://huggingface.co/stefan-it","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction\\n\\t\\n\\nThis inofficial dataset repository provides a CoNLL-like version of the CO-Fun NER dataset, that was proposed in the CO-Fun paper (https://arxiv.org/abs/2403.15322):\\n\\nThe process of cyber mapping gives insights in relationships among financial entities and service providers. Centered around the outsourcing practices of companies within fund prospectuses inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stefan-it/co-funer."},
	{"name":"JimmyTeddy","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emilderteddybaer/JimmyTeddy","creator_name":"Jimmy","creator_url":"https://huggingface.co/emilderteddybaer","description":"emilderteddybaer/JimmyTeddy dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"NTREX","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davidstap/NTREX","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nNTREX -- News Test References for MT Evaluation from English into a total of 128 target languages. See original GitHub repo for full details.\\nExample of loading:\\ndataset = load_dataset(\\\"davidstap/NTREX\\\", \\\"rus_Cyrl\\\", trust_remote_code=True)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe following languages are available:\\n\\n\\t\\n\\t\\t\\nLanguage Code\\nLanguage Name\\n\\n\\n\\t\\t\\nafr_Latn\\nAfrikaans\\n\\n\\namh_Ethi\\nAmharic\\n\\n\\narb_Arab\\nArabic\\n\\n\\naze_Latn\\nAzerbaijani\\n\\n\\nbak_Cyrl\\nBashkir\\n\\n\\nbel_Cyrl\\nBelarusianâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/NTREX."},
	{"name":"eurlex-multilingual","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/eurlex-multilingual","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"eurlex-multilingual\\\"\\n\\t\\n\\nMore Information needed\\n"},
	{"name":"biblenlp-corpus-mmteb","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\\nLoading example:\\n>>> from datasets import load_dataset\\n>>> dataset = load_dataset(\\\"davidstap/biblenlp-corpus-mmteb\\\", \\\"eng-arb\\\", trust_remote_code=True)\\n>>> dataset\\nDatasetDict({\\n    train: Dataset({\\n        features: ['eng', 'arb'],\\n        num_rows: 28723\\n    })\\n    validation: Dataset({\\n        features: ['eng', 'arb'],\\n        num_rows: 1578\\n    })â€¦ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb."},
	{"name":"biblenlp-corpus-mmteb","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\\nLoading example:\\n>>> from datasets import load_dataset\\n>>> dataset = load_dataset(\\\"davidstap/biblenlp-corpus-mmteb\\\", \\\"eng-arb\\\", trust_remote_code=True)\\n>>> dataset\\nDatasetDict({\\n    train: Dataset({\\n        features: ['eng', 'arb'],\\n        num_rows: 28723\\n    })\\n    validation: Dataset({\\n        features: ['eng', 'arb'],\\n        num_rows: 1578\\n    })â€¦ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb."},
	{"name":"gahd","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jagoldz/gahd","creator_name":"Janis Goldzycher","creator_url":"https://huggingface.co/jagoldz","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for GAHD\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nGAHD is a German Adversarial Hate speech Dataset containing 10,996 examples. We collected the dataset via four rounds of Dynamic Adversarial Data Collection and explored various methods of supporting annotators in finding adversarial examples.\\n\\nPaper: https://aclanthology.org/2024.naacl-long.248/\\nRepository: https://github.com/jagol/gahd\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\ngahd.csv contains the following columns:\\n\\ngahd_id:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jagoldz/gahd."},
	{"name":"FL_QA_GER","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JoeUnili/FL_QA_GER","creator_name":"Joel","creator_url":"https://huggingface.co/JoeUnili","description":"Question-Answer style Dataset containing 3069 different questions regarding the Principality of Liechtenstein.\\nContains 1409 questions in the legal domain and 1660 questions in the historical / cultural domain.\\nThe questions are generated using OpenAI ChatGPT 4, asking ChatGPT to produce question-answer pairs for the text content given. The text content is based on the documents and articles used in the FL_Legal_GER and FL_History_GER textual datasets.\\nDataset is made available in Germanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JoeUnili/FL_QA_GER."},
	{"name":"S4PLANTRAIN240427","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HDBrinkmann/S4PLANTRAIN240427","creator_name":"Brinkmann","creator_url":"https://huggingface.co/HDBrinkmann","description":"HDBrinkmann/S4PLANTRAIN240427 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"swim-ir-cross-lingual","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nthakur/swim-ir-cross-lingual","creator_name":"Nandan Thakur","creator_url":"https://huggingface.co/nthakur","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SWIM-IR (Cross-lingual)\\n\\t\\n\\n\\n\\n\\nThis is the cross-lingual subset of the SWIM-IR dataset, where the query generated is in the target language and the passage is in English.\\nThe SWIM-IR dataset is available as CC-BY-SA 4.0. 18 languages (including English) are available in the cross-lingual dataset.\\nFor full details of the dataset, please read our upcoming NAACL 2024 paper and check out our website.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhat is SWIM-IR?\\n\\t\\n\\nSWIM-IR dataset is a syntheticâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nthakur/swim-ir-cross-lingual."},
	{"name":"xsimplusplus","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jaygala24/xsimplusplus","creator_name":"Jay Gala","creator_url":"https://huggingface.co/jaygala24","description":"xSIM++ is an extension of xSIM. In comparison to xSIM, this evaluates using target-side data with additional synthetic, hard-to-distinguish examples. You can find more details about it in the publication: xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages.\\n"},
	{"name":"axolotl-wiktionary-definitions","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/adorkin/axolotl-wiktionary-definitions","creator_name":"Aleksei Dorkin","creator_url":"https://huggingface.co/adorkin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\n@misc{dorkin2024tartunlpaxolotl24leveraging,\\n      title={TartuNLP @ AXOLOTL-24: Leveraging Classifier Output for New Sense Detection in Lexical Semantics}, \\n      author={Aleksei Dorkin and Kairit Sirts},\\n      year={2024},\\n      eprint={2407.03861},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2407.03861}, \\n}\\n\\n"},
	{"name":"4PLANSMOBUDDY03","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HDBrinkmann/4PLANSMOBUDDY03","creator_name":"Brinkmann","creator_url":"https://huggingface.co/HDBrinkmann","description":"HDBrinkmann/4PLANSMOBUDDY03 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"biblenlp-corpus-mmteb","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\\nLoading example:\\n>>> from datasets import load_dataset\\n>>> dataset = load_dataset(\\\"davidstap/biblenlp-corpus-mmteb\\\", \\\"eng-arb\\\", trust_remote_code=True)\\n>>> dataset\\nDatasetDict({\\n    train: Dataset({\\n        features: ['eng', 'arb'],\\n        num_rows: 28723\\n    })\\n    validation: Dataset({\\n        features: ['eng', 'arb'],\\n        num_rows: 1578\\n    })â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb."},
	{"name":"biblenlp-corpus-mmteb","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\\nLoading example:\\n>>> from datasets import load_dataset\\n>>> dataset = load_dataset(\\\"davidstap/biblenlp-corpus-mmteb\\\", \\\"eng-arb\\\", trust_remote_code=True)\\n>>> dataset\\nDatasetDict({\\n    train: Dataset({\\n        features: ['eng', 'arb'],\\n        num_rows: 28723\\n    })\\n    validation: Dataset({\\n        features: ['eng', 'arb'],\\n        num_rows: 1578\\n    })â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb."},
	{"name":"Ger-RAG-eval","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/deutsche-telekom/Ger-RAG-eval","creator_name":"Deutsche Telekom AG","creator_url":"https://huggingface.co/deutsche-telekom","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman RAG LLM Evaluation Dataset\\n\\t\\n\\nThis dataset is intended for the evaluation of German RAG (retrieval augmented generation) capabilities of LLM models.\\nIt is based on the test set of the deutsche-telekom/wikipedia-22-12-de-dpr \\ndata set (also see wikipedia-22-12-de-dpr on GitHub) and\\nconsists of 4 subsets or tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTask Description\\n\\t\\n\\nThe dataset consists of 4 subsets for the following 4 tasks (each task with 1000 prompts):\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tchoose_context_by_questionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deutsche-telekom/Ger-RAG-eval."},
	{"name":"gahd","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/gahd","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"NOTE README copied from https://github.com/jagol/gahd\\nThis repository contains the dataset from our NAACL 2024 paper \\\"Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset\\\".\\ngahd.csv contains the following columns:\\n\\ngahd_id: unique identifier of the entry\\ntext: text of the entry\\nlabel: 0 = \\\"not-hate speech\\\", 1 = \\\"hate speech\\\"\\nround: round in which the entry was created\\nsplit: \\\"train\\\", \\\"dev\\\", or \\\"test\\\"\\ncontrastive_gahd_id: gahd_id of itsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/gahd."},
	{"name":"wikipedia_leipzig_de_2016","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Alienmaster/wikipedia_leipzig_de_2016","creator_name":"Robert Geislinger","creator_url":"https://huggingface.co/Alienmaster","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLeipzig Corpora Wikipedia 2016 German\\n\\t\\n\\nThis dataset contains different splits (between 10k and 1mio) from the german wikipedia 2016. The data were collected 2016.\\nEvery element in the dataset is labeled as \\\"neutral\\\".\\nThe source can be found here\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\n@inproceedings{goldhahn-etal-2012-building,\\n    title = \\\"Building Large Monolingual Dictionaries at the {L}eipzig Corpora Collection: From 100 to 200 Languages\\\",\\n    author = \\\"Goldhahn, Dirk  and\\n      Eckartâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Alienmaster/wikipedia_leipzig_de_2016."},
	{"name":"qonto-open-qa","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ThomasCdnns/qonto-open-qa","creator_name":"Thomas Chardonnens","creator_url":"https://huggingface.co/ThomasCdnns","description":"ThomasCdnns/qonto-open-qa dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"de_en","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sohy/de_en","creator_name":"Sohyun Son","creator_url":"https://huggingface.co/Sohy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sohy/de_en."},
	{"name":"sib200","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/sib200","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SIB-200\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSIB-200 is the largest publicly available topic classification dataset based on Flores-200 covering 205 languages and dialects.\\nThe train/validation/test sets are available for all the 205 languages.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\ntopic classification: categorize wikipedia sentences into topics e.g science/technology, sports or politics.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThere are 205 languages available :â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/sib200."},
	{"name":"MAiDE-up","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MichiganNLP/MAiDE-up","creator_name":"LIT @ UMich","creator_url":"https://huggingface.co/MichiganNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel Reviews\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMultilingual Deception Detection of GPT-generated Hotel Reviews. We compare real hotel reviews from Booking with LLM-generated hotel reviews in 10 languages.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe text in the dataset is in 10 languages: Chinese, English, French, German, Italian, Romanian, Korean, Russian, Spanish, Turkish\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboardsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MichiganNLP/MAiDE-up."},
	{"name":"bundestag_gesetze_index_bulk_20240507","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/orbiter/bundestag_gesetze_index_bulk_20240507","creator_name":"Michael Christen","creator_url":"https://huggingface.co/orbiter","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDeutsche Bundesgesetze und -verordnungen\\n\\t\\n\\nDieses Git Repository enthÃ¤lt alle Deutschen Bundesgesetze und -verordnungen\\nals Elasticsearch Index Bulk Format. Diese Dateien wurden mit Hilfe von\\nhttps://github.com/Orbiter/bundestag_gesetze_parser\\nerzeugt. Der Ursprung der Gesetze sind die Webseiten\\nhttps://www.gesetze-im-internet.de/\\nwelche auch in den DatensÃ¤tzen verlinkt sind.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVerwendungsmÃ¶glichkeiten\\n\\t\\n\\nDie Daten kÃ¶nnen beispielsweise als Grundlage fÃ¼r RAG (Retrievalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orbiter/bundestag_gesetze_index_bulk_20240507."},
	{"name":"FL_Legal_GER","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JoeUnili/FL_Legal_GER","creator_name":"Joel","creator_url":"https://huggingface.co/JoeUnili","description":"The regulations (legal articles) as well as other legal data are contained inside this dataset in text form. Only one column called â€œtextâ€ is contained in the dataset. The dataset is intended to be used for continual pretraining of a language model on legal data from Liechtenstein. The dataset is published exclusively in German language and around 5â€™700 rows are included in the dataset. The relatively low number of 5700 rows can be explained by the fact that each legal regulation is beingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JoeUnili/FL_Legal_GER."},
	{"name":"FL_History_GER","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JoeUnili/FL_History_GER","creator_name":"Joel","creator_url":"https://huggingface.co/JoeUnili","description":"All of the articles of the Liechtenstein historical encyclopaedia are contained in this dataset in textual form. Additionally, the historical and cultural publications and books gathered from eliechtensteinen-sia.li are also included. Only one column called â€œtextâ€ is contained in the dataset. The dataset is published exclusively in German language and around 17â€™000 rows are included in the dataset.\\nThe dataset is intended as is to be used for pretraining. However, further data cleaning isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JoeUnili/FL_History_GER."},
	{"name":"lua_code_dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tfcbhop/lua_code_dataset","creator_name":"Tim Gatzke","creator_url":"https://huggingface.co/tfcbhop","description":"tfcbhop/lua_code_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"newscrawl_enhanced_gender_balance","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jurasova/newscrawl_enhanced_gender_balance","creator_name":"Daniela JurÃ¡Å¡ovÃ¡","creator_url":"https://huggingface.co/jurasova","description":"This dataset is the product of our work focused on the investigation of the development of the representation of gender forms, specifically male and female forms of occupations, in textual data.\\nWe used the newscrawl dataset (Newscrawl doc-split data; WMT overview paper) for Czech, German, Spanish and Polish and employed clustering and filtering techniques (based on temporal and topic information) to extract a gender-balanced portion of the data. The dataset is therefore released under theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jurasova/newscrawl_enhanced_gender_balance."},
	{"name":"orpo-dpo-mix-40k-llama3-de","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/johannhartmann/orpo-dpo-mix-40k-llama3-de","creator_name":"Johann-Peter Hartmann","creator_url":"https://huggingface.co/johannhartmann","description":"johannhartmann/orpo-dpo-mix-40k-llama3-de dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"jina-embeddings-v2-base-en-14052024-5b5o-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-14052024-5b5o-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjina-embeddings-v2-base-en-14052024-5b5o-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"Fashion boutique products and reviews search\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jina-embeddings-v2-base-en-14052024-5b5o-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-14052024-5b5o-webapp."},
	{"name":"NTREX","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/NTREX","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nNTREX -- News Test References for MT Evaluation from English into a total of 128 target languages. See original GitHub repo for full details.\\nExample of loading:\\ndataset = load_dataset(\\\"davidstap/NTREX\\\", \\\"rus_Cyrl\\\", trust_remote_code=True)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe following languages are available:\\n\\n\\t\\n\\t\\t\\nLanguage Code\\nLanguage Name\\n\\n\\n\\t\\t\\nafr_Latn\\nAfrikaans\\n\\n\\namh_Ethi\\nAmharic\\n\\n\\narb_Arab\\nArabic\\n\\n\\naze_Latn\\nAzerbaijani\\n\\n\\nbak_Cyrl\\nBashkir\\n\\n\\nbel_Cyrl\\nBelarusianâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NTREX."},
	{"name":"ParaNames","keyword":"swiss german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bltlab/ParaNames","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bltlab/ParaNames."},
	{"name":"ParaNames","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bltlab/ParaNames","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bltlab/ParaNames."},
	{"name":"xm3600","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/floschne/xm3600","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tXM3600 - Crossmodal-3600\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is a copy from https://google.github.io/crossmodal-3600/\\n\\t\\n\\nIf you use this dataset, please cite the original authors:\\n@inproceedings{ThapliyalCrossmodal2022,\\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\\n  booktitle = {EMNLP},\\n  year = {2022}\\n}\\n\\nIt also includes the image features as PIL Image and has a uniform andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/floschne/xm3600."},
	{"name":"xm3600_1k","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/floschne/xm3600_1k","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tXM3600 - Crossmodal-3600 - 1K Split\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is a copy from https://google.github.io/crossmodal-3600/\\n\\t\\n\\nIf you use this dataset, please cite the original authors:\\n@inproceedings{ThapliyalCrossmodal2022,\\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\\n  booktitle = {EMNLP},\\n  year = {2022}\\n}\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is a 1K split of XM3600!\\n\\t\\n\\nFor this, weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/floschne/xm3600_1k."},
	{"name":"xflickrco","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/floschne/xflickrco","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","description":"floschne/xflickrco dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"xgqa","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/floschne/xgqa","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\txGQA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is a clone of the few_shot-test split of the xGQA dataset\\n\\t\\n\\nPlease find the original repository here: https://github.com/adapter-hub/xGQA\\nIf you use this dataset, please cite the original authors:\\n@inproceedings{pfeiffer-etal-2021-xGQA,\\n    title={{xGQA: Cross-Lingual Visual Question Answering}},\\n    author={ Jonas Pfeiffer and Gregor Geigle and Aishwarya Kamath and Jan-Martin O. Steitz and Stefan Roth and Ivan Vuli{\\\\'{c}} and Iryna Gurevych}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/floschne/xgqa."},
	{"name":"PearlDiver","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JoaoSchneider/PearlDiver","creator_name":"Joao Schneider","creator_url":"https://huggingface.co/JoaoSchneider","description":"Listing of the >7k books I've read recently. Some claim it's actually data scraped from website Perlentaucher. Which you believe is up to you, but here's a video of my occasional reading activity (haters gonna say it's fake).\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset card\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe data set contains information of books that have been reviewed at least once. \\nThe reviews are received from reputable German print media such as the Frankfurter Allgemeine Zeitung (FAZ), SÃ¼ddeutscheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JoaoSchneider/PearlDiver."},
	{"name":"riddles-qa-de","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/D4ve-R/riddles-qa-de","creator_name":"David","creator_url":"https://huggingface.co/D4ve-R","description":"D4ve-R/riddles-qa-de dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"GPT-CitizenService-QA-de","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/D4ve-R/GPT-CitizenService-QA-de","creator_name":"David","creator_url":"https://huggingface.co/D4ve-R","description":"D4ve-R/GPT-CitizenService-QA-de dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"terra-xplain-cc-de","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/D4ve-R/terra-xplain-cc-de","creator_name":"David","creator_url":"https://huggingface.co/D4ve-R","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Terra-Xplain-CC-de\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nA high quality text dataset in german. Scraped from https://terraxplaincommons.zdf.de/\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains 283 items, including title, short_text and text for various domains.\\n48833 words.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nText Generation, DPO finetuning for response length preference\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nGerman only\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n{â€¦ See the full description on the dataset page: https://huggingface.co/datasets/D4ve-R/terra-xplain-cc-de."},
	{"name":"ronaldo","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/noahgeiger2000/ronaldo","creator_name":"Noah Geiger","creator_url":"https://huggingface.co/noahgeiger2000","description":"noahgeiger2000/ronaldo dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"messio","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/noahgeiger2000/messio","creator_name":"Noah Geiger","creator_url":"https://huggingface.co/noahgeiger2000","description":"noahgeiger2000/messio dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"x-fact","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/utahnlp/x-fact","creator_name":"NLP at University of Utah","creator_url":"https://huggingface.co/utahnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"x-fact\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nX-FACT is a multilingual dataset for fact-checking with real world claims. The dataset contains short statments in 25 languages with top five evidence documents retrieved by performing google search with claim statements. The dataset contains two additional evaluation splits (in addition to a traditional test set): ood and zeroshot. ood measures out-of-domain generalization where whileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/utahnlp/x-fact."},
	{"name":"M4U","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/M4U-Benchmark/M4U","creator_name":"M4U-Benchmark","creator_url":"https://huggingface.co/M4U-Benchmark","description":"\\n\\t\\n\\t\\t\\n\\t\\tM4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models\\n\\t\\n\\nCode for the Paper M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models.\\n[Webpage] [Paper] [Huggingface Dataset] [Leaderboard]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ’¥ News ðŸ’¥\\n\\t\\n\\n\\n[2024.05.23] Our paper, dataset and code are public aviailable.\\n\\n\\n\\t\\n\\t\\n\\t\\n\\t\\tðŸ‘€ About M4U\\n\\t\\n\\n\\n     \\n\\n\\nMultilingual multimodal reasoning is a core component to achieve human-level intelligence. However, most of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/M4U-Benchmark/M4U."},
	{"name":"xgqa_1k","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/floschne/xgqa_1k","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\txGQA 1K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is a 1K subset of the few_shot-test split of the xGQA dataset\\n\\t\\n\\nPlease find the original repository here: https://github.com/adapter-hub/xGQA\\nIf you use this dataset, please cite the original authors:\\n@inproceedings{pfeiffer-etal-2021-xGQA,\\n    title={{xGQA: Cross-Lingual Visual Question Answering}},\\n    author={ Jonas Pfeiffer and Gregor Geigle and Aishwarya Kamath and Jan-Martin O. Steitz and Stefan Roth and Ivan Vuli{\\\\'{c}} and Iryna Gurevych}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/floschne/xgqa_1k."},
	{"name":"xflickrco_1k","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/floschne/xflickrco_1k","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","description":"floschne/xflickrco_1k dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"aya_german-sharegpt","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sroecker/aya_german-sharegpt","creator_name":"Steffen RÃ¶cker","creator_url":"https://huggingface.co/sroecker","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nDerived from Aya Collection Language Split\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): German\\nLicense: Apache 2.0\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/sroecker/aya_german-sharegpt."},
	{"name":"europa-random-split","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NCube/europa-random-split","creator_name":"Neural Network Nomads (We are GLOSS)","creator_url":"https://huggingface.co/NCube","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for EUROPA\\n\\t\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nEUROPA is a dataset designed for training and evaluating multilingual keyphrase generation models in the legal domain. It consists of legal judgments from the Court of Justice of the European Union (EU) and includes instances in all 24 official EU languages.\\nKey Features:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NCube/europa-random-split."},
	{"name":"paracrawl_context","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Proyag/paracrawl_context","creator_name":"Proyag Pal","creator_url":"https://huggingface.co/Proyag","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ParaCrawl_Context\\n\\t\\n\\n\\n\\nThis is a dataset for document-level machine translation introduced in the ACL 2024 paper Document-Level Machine Translation with Large-Scale Public Parallel Data. It is a dataset consisting of parallel sentence pairs from the ParaCrawl dataset along with corresponding preceding context extracted from the webpages the sentences were crawled from.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\nThis dataset addsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Proyag/paracrawl_context."},
	{"name":"span_absinth_german_faithfulness_detection_dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mtc/span_absinth_german_faithfulness_detection_dataset","creator_name":"MTC","creator_url":"https://huggingface.co/mtc","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Span Absinth - Hallucination Detection Dataset of German News Summarization\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nSpan Absinth is an extension of the Absinth dataset, where each hallucinated summary-sentence has been augmented with span annotations, that define which part of the sentence is hallucinated. Span annotations have the advantage of\\neffectively isolating hallucinations at the token level.\\nPlease refer to our paper and Absinth, for more details about theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mtc/span_absinth_german_faithfulness_detection_dataset."},
	{"name":"germeval14_no_wikipedia","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/stefan-it/germeval14_no_wikipedia","creator_name":"Stefan Schweter","creator_url":"https://huggingface.co/stefan-it","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiltered GermEval 2014 NER Dataset\\n\\t\\n\\nThis repository hosts a filtered version of the great GermEval 2014 NER Dataset.\\nAfter some analysis of the annotated examples in this dataset, it can be seen that the dataset is highly biased by Wikipedia articles.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Stats\\n\\t\\n\\nWe present an overview of the top 10 top-level domains where annotations were retrieved from for training, development and test splits:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTraining Split\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nTLD\\nNumber of examplesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stefan-it/germeval14_no_wikipedia."},
	{"name":"MultiPICo","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Multilingual-Perspectivist-NLU/MultiPICo","creator_name":"MultilingualPerspectivistNLU","creator_url":"https://huggingface.co/Multilingual-Perspectivist-NLU","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMultiPICo (Multilingual Perspectivist Irony Corpus) is a disaggregated multilingual corpus for irony detection, containing 18,778 pairs of short conversations (post-reply) from Twitter (8,956) and Reddit (9,822), along with the demographic information of each annotator (age, nationality, gender, and so on). \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nIrony classification task using soft labels (i.e., distribution of annotations) or hard labels (i.e.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Multilingual-Perspectivist-NLU/MultiPICo."},
	{"name":"europa","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NCube/europa","creator_name":"Neural Network Nomads (We are GLOSS)","creator_url":"https://huggingface.co/NCube","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for EUROPA\\n\\t\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nEUROPA is a dataset designed for training and evaluating multilingual keyphrase generation models in the legal domain. It consists of legal judgments from the Court of Justice of the European Union (EU) and includes instances in all 24 official EU languages.\\nKey Features:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NCube/europa."},
	{"name":"agb-de","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d4br4/agb-de","creator_name":"Daniel Braun","creator_url":"https://huggingface.co/d4br4","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to cite\\n\\t\\n\\n@inproceedings{braun-matthes-2024-agb,\\n    title = \\\"AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts\\\", \\n    author = \\\"Braun, Daniel and Matthes, Florian\\\",\\n    booktitle = \\\"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\\\",\\n    year = \\\"2024\\\",\\n    publisher =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d4br4/agb-de."},
	{"name":"dtaec-lexicon","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aehrm/dtaec-lexicon","creator_name":"Anton Ehrmanntraut","creator_url":"https://huggingface.co/aehrm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDTA EvalCorpus Lexicon\\n\\t\\n\\nThis dataset is a derivative of the DTA EvalCorpus which is a parallel corpus by the Deutsche Textarchiv (DTA), of the Deutsche Textarchiv (German Text Archive), who aligned historic prints of documents with their respective modern editions normalized contemporary orthography.\\nIn particular, this dataset is constructed from a subset of 85 literary documents of the DTA EvalCorpus, and then counting the normalization pairs. The dataset can be replicated fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aehrm/dtaec-lexicon."},
	{"name":"GermEval18","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jfrei/GermEval18","creator_name":"Johann Frei","creator_url":"https://huggingface.co/jfrei","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGermEval18 Loader\\n\\t\\n\\n\\nData Repository: https://github.com/uds-lsv/GermEval-2018-Data\\nData Reference: https://doi.org/10.11588/data/0B5VML\\nPaper: https://epub.oeaw.ac.at/0xc1aa5576_0x003a10d2.pdf\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInfo\\n\\t\\n\\nNote: This dataset is a loader script that pulls the data straight from the official GitHub repository.\\nWhat is the difference to philschmid/germeval18?: We did not get all samples, when using the former script.\\nOutput from philschmid/germeval18:\\nDatasetDict({\\n  train:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jfrei/GermEval18."},
	{"name":"kassenzettel-synth","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nimalu/kassenzettel-synth","creator_name":"Niklas","creator_url":"https://huggingface.co/nimalu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tkassenzettel-synth\\n\\t\\n\\n\\n\\nThis dataset contains generate images of receipts.\\nIt has been generated using github.com/nimalu/kassenzettel.\\nThe current version contains 1000 samples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nFor each instance, there is the underlying data, an image of the receipt, an image of the masks and both images augmented by adding crinkles.\\n \\n\\n"},
	{"name":"FL_Eval_GER","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JoeUnili/FL_Eval_GER","creator_name":"Joel","creator_url":"https://huggingface.co/JoeUnili","description":"Single-Choice Question Answering dataset used for evaluation regarding Liechtenstein questions. Consists of 100 question-answer pairs, each consisting of a question and 4 possible answers. Answer A is always correct while answers B, C and D are incorrect. All question-answer pairs are either assigned to the legal domain or to the historical / general domain.\\nIt is created using GPT 4 generations with prompts referencing information from the the FL_Legal_GER and FL_History_GER datasets.\\n"},
	{"name":"Chatgpt","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RajChat/Chatgpt","creator_name":"Rajdeep Chatterjee ","creator_url":"https://huggingface.co/RajChat","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenAssistant Conversations Dataset (OASST1)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \\nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \\ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \\nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \\nis a product of a worldwide crowd-sourcing effortâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RajChat/Chatgpt."},
	{"name":"ProgressGym-HistText","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PKU-Alignment/ProgressGym-HistText","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","description":"*Huggingface dataset preview for 19th, 20th, and 21st centuries is not available due to lack of support for array types. Instead, consider downloading those files for manual inspection, or see the Data Samples section below for more examples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProgressGym-HistText\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe ProgressGym Framework\\n\\t\\n\\n\\nProgressGym-HistText is part of the ProgressGym framework for research and experimentation on progress alignment - the emulation of moral progress in AIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/ProgressGym-HistText."},
	{"name":"synthetic_multilingual_llm_prompts","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gretelai/synthetic_multilingual_llm_prompts","creator_name":"Gretel.ai","creator_url":"https://huggingface.co/gretelai","description":"\\n  \\n  Image generated by DALL-E. See prompt for more details\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ“ðŸŒ Synthetic Multilingual LLM Prompts\\n\\t\\n\\nWelcome to the \\\"Synthetic Multilingual LLM Prompts\\\" dataset! This comprehensive collection features 1,250 synthetic LLM prompts generated using Gretel Navigator, available in seven different languages. To ensure accuracy and diversity in prompts, and translation quality and consistency across the different languages, we employed Gretel Navigator both as a generation tool and asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gretelai/synthetic_multilingual_llm_prompts."},
	{"name":"CaLMQA","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shanearora/CaLMQA","creator_name":"Shane Arora","creator_url":"https://huggingface.co/shanearora","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\n\\nCaLMQA is a long-form question answering (LFQA) dataset spanning 23 high- to low-resource languages. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nCaLMQA is an LFQA dataset with 2K questions from 23 languages, 11 high- to mid-resource and 12 low-resource.\\nQuestions are either culturally specific â€“ uniquely or more likely to be asked by people of a specific\\nculture â€“ or culturally agnostic (not culturally specific). These questions wereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shanearora/CaLMQA."},
	{"name":"SN-echoes","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SoccerNet/SN-echoes","creator_name":"SoccerNet","creator_url":"https://huggingface.co/SoccerNet","description":"[Paper] | [GitHub]\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for SoccerNet-Echoes\\n\\t\\n\\nThis dataset card aims to provide comprehensive details for the SoccerNet-Echoes dataset, an audio commentary dataset for soccer games.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nSoccerNet-Echoes is an audio commentary dataset for soccer games, curated by SimulaMet under the AI-Storyteller project. It is funded by the Research Council of Norway (project number 346671) and shared by the SoccerNet team. The datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SoccerNet/SN-echoes."},
	{"name":"MELA","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Geralt-Targaryen/MELA","creator_name":"Ziyin Zhang","creator_url":"https://huggingface.co/Geralt-Targaryen","description":"See the GitHub repo for details.\\n"},
	{"name":"bitext_sib200_miners","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gentaiscool/bitext_sib200_miners","creator_name":"Genta Indra Winata","creator_url":"https://huggingface.co/gentaiscool","description":"gentaiscool/bitext_sib200_miners dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"fleurs_clean","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Helloworld668/fleurs_clean","creator_name":"zhide Tang","creator_url":"https://huggingface.co/Helloworld668","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFLEURS\\n\\t\\n\\nFleurs is the speech version of the FLoRes machine translation benchmark. \\nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \\nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\\nused and â€unit error rateâ€ (characters, signs) of all languages is averaged. Languages and results are also grouped into sevenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Helloworld668/fleurs_clean."},
	{"name":"smartdata-corpus","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DFKI-SLT/smartdata-corpus","creator_name":"Speech and Language Technology, DFKI","creator_url":"https://huggingface.co/DFKI-SLT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SmartData Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSmartData Corpus is a German-language dataset which is human-annotated with entity types and a set of 15 traffic- and \\nindustry-related n-ary relations and events, such as accidents, traffic jams, acquisitions, and strikes. \\nThe corpus consists of newswire texts, Twitter messages, and traffic reports from radio stations, police and \\nrailway companies.\\nThis version of the dataset loader provides configurations for:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/DFKI-SLT/smartdata-corpus."},
	{"name":"xP3x-Kongo","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Svngoku/xP3x-Kongo","creator_name":"NIONGOLO Chrys FÃ©-Marty","creator_url":"https://huggingface.co/Svngoku","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for xP3x Kikongo Focus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nxP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts & datasets across 277 languages & 16 NLP tasks. It contains all of xP3 + much more! It is used for training future contenders of mT0 & BLOOMZ at project Aya @C4AI ðŸ§¡\\n\\n\\nCreation: The dataset can be recreated using instructions available here together with the file in this repository named xp3x_create.py. We provide this version to saveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Svngoku/xP3x-Kongo."},
	{"name":"German_RisingWorld_Alpaca-Dataset","keyword":"german","license":"GNU General Public License v2.0","license_url":"https://choosealicense.com/licenses/gpl-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Andzej-75/German_RisingWorld_Alpaca-Dataset","creator_name":"Andzej Ktowierzy","creator_url":"https://huggingface.co/Andzej-75","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman \\\"Rising World\\\"-Game Alpaca-Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Description\\n\\t\\n\\nThis HF data repository contains the German Alpaca dataset for the open-world sandbox game \\\"Rising World\\\".\\nDieses HF-Datenrepository enthÃ¤lt den deutschen Alpaca-Datensatz fÃ¼r das Open-World-Sandbox-Spiel \\\"Rising World\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\n\\nThis data is intended for fine-tuning\\nThis data is useful for \\\"Rising World\\\" plug-in developers\\nEach instance has an instruction, an output, and an optional input. Anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Andzej-75/German_RisingWorld_Alpaca-Dataset."},
	{"name":"German_RisingWorld_Alpaca-Dataset","keyword":"german","license":"GNU General Public License v2.0","license_url":"https://choosealicense.com/licenses/gpl-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Andzej-75/German_RisingWorld_Alpaca-Dataset","creator_name":"Andzej Ktowierzy","creator_url":"https://huggingface.co/Andzej-75","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman \\\"Rising World\\\"-Game Alpaca-Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Description\\n\\t\\n\\nThis HF data repository contains the German Alpaca dataset for the open-world sandbox game \\\"Rising World\\\".\\nDieses HF-Datenrepository enthÃ¤lt den deutschen Alpaca-Datensatz fÃ¼r das Open-World-Sandbox-Spiel \\\"Rising World\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\n\\nThis data is intended for fine-tuning\\nThis data is useful for \\\"Rising World\\\" plug-in developers\\nEach instance has an instruction, an output, and an optional input. Anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Andzej-75/German_RisingWorld_Alpaca-Dataset."},
	{"name":"german-disinformation-narratives-synthetic","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sami92/german-disinformation-narratives-synthetic","creator_name":"Sami Nenno","creator_url":"https://huggingface.co/Sami92","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"german-disinformation-narratives-synthetic\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is built using GPT-4o and contains 41 narratives that are frequently encountered by German fact-checkers. Each narrative has been expanded using GPT-4o to generate multiple supporting, unrelated, or contradicting text units. These expansions are presented from various perspectives, including that of a politician, an angry citizen, a conspiracy theorist, and others. \\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sami92/german-disinformation-narratives-synthetic."},
	{"name":"Multilingual-Benchmark","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TaiMingLu/Multilingual-Benchmark","creator_name":"TaiMing","creator_url":"https://huggingface.co/TaiMingLu","description":"These are the GSM8K and ARC dataset translated by Google Translate. \\nBibTex\\n@misc{lu2024languagecountslearnunlearn,\\n      title={Every Language Counts: Learn and Unlearn in Multilingual LLMs}, \\n      author={Taiming Lu and Philipp Koehn},\\n      year={2024},\\n      eprint={2406.13748},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2406.13748}, \\n}\\n\\n"},
	{"name":"modern","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CATMuS/modern","creator_name":"CATMuS: Consistent Approach to Transcribing ManuScripts","creator_url":"https://huggingface.co/CATMuS","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for CATMuS Modern and Contemporary (McCATMuS)\\n\\t\\n\\nJoin our Discord to ask questions about the dataset: \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nHandwritten Text Recognition (HTR) has emerged as a crucial tool for converting manuscripts images into machine-readable formats, enabling researchers and scholars to analyze vast collections efficiently. Despite significant technological progress, establishing consistent ground truth across projects for HTR tasks, particularly forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CATMuS/modern."},
	{"name":"course_competency_alignment_de","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/isy-thl/course_competency_alignment_de","creator_name":"Institut fÃ¼r Interaktive Systeme der Technischen Hochschule LÃ¼beck","creator_url":"https://huggingface.co/isy-thl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t!This is a draft!\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman Course Competency Alignment\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe \\\"German Course Competency Alignment\\\" dataset consists of triplets incorporating German course descriptions, positive competency labels, and negative competency labels. Its primary purpose is to train language models for information retrieval tasks, specifically to annotate courses with standardized competencies based on learning outcomes. This dataset leverages various competencyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/isy-thl/course_competency_alignment_de."},
	{"name":"German_RisingWorld_prompt-text-rejected_Jsonl","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Andzej-75/German_RisingWorld_prompt-text-rejected_Jsonl","creator_name":"Andzej Ktowierzy","creator_url":"https://huggingface.co/Andzej-75","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman \\\"Rising World\\\"-Game Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Description\\n\\t\\n\\nThis HF data repository contains the German dataset for the open-world sandbox game \\\"Rising World\\\".\\nDieses HF-Datenrepository enthÃ¤lt den deutschen Datensatz fÃ¼r das Open-World-Sandbox-Spiel \\\"Rising World\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\n\\nThis data is intended for fine-tuning\\nThis data is useful for \\\"Rising World\\\" plug-in developers\\n\\n"},
	{"name":"German_RisingWorld_prompt-text-rejected_Jsonl","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Andzej-75/German_RisingWorld_prompt-text-rejected_Jsonl","creator_name":"Andzej Ktowierzy","creator_url":"https://huggingface.co/Andzej-75","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman \\\"Rising World\\\"-Game Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Description\\n\\t\\n\\nThis HF data repository contains the German dataset for the open-world sandbox game \\\"Rising World\\\".\\nDieses HF-Datenrepository enthÃ¤lt den deutschen Datensatz fÃ¼r das Open-World-Sandbox-Spiel \\\"Rising World\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\n\\nThis data is intended for fine-tuning\\nThis data is useful for \\\"Rising World\\\" plug-in developers\\n\\n"},
	{"name":"weather_and_campsite_germany","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wilhelmine/weather_and_campsite_germany","creator_name":"Ronja Schwarz","creator_url":"https://huggingface.co/wilhelmine","description":"wilhelmine/weather_and_campsite_germany dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"genrescoh","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Johndfm/genrescoh","creator_name":"John MendonÃ§a","creator_url":"https://huggingface.co/Johndfm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for GenResCoh\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGenResCoh is a collection of positive and negative responses focused on coherence. It is generated using GPT-3.5-Turbo and GPT-4, and contains over 130k responses in different languages (English, French, German, Italian, and Chinese), together with their corresponding explanations (in English).\\nGenResCoh was used to train the ECoh family of models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n\\nEnglish\\nGerman\\nItalian\\nFrench\\nChineseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Johndfm/genrescoh."},
	{"name":"scala","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexandrainst/scala","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ScaLA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset consists of documents and whether they are grammatically correct or not. It has been automatically generated using this script, which corrupts documents from a universal dependencies treebank.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nEvaluation of linguistic acceptability (binary classification on correct/incorrect) is the intended task for this dataset. Leaderboards are live here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alexandrainst/scala."},
	{"name":"12-weltanschauungen","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Lafisrap/12-weltanschauungen","creator_name":"Michael Schmidt","creator_url":"https://huggingface.co/Lafisrap","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t12 Denkarten, 7 Erkenntnisstimmungen und 45 kulturgewordene Gedankenfehler\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset consists of writings on twelve thinking styles, seven moods of cognition, and forty-five culturally manifested cognitive errors. It includes a collection of texts that analyze and critique various aspects of human thought processes within a cultural context.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nThe dataset can be used for tasks such as text generation, text classificationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Lafisrap/12-weltanschauungen."},
	{"name":"librivox-tracks","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xacer/librivox-tracks","creator_name":"xacer","creator_url":"https://huggingface.co/xacer","description":"A dataset of all audio files uploaded to LibriVox before 26th September 2023.\\nForked from https://huggingface.co/datasets/pykeio/librivox-tracks\\nChanges:\\n\\nUsed archive.org metadata API to annotate rows with \\\"duration\\\" column\\n\\n"},
	{"name":"belgian-journal","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guust-franssens/belgian-journal","creator_name":"Guust Franssens","creator_url":"https://huggingface.co/guust-franssens","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nDataset contains the metadata + the text of company bylaws publications of Belgian companies on the Belgian Journal (Moniteur Belge/Belgisch Staatstblad).\\nThis data was collected by webscraping the Belgian Journal, for more info see: https://github.com/Guust-Franssens/belgian-journal.\\n\\nLanguage(s) (NLP): French, Dutch and a small subset German (official languages of Belgium.)\\nLicense: apache-2.0\\n\\n"},
	{"name":"EC-Guide","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AiMijie/EC-Guide","creator_name":"AiMijie","creator_url":"https://huggingface.co/AiMijie","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis repo is only used for dataset viewer. Please download from here.\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAmazon KDDCup 2024 Team ZJU-AI4Hâ€™s Solution and Dataset (Track 2 Top 2; Track 5 Top 5)\\n\\t\\n\\nThe Amazon KDD Cupâ€™24 competition presents a unique challenge by focusing on the application of LLMs in E-commerce across multiple tasks. Our solution for addressing Tracks 2 and 5 involves a comprehensive pipeline encompassing dataset construction, instruction tuning, post-training quantization, and inferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AiMijie/EC-Guide."},
	{"name":"ValueConsistency","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jlcmoore/ValueConsistency","creator_name":"Jared","creator_url":"https://huggingface.co/jlcmoore","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ValueConsistency\\n\\t\\n\\n\\n\\nThis is the ValueConsistency data set as introduced in the paper\\n\\\"Are Large Language Models Consistent over Value-laden Questions?\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\nValueConsistency is a dataset of both controversial and uncontroversial questions \\nin English, Chinese, German, and Japanese for topics from the U.S., China, Germany, and Japan. \\nIt was generated via prompting by GPT-4 and validated manually.\\nYouâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jlcmoore/ValueConsistency."},
	{"name":"another-german-alpaca-dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Fischerboot/another-german-alpaca-dataset","creator_name":"Moritz Nickel","creator_url":"https://huggingface.co/Fischerboot","description":"Fischerboot/another-german-alpaca-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"fact-check-bureau","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NaughtyConstrictor/fact-check-bureau","creator_name":"Younes","creator_url":"https://huggingface.co/NaughtyConstrictor","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFact-Check Retrieval Dataset\\n\\t\\n\\nThis dataset is designed to support the development and evaluation of fact-check retrieval pipelines. It is structured to work with FactCheckBureau, a tool for designing and evaluating fact-check retrieval pipelines. The dataset comprises a list of claims, fact-check articles, and precomputed embeddings for English and French fact-checks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of the following files and directories:\\n\\narticles.csv:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NaughtyConstrictor/fact-check-bureau."},
	{"name":"German-RAG-SFT-Alpaca-HESSIAN-AI","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-SFT-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-SFT (Supervised Fine-Tuning) Alpaca-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SFT Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-SFT-Alpaca-HESSIAN-AI."},
	{"name":"German-RAG-SFT-Alpaca-HESSIAN-AI","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-SFT-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-SFT (Supervised Fine-Tuning) Alpaca-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SFT Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-SFT-Alpaca-HESSIAN-AI."},
	{"name":"German-RAG-SFT-ShareGPT-HESSIAN-AI","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-SFT-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-SFT (Supervised Fine-Tuning) Share-GPT Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SFT Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-SFT-ShareGPT-HESSIAN-AI."},
	{"name":"German-RAG-SFT-ShareGPT-HESSIAN-AI","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-SFT-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-SFT (Supervised Fine-Tuning) Share-GPT Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SFT Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-SFT-ShareGPT-HESSIAN-AI."},
	{"name":"medical-translation-test-set","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ai-amplified/medical-translation-test-set","creator_name":"admin","creator_url":"https://huggingface.co/ai-amplified","description":"ai-amplified/medical-translation-test-set dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"zenless_zone_zero_interknots_v1.0","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/zenless_zone_zero_interknots_v1.0","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tZZZ Interknots\\n\\t\\n\\nThis datasets contains extracted Interknot posts and comments (ç»³ç½‘çš„åšå®¢ä¸Žè¯„è®º) in multi-language.\\nUp to game version: 1.0\\n\\nInterknot posts and comments examples\\n\\n{\\n  \\\"id\\\": \\\"1021\\\",\\n  \\\"poster\\\": \\\"Sorrowful Intern\\\",\\n  \\\"title\\\": \\\"[Commission] Missing Bangboo merchants\\\",\\n  \\\"text\\\": \\\"Hello, pros... Some of the senior Bangboo of our merchant association have gone missing! We urgently need an expert to help find them!!\\\\nLet me explain, I recently joined a very prestigiousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/zenless_zone_zero_interknots_v1.0."},
	{"name":"GraSCCo","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/johannsr/GraSCCo","creator_name":"Johann Simon Reichel","creator_url":"https://huggingface.co/johannsr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for GraSCCo\\n\\t\\n\\nThe dataset contains clinical discharge summaries along with their corresponding summarizations. The source texts are from the GraSCCo dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe summarization texts were created using ChatGPT-3.5, followed by manual corrections and refinements. As mentioned in the GraSCCo paper, the source texts were synthetically generated based on real medical data. Consequently, the texts are noâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/johannsr/GraSCCo."},
	{"name":"ARK-Metadata","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SBB/ARK-Metadata","creator_name":"Staatsbibliothek zu Berlin - PreuÃŸischer Kulturbesitz","creator_url":"https://huggingface.co/SBB","description":"\\n\\t\\n\\t\\t\\n\\t\\tMetadata of the \\\"Alter Realkatalog\\\" (ARK) of Berlin State Library (SBB)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tMotivation\\n\\t\\n\\nThis dataset was created with the intent to provide a single larger set of metadata from Berlin State Library for research purposes and the development of AI applications.\\nThe dataset comprises of descriptive metadata of 2.619.397 titles, which together form the \\\"Alte Realkatalog\\\" of Berlin State Libray, which may be translated to \\\"Old Subject Catalogue\\\". The data are stored in columnarâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SBB/ARK-Metadata."},
	{"name":"moniteur-belge","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guust-franssens/moniteur-belge","creator_name":"Guust Franssens","creator_url":"https://huggingface.co/guust-franssens","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nThe most up to date data can be found here: https://huggingface.co/datasets/guust-franssens/belgian-journal\\nDue to the different languages in Belgium, I decided to create three datasets belgian-journal/moniteur-belge/belgisch-staatsblad in order to make it more visible.\\nDataset contains the metadata + the text of bylaw publications of Belgian companies on the Belgian Journal (Moniteur Belge/Belgisch Staatstblad).\\nThis data was collected by webscraping theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/guust-franssens/moniteur-belge."},
	{"name":"belgisch-staatsblad","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guust-franssens/belgisch-staatsblad","creator_name":"Guust Franssens","creator_url":"https://huggingface.co/guust-franssens","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nThe most up to date data can be found here: https://huggingface.co/datasets/guust-franssens/belgian-journal\\nDue to the different languages in Belgium, I decided to create three datasets belgian-journal/moniteur-belge/belgisch-staatsblad in order to make it more visible.\\nDataset contains the metadata + the text of bylaw publications of Belgian companies on the Belgian Journal (Moniteur Belge/Belgisch Staatstblad).\\nThis data was collected by webscraping theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/guust-franssens/belgisch-staatsblad."},
	{"name":"mmarco-hard-negatives-reranker-score","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hotchpotch/mmarco-hard-negatives-reranker-score","creator_name":"Yuichi Tateno","creator_url":"https://huggingface.co/hotchpotch","description":"\\nhotchpotch/mmarco-hard-negatives-reranker-score\\n\\nThis repository contains data from mMARCO scored using the reranker BAAI/bge-reranker-v2-m3.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages Covered\\n\\t\\n\\ntarget_languages = [\\n    \\\"english\\\",\\n    \\\"chinese\\\", \\n    \\\"french\\\",\\n    \\\"german\\\",\\n    \\\"indonesian\\\",\\n    \\\"italian\\\",\\n    \\\"portuguese\\\",\\n    \\\"russian\\\",\\n    \\\"spanish\\\",\\n    \\\"arabic\\\",\\n    \\\"dutch\\\",\\n    \\\"hindi\\\",\\n    \\\"japanese\\\",\\n    \\\"vietnamese\\\"\\n]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHard Negative Data\\n\\t\\n\\nThe hard negative data is derived fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hotchpotch/mmarco-hard-negatives-reranker-score."},
	{"name":"wiki-talks","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lflage/wiki-talks","creator_name":"Lucas Fonseca Lage","creator_url":"https://huggingface.co/lflage","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWiki-Talks\\n\\t\\n\\nThe Wiki-Talks dataset is a collection of conversational threads extracted from the talk pages on Wikipedia.\\nThis dataset captures collaborative dialogue, discussion patterns, and consensus-building among Wikipedia contributors.\\nIt is useful for NLP research focused on dialogue, sentiment analysis, and community dynamics.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDetails\\n\\t\\n\\nCurrently due to PyArrow incompatibility to the long recursive structures in the dataset there is an intrinsicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lflage/wiki-talks."},
	{"name":"muri-it-language-split","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/akoksal/muri-it-language-split","creator_name":"Abdullatif Koksal","creator_url":"https://huggingface.co/akoksal","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMURI-IT: Multilingual Instruction Tuning Dataset for 200 Languages via Multilingual Reverse Instructions\\n\\t\\n\\nMURI-IT is a large-scale multilingual instruction tuning dataset containing 2.2 million instruction-output pairs across 200 languages. It is designed to address the challenges of instruction tuning in low-resource languages with Multilingual Reverse Instructions (MURI), which ensures that the output is human-written, high-quality, and authentic to the cultural and linguisticâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/akoksal/muri-it-language-split."},
	{"name":"IdiomsInCtx-MT","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davidstap/IdiomsInCtx-MT","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIdiomsInCtx-MT Dataset\\n\\t\\n\\nThis repository contains the IdiomsInCtx-MT dataset used in our ACL 2024 paper: The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities. See this GitHub repo for the origin of the data.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of idiomatic expressions in context and their human-written translations. There are 1000 translations per direction. The dataset covers 2 language pairs (English-German and English-Russian) withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/IdiomsInCtx-MT."},
	{"name":"German-RAG-DPO-Alpaca-HESSIAN-AI","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-DPO-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-DPO Alpaca Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe DPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge graph where Question-Answerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-DPO-Alpaca-HESSIAN-AI."},
	{"name":"German-RAG-DPO-Alpaca-HESSIAN-AI","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-DPO-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-DPO Alpaca Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe DPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge graph where Question-Answerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-DPO-Alpaca-HESSIAN-AI."},
	{"name":"German-RAG-DPO-ShareGPT-HESSIAN-AI","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-DPO-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-DPO Share-GPT Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe DPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge graph whereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-DPO-ShareGPT-HESSIAN-AI."},
	{"name":"German-RAG-DPO-ShareGPT-HESSIAN-AI","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-DPO-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-DPO Share-GPT Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe DPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge graph whereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-DPO-ShareGPT-HESSIAN-AI."},
	{"name":"my_iris","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/beierr1/my_iris","creator_name":"Ralf Beier","creator_url":"https://huggingface.co/beierr1","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIris Species Dataset\\n\\t\\n\\nThe Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\\nThe dataset is taken from UCI Machine Learning Repository'sâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/beierr1/my_iris."},
	{"name":"mls-annotated","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PHBJT/mls-annotated","creator_name":"Paul Henri Biojout","creator_url":"https://huggingface.co/PHBJT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Annotations of non English MLS\\n\\t\\n\\nThis dataset consists in annotations of a the Non English subset of the Multilingual LibriSpeech (MLS) dataset. \\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \\n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours for otherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PHBJT/mls-annotated."},
	{"name":"German-RAG-ORPO-Alpaca-HESSIAN-AI","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) Alpaca-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe ORPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \\nThe subsets can be for this training step are derived from 2 different sources:\\n\\nSauerkrautLM Preference Datasets:\\nSauerkrautLM-Fermented-GER-DPO:  is a specialized dataset designed for trainingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-Alpaca-HESSIAN-AI."},
	{"name":"German-RAG-ORPO-Alpaca-HESSIAN-AI","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) Alpaca-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe ORPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \\nThe subsets can be for this training step are derived from 2 different sources:\\n\\nSauerkrautLM Preference Datasets:\\nSauerkrautLM-Fermented-GER-DPO:  is a specialized dataset designed for trainingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-Alpaca-HESSIAN-AI."},
	{"name":"ManyToDanishTranslations-tatoeba","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/trollek/ManyToDanishTranslations-tatoeba","creator_name":"Trolle Karlsson","creator_url":"https://huggingface.co/trollek","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDanske oversÃ¦ttelser\\n\\t\\n\\nTak til Helsinki-NLP for deres tatoeba dataset (CC-BY-2.0).\\nModeller der kan adskillige sprog kan sjÃ¦ldent dansk. At oversÃ¦tte eksisterende dataset virker som en fornuftig lÃ¸sning pÃ¥ det problem, men af fornuftige oversÃ¦ttelsesvÃ¦rktÃ¸jer er der kun fÃ¥. Mad props til Mabeck for arbejdet med SlimOrca. Much inspired. Great thank.\\nSom sagt kan polylinvistiske modeller som regel engelsk, kinesisk, fransk, tysk, osv. og mÃ¥ske mangler der bare nogetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/trollek/ManyToDanishTranslations-tatoeba."},
	{"name":"GermanPhoneConversation","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/masterchatbot/GermanPhoneConversation","creator_name":"Eugen Graf","creator_url":"https://huggingface.co/masterchatbot","description":"masterchatbot/GermanPhoneConversation dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"PangeaBench-xm100","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/neulab/PangeaBench-xm100","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tXM100\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is a copy from https://google.github.io/crossmodal-3600/\\n\\t\\n\\nIf you use this dataset, please cite the original authors:\\n@inproceedings{ThapliyalCrossmodal2022,\\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\\n  booktitle = {EMNLP},\\n  year = {2022}\\n}\\n\\n"},
	{"name":"german_ner_dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HUMADEX/german_ner_dataset","creator_name":"HUMADEX Research Group","creator_url":"https://huggingface.co/HUMADEX","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman NER dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAcknowledgement\\n\\t\\n\\nThis dataset had been created as part of joint research of HUMADEX research group (https://www.linkedin.com/company/101563689/) and has received funding by the European Union Horizon Europe Research and Innovation Program project SMILE (grant number 101080923) and Marie SkÅ‚odowska-Curie Actions (MSCA) Doctoral Networks, project BosomShield ((rant number 101073222). Responsibility for the information and views expressed hereinâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HUMADEX/german_ner_dataset."},
	{"name":"german_ner_dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HUMADEX/german_ner_dataset","creator_name":"HUMADEX Research Group","creator_url":"https://huggingface.co/HUMADEX","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman NER dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAcknowledgement\\n\\t\\n\\nThis dataset had been created as part of joint research of HUMADEX research group (https://www.linkedin.com/company/101563689/) and has received funding by the European Union Horizon Europe Research and Innovation Program project SMILE (grant number 101080923) and Marie SkÅ‚odowska-Curie Actions (MSCA) Doctoral Networks, project BosomShield ((rant number 101073222). Responsibility for the information and views expressed hereinâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HUMADEX/german_ner_dataset."},
	{"name":"comment-translation-01","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ifmain/comment-translation-01","creator_name":"Mike Afton","creator_url":"https://huggingface.co/ifmain","description":"This dataset is based on Kaggle.  \\nThis dataset includes translations of 69,000 Reddit comments into 17 languages (English to 16 languages):\\nBelarusian, Czech, German,\\nEnglish, Spanish, Finnish,\\nFrench, Italian, Japanese,\\nKazakh, Korean, Latvian,\\nPolish, Russian, Swedish,\\nUkrainian, and Chinese.\\nIt contains 50% regular comments and 50% highly negative ones.\\nEnjoy using it!\\n"},
	{"name":"ApolloMoEDataset","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEDataset","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDemocratizing Medical LLMs For Much More Languages\\n\\t\\n\\nCovering 12 Major Languages including English, Chinese, French, Hindi, Spanish, Arabic, Russian, Japanese, Korean, German, Italian, Portuguese and 38 Minor Languages So far.\\n\\n   ðŸ“ƒ Paper â€¢ ðŸŒ Demo â€¢ ðŸ¤— ApolloMoEDataset â€¢ ðŸ¤— ApolloMoEBench  â€¢ ðŸ¤— Models  â€¢ðŸŒ Apollo  â€¢ ðŸŒ ApolloMoE\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸŒˆ Update\\n\\t\\n\\n\\n[2024.10.15] ApolloMoE repo is publishedï¼ðŸŽ‰\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages Coverage\\n\\t\\n\\n12 Major Languages and 38 Minor Languagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEDataset."},
	{"name":"ApolloMoEBench","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEBench","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDemocratizing Medical LLMs For Much More Languages\\n\\t\\n\\nCovering 12 Major Languages including English, Chinese, French, Hindi, Spanish, Arabic, Russian, Japanese, Korean, German, Italian, Portuguese and 38 Minor Languages So far.\\n\\n   ðŸ“ƒ Paper â€¢ ðŸŒ Demo â€¢ ðŸ¤— ApolloMoEDataset â€¢ ðŸ¤— ApolloMoEBench  â€¢ ðŸ¤— Models  â€¢ðŸŒ Apollo  â€¢ ðŸŒ ApolloMoE\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸŒˆ Update\\n\\t\\n\\n\\n[2024.10.15] ApolloMoE repo is publishedï¼ðŸŽ‰\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages Coverage\\n\\t\\n\\n12 Major Languages and 38 Minor Languagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEBench."},
	{"name":"PangeaBench-xgqa","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/neulab/PangeaBench-xgqa","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\txGQA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is a clone of the few_shot-test split of the xGQA dataset\\n\\t\\n\\nPlease find the original repository here: https://github.com/adapter-hub/xGQA\\nIf you use this dataset, please cite the original authors:\\n@inproceedings{pfeiffer-etal-2021-xGQA,\\n    title={{xGQA: Cross-Lingual Visual Question Answering}},\\n    author={ Jonas Pfeiffer and Gregor Geigle and Aishwarya Kamath and Jan-Martin O. Steitz and Stefan Roth and Ivan Vuli{\\\\'{c}} and Iryna Gurevych}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neulab/PangeaBench-xgqa."},
	{"name":"amilia_sim_conv","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Blubbe/amilia_sim_conv","creator_name":"Linus","creator_url":"https://huggingface.co/Blubbe","description":"Blubbe/amilia_sim_conv dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"grundschutz-dataset","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/indigosphere/grundschutz-dataset","creator_name":"Gernot Weiser","creator_url":"https://huggingface.co/indigosphere","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIT-Grundschutz Dataset\\n\\t\\n\\nTraining data for IT-Grundschutz fine-tuning.\\n"},
	{"name":"NaVAB","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JadenGGGeee/NaVAB","creator_name":"JCY","creator_url":"https://huggingface.co/JadenGGGeee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for NaVAB\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nNaVAB is a comprehensive benchmark designed to evaluate the alignment of Large Language Models (LLMs) with the values of five major nations: China, the United States, the United Kingdom, France, and Germany. The dataset addresses the limitations of existing benchmarks, which often fail to capture the dynamic nature of values across countries and lack sufficient evaluation data.\\nThe datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JadenGGGeee/NaVAB."},
	{"name":"Schule-DPO","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nbeerbower/Schule-DPO","creator_name":"Nicholas Beerbower","creator_url":"https://huggingface.co/nbeerbower","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSchule-DPO\\n\\t\\n\\nA collection of various writing assignments I completed in high school and college.\\nMost were written in English, some in German. Everything has been translated to both languages and marked en for English or de for German under the lang column.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMethod\\n\\t\\n\\nEssays were cleaned up and formatted as plain-text (some have a bit of markdown) and used as 'chosen'. Claude 3.5 Sonnet was used for translations and generating synthetic prompts. The prompts were thenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nbeerbower/Schule-DPO."},
	{"name":"MegaWika","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/conceptofmind/MegaWika","creator_name":"Enrico Shippole","creator_url":"https://huggingface.co/conceptofmind","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MegaWika\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMegaWika is a multi- and crosslingual text dataset containing 30 million Wikipedia passages with their scraped and cleaned web citations. The passages span\\n50 Wikipedias in 50 languages, and the articles in which the passages were originally embedded are included for convenience. Where a Wikipedia passage is in a\\nnon-English language, an automated English translation is provided. Furthermore, nearly 130 million Englishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/conceptofmind/MegaWika."},
	{"name":"WiNNL","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/peemil/WiNNL","creator_name":"Emile Peetermans","creator_url":"https://huggingface.co/peemil","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWiNNL\\n\\t\\n\\nWikiNews Named entity recognition and Linking (WiNNL) is a multilingual news NER & NEL benchmark based on Wikinews articles.\\nThe dataset was created by automatically scraping and tagging news articles, and manually corrected by native speakers to ensure accuracy.\\nYou can find more information in the paper:\\nhttps://aclanthology.org/2024.dlnld-1.3.pdf\\nThe dataset includes the following NER classes in IOB format (labels):\\n\\nPER (Person): person names \\nLOC (Location):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/peemil/WiNNL."},
	{"name":"wiktionary-data","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QubitPi/wiktionary-data","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWiktionary Data on Hugging Face Datasets\\n\\t\\n\\n\\n\\n\\n\\n\\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\\nsupports the following languages:\\n\\nDeutsch - German\\nLatinum - Latin\\ná¼™Î»Î»Î·Î½Î¹ÎºÎ® - Ancient Greek\\ní•œêµ­ì–´ - Korean\\nðŽ ðŽ¼ðŽ¹ - Old Persian\\nð’€ð’…—ð’ºð’Œ‘(ð’Œ) - Akkadian\\nElamite\\nà¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤®à¥ - Sanskrit, or Classical Sanskrit\\n\\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\\nthe dataset it's getting bigger, I noticed a wave of more exciting potentialsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wiktionary-data."},
	{"name":"wiktionary-data","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QubitPi/wiktionary-data","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWiktionary Data on Hugging Face Datasets\\n\\t\\n\\n\\n\\n\\n\\n\\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\\nsupports the following languages:\\n\\nDeutsch - German\\nLatinum - Latin\\ná¼™Î»Î»Î·Î½Î¹ÎºÎ® - Ancient Greek\\ní•œêµ­ì–´ - Korean\\nðŽ ðŽ¼ðŽ¹ - Old Persian\\nð’€ð’…—ð’ºð’Œ‘(ð’Œ) - Akkadian\\nElamite\\nà¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤®à¥ - Sanskrit, or Classical Sanskrit\\n\\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\\nthe dataset it's getting bigger, I noticed a wave of more exciting potentialsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wiktionary-data."},
	{"name":"GermanDefinitionGeneration-Distillation","keyword":"german","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jfeil/GermanDefinitionGeneration-Distillation","creator_name":"Jan","creator_url":"https://huggingface.co/jfeil","description":"jfeil/GermanDefinitionGeneration-Distillation dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"SteamGRS","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GabrielML/SteamGRS","creator_name":"Gabriel Schurr","creator_url":"https://huggingface.co/GabrielML","description":"Steam German Review Sentiment (SteamGRS) for seminar work (LoRA experiments).\\n"},
	{"name":"germeval21_detox","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lschoen/germeval21_detox","creator_name":"Levente Schoenherr","creator_url":"https://huggingface.co/lschoen","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset for DeTox at GermEval 2021: Fine grained Comment Classification\\n\\t\\n\\nHas a train test split and 3 labels for each comment: Sub1_Toxic, Sub2_Engaging, and Sub3_Factclaiming.\\nDatasetDict({\\n    train: Dataset({\\n        features: ['comment_id', 'comment_text', 'Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming'],\\n        num_rows: 3244\\n    })\\n    test: Dataset({\\n        features: ['comment_id', 'comment_text', 'Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming'],\\n        num_rows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lschoen/germeval21_detox."},
	{"name":"germeval14_ner","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lschoen/germeval14_ner","creator_name":"Levente Schoenherr","creator_url":"https://huggingface.co/lschoen","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGermEval 14 NER dataset\\n\\t\\n\\nThis dataset includes the actual NER tags (B-PER, B-LOC, etc.) besides the labels (0, 1, 2, ...) and requires no code execution when loading. Structured as follow\\nDatasetDict({\\n    train: Dataset({\\n        features: ['id', 'source', 'source_date', 'tokens', 'ner_label', 'ner_tag', 'nested_ner_label', 'nested_ner_tag'],\\n        num_rows: 24002\\n    })\\n    validation: Dataset({\\n        features: ['id', 'source', 'source_date', 'tokens', 'ner_label'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lschoen/germeval14_ner."},
	{"name":"10kgnad","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lschoen/10kgnad","creator_name":"Levente Schoenherr","creator_url":"https://huggingface.co/lschoen","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t10kGNAD - Ten Thousand German News Articles Dataset\\n\\t\\n\\nGerman news articels, categorized. Category is the actual category name, one of: 'Etat', 'Inland', 'International', 'Kultur', 'Panorama', 'Sport', 'Web', 'Wirtschaft', 'Wissenschaft'. The label is the index as an integer.\\nDatasetDict({\\n    train: Dataset({\\n        features: ['category', 'text', 'label'],\\n        num_rows: 9245\\n    })\\n    test: Dataset({\\n        features: ['category', 'text', 'label'],\\n        num_rows: 1028â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lschoen/10kgnad."},
	{"name":"Tridis","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/magistermilitum/Tridis","creator_name":"Sergio Torres","creator_url":"https://huggingface.co/magistermilitum","description":"This is the first dataset version of the corpora used in TRIDIS (Tria Digita Scribunt) which is a series of Handwriting Text Recognition models trained on semi-diplomatic transcriptions \\nfrom medieval and Early Modern Manuscripts.\\nThe dataset involves 4k pages of manuscripts and is suitable for work on documentary manuscripts, that is, manuscripts arising  from legal, administrative, and memorial practices such as registers, feudal books, charters, proceedings, comptability more commonly fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/magistermilitum/Tridis."},
	{"name":"Topic-specific-genre-classification_german_historical-newspapers","keyword":"german","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/oberbics/Topic-specific-genre-classification_german_historical-newspapers","creator_name":"Oberbichler","creator_url":"https://huggingface.co/oberbics","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Topic-specific Genre Classification of German Historical Newspapers\\n\\t\\n\\nThis dataset was developed to train and evaluate topic-specific genre classification of German-language historical newspaper clippings. \\n\\nCurated by: [Sarah Oberbichler]\\nLanguage(s) (NLP): [German]\\nLicense: [afl-3.0]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tUses\\n\\t\\n\\n\\nEvaluation of machine learning models for topic-specific classification of ocr-processed historical texts with varying quality levels.\\nFine-tuning models onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/oberbics/Topic-specific-genre-classification_german_historical-newspapers."},
	{"name":"Topic-specific-disambiguation_evaluation-dataset_German_historical-newspapers","keyword":"german","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/oberbics/Topic-specific-disambiguation_evaluation-dataset_German_historical-newspapers","creator_name":"Oberbichler","creator_url":"https://huggingface.co/oberbics","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for German Topic-specific Disambiguation Dataset Historical Newspapers\\n\\t\\n\\nThis dataset contains German newspaper articles (1850-1950) labeled as either about relevant for \\\"return migration\\\" or not. It helps researchers develop word sense disambiguation methods - technology that can tell when the German word \\\"Heimkehr\\\" or \\\"RÃ¼ckkehr\\\" (return/homecoming) is being used to discuss people returning to their home countries versus when it's used in different contexts. This mattersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/oberbics/Topic-specific-disambiguation_evaluation-dataset_German_historical-newspapers."},
	{"name":"German-RAG-HARD-REASONING-DE-THINKING","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-HARD-REASONING-DE-THINKING","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"avemio/German-RAG-HARD-REASONING-DE-THINKING dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"GammaCorpus-Polylingo-50k","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus Polylingo 50k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus Polylingo 50k dataset consists of 50,000 structured, unfiltered, single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\nLanguage: The language used in the interaction.\\n\\nThis dataset is designed to help in the training and evaluation of conversational AI models for linguistic purposes. This dataset can be especially helpful if youâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k."},
	{"name":"synthetic_call_center_summaries","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marccgrau/synthetic_call_center_summaries","creator_name":"Marc Christopher Grau","creator_url":"https://huggingface.co/marccgrau","description":"\\n\\t\\n\\t\\t\\n\\t\\tSynthetic Call Center Summaries Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains synthetic summaries of call center conversations generated by different prompt configurations.\\nEach record (in JSON Lines format) includes:\\n\\nThe original dialogue metadata.\\nA generated summary tailored to provide quick insights for call center service agents.\\nExtensive evaluation metrics and attributes such as conciseness, formatting, contextual relevance, tone, and actionability.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntendedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/marccgrau/synthetic_call_center_summaries."},
	{"name":"klexikon","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bbunzeck/klexikon","creator_name":"Bastian Bunzeck","creator_url":"https://huggingface.co/bbunzeck","description":"Klexikon\\nThis dataset represents a full scrape (approx. 1.3M lexical tokens) of the website Klexikon (Kinder-Lexikon), a German children's Wikipedia. Texts were collected on Feb 20, 2025.\\nAll texts are licensed under cc-by-sa-4.0.\\nThe individual text files contain only the body text of the individual articles (and sometimes metatextual information, like author names). We did some light cleaning, but otherwise the texts are provided as they were scraped. \\nAnother older (2022) Klexikon dump canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bbunzeck/klexikon."},
	{"name":"mini-klexikon","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bbunzeck/mini-klexikon","creator_name":"Bastian Bunzeck","creator_url":"https://huggingface.co/bbunzeck","description":"MiniKlexikon\\nThis dataset represents a full scrape (approx. 270K lexical tokens) of the website MiniKlexikon (Mini Kinder-Lexikon), a German children's Wikipedia that contains texts in simplified German (Einfache Sprache). Texts were collected on Feb 20, 2025.\\nAll texts are licensed under cc-by-sa-4.0.\\nThe individual text files contain only the body text of the individual articles (and sometimes metatextual information, like author names). We did some light cleaning, but otherwise the textsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bbunzeck/mini-klexikon."},
	{"name":"AIME2025-Multilingual","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fedric95/AIME2025-Multilingual","creator_name":"Federico Ricciuti","creator_url":"https://huggingface.co/fedric95","description":"\\n\\t\\n\\t\\t\\n\\t\\tDescription\\n\\t\\n\\nThis repository contains a multi language version of the AIME2025 dataset. \\nAs the english reference version, we haved used the one created by the authors of MathArena.\\nFor completness, we have included the english version also in this repository, please, refer to the one contained in the MathArena github repository for the original one (https://github.com/eth-sri/matharena/tree/main/data/aime). Many thanks to Jasper Dekoninck for the help in understanding the structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fedric95/AIME2025-Multilingual."},
	{"name":"dtak-transnormer-full-v1","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/textplus-bbaw/dtak-transnormer-full-v1","creator_name":"Text+ at Berlin-Brandenburgische Akademie der Wissenschaften","creator_url":"https://huggingface.co/textplus-bbaw","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for DTAK-transnormer-full (v1.0)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nDTAK-transnormer-full is a modified subset of the DTA-Kernkorpus (Deutsches Textarchiv, German Text Archive Core Corpus).\\nIt is a parallel corpus of German texts from the period between 1600 to 1899, that aligns sentences in historical spelling with their normalizations.\\nA normalization is a modified version of the original text that is adapted to modern spelling conventions.\\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/textplus-bbaw/dtak-transnormer-full-v1."},
	{"name":"InVar-100","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vivek9chavan/InVar-100","creator_name":"Vivek Chavan","creator_url":"https://huggingface.co/vivek9chavan","description":"\\n\\t\\n\\t\\t\\n\\t\\tTowards Realistic Evaluation of Industrial Continual Learning Scenarios with an Emphasis on Energy Consumption and Computational Footprint\\n\\t\\n\\n[Paper] [Poster] [Summary Video]\\nAbstract: Incremental Learning (IL) aims to develop Machine Learning (ML) models that can learn from continuous streams of data and mitigate catastrophic forgetting. We analyze the current state-of-the-art Class-IL implementations and demonstrate why the current body of research tends to be one-dimensional, withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vivek9chavan/InVar-100."},
	{"name":"multilingualcrowspairs","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FrancophonIA/multilingualcrowspairs","creator_name":"FrancophonIA","creator_url":"https://huggingface.co/FrancophonIA","description":"\\nDataset origin: https://gitlab.inria.fr/corpus4ethics/multilingualcrowspairs/\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMultiLingualCrowsPairs\\n\\t\\n\\nMultilingual CrowS-Pairs, a challenge dataset for measuring stereotypical biases present in the masked language models (MLMs) in 7 different languages. \\nThis challenge dataset was built on the Crows-Pairs corpus (Nangia et al. 2020) using the methodology described in (NÃ©vÃ©ol et al. 2023). \\nThe 7 new languages are the following:\\n\\nArabic from Maghreb and the Arab world in generalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FrancophonIA/multilingualcrowspairs."},
	{"name":"SpeechQE-CoVoST2","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/h-j-han/SpeechQE-CoVoST2","creator_name":"HyoJung Han","creator_url":"https://huggingface.co/h-j-han","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSpeechQE: Estimating the Quality of Direct Speech Translation\\n\\t\\n\\nThis is a benchmark and training corpus for the task of quality estimation for speech translation (SpeechQE).\\nWe subsample about 80k segments from the training set and 500 from the dev and test of CoVoST2, then run seven different direct ST models to generate the ST hypotheses.\\nSo,test split consists of 3500 instances(500*7). We also provide splits for each translation model.\\n*(We provide test split first, and theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/h-j-han/SpeechQE-CoVoST2."},
	{"name":"GermanDefinitionGeneration-Wiktionary","keyword":"german","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jfeil/GermanDefinitionGeneration-Wiktionary","creator_name":"Jan","creator_url":"https://huggingface.co/jfeil","description":"jfeil/GermanDefinitionGeneration-Wiktionary dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"parallel_corpus_game_2024","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bot-yaya/parallel_corpus_game_2024","creator_name":"yaya","creator_url":"https://huggingface.co/bot-yaya","description":"https://github.com/mnbvc-parallel-corpus-team/parallel_corpus_mnbvc\\nMNBVCå¹³è¡Œè¯­æ–™å°ç»„ï¼šæ¸¸æˆè¯­æ–™\\nä¸å®šæœŸæ›´æ–°ï¼Œç›®å‰å·²æ”¶å½•çš„æ¸¸æˆè¯­æ–™æ–‡ä»¶ï¼Œå…±29ä»½ï¼š\\n\\nåšå¾·ä¹‹é—¨3\\nèµ›åšæœ‹å…‹2077\\né»‘æš—ä¹‹é­‚3\\nåº•ç‰¹å¾‹ï¼šåŒ–èº«ä¸ºäºº\\né¥¥è’\\nè‰¾å°”ç™»æ³•çŽ¯\\nåŽŸç¥ž\\né»‘å¸æ–¯\\néœæ ¼æ²ƒå…¹ä¹‹é—\\nIb\\nå¦‚é¾™8\\nå¦‚é¾™7å¤–ä¼ \\nè’é‡Žå¤§é•–å®¢2\\nåªç‹¼ï¼šå½±é€äºŒåº¦\\næ–‡æ˜Ž6\\næ€æˆ®å°–å¡”\\nå´©åæ˜Ÿç©¹é“é“\\nç¾¤æ˜Ÿ\\næ³°æ‹‰ç‘žäºš\\nå·«å¸ˆ3\\né­”å¥³ä¹‹æ³‰3\\né­”å¥³ä¹‹æ³‰R\\né¸£æ½®\\nå¦‚é¾™3\\nå¦‚é¾™4\\nå¦‚é¾™5\\nå¦‚é¾™6\\nå¦‚é¾™æž2\\nå¦‚é¾™7\\n\\n"},
	{"name":"german-mongotom","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Fischerboot/german-mongotom","creator_name":"Moritz Nickel","creator_url":"https://huggingface.co/Fischerboot","description":"Fischerboot/german-mongotom dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"cml-tts-filtered-annotated","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PHBJT/cml-tts-filtered-annotated","creator_name":"Paul Henri Biojout","creator_url":"https://huggingface.co/PHBJT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Filtred and annotated CML TTS\\n\\t\\n\\nThis dataset is an annotated and filtred version of a CML-TTS [1]. \\nCML-TTS [1] CML-TTS is a recursive acronym for CML-Multi-Lingual-TTS, a Text-to-Speech (TTS) dataset developed at the Center of Excellence in Artificial Intelligence (CEIA) of the Federal University of Goias (UFG). CML-TTS is a dataset comprising audiobooks sourced from the public domain books of Project Gutenberg, read by volunteers from the LibriVox project. Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PHBJT/cml-tts-filtered-annotated."},
	{"name":"COLING-2025-GENAI-3","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/1-800-SHARED-TASKS/COLING-2025-GENAI-3","creator_name":"1-800-Shared-Tasks","creator_url":"https://huggingface.co/1-800-SHARED-TASKS","description":"\\nðŸš¨ RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors ðŸš¨\\nðŸŒ Website, ðŸ–¥ï¸ Github, ðŸ“ Paper\\n\\n\\nRAID is the largest & most comprehensive dataset for evaluating AI-generated text detectors. \\nIt contains over 10 million documents spanning 11 LLMs, 11 genres, 4 decoding strategies, and 12 adversarial attacks. \\nIt is designed to be the go-to location for trustworthy third-party evaluation of both open-source and closed-source generated text detectors.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLoadâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/1-800-SHARED-TASKS/COLING-2025-GENAI-3."},
	{"name":"X-ALMA-Preference","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/haoranxu/X-ALMA-Preference","creator_name":"Haoran Xu","creator_url":"https://huggingface.co/haoranxu","description":"This is the translation preference dataset used by X-ALMA.\\nsource: the source sentence.\\nchosen: the preferred translation.\\nreject: the dis-preferred translation.\\ndirections: the translation direction.\\n@misc{xu2024xalmaplugplay,\\n      title={X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale}, \\n      author={Haoran Xu and Kenton Murray and Philipp Koehn and Hieu Hoang and Akiko Eriguchi and Huda Khayrallah},\\n      year={2024},\\n      eprint={2410.03115}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/haoranxu/X-ALMA-Preference."},
	{"name":"German-RAG-ORPO-Long-Context-Alpaca-HESSIAN-AI","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-Long-Context-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) Long-Context Alpaca-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe ORPO Long Context Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \\nThe subsets are derived from Synthetic generation inspired by Tencent's (â€œScaling Synthetic Data Creation with 1,000,000,000 Personasâ€).\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-Long-Context-Alpaca-HESSIAN-AI."},
	{"name":"German-RAG-ORPO-Long-Context-Alpaca-HESSIAN-AI","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-Long-Context-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) Long-Context Alpaca-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe ORPO Long Context Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \\nThe subsets are derived from Synthetic generation inspired by Tencent's (â€œScaling Synthetic Data Creation with 1,000,000,000 Personasâ€).\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-Long-Context-Alpaca-HESSIAN-AI."},
	{"name":"TV-44kHz-Full","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Thorsten-Voice/TV-44kHz-Full","creator_name":"Thorsten MÃ¼ller","creator_url":"https://huggingface.co/Thorsten-Voice","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe \\\"Thorsten-Voice\\\" dataset\\n\\t\\n\\nThis truly open source (CC0 license) german (ðŸ‡©ðŸ‡ª) voice dataset contains about 40 hours of transcribed voice recordings by Thorsten MÃ¼ller, \\na single male, native speaker in over 38.000 wave files.\\n\\nMono\\nSamplerate: 44.100Hz\\nTrimmed silence at begin/end\\nDenoised\\nNormalized to -24dB\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDisclaimer\\n\\t\\n\\n\\\"Please keep in mind, I am not a professional speaker, just an open source speech technology enthusiast who donates his voice. I contribute myâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Thorsten-Voice/TV-44kHz-Full."},
	{"name":"llm_filtered_customer_service_conversations","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marccgrau/llm_filtered_customer_service_conversations","creator_name":"Marc Christopher Grau","creator_url":"https://huggingface.co/marccgrau","description":"\\n\\t\\n\\t\\t\\n\\t\\tLLM-filtered Customer Service Conversations Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains simulated conversations generated by our agentic simulation system.\\nThe conversations are filtered by a LLM to ensure they are of high quality.\\nEach record is stored in JSON Lines (JSONL) format and includes:\\n\\nInput Settings: Metadata such as selected bank, customer, agent profiles, and task details.\\nMessages: The full conversation messages.\\nSummary: A German summary of the conversation.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/marccgrau/llm_filtered_customer_service_conversations."},
	{"name":"llm_filtered_customer_service_conversations_cleaned","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marccgrau/llm_filtered_customer_service_conversations_cleaned","creator_name":"Marc Christopher Grau","creator_url":"https://huggingface.co/marccgrau","description":"\\n\\t\\n\\t\\t\\n\\t\\tLLM-filtered Customer Service Conversations Dataset (cleaned)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains simulated conversations generated by our agentic simulation system.\\nThe conversations are filtered by a LLM to ensure they are of high quality.\\nEach record is stored in JSON Lines (JSONL) format and includes:\\n\\nInput Settings: Metadata such as selected bank, customer, agent profiles, and task details.\\nMessages: The full conversation messages.\\nSummary: A German summary of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/marccgrau/llm_filtered_customer_service_conversations_cleaned."},
	{"name":"BRIGHTER-emotion-intensities","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/brighter-dataset/BRIGHTER-emotion-intensities","creator_name":"BRIGHTER Dataset","creator_url":"https://huggingface.co/brighter-dataset","description":"\\n\\t\\n\\t\\t\\n\\t\\tBRIGHTER Emotion Intensities Dataset\\n\\t\\n\\nThis dataset contains the emotion intensities data from the BRIGHTER paper: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe BRIGHTER Emotion Intensities dataset is a comprehensive multi-language emotion intensity dataset with separate configurations for each language. It represents one of the largest human-annotated emotion datasets across multiple languages, providingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/brighter-dataset/BRIGHTER-emotion-intensities."},
	{"name":"semeval-2025-task11-track-a","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vgaraujov/semeval-2025-task11-track-a","creator_name":"Vladimir Araujo","creator_url":"https://huggingface.co/vgaraujov","description":"\\n\\t\\n\\t\\t\\n\\t\\tSemEval 2025 Task 11 - Track A Dataset\\n\\t\\n\\nThis dataset contains the data for SemEval 2025 Task 11: Bridging the Gap in Text-Based Emotion Detection - Track A, organized as language-specific configurations.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset is a multi-language, multi-label emotion classification dataset with separate configurations for each language.\\n\\nTotal languages: 26 standard ISO codes\\nTotal examples: 115159\\nSplits: train, dev, test\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguage Configurations\\n\\t\\n\\nEachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vgaraujov/semeval-2025-task11-track-a."},
	{"name":"semeval-2025-task11-track-c","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vgaraujov/semeval-2025-task11-track-c","creator_name":"Vladimir Araujo","creator_url":"https://huggingface.co/vgaraujov","description":"\\n\\t\\n\\t\\t\\n\\t\\tSemEval 2025 Task 11 - Track C Dataset\\n\\t\\n\\nThis dataset contains the data for SemEval 2025 Task 11: Bridging the Gap in Text-Based Emotion Detection - Track C, organized as language-specific configurations.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset is a multi-language, multi-label emotion classification dataset with separate configurations for each language.\\n\\nTotal languages: 30 standard ISO codes\\nTotal examples: 57254\\nSplits: dev, test (Track C has no train split)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tTrackâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vgaraujov/semeval-2025-task11-track-c."},
	{"name":"small_data_text","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Miti-Komon/small_data_text","creator_name":"Miti Komon Hatashi","creator_url":"https://huggingface.co/Miti-Komon","description":"Miti-Komon/small_data_text dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"mosel","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FBK-MT/mosel","creator_name":"FBK-MT","creator_url":"https://huggingface.co/FBK-MT","description":"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description, Collection, and Source\\n\\t\\n\\nThe MOSEL corpus is a multilingual dataset collection including up to 950K hours of open-source speech recordings covering the 24 official languages of the European Union. We collect data by surveying labeled and unlabeled speech corpora under open-source compliant licenses.\\nIn particular, MOSEL includes the automatic transcripts of 441k hours of unlabeled speech from VoxPopuli and LibriLight. The data is transcribed using Whisper largeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FBK-MT/mosel."},
	{"name":"mac-app-store-apps-metadata","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MacPaw/mac-app-store-apps-metadata","creator_name":"MacPaw Inc.","creator_url":"https://huggingface.co/MacPaw","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Macappstore Applications Metadata\\n\\t\\n\\n\\nMac App Store Applications Metadata sourced by the public API.\\n\\nCurated by: MacPaw Inc.\\n\\nLanguage(s) (NLP): Mostly EN, DE\\nLicense: MIT\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nThis data aims to cover our internal company research needs and start collecting and sharing the macOS app dataset since we have yet to find a suitable existing one.\\nFull application metadata was sourced by the public iTunes search API for the US, Germany, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MacPaw/mac-app-store-apps-metadata."},
	{"name":"mac-app-store-apps-release-notes","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MacPaw/mac-app-store-apps-release-notes","creator_name":"MacPaw Inc.","creator_url":"https://huggingface.co/MacPaw","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Macappstore Applications Release Notes\\n\\t\\n\\n\\nMac App Store Applications release notes extracted from the metadata from the public API.\\n\\nCurated by: MacPaw Inc.\\n\\nLanguage(s) (NLP): Mostly EN, DE\\nLicense: MIT\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nThis dataset is a combined and refined Mac App Store Applications Metadata dataset subset. \\nThe main idea behind its creation is to separate the release notes texts of the macOS apps for the convenience of further analysis.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MacPaw/mac-app-store-apps-release-notes."},
	{"name":"gutenberg_de","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LemiSt/gutenberg_de","creator_name":"Lennard Michael Strohmeyer","creator_url":"https://huggingface.co/LemiSt","description":"German subset of sedthh/gutenberg_multilang\\n"},
	{"name":"mc-translation","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/mc-translation","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"This dataset contains professional human translations from OpenAI's MMMLU dataset, repurposed to train translation models that can help translate future evaluation datasets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy This Dataset?\\n\\t\\n\\nTranslation of evaluation benchmarks is a critical but challenging task. While automated translations may introduce errors or biases, professional human translations are expensive and time-consuming. This dataset leverages existing professional translations (MMMLU) to train specializedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/efederici/mc-translation."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-eh35-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-eh35-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-eh35-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-eh35-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-eh35-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-k007-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-k007-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-k007-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information in German\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-k007-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-k007-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-12h5-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-12h5-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-12h5-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-12h5-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-12h5-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-de-932024-59f9-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-de-932024-59f9-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp."},
	{"name":"CTA-synthetic-dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chaichy/CTA-synthetic-dataset","creator_name":"Michael Achmann-Denkler","creator_url":"https://huggingface.co/chaichy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman Instagram Political Communication 2021 - Synthetic Call to Action Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThis dataset consists of synthetic training data used to detect Calls to Action (CTAs) in German political Instagram content from the 2021 Federal Election. The synthetic data was generated using OpenAI's GPT-4o to augment original human-annotated examples. The dataset aims to address class imbalance issues for improved model performance in political communicationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/chaichy/CTA-synthetic-dataset."},
	{"name":"RAGAS_xquad","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xxizhouu/RAGAS_xquad","creator_name":"Zhou","creator_url":"https://huggingface.co/xxizhouu","description":"data source: https://github.com/google-deepmind/xquad (base on squad 1.0)\\ndocument_en/de.zip contains all 48 documents for answer the question: created by concatenate continous pragraphs\\nQuestion,Answer, Context (QAC) pair in english(en) AND german(de)\\n\\nclean_full_en_de: 48 documents, 5 paragraphs per document, multiple questions per paragraph\\nsingle_qa_en_de: 48 documents, 5 paragraphs per document, one question per paragraph\\ntest_half: 24 documents, 5 paragraphs per document, one questionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xxizhouu/RAGAS_xquad."},
	{"name":"German-RAG-EMBEDDING-BENCHMARK","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-EMBEDDING-BENCHMARK","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-EMBEDDING-BENCHMARK\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis German-RAG-EMBEDDING-BENCHMARK represents a specialized collection for evaluating embedding models.\\nYou can use this dataset with this Evaluation Notebook in Colab:\\nhttps://colab.research.google.com/drive/1KlIRchSlCtKg9C_bZkdWaYLxROeB0Fr_?usp=sharing\\nAdditonally we have evaluated our Models on German Subsets of the MTEB Benchmark with this Evaluation Notebookâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-EMBEDDING-BENCHMARK."},
	{"name":"product-database","keyword":"german","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/openfoodfacts/product-database","creator_name":"Open Food Facts","creator_url":"https://huggingface.co/openfoodfacts","description":"\\n\\t\\n\\t\\t\\n\\t\\tOpen Food Facts Database\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is ðŸŠ Open Food Facts?\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tA food products database\\n\\t\\n\\nOpen Food Facts is a database of food products with ingredients, allergens, nutrition facts and all the tidbits of information we can find on product labels.\\n\\n\\t\\n\\t\\t\\n\\t\\tMade by everyone\\n\\t\\n\\nOpen Food Facts is a non-profit association of volunteers. 25.000+ contributors like you have added 1.7 million + products from 150 countries using our Android or iPhone app or their camera to scanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openfoodfacts/product-database."},
	{"name":"ss_members_dataset","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jotone/ss_members_dataset","creator_name":"John Toniutti","creator_url":"https://huggingface.co/jotone","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSS members dataset\\n\\t\\n\\nThis dataset contains information about some of the members of the SS during the Third Reich.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource\\n\\t\\n\\nThis dataset was made by parsing this page and all its subpages: https://www.dws-xip.com/reich/biografie/numery/numerA.html\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDisclaimer\\n\\t\\n\\nThis dataset is provided solely for archival and educational purposes. I do not support or endorse Nazist ideology, the actions of the SS, or any related beliefs. The information contained withinâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jotone/ss_members_dataset."},
	{"name":"SemEval2024-task8","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/SemEval2024-task8","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSemEval2024-task8\\n\\t\\n\\nUnofficial mirror of M4 dataset from mbzuai-nlp/SemEval2024-task8 (website, github, codabench).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSubtask A\\n\\t\\n\\nAn object in the JSON format:\\n{\\n  id -> identifier of the example,\\n  label -> label (human text: 0, machine text: 1,),\\n  text -> text generated by a machine or written by a human,\\n  model -> model that generated the data,\\n  source -> source (Wikipedia, Wikihow, Peerread, Reddit, Arxiv)  on English or languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/SemEval2024-task8."},
	{"name":"global-festivals-translated","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/azminetoushikwasi/global-festivals-translated","creator_name":"Azmine Toushik Wasi","creator_url":"https://huggingface.co/azminetoushikwasi","description":"azminetoushikwasi/global-festivals-translated dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"test_4","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tunngo/test_4","creator_name":"nguyen tuan","creator_url":"https://huggingface.co/tunngo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFLEURS\\n\\t\\n\\nFleurs is the speech version of the FLoRes machine translation benchmark. \\nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \\nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\\nused and â€unit error rateâ€ (characters, signs) of all languages is averaged. Languages and results are also grouped into sevenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tunngo/test_4."},
	{"name":"prompt-injection-multilingual","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/rikka-snow/prompt-injection-multilingual","creator_name":"Le Xuan Hoang","creator_url":"https://huggingface.co/rikka-snow","description":"rikka-snow/prompt-injection-multilingual dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"mac-app-store-apps-descriptions","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MacPaw/mac-app-store-apps-descriptions","creator_name":"MacPaw Inc.","creator_url":"https://huggingface.co/MacPaw","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Macappstore Applications Descriptions\\n\\t\\n\\n\\nMac App Store Applications descriptions extracted from the metadata from the public API.\\n\\nCurated by: MacPaw Inc.\\n\\nLanguage(s) (NLP): Mostly EN, DE\\nLicense: MIT\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nThis dataset is a combined and refined Mac App Store Applications Metadata dataset subset. \\nThe main idea behind its creation is to separate the description texts of the macOS apps for the convenience of further analysis.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MacPaw/mac-app-store-apps-descriptions."},
	{"name":"xtreme-up-semantic-parsing","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davlan/xtreme-up-semantic-parsing","creator_name":"David Adelani","creator_url":"https://huggingface.co/Davlan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for afrixnli\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSee XTREME-UP GitHub\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThere are 20 languages available :\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe examples look like this for English:\\nfrom datasets import load_dataset\\ndata = load_dataset('Davlan/xtreme-up-semantic-parsing', 'yor') \\n# Please, specify the language code\\n# A data point example is below:\\n{\\n\\\"id\\\": \\\"3231323330393336\\\",\\n\\\"split\\\": \\\"test\\\",\\n\\\"intent\\\": \\\"IN:GET_REMINDER\\\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davlan/xtreme-up-semantic-parsing."},
	{"name":"integreat-qa","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/digitalfabrik/integreat-qa","creator_name":"TÃ¼r an TÃ¼r - Digitalfabrik gGmbH","creator_url":"https://huggingface.co/digitalfabrik","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset\\n\\t\\n\\nOur dataset consists of 906 diverse QA pairs in German and English.\\nThe dataset is extractive, i.e., answers are given as sentence indices (breaking at the newline character \\\\n).\\nQuestions are automatically generated using an LLM.\\nThe answers are manually annotated using voluntary crowdsourcing.\\nRepository: More Information Needed\\nPaper:\\n\\nhttps://arxiv.org/abs/1806.03822\\nhttps://aclanthology.org/2024.konvens-main.25/\\n\\nOur dataset is licensed under cc-by-4.0.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/digitalfabrik/integreat-qa."},
	{"name":"cml-tts-filtered","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PHBJT/cml-tts-filtered","creator_name":"Paul Henri Biojout","creator_url":"https://huggingface.co/PHBJT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Filtred and CML-TTS\\n\\t\\n\\nThis dataset is a filtred version of a CML-TTS [1]. \\nCML-TTS [1] CML-TTS is a recursive acronym for CML-Multi-Lingual-TTS, a Text-to-Speech (TTS) dataset developed at the Center of Excellence in Artificial Intelligence (CEIA) of the Federal University of Goias (UFG). CML-TTS is a dataset comprising audiobooks sourced from the public domain books of Project Gutenberg, read by volunteers from the LibriVox project. The dataset includesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PHBJT/cml-tts-filtered."},
	{"name":"testing_arc_easy_de","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TDN007/testing_arc_easy_de","creator_name":"Nguyen","creator_url":"https://huggingface.co/TDN007","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\ttesting_arc_easy_de\\n\\t\\n\\nThis is a copy of the dataset openGPT-X/arcx. \\n"},
	{"name":"German-RAG-LLM-EASY-BENCHMARK","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-LLM-EASY-BENCHMARK","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-LLM-EASY-BENCHMARK\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis German-RAG-LLM-BENCHMARK represents a specialized collection for evaluating language models with a focus on source citation, time difference stating in RAG-specific tasks.\\nTo evaluate models compatible with OpenAI-Endpoints you can refer to our Github Repo: https://github.com/avemio-digital/German-RAG-LLM-EASY-BENCHMARK/\\nMost of the Subsets are syntheticallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-LLM-EASY-BENCHMARK."},
	{"name":"German-RAG-LLM-EASY-BENCHMARK","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-LLM-EASY-BENCHMARK","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-LLM-EASY-BENCHMARK\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis German-RAG-LLM-BENCHMARK represents a specialized collection for evaluating language models with a focus on source citation, time difference stating in RAG-specific tasks.\\nTo evaluate models compatible with OpenAI-Endpoints you can refer to our Github Repo: https://github.com/avemio-digital/German-RAG-LLM-EASY-BENCHMARK/\\nMost of the Subsets are syntheticallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-LLM-EASY-BENCHMARK."},
	{"name":"German-RAG-ORPO-Long-Context-ShareGPT-HESSIAN-AI","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-Long-Context-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) Long Context ShareGPT-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe ORPO Long Context Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \\nThe subsets are derived from Synthetic generation inspired by Tencent's (â€œScaling Synthetic Data Creation with 1,000,000,000 Personasâ€).\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-Long-Context-ShareGPT-HESSIAN-AI."},
	{"name":"German-RAG-ORPO-Long-Context-ShareGPT-HESSIAN-AI","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-Long-Context-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) Long Context ShareGPT-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe ORPO Long Context Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \\nThe subsets are derived from Synthetic generation inspired by Tencent's (â€œScaling Synthetic Data Creation with 1,000,000,000 Personasâ€).\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-Long-Context-ShareGPT-HESSIAN-AI."},
	{"name":"DataMix","keyword":"german","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emogie3D/DataMix","creator_name":"Andreas","creator_url":"https://huggingface.co/emogie3D","description":"this dataset is a mix of of some datasets available on huggingface for text-to-text generation, it's not cleaned, it's not deduplicated.\\nIt's not recommend to use it in it's current state.\\nas the higghest restriction counts, it's only for private and research usable - it's not allowed to create an competitive model to \\\"whatever\\\".\\nNever use it for commercial purposes describes it for all Datasets in this repository.\\n"},
	{"name":"SemEval2025-Task5-Curated-Data","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jimfhahn/SemEval2025-Task5-Curated-Data","creator_name":"jim hahn","creator_url":"https://huggingface.co/jimfhahn","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for LLMs4Subjects SemEval 2025 Shared Task Curated Dataset\\n\\t\\n\\nThis is a curated set of data made available as a part of the LLMs4Subjects SemEval 2025 Shared Task.\\n\\n\\t\\n\\t\\t\\n\\t\\tOriginal Data Citation\\n\\t\\n\\nD'Souza, J., Sadruddin, S., Israel, H., & Begoin, M. (2024). The SemEval 2025 LLMs4Subjects Shared Task Dataset (Version 1.0.0) [Data set]. https://github.com/jd-coderepos/llms4subjects/\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\nContains English and German language text, with corresponding GNDâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jimfhahn/SemEval2025-Task5-Curated-Data."},
	{"name":"filtered_convos_research_llm_summaries","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marccgrau/filtered_convos_research_llm_summaries","creator_name":"Marc Christopher Grau","creator_url":"https://huggingface.co/marccgrau","description":"\\n\\t\\n\\t\\t\\n\\t\\tSynthetic Call Center Summaries Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains synthetic summaries of call center conversations generated by different prompt configurations.\\nEach record (in JSON Lines format) includes:\\n\\nThe original dialogue metadata.\\nA generated summary tailored to provide quick insights for call center service agents.\\nEvaluation metrics\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tPrompts for summarization\\n\\t\\n\\n\\nNarrative: A narrative summary of the conversation.\\nBullet Points: A summary of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/marccgrau/filtered_convos_research_llm_summaries."},
	{"name":"filtered_convos_research_llm_summaries_cleaned","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marccgrau/filtered_convos_research_llm_summaries_cleaned","creator_name":"Marc Christopher Grau","creator_url":"https://huggingface.co/marccgrau","description":"\\n\\t\\n\\t\\t\\n\\t\\tSynthetic Call Center Summaries Dataset Cleaned\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains synthetic summaries of call center conversations generated by different prompt configurations.\\nEach record (in JSON Lines format) includes:\\n\\nThe original dialogue metadata.\\nA generated summary tailored to provide quick insights for call center service agents.\\nEvaluation metrics\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tPrompts for summarization\\n\\t\\n\\n\\nNarrative: A narrative summary of the conversation.\\nBullet Points: A summary ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/marccgrau/filtered_convos_research_llm_summaries_cleaned."},
	{"name":"semeval-2025-task11-track-b","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vgaraujov/semeval-2025-task11-track-b","creator_name":"Vladimir Araujo","creator_url":"https://huggingface.co/vgaraujov","description":"\\n\\t\\n\\t\\t\\n\\t\\tSemEval 2025 Task 11 - Track B Dataset\\n\\t\\n\\nThis dataset contains the data for SemEval 2025 Task 11: Bridging the Gap in Text-Based Emotion Detection - Track B, organized as language-specific configurations.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset is a multi-language, multi-label emotion classification dataset with separate configurations for each language.\\n\\nTotal languages: 11 standard ISO codes\\nTotal examples: 47111\\nSplits: train, dev, test\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tTrack Information\\n\\t\\n\\nTrack B hasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vgaraujov/semeval-2025-task11-track-b."},
	{"name":"openlegaldata","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/harshildarji/openlegaldata","creator_name":"Harshil","creator_url":"https://huggingface.co/harshildarji","description":"Clean Open Legal Data\\n\\n   \\n       Overview |\\n       Dataset Structure  |\\n       Key Fields  |\\n       Example Entry |\\n       Using the Dataset with Python |\\n       License\\n   \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is a comprehensive collection of open legal case records in JSONL format. It comprises 251,038 cases extracted and processed from the Open Legal Data dump (as of 2022-10-18). The dataset is designed for legal research, data science, and natural language processing applications. Whileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/harshildarji/openlegaldata."},
	{"name":"multiCHILDES","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/IParraMartin/multiCHILDES","creator_name":"IÃ±igo Parra","creator_url":"https://huggingface.co/IParraMartin","description":"\\n\\t\\n\\t\\t\\n\\t\\tmultiCHILDES: Multilingual Child-Directed Speech Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains child-directed speech from 19 languages, extracted from the CHILDES corpus. The text has been cleaned and is designed for text generation tasks, particularly in studying early language acquisition.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nSource: CHILDES corpus\\nLanguages: 19 languages\\nText Type: Child-directed speech\\nTask: Text Generation, Language Modeling\\nData Processing: The datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/IParraMartin/multiCHILDES."},
	{"name":"IFA_LKW_W50_4x4_Expedition","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RenatoWBS/IFA_LKW_W50_4x4_Expedition","creator_name":"Renato Pietsch","creator_url":"https://huggingface.co/RenatoWBS","description":"RenatoWBS/IFA_LKW_W50_4x4_Expedition dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"taco-de","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LenDigLearn/taco-de","creator_name":"Lennard Michael Strohmeyer","creator_url":"https://huggingface.co/LenDigLearn","description":"\\n\\t\\n\\t\\t\\n\\t\\tTACO subset with all of the prompts translated to German\\n\\t\\n\\nAll examples were taken from BAAI/TACO and the prompts were translated to German using educa-ai-nemo-dpo.\\nNo filtering or processing has been done afterwards, use with care.\\n"},
	{"name":"BenchMAX_Question_Answering","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Question_Answering","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\\nLink: https://arxiv.org/pdf/2502.07346\\nRepository: https://github.com/CONE-MT/BenchMAX\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBenchMAX_Function_Completion is a dataset of BenchMAX, sourcing from UN Parallel Corpus and xquad.\\nThe haystacks are from UN Parallel Corpus Test and Development Sets and we translate them to other languages by Google Translate.\\nThe multilingual QA data is fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Question_Answering."},
	{"name":"M-ABSA","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Multilingual-NLP/M-ABSA","creator_name":"multilingual-NLP","creator_url":"https://huggingface.co/Multilingual-NLP","description":"\\n\\t\\n\\t\\t\\n\\t\\tM-ABSA\\n\\t\\n\\nThis repo contains the data for our paper M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Description:\\n\\t\\n\\nThis is a dataset suitable for the multilingual ABSA task with triplet extraction.\\nAll datasets are stored in the data/ folder:\\n\\nAll dataset contains 7 domains.\\n\\ndomains = [\\\"coursera\\\", \\\"hotel\\\", \\\"laptop\\\", \\\"restaurant\\\", \\\"phone\\\", \\\"sight\\\", \\\"food\\\"]\\n\\n\\nEach dataset contains 21 languages.\\n\\nlangs = [\\\"ar\\\", \\\"da\\\", \\\"de\\\", \\\"en\\\", \\\"es\\\", \\\"fr\\\", \\\"hi\\\", \\\"hr\\\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Multilingual-NLP/M-ABSA."},
	{"name":"bsi_grundschutz","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/schdav/bsi_grundschutz","creator_name":"David S","creator_url":"https://huggingface.co/schdav","description":"schdav/bsi_grundschutz dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"LLM-Interaction","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Yliu566/LLM-Interaction","creator_name":"Yiqi Liu","creator_url":"https://huggingface.co/Yliu566","description":"This is a temporary dataset used for initial testing.\\n"},
	{"name":"wiktionary-data","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/paion-data/wiktionary-data","creator_name":"Paion Data","creator_url":"https://huggingface.co/paion-data","description":"\\n\\t\\n\\t\\t\\n\\t\\tWiktionary Data on Hugging Face Datasets\\n\\t\\n\\n\\n\\n\\n\\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\\nsupports the following languages:\\n\\nDeutsch - German\\nLatinum - Latin\\ná¼™Î»Î»Î·Î½Î¹ÎºÎ® - Ancient Greek\\ní•œêµ­ì–´ - Korean\\nðŽ ðŽ¼ðŽ¹- Old Persian\\nð’€ð’…—ð’ºð’Œ‘(ð’Œ) - Akkadian\\nElamite\\nà¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤®à¥ - Sanskrit, or Classical Sanskrit\\n\\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\\nthe dataset it's getting bigger, I noticed a wave of more exciting potentials thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/paion-data/wiktionary-data."},
	{"name":"wiktionary-data","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/paion-data/wiktionary-data","creator_name":"Paion Data","creator_url":"https://huggingface.co/paion-data","description":"\\n\\t\\n\\t\\t\\n\\t\\tWiktionary Data on Hugging Face Datasets\\n\\t\\n\\n\\n\\n\\n\\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\\nsupports the following languages:\\n\\nDeutsch - German\\nLatinum - Latin\\ná¼™Î»Î»Î·Î½Î¹ÎºÎ® - Ancient Greek\\ní•œêµ­ì–´ - Korean\\nðŽ ðŽ¼ðŽ¹- Old Persian\\nð’€ð’…—ð’ºð’Œ‘(ð’Œ) - Akkadian\\nElamite\\nà¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤®à¥ - Sanskrit, or Classical Sanskrit\\n\\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\\nthe dataset it's getting bigger, I noticed a wave of more exciting potentials thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/paion-data/wiktionary-data."},
	{"name":"arc_challenge_de_fixed","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TDN007/arc_challenge_de_fixed","creator_name":"Nguyen","creator_url":"https://huggingface.co/TDN007","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tARC Challenge DE fixed\\n\\t\\n\\nThis is a copy of the dataset LeoLM/ArcChallenge_de which is a German translation of the original allenai/ai2_arc. \\nBug fixesWe copied it to fix the numeric labels 1,2,3,4 into A,B,C,D. \\nThanksThe translation of the arc_challenge was done by Bjoern: https://github.com/bjoernpl/GermanBenchmark\\n"},
	{"name":"Wikipedia-Abstract","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/laion/Wikipedia-Abstract","creator_name":"LAION eV","creator_url":"https://huggingface.co/laion","description":"Wikipedia Abstract\\n\\n\\n  \\n\\n\\n\\nIntroducing Wikipedia Abstract, a comprehensive dataset encompassing abstracts, complete articles, and a popularity score index for both widely spoken and lesser-known Wikipedia subsets. Our dedication to Wikipedia-X ensures a centralized Wikipedia dataset that undergoes regular updates and adheres to the highest standards.\\nA central focus of our efforts was to include exotic languages that often lack up-to-date Wikipedia dumps or may not have any dumps at all.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/laion/Wikipedia-Abstract."},
	{"name":"new-new-carlotta-data","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Fischerboot/new-new-carlotta-data","creator_name":"Moritz Nickel","creator_url":"https://huggingface.co/Fischerboot","description":"Fischerboot/new-new-carlotta-data dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"jinaai_jina-embeddings-v2-base-en-02092024-jqg1-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-02092024-jqg1-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-02092024-jqg1-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-02092024-jqg1-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-02092024-jqg1-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information retrieval system\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information retrieval system\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-9evb-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-9evb-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-9evb-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-9evb-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-9evb-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-2ayt-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-2ayt-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-2ayt-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-2ayt-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-2ayt-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information in German language\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information in German language\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp."},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-wh9d-webapp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-wh9d-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-03092024-wh9d-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"health insurance information\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-wh9d-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-wh9d-webapp."},
	{"name":"Wikipedia-X-Concat","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/laion/Wikipedia-X-Concat","creator_name":"LAION eV","creator_url":"https://huggingface.co/laion","description":"We have combined title and abstracts together into the Concat Abstract column in this dataset. It's a slight modification over our original Wikipedia X dataset. It's done for RAG project purposes.\\n"},
	{"name":"German-RAG-LLM-HARD-BENCHMARK","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-LLM-HARD-BENCHMARK","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-LLM-HARD Benchmark\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis German-RAG-LLM-HARD-BENCHMARK represents a specialized collection for evaluate language models with a focus on hard to solve RAG-specific capabilities. To evaluate models compatible with OpenAI-Endpoints you can refer to our Github Repo: https://github.com/avemio-digital/GRAG-LLM-HARD-BENCHMARK\\nThe subsets are derived from Synthetic generation inspired byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-LLM-HARD-BENCHMARK."},
	{"name":"German-RAG-LLM-HARD-BENCHMARK","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-LLM-HARD-BENCHMARK","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-LLM-HARD Benchmark\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis German-RAG-LLM-HARD-BENCHMARK represents a specialized collection for evaluate language models with a focus on hard to solve RAG-specific capabilities. To evaluate models compatible with OpenAI-Endpoints you can refer to our Github Repo: https://github.com/avemio-digital/GRAG-LLM-HARD-BENCHMARK\\nThe subsets are derived from Synthetic generation inspired byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-LLM-HARD-BENCHMARK."},
	{"name":"multilingual_refusals","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/s-nlp/multilingual_refusals","creator_name":"s-nlp","creator_url":"https://huggingface.co/s-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\tData description\\n\\t\\n\\nThis dataset is designed to train and evaluate models for the task of refusal detection in generated responses. The dataset consists of input prompts sourced from the lmsys/lmsys-chat-1m collection, encompassing a variety of languages including English, German, French, Russian, and Spanish. To increase refusal diversity, the responses and refusals were generated using two models, Gemini Flash 1.5 and LLaMA-3.3-70b.\\nThe dataset is primarily intended to trainâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/s-nlp/multilingual_refusals."},
	{"name":"ops-volltext-klassifizierung","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/regmibijay/ops-volltext-klassifizierung","creator_name":"Bijay Regmi","creator_url":"https://huggingface.co/regmibijay","description":"\\n\\t\\n\\t\\t\\n\\t\\tSynthetisches Dataset fÃ¼r OPS-Klassifizierung\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tHaftungsausschluss\\n\\t\\n\\nDiese Daten wurden von https://gesund.bund.de gescraped und sind Eigentum des Urheberrechtsinhabers. Der alleinige Zweck dieses Datensatzes und der zugehÃ¶rigen Codebasis sowie anderer Materialien ist es, die deutsche medizinische Gemeinschaft bei der Erstellung hochspezialisierter deutscher Modelle zu unterstÃ¼tzen.\\nWenn Sie an vorab geparsten Daten interessiert sind, die als Baseline fÃ¼r diese synthetischenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/regmibijay/ops-volltext-klassifizierung."},
	{"name":"multi30k","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/romrawinjp/multi30k","creator_name":"Romrawin Chumpu","creator_url":"https://huggingface.co/romrawinjp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMulti30k Dataset\\n\\t\\n\\nThis dataset is a rearrangement version of the Multi30k dataset. The dataset was partially retrived from Multi30k original github.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to use\\n\\t\\n\\nThis dataset can be downloaded from datasets library. train, validation, and test set are included in the dataset.\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"romrawinjp/multi30k\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReference\\n\\t\\n\\nIf you find this dataset beneficial, please directly cite to their incredible work.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/romrawinjp/multi30k."},
	{"name":"mmmlu_lite","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/opencompass/mmmlu_lite","creator_name":"OpenCompass","creator_url":"https://huggingface.co/opencompass","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMMLU-Lite\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nA lite version of the MMMLU dataset, which is an community version of the MMMLU dataset by OpenCompass. Due to the large size of the original dataset (about 200k questions), we have created a lite version of the dataset to make it easier to use. We sample 25 examples from each language subject in the original dataset with fixed seed to ensure reproducibility, finally we have 19950 examples in the lite version of the dataset, which is aboutâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/opencompass/mmmlu_lite."},
	{"name":"wilhelm-vocabulary","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","description":"\\n\\t\\n\\t\\t\\n\\t\\tWilhelm Vocabulary\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWilhelm Vocabulary\\nDevelopment\\nEnvironment Setup\\nInstalling Dependencies\\nData Format\\nEncoding Table in YAML\\n\\n\\nData Pipeline\\nHow Data (Vocabulary) is Stored in a Graph Database\\nWhy Graph Database\\nBase Schema\\n\\n\\n\\n\\nLanguages\\nGerman\\nPronoun\\nNoun\\nVerb\\n\\n\\nAncient Greek\\nDiacritic Mark Convention\\nPronoun\\nNoun\\nAdjective\\n1. Three-Ending Adjectives: 1st and 2nd Declension (2-1-2)2. Two-Ending 2nd Declension Adjectives (2-2)\\n3. Two-Ending 3rd Declension Adjectivesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary."},
	{"name":"wilhelm-vocabulary","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","description":"\\n\\t\\n\\t\\t\\n\\t\\tWilhelm Vocabulary\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWilhelm Vocabulary\\nDevelopment\\nEnvironment Setup\\nInstalling Dependencies\\nData Format\\nEncoding Table in YAML\\n\\n\\nData Pipeline\\nHow Data (Vocabulary) is Stored in a Graph Database\\nWhy Graph Database\\nBase Schema\\n\\n\\n\\n\\nLanguages\\nGerman\\nPronoun\\nNoun\\nVerb\\n\\n\\nAncient Greek\\nDiacritic Mark Convention\\nPronoun\\nNoun\\nAdjective\\n1. Three-Ending Adjectives: 1st and 2nd Declension (2-1-2)2. Two-Ending 2nd Declension Adjectives (2-2)\\n3. Two-Ending 3rd Declension Adjectivesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary."},
	{"name":"text-moderation-02-multilingual","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ifmain/text-moderation-02-multilingual","creator_name":"Mike Afton","creator_url":"https://huggingface.co/ifmain","description":"This dataset is based on Kaggle.It represents a version of @ifmain/text-moderation-410K that has been cleansed of semantically similar values and normalized to a 50/50 ratio of negative and neutral entries.\\nThe dataset contains 1.5M entries (91K * 17 languages).  \\nBefore use, augmentation is recommended! (e.g., character substitution to bypass moderation).\\nFor augmentation, you can use @ifmain/StringAugmentor.  \\nEnjoy using it!\\n"},
	{"name":"Multilingal-sakalt-data","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sakalti/Multilingal-sakalt-data","creator_name":"sakasan","creator_url":"https://huggingface.co/Sakalti","description":"ãƒžãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚mitãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§ã™ã€‚\\n"},
	{"name":"multilingual-coco","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/romrawinjp/multilingual-coco","creator_name":"Romrawin Chumpu","creator_url":"https://huggingface.co/romrawinjp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMultilingual Common Objects in Context (COCO) Dataset\\n\\t\\n\\nThis dataset is a collection of multiple language open-source captions of COCO dataset. \\nThe split in this dataset is set according to Andrej Karpathy's split from dataset_coco.json file. The collection was created specifically for simplicity of use in training and evaluation pipeline by non-commercial and research purposes. The COCO images dataset is licensed under a Creative Commons Attribution 4.0 License.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/romrawinjp/multilingual-coco."},
	{"name":"AyaVisionBench","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/AyaVisionBench","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Aya Vision Benchmark\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\nThe Aya Vision Benchmark is designed to evaluate vision-language models in real-world multilingual scenarios. It spans 23 languages and 9 distinct task categories, with 15 samples per category, resulting in 135 image-question pairs per language. \\nEach question requires visual context for the answer and covers languages that half of the world's population speaks, making this dataset particularly suited forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/AyaVisionBench."},
	{"name":"webfaq","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PaDaS-Lab/webfaq","creator_name":"Chair of Data Science, University of Passau","creator_url":"https://huggingface.co/PaDaS-Lab","description":"WebFAQ Q&A Dataset\\n\\n   \\n       Overview |\\n       Details  |\\n       Structure  |\\n       Examples |\\n       Considerations |\\n       License |\\n       Citation |\\n       Contact |\\n       Acknowledgement\\n   \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe WebFAQ Q&A Dataset is a broad-coverage corpus of 96 million natural question-answer (QA) pairs in 75 languages, gathered from FAQ pages on the web. It leverages structured schema.org FAQPage annotations, making it a unique resource for large-scale Question Answeringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PaDaS-Lab/webfaq."},
	{"name":"oasst1","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenAssistant/oasst1","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenAssistant Conversations Dataset (OASST1)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \\nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \\ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \\nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \\nis a product of a worldwide crowd-sourcing effortâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst1."},
	{"name":"Everything_Instruct_Multilingual","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rombodawg/Everything_Instruct_Multilingual","creator_name":"rombo dawg","creator_url":"https://huggingface.co/rombodawg","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEverything Instruct (Multilingual Edition)\\n\\t\\n\\nEverything you need... all in one place ðŸ’˜\\n\\nEverything instruct (Multilingual Edition) is a massive alpaca instruct formatted dataset consisting of a wide variety of topics meant to bring LLM's to the next level in open source AI.\\nNote: This dataset is fully uncensored (No model will refuse any request trained on this dataset unless otherwise aligned)\\nNote2: This version of the dataset supports the following languages:\\n\\nEnglish\\nRussianâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rombodawg/Everything_Instruct_Multilingual."},
	{"name":"ShareGPT52K","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RyokoAI/ShareGPT52K","creator_name":"Ryoko AI","creator_url":"https://huggingface.co/RyokoAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ShareGPT52K90K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\\nThis repository now contains the new 90K conversations version. The previous 52K may\\nbe found in the old/ directory.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\ntext-generation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RyokoAI/ShareGPT52K."},
	{"name":"oasst2","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenAssistant/oasst2","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpen Assistant Conversations Dataset Release 2 (OASST2)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThis dataset contains message trees. Each message tree has an initial prompt message as the root node, \\nwhich can have multiple child messages as replies, and these child messages can have multiple replies. \\nAll messages have a role property: this can either be \\\"assistant\\\" or \\\"prompter\\\". The roles in \\nconversation threads from prompt to leaf node strictly alternate between \\\"prompter\\\" andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst2."},
	{"name":"nn-auto-bench-ds","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nanonets/nn-auto-bench-ds","creator_name":"Nanonets","creator_url":"https://huggingface.co/nanonets","description":"\\n\\t\\n\\t\\t\\n\\t\\tnn-auto-bench-ds\\n\\t\\n\\nnn-auto-bench-ds is a dataset designed for key information extraction (KIE) and serves as a benchmark dataset for nn-auto-bench.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThe dataset comprises 1,000 documents, categorized into the following types:\\n\\nInvoice\\nReceipt\\nPassport\\nBank Statement\\n\\nThe documents are primarily available in English, with some also in German and Arabic. Each document is annotated for key information extraction and specific tasks. The dataset can be used toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nanonets/nn-auto-bench-ds."},
	{"name":"Global-MMLU","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/Global-MMLU","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGlobal-MMLU ðŸŒ is a multilingual evaluation set spanning 42 languages, including English. This dataset combines machine translations for MMLU questions along with professional translations and crowd-sourced post-edits.\\nIt also includes cultural sensitivity annotations for a subset of the questions (2850 questions per language) and classifies them as Culturally Sensitive (CS) ðŸ—½ or Culturally Agnostic (CA) âš–ï¸. These annotations were collected as part of an openâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/Global-MMLU."},
	{"name":"reasoning-multilingual-R1-Llama-70B-train","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lightblue/reasoning-multilingual-R1-Llama-70B-train","creator_name":"Lightblue KK.","creator_url":"https://huggingface.co/lightblue","description":"\\n\\t\\n\\t\\t\\n\\t\\tlightblue/reasoning-multilingual-R1-Llama-70B-train\\n\\t\\n\\nThis is a multilingual reasoning dataset covering more than 30 languages.\\nThis dataset was made by:\\n\\nSampling prompts from English datasets and translating them to various languages\\nGenerating responses to these prompts 8 times using deepseek-ai/DeepSeek-R1-Distill-Llama-70B\\nFiltering out <think> sections with incorrect language, non-fluent language, and incorrect answers\\n\\nThis dataset was then used to train a multilingualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lightblue/reasoning-multilingual-R1-Llama-70B-train."},
	{"name":"wmt24pp","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/wmt24pp","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\\n\\t\\n\\t\\t\\n\\t\\tWMT24++\\n\\t\\n\\nThis repository contains the human translation and post-edit data for the 55 en->xx language pairs released in\\nthe publication\\nWMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects.\\nIf you are interested in the MT/LLM system outputs and automatic metric scores, please see MTME.\\nIf you are interested in the images of the source URLs for each document, please see here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSchema\\n\\t\\n\\nEach language pair is stored in its own jsonl file.\\nEach row isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google/wmt24pp."},
	{"name":"thinking-multilingual-30-23-small-690","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/thinking-multilingual-30-23-small-690","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"\\nBased on OPENO1 math, this is a translated dataset of 30 high quality questions & answers in 23 languages. \\nOr use the \\\"big\\\" version: big 10k rows version\\n"},
	{"name":"ComMT","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NiuTrans/ComMT","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","description":"\\n\\t\\n\\t\\t\\n\\t\\tComMT\\n\\t\\n\\n\\nGithub: https://github.com/NiuTrans/LaMaTE/\\nPaper: https://arxiv.org/abs/2503.06594\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nComMT is a comprehensive dataset suite designed to support the development and evaluation of universal translation models. \\nIt includes diverse translation-related tasks, providing a well-curated data resource for training and testing LLM-based machine translation systems.\\nThe dataset is meticulously curated from over 60+ publicly available data sources. \\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/ComMT."},
	{"name":"bbaw_egyptian","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/phiwi/bbaw_egyptian","creator_name":"Wiesenbach","creator_url":"https://huggingface.co/phiwi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"bbaw_egyptian\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset comprises parallel sentences of hieroglyphic encodings, transcription and translation as used in the paper Multi-Task Modeling of Phonographic Languages: Translating Middle Egyptian Hieroglyph. The data triples are extracted from the digital corpus of Egyptian texts compiled by the project \\\"Strukturen und Transformationen des Wortschatzes der Ã¤gyptischen Sprache\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/phiwi/bbaw_egyptian."},
	{"name":"conceptnet5","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/conceptnet5/conceptnet5","creator_name":"conceptnet5","creator_url":"https://huggingface.co/conceptnet5","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Conceptnet5\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nConceptNet is a multilingual knowledge base, representing words and\\nphrases that people use and the common-sense relationships between\\nthem. The knowledge in ConceptNet is collected from a variety of\\nresources, including crowd-sourced resources (such as Wiktionary and\\nOpen Mind Common Sense), games with a purpose (such as Verbosity and\\nnadya.jp), and expert-created resources (such as WordNet and JMDict).\\nYou can browseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/conceptnet5/conceptnet5."},
	{"name":"exams","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mhardalov/exams","creator_name":"Momchil Hardalov","creator_url":"https://huggingface.co/mhardalov","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for [Dataset Name]\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nEXAMS is a benchmark dataset for multilingual and cross-lingual question answering from high school examinations. It consists of more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mhardalov/exams."},
	{"name":"xquad","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/xquad","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"xquad\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nXQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering\\nperformance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set\\nof SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German,\\nGreek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google/xquad."},
	{"name":"xtreme","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/xtreme","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"xtreme\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Cross-lingual Natural Language Inference (XNLI) corpus is a crowd-sourced collection of 5,000 test and\\n2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into\\n14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese,\\nHindi, Swahili and Urdu. This results in 112.5k annotated pairs. Each premise can be associated withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google/xtreme."},
	{"name":"mfaq","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clips/mfaq","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","description":"We present the first multilingual FAQ dataset publicly available. We collected around 6M FAQ pairs from the web, in 21 different languages."},
	{"name":"mqa","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clips/mqa","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","description":"MQA is a multilingual corpus of questions and answers parsed from the Common Crawl. Questions are divided between Frequently Asked Questions (FAQ) pages and Community Question Answering (CQA) pages."},
	{"name":"klexikon","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dennlinger/klexikon","creator_name":"Dennis Aumiller","creator_url":"https://huggingface.co/dennlinger","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for the Klexikon Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVersion History\\n\\t\\n\\n\\nv0.3 (2022-09-01): Removing some five samples from the dataset due to duplication conflicts with other samples.\\nv0.2 (2022-02-28): Updated the files to no longer contain empty sections and removing otherwise empty lines at the end of files. Also removing lines with some sort of coordinate.\\nv0.1 (2022-01-19): Initial data release on Huggingface datasets.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Klexikon dataset is aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dennlinger/klexikon."},
	{"name":"multilingual_librispeech","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/multilingual_librispeech","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MultiLingual LibriSpeech\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis is a streamable version of the Multilingual LibriSpeech (MLS) dataset. \\nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \\n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/facebook/multilingual_librispeech."},
	{"name":"voxpopuli","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/voxpopuli","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation."},
	{"name":"multilingual-sentiments","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tyqiangz/multilingual-sentiments","creator_name":"Tay Yong Qiang","creator_url":"https://huggingface.co/tyqiangz","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMultilingual Sentiments Dataset\\n\\t\\n\\nA collection of multilingual sentiments datasets grouped into 3 classes -- positive, neutral, negative.\\nMost multilingual sentiment datasets are either 2-class positive or negative, 5-class ratings of products reviews (e.g. Amazon multilingual dataset) or multiple classes of emotions. However, to an average person, sometimes positive, negative and neutral classes suffice and are more straightforward to perceive and annotate. Also, aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tyqiangz/multilingual-sentiments."},
	{"name":"NLU-few-shot-benchmark-en-de","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/deutsche-telekom/NLU-few-shot-benchmark-en-de","creator_name":"Deutsche Telekom AG","creator_url":"https://huggingface.co/deutsche-telekom","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNLU Few-shot Benchmark - English and German\\n\\t\\n\\nThis is a few-shot training dataset from the domain of human-robot interaction.\\nIt contains texts in German and English language with 64 different utterances (classes).\\nEach utterance (class) has exactly 20 samples in the training set.\\nThis leads to a total of 1280 different training samples.\\nThe dataset is intended to benchmark the intent classifiers of chat bots in English and especially in German language.\\nWe are building on ourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deutsche-telekom/NLU-few-shot-benchmark-en-de."},
	{"name":"lambada_openai","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/EleutherAI/lambada_openai","creator_name":"EleutherAI","creator_url":"https://huggingface.co/EleutherAI","description":"The LAMBADA dataset as processed by OpenAI. It is used to evaluate the capabilities\\nof computational models for text understanding by means of a word prediction task.\\nLAMBADA is a collection of narrative texts sharing the characteristic that human subjects\\nare able to guess their last word if they are exposed to the whole text, but not\\nif they only see the last sentence preceding the target word. To succeed on LAMBADA,\\ncomputational models cannot simply rely on local context, but must be able to keep track\\nof information in the broader discourse.\\n\\nReference: https://github.com/openai/gpt-2/issues/131#issuecomment-497136199"},
	{"name":"wikipedia-22-12-de-embeddings","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-de-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWikipedia (de) embedded with cohere.ai multilingual-22-12 encoder\\n\\t\\n\\nWe encoded Wikipedia (de) using the cohere.ai multilingual-22-12 embedding model.\\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEmbeddings\\n\\t\\n\\nWe compute for title+\\\" \\\"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-de-embeddings."},
	{"name":"legal-mc4","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/joelniklaus/legal-mc4","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","description":"Legal-MC4: A Corpus Covering the Legal Part of MC4 for European Languages"},
	{"name":"wmt-da-human-evaluation","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RicardoRei/wmt-da-human-evaluation","creator_name":"Ricardo Costa Dias Rei","creator_url":"https://huggingface.co/RicardoRei","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains all DA human annotations from previous WMT News Translation shared tasks.\\nThe data is organised into 8 columns:\\n\\nlp: language pair\\nsrc: input text\\nmt: translation\\nref: reference translation\\nscore: z score\\nraw: direct assessment\\nannotators: number of annotators\\ndomain: domain of the input text (e.g. news)\\nyear: collection year\\n\\nYou can also find the original data for each year in the results sectionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RicardoRei/wmt-da-human-evaluation."},
	{"name":"multiconer_v2","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MultiCoNER/multiconer_v2","creator_name":"MultiCoNER","creator_url":"https://huggingface.co/MultiCoNER","description":"Complex named entities (NE), like the titles of creative works, are not simple nouns and pose challenges for NER systems (Ashwini and Choi, 2014). They can take the form of any linguistic constituent, like an imperative clause (â€œDial M for Murderâ€), and do not look like traditional NEs (Persons, Locations, etc.). This syntactic ambiguity makes it challenging to recognize them based on context. We organized the MultiCoNER task (Malmasi et al., 2022) at SemEval-2022 to address these challenges in 11 languages, receiving a very positive community response with 34 system papers. Results confirmed the challenges of processing complex and long-tail NEs: even the largest pre-trained Transformers did not achieve top performance without external knowledge. The top systems infused transformers with knowledge bases and gazetteers. However, such solutions are brittle against out of knowledge-base entities and noisy scenarios like the presence of spelling mistakes and typos. We propose MultiCoNER II which represents novel challenges through new tasks that emphasize the shortcomings of the current top models.\\n\\nMultiCoNER II features complex NER in these languages:\\n\\n1. English\\n2. Spanish\\n3. Hindi\\n4. Bangla\\n5. Chinese\\n6. Swedish\\n7. Farsi\\n8. French\\n9. Italian\\n10. Portugese\\n11. Ukranian\\n12. German\\n\\nFor more details see https://multiconer.github.io/\\n\\n## References\\n* Sandeep Ashwini and Jinho D. Choi. 2014. Targetable named entity recognition in social media. CoRR, abs/1408.0782.\\n* Shervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta Kar, Oleg Rokhlenko. 2022. SemEval-2022 Task 11: Multilingual Complex Named Entity Recognition (MultiCoNER)."},
	{"name":"swissner","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZurichNLP/swissner","creator_name":"University of Zurich, Department of Computational Linguistics","creator_url":"https://huggingface.co/ZurichNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSwissNER\\n\\t\\n\\nA multilingual test set for named entity recognition (NER) on Swiss news articles.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nSwissNER is a dataset for named entity recognition based on manually annotated news articles in Swiss Standard German, French, Italian, and Romansh Grischun.\\nWe have manually annotated a selection of articles that have been published in February 2023 in the categories \\\"Switzerland\\\" or \\\"Regional\\\" on the following online news portals:\\n\\nSwiss Standard German:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZurichNLP/swissner."},
	{"name":"rte3-multi","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maximoss/rte3-multi","creator_name":"Maximos Skandalis","creator_url":"https://huggingface.co/maximoss","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository contains all manually translated versions of RTE-3 dataset, plus the original English one. The languages into which RTE-3 dataset has so far been translated are Italian (2012), German (2013), and French (2023).\\nUnlike in other repositories, both our own French version and the older Italian and German ones are here annotated in 3 classes (entailment, neutral, contradiction), and not in 2 (entailment, notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/maximoss/rte3-multi."},
	{"name":"oa-stackexchange","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/donfu/oa-stackexchange","creator_name":"Donfu","creator_url":"https://huggingface.co/donfu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tStackexchange Instructions for OpenAssistant\\n\\t\\n\\nThis dataset is taken from https://archive.org/details/stackexchange.\\nThere's a single parquet file combining all stackexchange sites. The threads\\nhave been filtered as follows: only threads with an accepted answer, for which\\nboth the question and response is less than 1000 characters have been choosen.\\nOther answers, or questions without accepted answers, or long entries have been\\ndroppped.\\nEach row consists of\\n\\nINSTRUCTION\\nRESPONSEâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/donfu/oa-stackexchange."},
	{"name":"openlegaldata-bulk-data","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LennardZuendorf/openlegaldata-bulk-data","creator_name":"Lennard ZÃ¼ndorf","creator_url":"https://huggingface.co/LennardZuendorf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for openlegaldata.io bulk case data\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is the copy of the lastest dump from openlegaldata.io. I will try to keep this updated, since there is no offical Huggingface Dataset Repo. \\n\\nHomepage: https://de.openlegaldata.io/\\nRepository: Bulk Data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis is the openlegaldata bulk case download from October 2022. Please refer to the offical website (above) for any more information. I have not made anyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LennardZuendorf/openlegaldata-bulk-data."},
	{"name":"mgsm","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/juletxara/mgsm","creator_name":"Julen Etxaniz","creator_url":"https://huggingface.co/juletxara","description":"Multilingual Grade School Math Benchmark (MGSM) is a benchmark of grade-school math problems, proposed in the paper [Language models are multilingual chain-of-thought reasoners](http://arxiv.org/abs/2210.03057).\\n\\nThe same 250 problems from [GSM8K](https://arxiv.org/abs/2110.14168) are each translated via human annotators in 10 languages. The 10 languages are:\\n- Spanish\\n- French\\n- German\\n- Russian\\n- Chinese\\n- Japanese\\n- Thai\\n- Swahili\\n- Bengali\\n- Telugu\\n\\nYou can find the input and targets for each of the ten languages (and English) as `.tsv` files.\\nWe also include few-shot exemplars that are also manually translated from each language in `exemplars.py`."},
	{"name":"sumstew","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Joemgu/sumstew","creator_name":"Jonas","creator_url":"https://huggingface.co/Joemgu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"sumstew\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTL;DR:\\n\\t\\n\\nSumstew is a abstractive, multilingual Dataset, with a balanced number of samples from a diverse set of summarization Datasets. The input sizes range up to 16384 tokens.\\nFiltered using a diverse set of heuristics to encourage high coverage, accuracy and factual consistency. Code to reproduce Dataset available at TODO\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTask Information\\n\\t\\n\\n\\nTask Categories: The tasks covered by this dataset are primarily summarizationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Joemgu/sumstew."},
	{"name":"swiss_leading_decision_summarization","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rcds/swiss_leading_decision_summarization","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","description":"This dataset contains court decisions for the swiss ruling summarization task."},
	{"name":"toxi-text-3M","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FredZhang7/toxi-text-3M","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","description":"This is a large multilingual toxicity dataset with 3M rows of text data from 55 natural languages, all of which are written/sent by humans, not machine translation models.\\nThe preprocessed training data alone consists of 2,880,667 rows of comments, tweets, and messages. Among these rows, 416,529 are classified as toxic, while the remaining 2,463,773 are considered neutral. Below is a table to illustrate the data composition:\\n\\n\\t\\n\\t\\t\\n\\nToxic\\nNeutral\\nTotal\\n\\n\\n\\t\\t\\nmultilingual-train-deduplicated.csvâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/toxi-text-3M."},
	{"name":"GuanacoDataset-de","keyword":"german","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sixf0ur/GuanacoDataset-de","creator_name":"David","creator_url":"https://huggingface.co/sixf0ur","description":"This dataset was taken from JosephusCheung/GuanacoDataset and filtered to German entries.\\n"},
	{"name":"massive_translation_dataset","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Amani27/massive_translation_dataset","creator_name":"Amani N","creator_url":"https://huggingface.co/Amani27","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Massive Dataset for Translation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is derived from AmazonScience/MASSIVE dataset for translation task purpose.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nTranslation\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n\\nEnglish (en_US)\\nGerman (de_DE)\\nHindi (hi_IN)\\nSpanish (es_ES)\\nFrench (fr_FR)\\nItalian (it_IT)\\nArabic (ar_SA)\\nDutch (nl_NL)\\nJapanese (ja_JP)\\nPortugese (pt_PT)\\n\\n"},
	{"name":"belebele","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/belebele","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Belebele Benchmark for Massively Multilingual NLU Evaluation\\n\\t\\n\\nBelebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. This dataset enables the evaluation of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the FLORES-200 dataset. The human annotation procedure was carefully curated to create questions thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/facebook/belebele."},
	{"name":"GSM8KInstruct_Parallel","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mathoctopus/GSM8KInstruct_Parallel","creator_name":"Mathoctopus","creator_url":"https://huggingface.co/Mathoctopus","description":"Mathoctopus/GSM8KInstruct_Parallel dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"ultrachat_de","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bjoernp/ultrachat_de","creator_name":"BjÃ¶rn PlÃ¼ster","creator_url":"https://huggingface.co/bjoernp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman UltraChat\\n\\t\\n\\nThis dataset contains the first 1k prompts from HuggingFaceH4/ultrachat_200k translated to German and inference on with GPT-4.\\n"},
	{"name":"Open_Assistant_Conversation_Chains","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-ric/Open_Assistant_Conversation_Chains","creator_name":"Aymeric Roucher","creator_url":"https://huggingface.co/m-ric","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset description\\n\\t\\n\\n\\n\\nThis dataset is a reformatting of OpenAssistant Conversations (OASST1), which is\\n\\na human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.\\n\\nIt wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-ric/Open_Assistant_Conversation_Chains."},
	{"name":"cml-tts","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ylacombe/cml-tts","creator_name":"Yoach Lacombe","creator_url":"https://huggingface.co/ylacombe","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for CML-TTS\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nCML-TTS is a recursive acronym for CML-Multi-Lingual-TTS, a Text-to-Speech (TTS) dataset developed at the Center of Excellence in Artificial Intelligence (CEIA) of the Federal University of Goias (UFG).\\nCML-TTS is a dataset comprising audiobooks sourced from the public domain books of Project Gutenberg, read by volunteers from the LibriVox project. The dataset includes recordings in Dutch, German, French, Italian, Polishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ylacombe/cml-tts."},
	{"name":"librivox-tracks","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pykeio/librivox-tracks","creator_name":"pyke.io","creator_url":"https://huggingface.co/pykeio","description":"A dataset of all audio files uploaded to LibriVox before 26th September 2023.\\n"},
	{"name":"qald_9_plus","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/casey-martin/qald_9_plus","creator_name":"Casey","creator_url":"https://huggingface.co/casey-martin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQALD-9-plus Dataset Description\\n\\t\\n\\nQALD-9-plus is the dataset for Knowledge Graph Question Answering (KGQA) based on well-known QALD-9.\\nQALD-9-plus enables to train and test KGQA systems over DBpedia and Wikidata using questions in 9 different languages: English, German, Russian, French, Armenian, Belarusian, Lithuanian, Bashkir, and Ukrainian.\\nSome of the questions have several alternative writings in particular languages which enables to evaluate the robustness of KGQA systemsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/casey-martin/qald_9_plus."},
	{"name":"multilingual-tts","keyword":"german","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MohamedRashad/multilingual-tts","creator_name":"Mohamed Rashad","creator_url":"https://huggingface.co/MohamedRashad","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBefore Anything and Everything âš±\\n\\t\\n\\nIn the time of writing this Dataset Card, 17,490 18,412 civilian has been killed in Palestine (7,870 8,000 are children and 6,121 6,200 are women).\\nSeek any non-profit organization to help them with what you can (For myself, I use Mersal) ðŸ‡µðŸ‡¸\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Multilingual TTS dataset is an exceptional compilation of text-to-speech (TTS) samples, meticulously crafted to showcase the richness and diversity of human languages.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MohamedRashad/multilingual-tts."},
	{"name":"multilingual-pl-bert","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/styletts2-community/multilingual-pl-bert","creator_name":"StyleTTS 2 Community","creator_url":"https://huggingface.co/styletts2-community","description":"Attribution: Wikipedia.org\\n"},
	{"name":"tla-demotic-v18-premium","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thesaurus-linguae-aegyptiae/tla-demotic-v18-premium","creator_name":"Thesaurus Linguae Aegyptiae","creator_url":"https://huggingface.co/thesaurus-linguae-aegyptiae","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset tla-demotic-v18-premium\\n\\t\\n\\n\\nThis data set contains demotic sentences in transliteration, with lemmatization, with POS glossing and with a German translation. \\nThe data comes from the database of the Thesaurus Linguae Agegyptiae, corpus version 18, and contains only fully intact, \\nunambiguously readable sentences (13,383 of 31,156 sentences), adjusted for philological and editorial markup.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Descriptionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thesaurus-linguae-aegyptiae/tla-demotic-v18-premium."},
	{"name":"openassistant-deepseek-coder","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-deepseek-coder","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChat Fine-tuning Dataset - OpenAssistant DeepSeek Coder\\n\\t\\n\\nThis dataset allows for fine-tuning chat models using:\\nB_INST = '\\\\n### Instruction:\\\\n'\\nE_INST = '\\\\n### Response:\\\\n'\\nBOS = '<ï½œbeginâ–ofâ–sentenceï½œ>'\\nEOS = '\\\\n<|EOT|>\\\\n'\\n\\nSample Preparation:\\n\\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-deepseek-coder."},
	{"name":"sib200","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davlan/sib200","creator_name":"David Adelani","creator_url":"https://huggingface.co/Davlan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SIB-200\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSIB-200 is the largest publicly available topic classification dataset based on Flores-200 covering 205 languages and dialects.\\nThe train/validation/test sets are available for all the 205 languages.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\ntopic classification: categorize wikipedia sentences into topics e.g science/technology, sports or politics.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThere are 205 languages available :â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davlan/sib200."},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","description":"(WIP)\\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \\nIn the next days, they will be filtered out. To do so manually, just look for \\\"Ã¼bersetz\\\" in the columns \\\"input\\\", \\\"chosen\\\" or \\\"rejected\\\"\\nand exclude them from your training pipeline.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tULTRA Distilabel Intel Orca DPO (German):\\n\\t\\n\\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\\nThe provided dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de."},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","description":"(WIP)\\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \\nIn the next days, they will be filtered out. To do so manually, just look for \\\"Ã¼bersetz\\\" in the columns \\\"input\\\", \\\"chosen\\\" or \\\"rejected\\\"\\nand exclude them from your training pipeline.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tULTRA Distilabel Intel Orca DPO (German):\\n\\t\\n\\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\\nThe provided dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de."},
	{"name":"aya_dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/aya_dataset","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Aya Dataset is a multilingual instruction fine-tuning dataset curated by an open-science community via Aya Annotation Platform from Cohere For AI. The dataset contains a total of 204k human-annotated prompt-completion pairs along with the demographics data of the annotators.\\nThis dataset can be used to train, finetune, and evaluate multilingual LLMs.\\n\\nCurated by: Contributors of Aya Open Science Intiative.\\n\\nLanguage(s): 65 languages (71 including dialects &â€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/aya_dataset."},
	{"name":"aya_collection","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/aya_collection","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\nThis dataset is uploaded in two places: here and additionally here as 'Aya Collection Language Split.' These datasets are identical in content but differ in structure of upload. This dataset is structured by folders split according to dataset name. The version here instead divides the Aya collection into folders split by language. We recommend you use the language split version if you are only interested in downloading data for a single or smaller set of languages, and this version if youâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/aya_collection."},
	{"name":"intensified-phoenix-14-t","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/merterm/intensified-phoenix-14-t","creator_name":"Mert Inan","creator_url":"https://huggingface.co/merterm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntensified PHOENIX 14-T German Sign Language Dataset\\n\\t\\n\\n\\n\\nThis is a German-to-German Sign Language (DGS) dataset of weather forecasts. It is a prosodically-enhanced version of the RWTH-PHOENIX-Weather-2014T dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\nCurated by: [Mert Inan]\\nLanguage(s) (NLP): German, DGS (German Sign Language)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\nRepository: Modeling Intensification for Sign Language Generation\\nPaper:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/merterm/intensified-phoenix-14-t."},
	{"name":"aya_evaluation_suite","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/aya_evaluation_suite","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAya Evaluation Suite contains a total of 26,750 open-ended conversation-style prompts to evaluate multilingual open-ended generation quality.To strike a balance between language coverage and the quality that comes with human curation, we create an evaluation suite that includes:\\n\\nhuman-curated examples in 7 languages (tur, eng, yor, arb, zho, por, tel) â†’ aya-human-annotated.\\nmachine-translations of handpicked examples into 101 languages â†’â€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/aya_evaluation_suite."},
	{"name":"CulturaY","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ontocord/CulturaY","creator_name":"Ontocord.AI","creator_url":"https://huggingface.co/ontocord","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCulturaY: A Large Cleaned Multilingual Dataset of 75 Languages\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nFrom the team that brought you CulturaX, we present CulturaY, another substantial multilingual dataset of 15TB (uncompressed)/3TB (zstd-compressed) that applies the same dataset cleaning methodology to the HPLT v1.1 dataset. \\nPlease note that HPLT v1.2 has also been released and is an alternative verison with different cleaning methodolgies. \\nThis data was used in part to train our SOTAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ontocord/CulturaY."},
	{"name":"distilabel-math-preference-dpo-de","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mayflowergmbh/distilabel-math-preference-dpo-de","creator_name":"Mayflower GmbH","creator_url":"https://huggingface.co/mayflowergmbh","description":"German azureml translation of argilla/distilabel-math-preference-dpo\\nfor dpo finetuning.\\n"},
	{"name":"bio-mqm-dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zouharvi/bio-mqm-dataset","creator_name":"VilÃ©m Zouhar","creator_url":"https://huggingface.co/zouharvi","description":"This dataset is compiled from the official Amazon repository (all respective licensing applies).\\nIt contains system translations, multiple references, and their quality evaluation on the MQM scale. It accompanies the ACL 2024 paper Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains.\\nWatch a brief 4 minutes-long video.\\n\\nAbstract: We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zouharvi/bio-mqm-dataset."},
	{"name":"aya_collection_language_split","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/aya_collection_language_split","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\nThis is a re-upload of the aya_collection, and only differs in the structure of upload. While the original aya_collection is structured by folders split according to dataset name, this dataset is split by language. We recommend you use this version of the dataset if you are only interested in downloading all of the Aya collection for a single or smaller set of languages.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Aya Collection is a massive multilingual collection consisting of 513 million instancesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/aya_collection_language_split."},
	{"name":"tokenizer-wiki-bench","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/occiglot/tokenizer-wiki-bench","creator_name":"Occiglot","creator_url":"https://huggingface.co/occiglot","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMultilingual Tokenizer Benchmark\\n\\t\\n\\nThis dataset includes pre-processed wikipedia data for tokenizer evaluation in 45 languages. We provide more information on the evaluation task in general this blogpost.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nThe dataset allows us to easily calculate tokenizer fertility and the proportion of continued words on any of the supported languages. In the example below we take the Mistral tokenizer and evaluate its performance on Slovak. \\nfrom transformers importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/occiglot/tokenizer-wiki-bench."},
	{"name":"intel_orca_dpo_pairs_de","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cstr/intel_orca_dpo_pairs_de","creator_name":"cstr","creator_url":"https://huggingface.co/cstr","description":"german auzureml translation from mayflowergmbh/intel_orca_dpo_pairs_de, here only put back to original jsonl structure\\n"},
	{"name":"mCoT-MATH","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/laihuiyuan/mCoT-MATH","creator_name":"Huiyuan Lai","creator_url":"https://huggingface.co/laihuiyuan","description":"\\n\\t\\n\\t\\t\\n\\t\\tmCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models\\n\\t\\n\\nPaper: https://arxiv.org/abs/2406.02301\\nCode: https://github.com/laihuiyuan/mCoT\\nModel: https://huggingface.co/laihuiyuan/mCoT\\n\\n\\t\\n\\t\\t\\n\\t\\tIntroduction\\n\\t\\n\\nBased on MetaMathQA and MathInstruct\\n, we use machine translation to compile mCoT-MATH, the first large-scale multilingual math CoT reasoning dataset containing around 6.3 million samples for 11 diverse languages.\\nWe train a 7B parameter model mCoT forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/laihuiyuan/mCoT-MATH."},
	{"name":"orca_dpo_pairs","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/multilingual/orca_dpo_pairs","creator_name":"mLLM multilingual","creator_url":"https://huggingface.co/multilingual","description":"\\n    \\n\\n\\nmLLM IMPLEMENTATION OF Intel/orca_dpo_pairs.\\nLANGUAGES:\\nARABIC\\nCHINESE\\nFRENCH\\nGERMAN\\nRUSSIAN\\nSPANISH\\nTURKISH\\n(WIP)\\n"},
	{"name":"Post-OCR-Correction","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PleIAs/Post-OCR-Correction","creator_name":"PleIAs","creator_url":"https://huggingface.co/PleIAs","description":"Post-OCR correction is a large corpus of 1 billion words containing original texts with a varying number of OCR mistakes and an experimental multilingual post-OCR correction output created by Pleias.\\nGeneration of Post-OCR correction was performed using HPC resources from GENCIâ€“IDRIS (Grant 2023-AD011014736) on Jean-Zay.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nAll the texts come from collections integrated into Common Corpus, the largest open corpus for pretraining previously released by Pleias onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PleIAs/Post-OCR-Correction."},
	{"name":"synthetic-pii-ner-mistral-v1","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/urchade/synthetic-pii-ner-mistral-v1","creator_name":"Urchade Zaratiana","creator_url":"https://huggingface.co/urchade","description":"This the synthetic dataset used for training https://huggingface.co/urchade/gliner_multi_pii-v1. You can get it by browsing the files and dowloading the data.json file.\\n"},
	{"name":"swim-ir-monolingual","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nthakur/swim-ir-monolingual","creator_name":"Nandan Thakur","creator_url":"https://huggingface.co/nthakur","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SWIM-IR (Monolingual)\\n\\t\\n\\n\\n\\n\\nThis is the monolingual subset of the SWIM-IR dataset, where the query generated and the passage are both in the same language.\\nA few remaining languages will be added in the upcoming v2 version of SWIM-IR. The dataset is available as CC-BY-SA 4.0.\\nFor full details of the dataset, please read our upcoming NAACL 2024 paper and check out our website.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhat is SWIM-IR?\\n\\t\\n\\nSWIM-IR dataset is a synthetic multilingual retrievalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nthakur/swim-ir-monolingual."},
	{"name":"webui-dom-snapshots","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gbenson/webui-dom-snapshots","creator_name":"Gary Benson","creator_url":"https://huggingface.co/gbenson","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WebUI DOM snapshots\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: Gary Benson\\nLanguages: Mostly English (87%);\\nDutch, French, Chinese, Japanese (1-2% each); 30+ others (<1% each)\\nLicense: CC0 1.0 Universal\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More Information Needed]\\nPaper [optional]: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gbenson/webui-dom-snapshots."},
	{"name":"GlotCC-V1","keyword":"swiss german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n \\n\\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \\nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage (Huggingface Hub --â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1."},
	{"name":"CIVICS","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CIVICS-dataset/CIVICS","creator_name":"CIVICS dataset organization","creator_url":"https://huggingface.co/CIVICS-dataset","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUses\\n\\t\\n\\nEvaluating a language modelâ€™s treatment of different ethical values, specifically for different civics topics relevant to sensitive groups. â€œTreatmentâ€ includes the likelihood a model gives to different value-laden statements and whether different implicit values in inputs lead to different generations by the model, in response to the provided prompts.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\nLanguage: One of â€œGermanâ€, â€œEnglishâ€, â€œFrenchâ€, â€œItalianâ€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/CIVICS-dataset/CIVICS."},
	{"name":"gsm-1k-de","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/D4ve-R/gsm-1k-de","creator_name":"David","creator_url":"https://huggingface.co/D4ve-R","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGSM 1k DE\\n\\t\\n\\nGSM-1k-de is a translated(english -> german) subset of the first 1000 items of GSM8K\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe text in the dataset is in german. The associated BCP-47 code is de.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tStructure\\n\\t\\n\\nEach instance contains a string for the grade-school level math question and a string for the corresponding answer with multiple steps of reasoning and calculator annotations.\\n{\\n  \\\"question\\\": \\\"Natalie ...\\\",\\n  \\\"answer\\\": \\\"Natalie ...\\\"\\n}\\n\\nquestion: The question stringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/D4ve-R/gsm-1k-de."},
	{"name":"german_handwriting","keyword":"german","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fhswf/german_handwriting","creator_name":"Fachhochschule SÃ¼dwestfalen","creator_url":"https://huggingface.co/fhswf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman handwriting\\n\\t\\n\\nThis dataset contains German handwriting images and corresponding text labels. In total, the dataset contains around 10,000 entries with handwriting from 15 different people.\\nThe data was created with the help of transcripts from school and university.\\nThe dataset was created as part of a handwriting recognition project at the FH-SWF.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to use:\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset('fhswf/german_handwriting')\\n\\n"},
	{"name":"GlotCC-V1","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n \\n\\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \\nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage (Huggingface Hub --â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1."},
	{"name":"GlotCC-V1","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n \\n\\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \\nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage (Huggingface Hub --â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1."},
	{"name":"synthetic_pii_finance_multilingual","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gretelai/synthetic_pii_finance_multilingual","creator_name":"Gretel.ai","creator_url":"https://huggingface.co/gretelai","description":"\\n  \\n  Image generated by DALL-E. See prompt for more details\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tðŸ’¼ ðŸ“Š Synthetic Financial Domain Documents with PII Labels\\n\\t\\n\\ngretelai/synthetic_pii_finance_multilingual is a dataset of full length synthetic financial documents containing Personally Identifiable Information (PII), generated using Gretel Navigator and released under Apache 2.0.\\nThis dataset is designed to assist with the following use cases:\\n\\nðŸ·ï¸ Training NER (Named Entity Recognition) models to detect and label PII inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gretelai/synthetic_pii_finance_multilingual."},
	{"name":"nomiracl-instruct","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/miracl/nomiracl-instruct","creator_name":"MIRACL","creator_url":"https://huggingface.co/miracl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for NoMIRACL (EMNLP 2024 Findings Track)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Overview\\n\\t\\n\\nThis repository contains the fine-tuning (training & development split) of the NoMIRACL instruct dataset for fine-tuning LLMs on multilingual relevance assessment.\\nThe training dataset is a binary classification task; they need to explicitly output either Yes, answer is present or I don't know. \\nThe dataset contains training pairs from all 18 languages for both splits: relevant & non-relevant.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/miracl/nomiracl-instruct."},
	{"name":"Tuda-De","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/uhhlt/Tuda-De","creator_name":"LT Group at UHH","creator_url":"https://huggingface.co/uhhlt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpen speech data for German speech recognition\\n\\t\\n\\nLanguage Technology, UniversitÃ¤t Hamburg, Germany\\nhttps://www.inf.uni-hamburg.de/en/inst/ab/lt (formerly TU-Darmstadt)\\nhttps://www.lt.tu-darmstadt.de\\nTelecooperation labs, TU-Darmstadt, Germany\\nhttps://www.tk.informatik.tu-darmstadt.de\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGeneral information\\n\\t\\n\\n\\nThe speech data was collected in a controlled environment (same room, same microphone distances, etc. )\\nDistance between speakers and the microphones is about 1â€¦ See the full description on the dataset page: https://huggingface.co/datasets/uhhlt/Tuda-De."},
	{"name":"wikipedia-2024-06-bge-m3","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Upstash/wikipedia-2024-06-bge-m3","creator_name":"Upstash","creator_url":"https://huggingface.co/Upstash","description":"\\n\\t\\n\\t\\t\\n\\t\\tWikipedia Embeddings with BGE-M3\\n\\t\\n\\nThis dataset contains embeddings from the\\nJune 2024 Wikipedia dump\\nfor the 11 most popular languages.\\nThe embeddings are generated with the multilingual\\nBGE-M3 model.\\nThe dataset consists of Wikipedia articles split into paragraphs,\\nand embedded with the aforementioned model.\\nTo enhance search quality, the paragraphs are prefixed with their\\nrespective article titles before embedding.\\nAdditionally, paragraphs containing fewer than 100 charactersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Upstash/wikipedia-2024-06-bge-m3."},
	{"name":"text_ratings","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lightblue/text_ratings","creator_name":"Lightblue KK.","creator_url":"https://huggingface.co/lightblue","description":"Todo - Write dataset card\\n"},
	{"name":"raid","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/liamdugan/raid","creator_name":"Liam Dugan","creator_url":"https://huggingface.co/liamdugan","description":"\\nðŸš¨ RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors ðŸš¨\\nðŸŒ Website, ðŸ–¥ï¸ Github, ðŸ“ Paper\\n\\n\\nRAID is the largest & most comprehensive dataset for evaluating AI-generated text detectors. \\nIt contains over 10 million documents spanning 11 LLMs, 11 genres, 4 decoding strategies, and 12 adversarial attacks. \\nIt is designed to be the go-to location for trustworthy third-party evaluation of both open-source and closed-source generated text detectors.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLoadâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/liamdugan/raid."},
	{"name":"MMMLU","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/openai/MMMLU","creator_name":"OpenAI","creator_url":"https://huggingface.co/openai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMultilingual Massive Multitask Language Understanding (MMMLU)\\n\\t\\n\\nThe MMLU is a widely recognized benchmark of general knowledge attained by AI models. It covers a broad range of topics from 57 different categories, covering elementary-level knowledge up to advanced professional subjects like law, physics, history, and computer science.\\nWe translated the MMLUâ€™s test set into 14 languages using professional human translators. Relying on human translators for this evaluation increasesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openai/MMMLU."},
	{"name":"muri-it","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/akoksal/muri-it","creator_name":"Abdullatif Koksal","creator_url":"https://huggingface.co/akoksal","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMURI-IT: Multilingual Instruction Tuning Dataset for 200 Languages via Multilingual Reverse Instructions\\n\\t\\n\\nMURI-IT is a large-scale multilingual instruction tuning dataset containing 2.2 million instruction-output pairs across 200 languages. It is designed to address the challenges of instruction tuning in low-resource languages with Multilingual Reverse Instructions (MURI), which ensures that the output is human-written, high-quality, and authentic to the cultural and linguisticâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/akoksal/muri-it."},
	{"name":"mgsm","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jbross-ibm-research/mgsm","creator_name":"J Bross","creator_url":"https://huggingface.co/jbross-ibm-research","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MGSM\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nCopy and merge of this MGSM Dataset, Catalan version, Basque version, Galician version, but in training samples we removed the prompt formatting, e.g. removed Question: ... in question field or Answer: ... in answer field.\\nMultilingual Grade School Math Benchmark (MGSM) is a benchmark of grade-school math problems, proposed in the paper Language models are multilingual chain-of-thought reasoners.\\nThe same 250 problems fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jbross-ibm-research/mgsm."},
	{"name":"synthetic-multi-pii-ner-v1","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/E3-JSI/synthetic-multi-pii-ner-v1","creator_name":"Department for Artificial Intelligence, JoÅ¾ef Stefan Institute","creator_url":"https://huggingface.co/E3-JSI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multilingual PII NER Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tModels Trained Using this Dataset\\n\\t\\n\\n\\nE3-JSI/gliner-multi-pii-domains-v1\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis is a synthetic dataset created for the purposes for training multilingual personally identifiable information (PII) named entity recognition (NER) models.\\nThe examples were generated using a prompt that generates the text and the entities present in the text. In addition, the generated response had to follow theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/E3-JSI/synthetic-multi-pii-ner-v1."},
	{"name":"MultiSimV2","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MichaelR207/MultiSimV2","creator_name":"Michael Ryan","creator_url":"https://huggingface.co/MichaelR207","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MultiSim Benchmark\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe MultiSim benchmark is a growing collection of text simplification datasets targeted at sentence simplification in several languages.  Currently, the benchmark spans 12 languages.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\n\\nSentence Simplification\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"MichaelR207/MultiSimV2\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this benchmark, please cite ourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MichaelR207/MultiSimV2."},
	{"name":"German-RAG-ORPO-ShareGPT-HESSIAN-AI","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) ShareGPT-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe ORPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \\nThe subsets can be for this training step are derived from 3 different sources:\\n\\nSauerkrautLM Preference Datasets:\\nSauerkrautLM-Fermented-GER-DPO:  is a specialized dataset designed for trainingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-ShareGPT-HESSIAN-AI."},
	{"name":"German-RAG-ORPO-ShareGPT-HESSIAN-AI","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) ShareGPT-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe ORPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \\nThe subsets can be for this training step are derived from 3 different sources:\\n\\nSauerkrautLM Preference Datasets:\\nSauerkrautLM-Fermented-GER-DPO:  is a specialized dataset designed for trainingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-ShareGPT-HESSIAN-AI."},
	{"name":"wikipedia","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenLLM-France/wikipedia","creator_name":"OpenLLM France","creator_url":"https://huggingface.co/OpenLLM-France","description":"\\n\\t\\n\\t\\t\\n\\t\\tPlain text of Wikipedia\\n\\t\\n\\n\\nDataset Description\\nSize\\nExample use (python)\\nData fields\\nNotes on data formatting\\n\\n\\nLicense\\nAknowledgements\\nCitation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is a plain text version of pages from wikipedia.org spaces for several languages\\n(English,\\nGerman,\\nFrench,\\nSpanish,\\nItalian).\\nThe text is without HTML tags nor wiki templates.\\nIt just includes markdown syntax for headers, lists and tables.\\nSee Notes on data formatting for more details.\\nIt wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenLLM-France/wikipedia."},
	{"name":"HPLT2.0_cleaned","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPLT/HPLT2.0_cleaned","creator_name":"HPLT","creator_url":"https://huggingface.co/HPLT","description":"This is a large-scale collection of web-crawled documents in 191 world languages, produced by the HPLT project. \\nThe source of the data is mostly Internet Archive with some additions from Common Crawl.\\nFor a detailed description of the dataset, please refer to https://hplt-project.org/datasets/v2.0\\nThe Cleaned variant of HPLT Datasets v2.0\\nThis is the cleaned variant of the HPLT Datasets v2.0 converted to the Parquet format semi-automatically when being uploaded here. \\nThe original JSONL filesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HPLT/HPLT2.0_cleaned."},
	{"name":"m-ArenaHard","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/m-ArenaHard","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for m-ArenaHard\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\nThe m-ArenaHard dataset is a multilingual LLM evaluation set. This dataset was created by translating the prompts from the originally English-only LMarena (formerly LMSYS) arena-hard-auto-v0.1 test dataset using Google Translate API v3 to 22 languages. The original English-only prompts were created by Li et al. (2024) and consist of 500 challenging user queries sourced from Chatbot Arena. The authors show that these can be usedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/m-ArenaHard."},
	{"name":"ToxicCommons","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PleIAs/ToxicCommons","creator_name":"PleIAs","creator_url":"https://huggingface.co/PleIAs","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tToxic Commons\\n\\t\\n\\nToxic Commons is a release of 2 million samples of annotated, public domain, multilingual text that was used to train Celadon. \\nIt is being released alongside Celadon, in order to better understand multilingual and multicultural toxicity. \\nEach sample was classified across 5 axes of toxicity:\\n\\nRace and origin-based bias: includes racism as well as bias against someoneâ€™s country or region of origin or immigration status, especially immigrant or refugee status.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/PleIAs/ToxicCommons."},
	{"name":"SauerkrautLM-Fermented-GER-DPO","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-GER-DPO","creator_name":"VAGO solutions","creator_url":"https://huggingface.co/VAGOsolutions","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSauerkrautLM-Fermented-GER-DPO Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nSauerkrautLM-Fermented-GER-DPO is a high-quality German instruction-response dataset specifically designed for Direct Preference Optimization (DPO) training. The dataset consists of 3,305 instruction-response pairs. Rather than being merged from existing German datasets, it was carefully created through a sophisticated augmentation process, transforming curated English instructions and responses into culturallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-GER-DPO."},
	{"name":"SauerkrautLM-Fermented-Irrelevance-GER-DPO","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-Irrelevance-GER-DPO","creator_name":"VAGO solutions","creator_url":"https://huggingface.co/VAGOsolutions","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSauerkrautLM-Fermented-Irrelevance-GER-DPO Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nSauerkrautLM-Fermented-Irrelevance-GER-DPO  is a specialized dataset designed for training language models in function calling irrelevance detection using Direct Preference Optimization (DPO). The dataset consists of 2,000 carefully evaluated instruction-response pairs, specifically curated to help models recognize situations where function calls are unnecessary and direct responses are more appropriate.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-Irrelevance-GER-DPO."},
	{"name":"belebele-fleurs","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WueNLP/belebele-fleurs","creator_name":"WÃ¼NLP","creator_url":"https://huggingface.co/WueNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBelebele-Fleurs\\n\\t\\n\\nBelebele-Fleurs is a dataset suitable to evaluate two core tasks:\\n\\nMultilingual Spoken Language Understanding (Listening Comprehension): For each spoken paragraph, the task is to answer a multiple-choice question. The question and four answer choices are provided in text form.\\nMultilingual Long-Form Automatic Speech Recognition (ASR) with Diverse Speakers: By concatenating sentence-level utterances, long-form audio clips (ranging from 30 seconds to 1 minute 30â€¦ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/belebele-fleurs."},
	{"name":"include-base-44","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/include-base-44","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tINCLUDE-base (44 languages)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nPaper: http://arxiv.org/abs/2411.19799\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \\nIt contains 22,637 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regionalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/include-base-44."},
	{"name":"unlabelled-sti-corpus","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SIRIS-Lab/unlabelled-sti-corpus","creator_name":"SIRIS Lab, Research Division of SIRIS Academic","creator_url":"https://huggingface.co/SIRIS-Lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe unlabelled-sti-corpus is a diverse dataset designed for developing information extraction datasets (i.e. text classification or NER) for Science, Technology, and Innovation (STI) records. The corpus contains approximately 35,000 records sourced from four major repositories:\\n\\n22,500 publications from OpenAlex\\n10,000 European research projects from CORDIS\\n5,000 regional projects from Interreg and Kohesio\\n7,000 patents from Lens.org\\n\\nThe dataset isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SIRIS-Lab/unlabelled-sti-corpus."},
	{"name":"open-dict-words-ipa","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/StephanAkkerman/open-dict-words-ipa","creator_name":"Stephan Akkerman","creator_url":"https://huggingface.co/StephanAkkerman","description":"\\n\\t\\n\\t\\t\\n\\t\\tOpen-dict Words IPA\\n\\t\\n\\nThis dataset is a copy of https://github.com/open-dict-data/ipa-dict\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguages\\n\\t\\n\\nIPA data is currently available for the following languages:\\n\\n\\t\\n\\t\\t\\nLanguage\\nCode\\n\\n\\n\\t\\t\\nar\\nArabic (Modern Standard)\\n\\n\\nde\\nGerman\\n\\n\\nen_UK\\nEnglish (Received Pronunciation)\\n\\n\\nen_US\\nEnglish (General American)\\n\\n\\neo\\nEsperanto\\n\\n\\nes_ES\\nSpanish (Spain)\\n\\n\\nes_MX\\nSpanish (Mexico)\\n\\n\\nfa\\nPersian\\n\\n\\nfi\\nFinnish\\n\\n\\nfr_FR\\nFrench (France)\\n\\n\\nfr_QC\\nFrench (QuÃ©bec)\\n\\n\\nis\\nIcelandic\\n\\n\\nja\\nJapanese\\n\\n\\njamâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/StephanAkkerman/open-dict-words-ipa."},
	{"name":"include-lite-44","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/include-lite-44","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tINCLUDE-lite (44 languages)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nPaper: http://arxiv.org/abs/2411.19799\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \\nIt contains 11,095 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regionalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/include-lite-44."},
	{"name":"sib-fleurs","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WueNLP/sib-fleurs","creator_name":"WÃ¼NLP","creator_url":"https://huggingface.co/WueNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSIB-Fleurs\\n\\t\\n\\nSIB-Fleurs is a dataset suitable to evaluate Multilingual Spoken Language Understanding. For each utterance in Fleurs, the task is to determine the topic the utterance belongs to. \\nThe topics are:\\n\\nScience/Technology\\nTravel\\nPolitics\\nSports\\nHealth\\nEntertainment\\nGeography\\n\\nPreliminary evaluations can be found at the bottom of the README. The preliminary results in full detail are available in ./results.csv*.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset creation\\n\\t\\n\\nThis dataset processes andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/sib-fleurs."},
	{"name":"Diplomatarium-Fennicum","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kansallisarkisto/Diplomatarium-Fennicum","creator_name":"National Archives of Finland","creator_url":"https://huggingface.co/Kansallisarkisto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDiplomatarium Fennicum-dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset summary\\n\\t\\n\\nDataset consisting of eight data fields taken from the Diplomatarium Fennicum -database. \\nDiplomatarium Fennicum -database contains medieval charters and text excerpts conserning Finland and Finns, \\nand is published and maintained by the National Archives of Finland.\\nThe data fields selected are central to identifying, categorizing and analyzing the texts. \\nThe dataset represents only very minimally the wholeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kansallisarkisto/Diplomatarium-Fennicum."},
	{"name":"Global-MMLU-Lite","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGlobal-MMLU-Lite is a multilingual evaluation set spanning 15 languages, including English. It is \\\"lite\\\" version of the original Global-MMLU dataset ðŸŒ.\\nIt includes 200 Culturally Sensitive (CS) and 200 Culturally Agnostic (CA) samples per language. The samples in Global-MMLU-Lite are corresponding to languages which are fully human translated or post-edited in the original Global-MMLU dataset. \\n\\nCurated by: Professional annotators and contributors of Cohere Forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite."},
	{"name":"2M-Belebele","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/2M-Belebele","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t2M-Belebele\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHighly-Multilingual Speech and American Sign Language Comprehension Dataset\\n\\t\\n\\nWe introduce 2M-Belebele as the first highly multilingual speech and American Sign Language (ASL) comprehension dataset. Our dataset, which is an extension of the existing Belebele only-text dataset, covers 74 spoken languages at the intersection of Belebele and Fleurs, and one sign language (ASL). \\nThe speech dataset is built from aligning Belebele, Flores200 and Fleursâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/facebook/2M-Belebele."},
	{"name":"reranker_continuous_filt_max7_train","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lightblue/reranker_continuous_filt_max7_train","creator_name":"Lightblue KK.","creator_url":"https://huggingface.co/lightblue","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReranker training data\\n\\t\\n\\nThis data was generated using 4 steps:\\n\\nWe gathered queries and corresponding text data from 35 high quality datasets covering more than 95 languages.\\nFor datasets which did not already have negative texts for queries, we mined hard negatives using the BAAI/bge-m3 embedding model.\\nFor each query, we selected one positive and one negative text and used Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4 to rate the relatedness of each query-text pair using a token \\\"1\\\", \\\"2\\\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lightblue/reranker_continuous_filt_max7_train."},
	{"name":"reranking-datasets-light","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light","creator_name":"unkown","creator_url":"https://huggingface.co/abdoelsayed","description":"\\n\\t\\n\\t\\t\\n\\t\\tðŸ”¥ Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation ðŸ”¥\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReRanking Datasets : A comprehensive collection of retrieval and reranking datasets with full passage contexts, including titles, text, and metadata for in-depth research.\\n\\t\\n\\n\\n    \\n    \\n    \\n    \\n    \\n    \\n        \\n    \\n    \\n        \\n    \\n    \\n        \\n    \\n    \\n        \\n    \\n    \\n    \\n\\n\\n\\nA curated collection of ready-to-use datasets for retrieval and rerankingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light."},
	{"name":"MultiLingualSentiment","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clapAI/MultiLingualSentiment","creator_name":"clapAI","creator_url":"https://huggingface.co/clapAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nMultilingualSentiment is a sentiment classification dataset that encompasses three sentiment labels: Positive, Neutral, Negative\\nThe dataset spans multiple languages and covers a wide range of domains, making it ideal for multilingual sentiment analysis tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\nThe dataset was meticulously collected and aggregated from various sources, including Hugging Face and Kaggle. These sources provide diverse languages and domains to ensure aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/clapAI/MultiLingualSentiment."},
	{"name":"Multilingual_Topic-Specific_Article-Extraction_and_Classification","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/oberbics/Multilingual_Topic-Specific_Article-Extraction_and_Classification","creator_name":"Oberbichler","creator_url":"https://huggingface.co/oberbics","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Multilingual Historical News Article Extraction and Classification Dataset\\n\\t\\n\\nThis dataset was created specifically to test Large Language Models' (LLMs) capabilities in processing and extracting topic-specific content from historical newspapers based on OCR'd text.\\n\\n\\t\\n\\t\\t\\n\\t\\tCite the Dataset\\n\\t\\n\\nMauermann, Johanna, GonzÃ¡lez-Gallardo, Carlos-Emiliano, and Oberbichler, Sarah. (2025). Multilingual Topic-Specific Article-Extraction and Classification [Data set]. Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/oberbics/Multilingual_Topic-Specific_Article-Extraction_and_Classification."},
	{"name":"BoundingDocs","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/letxbe/BoundingDocs","creator_name":"Letxbe","creator_url":"https://huggingface.co/letxbe","description":"\\n\\nBoundingDocs\\n\\nðŸ” The largest spatially-annotated dataset for Document Question Answering\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBoundingDocs is a unified dataset for Document Question Answering (QA) that includes spatial annotations. It consolidates multiple public datasets from Document AI and Visually Rich Document Understanding (VRDU) domains. The dataset reformulates Information Extraction (IE) tasks into QA tasks, making it a valuable resource for training and evaluating Large Languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/letxbe/BoundingDocs."},
	{"name":"vdr-multilingual-train","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llamaindex/vdr-multilingual-train","creator_name":"LlamaIndex","creator_url":"https://huggingface.co/llamaindex","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMultilingual Visual Document Retrieval Dataset\\n\\t\\n\\n\\n\\nThis dataset consists of 500k multilingual query image samples, collected and generated from scratch using public internet pdfs. The queries are synthetic and generated using VLMs (gemini-1.5-pro and Qwen2-VL-72B).\\n\\nIt was used to train the vdr-2b-multi-v1 retrieval multimodal, multilingual embedding model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow it was created\\n\\t\\n\\nThis is the entire data pipeline used to create the Italian subset of this dataset. Eachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llamaindex/vdr-multilingual-train."},
	{"name":"vdr-multilingual-test","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llamaindex/vdr-multilingual-test","creator_name":"LlamaIndex","creator_url":"https://huggingface.co/llamaindex","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMultilingual Visual Document Retrieval Benchmarks\\n\\t\\n\\n\\nThis dataset consists of 15 different benchmarks used to initially evaluate the vdr-2b-multi-v1 multimodal retrieval embedding model. These benchmarks allow the testing of multilingual, multimodal retrieval capabilities on text-only, visual-only and mixed page screenshots.\\nEach language subset contains queries and images in that language and is divided into three different categories by the \\\"pagetype\\\" column. Each categoryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llamaindex/vdr-multilingual-test."},
	{"name":"SMPQA","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WueNLP/SMPQA","creator_name":"WÃ¼NLP","creator_url":"https://huggingface.co/WueNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSMPQA (Synthetic Multilingual Plot QA)\\n\\t\\n\\n\\n\\nThe SMPQA evaluation dataset proposed in Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model.\\nSMPQA is composed of synthetic bar plots and pie charts (generated using word lists of different languages) together with questions about those plots.\\nThe datasets aims at providing an initial way of evaluating multilingual OCR capabilities of models in arbritrary languages.\\nThere are two sub-tasks: \\n\\nGrounding text labelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/SMPQA."},
	{"name":"munich-public-services","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/it-at-m/munich-public-services","creator_name":"it@M","creator_url":"https://huggingface.co/it-at-m","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for \\\"Munich Public Services\\\"\\n\\t\\n\\nzur deutschen Dokumentation\\n\\nThis dataset contains information about the services provided to the public by the City of Munich in the form of written articles, corresponding metadata as well as embeddings.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\nThe Munich Public Services dataset contains around 1.400 articles about various public services provided by the City of Munich.\\nNext to the content of the articles, the datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/it-at-m/munich-public-services."},
	{"name":"munich-public-services","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/it-at-m/munich-public-services","creator_name":"it@M","creator_url":"https://huggingface.co/it-at-m","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for \\\"Munich Public Services\\\"\\n\\t\\n\\nzur deutschen Dokumentation\\n\\nThis dataset contains information about the services provided to the public by the City of Munich in the form of written articles, corresponding metadata as well as embeddings.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\nThe Munich Public Services dataset contains around 1.400 articles about various public services provided by the City of Munich.\\nNext to the content of the articles, the datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/it-at-m/munich-public-services."},
	{"name":"lib3m_qa_dataset_v1","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lib3m/lib3m_qa_dataset_v1","creator_name":"Libertarian Padawan","creator_url":"https://huggingface.co/lib3m","description":"\\n\\t\\n\\t\\t\\n\\t\\tLibertarian Large Language Model QA Dataset (Lib3M QAD)\\n\\t\\n\\nVersion: 1.0.0\\nThis repository contains a large-scale Question-Answer (QA) dataset generated from libertarian literature and content. The dataset is designed to help train and fine-tune language models with libertarian economic and philosophical concepts.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of question-answer pairs automatically generated from a curated collection of libertarian books and content. The data isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lib3m/lib3m_qa_dataset_v1."},
	{"name":"degeneration-html-multilingual","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\\n\\t\\n\\t\\t\\n\\t\\tThe Degeneration of the Nation Multilingual Dataset\\n\\t\\n\\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual."},
	{"name":"wmt-da-human-evaluation-long-context","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ymoslem/wmt-da-human-evaluation-long-context","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nLong-context / document-level dataset for Quality Estimation of Machine Translation.\\nIt is an augmented variant of the sentence-level WMT DA Human Evaluation dataset.\\nIn addition to individual sentences, it contains augmentations of 2, 4, 8, 16, and 32 sentences, among each language pair lp and domain.\\nThe raw column represents a weighted average of scores of augmented sentences using character lengths of src and mt as weights.\\nThe code used to apply the augmentationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/wmt-da-human-evaluation-long-context."},
	{"name":"opendata-iisys-hui","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Paradoxia/opendata-iisys-hui","creator_name":"Firat Tay","creator_url":"https://huggingface.co/Paradoxia","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\tHUI-Audio-Corpus-German Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThe HUI-Audio-Corpus-German is a high-quality Text-To-Speech (TTS) dataset developed by researchers at the Institute of Information Systems (IISYS). This dataset is designed to facilitate the development and training of TTS applications, particularly in the German language. The associated research paper can be found here.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Contents\\n\\t\\n\\nThe dataset comprises recordings from multiple speakers, with the five mostâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Paradoxia/opendata-iisys-hui."},
	{"name":"imatrix-calibration","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/eaddario/imatrix-calibration","creator_name":"Ed Addario","creator_url":"https://huggingface.co/eaddario","description":"\\n\\t\\n\\t\\t\\n\\t\\tImportance Matrix (imatrix) calibration datasets\\n\\t\\n\\nThis dataset consists of over 10M tokens of cleaned and de-duplicated text files for 13 different languages. Each language file is available in five sizes, ranging from large (~ 26,000 lines equivalent to approx. 750K tokens), to micro (~ 1,625 lines and 125K tokens avg).\\nOriginal data sourced from HuggingFaceFW/fineweb and HuggingFaceFW/fineweb-2\\n\\n\\t\\n\\t\\t\\nFile\\nLanguage\\nLines\\nSize\\n\\n\\n\\t\\t\\ncalibration_ar_large\\nArabic\\n26,000\\n3.1Mâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/eaddario/imatrix-calibration."},
	{"name":"open_government","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AgentPublic/open_government","creator_name":"AgentPublic","creator_url":"https://huggingface.co/AgentPublic","description":"\\n\\t\\n\\t\\t\\n\\t\\tOpen Government Dataset\\n\\t\\n\\nOpen Government is the largest agregation of governement text and data made available as part of open data programs. \\nIn total, the dataset contains approximately 380B tokens. While Open Government aims to become a global resource, in its current state it mostly features open datasets from the US, France, European and international organizations.\\nThe dataset comprises 16 collections curated through two different initiaties: Finance commons and Legal commons.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AgentPublic/open_government."},
	{"name":"Synthdog-Multilingual-100","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WueNLP/Synthdog-Multilingual-100","creator_name":"WÃ¼NLP","creator_url":"https://huggingface.co/WueNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\tSynthdog Multilingual\\n\\t\\n\\n\\n\\nThe Synthdog dataset created for training in Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model.\\nUsing the official Synthdog code, we created >1 million training samples for improving OCR capabilities in Large Vision-Language Models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nWe provide the images for download in two .tar.gz files. Download and extract them in folders of the same name (so cat images.tar.gz.* | tar xvzf -C images; tar xvzfâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/Synthdog-Multilingual-100."},
	{"name":"BenchMAX_Math","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Math","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\\nLink: https://arxiv.org/pdf/2502.07346\\nRepository: https://github.com/CONE-MT/BenchMAX\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBenchMAX_Math is a dataset of BenchMAX, sourcing from MGSM.\\nWe extend the original dataset to 6 more languages.\\nThe data is first translated by Google Translate, and then post-editing by native speakers.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Languages\\n\\t\\n\\nArabic, Bengali, Chineseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Math."},
	{"name":"BenchMAX_Science","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Science","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\\nLink: https://arxiv.org/pdf/2502.07346\\nRepository: https://github.com/CONE-MT/BenchMAX\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBenchMAX_Science is a dataset of BenchMAX, sourcing from GPQA.\\nWe extend the original English dataset to 16 non-English languages.\\nThe data is first translated by Google Translate, and then post-editing by native speakers.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Languages\\n\\t\\n\\nArabicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Science."},
	{"name":"BenchMAX_Function_Completion","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Function_Completion","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\\nLink: https://arxiv.org/pdf/2502.07346\\nRepository: https://github.com/CONE-MT/BenchMAX\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBenchMAX_Function_Completion is a dataset of BenchMAX, sourcing from humanevalplus.\\nWe extend the original English dataset to 16 non-English languages.\\nThe data is first translated by GPT-4o, and then post-editing by native speakers.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Languagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Function_Completion."},
	{"name":"BenchMAX_Problem_Solving","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Problem_Solving","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\\nLink: https://arxiv.org/pdf/2502.07346\\nRepository: https://github.com/CONE-MT/BenchMAX\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBenchMAX_Problem_Solving is a dataset of BenchMAX, sourcing from LiveCodeBench.\\nWe extend the original English dataset to 16 non-English languages.\\nThe data is first translated by GPT-4o, and then post-editing by native speakers.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Languagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Problem_Solving."},
	{"name":"Retrieval-SFT-Chat","keyword":"german","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat","creator_name":"CausalLM","creator_url":"https://huggingface.co/CausalLM","description":"\\n\\t\\n\\t\\t\\n\\t\\tRetrieval-Based Multi-Turn Chat SFT Synthetic Data\\n\\t\\n\\nA year ago, we released CausalLM/Refined-Anime-Text, a thematic subset of a dataset generated using the then state-of-the-art LLMs. This dataset comprises 1 million entries synthesized through long-context models that rewrote multi-document web text inputs, intended for continued pre-training. We are pleased to note that this data has been employed in various training scenarios and in studies concerning data and internet culture.\\nInâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CausalLM/Retrieval-SFT-Chat."},
	{"name":"DATA-AI_Chat","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mattimax/DATA-AI_Chat","creator_name":"M.INC.","creator_url":"https://huggingface.co/Mattimax","description":"\\n\\t\\n\\t\\t\\n\\t\\tDATA-AI: Il Modello di IA di M.INC.\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tðŸ“Œ Introduzione\\n\\t\\n\\nDATA-AI Ã¨ un avanzato modello di intelligenza artificiale sviluppato da *M.INC., un'azienda italiana fondata da *Mattimax (M. Marzorati).Questo modello Ã¨ basato sull'architettura ELNS (Elaborazione del Linguaggio Naturale Semplice), un sistema innovativo progettato per rendere l'IA accessibile su quasi qualsiasi dispositivo, garantendo prestazioni ottimali anche su hardware limitato.  \\nDATA-AI Ã¨ stato addestrato su unâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mattimax/DATA-AI_Chat."},
	{"name":"ea-mt-benchmark","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sapienzanlp/ea-mt-benchmark","creator_name":"Sapienza NLP, Sapienza University of Rome","creator_url":"https://huggingface.co/sapienzanlp","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for EA-MT\\n\\t\\n\\nEA-MT (Entity-Aware Machine Translation) is a multilingual benchmark for evaluating the capabilities of Large Language Models (LLMs) and Machine Translation (MT) models in translating simple sentences with potentially challenging entity mentions, e.g., entities for which a word-for-word translation may not be accurate.\\nHere is an example of a simple sentence with a challenging entity mention:\\n\\nEnglish: \\\"What is the plot of The Catcher in the Rye?\\\"\\nItalian:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/sapienzanlp/ea-mt-benchmark."},
	{"name":"u-sticker","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/metchee/u-sticker","creator_name":"Metilda Chee","creator_url":"https://huggingface.co/metchee","description":"\\n\\t\\n\\t\\t\\n\\t\\tU-Sticker\\n\\t\\n\\nUser-Sticker is a stickers dataset with multi-domain conversations.\\nFeatures of U-Sticker:\\n\\nMulti-domain interactions âœ…\\nTemporal âœ…\\nUser information âœ…\\n370.2k stickers âœ… (104k unique)\\n22.6k users âœ…\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nU-Sticker contains three files:\\n\\nConversation files: 1 to 67.json\\nDomain mapping files idx_to_domain.txt.\\nSticker files.\\n\\n\\nSticker files are available here and Baidu Cloud.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tConversation file\\n\\t\\n\\n\\nEmpty lines areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/metchee/u-sticker."},
	{"name":"high-quality-multilingual-sentences","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/high-quality-multilingual-sentences","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\tHigh Quality Multilingual Sentences\\n\\t\\n\\n\\nThis dataset contains multilingual sentences derived from the agentlans/LinguaNova dataset.\\nIt includes 1.58 million rows across 51 different languages, each in its own configuration.\\n\\nExample row (from the all config):\\n{\\n    \\\"text\\\": \\\"Ø§Ù…Ø§Ù… Ø¬Ù…Ø¹Ù‡ Ø§ØµÙÙ‡Ø§Ù† Ú¯ÙØª: Ù…ÛŒØ²Ø§Ù† Ù†ÛŒØ§Ø² Ø¢Ø¨ Ø´Ø±Ø¨ Ø§ØµÙÙ‡Ø§Ù† Û±Û±.Ûµ Ù…ØªØ± Ù…Ú©Ø¹Ø¨ Ø§Ø³Øª Ú©Ù‡ ØªÙ…Ø§Ù… Ø§Ø³ØªØ§Ù† Ø§ØµÙÙ‡Ø§Ù† Ø±Ø§ Ù¾ÙˆØ´Ø´ Ù…ÛŒØ¯Ù‡Ø¯ Ùˆ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ø§Ù†Ù‚Ù„Ø§Ø¨ ÛŒÚ©ÛŒ Ø§Ø² Ù¾ÛŒØ´Ø±ÙØªÙ‡Ø§ Ø¯Ø± Ø­ÙˆØ²Ù‡ Ø¢Ø¨ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª.\\\",\\n    \\\"fasttext\\\": \\\"fa\\\",\\n    \\\"gcld3\\\": \\\"fa\\\"\\n}\\n\\nFields:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/high-quality-multilingual-sentences."},
	{"name":"Open-R1-Mulitlingual-SFT","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/amphora/Open-R1-Mulitlingual-SFT","creator_name":"GUIJIN SON","creator_url":"https://huggingface.co/amphora","description":"\\n\\t\\n\\t\\t\\n\\t\\tOpen-R1-Mulitlingual-SFT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nOpen-R1-Mulitlingual-SFT is a curated dataset designed for multilingual supervised fine-tuning.\\nThe source data comprises multiple datasets containing original prompts and responses, which were subsequently translated into 14 languages using GPT-4o.\\n\\n\\t\\n\\t\\t\\n\\t\\tSources\\n\\t\\n\\nThe dataset is derived from:\\n\\nopen-thoughts/OpenThoughts-114kHugging Face: open-thoughts/OpenThoughts-114k\\nbespokelabs/Bespoke-Stratos-17kHugging Face:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/amphora/Open-R1-Mulitlingual-SFT."},
	{"name":"klexikon","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FabianToSpace/klexikon","creator_name":"Fabian Haeger","creator_url":"https://huggingface.co/FabianToSpace","description":"FabianToSpace/klexikon dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"wikis","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/wikis","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Public MediaWiki Collection\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains 1,662,448 articles harvested from 930 random public MediaWiki instances found across the Internet. The collection was created by extracting current page content from these wikis, preserving article text, metadata, and structural information. The dataset represents a diverse cross-section of public wiki content spanning multiple domains, topics, and languages.\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguages\\n\\t\\n\\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/wikis."},
	{"name":"wikipedia_quality_wikirank","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank","creator_name":"WÅ‚odzimierz Lewoniewski","creator_url":"https://huggingface.co/lewoniewski","description":"Datasets with WikiRank quality score as of 1 August 2024 for 47 million Wikipedia articles in 55 language versions (also simplified version for each language in separate files).\\nThe WikiRank quality score is a metric designed to assess the overall quality of a Wikipedia article. Although its specific algorithm can vary depending on the implementation, the score typically combines several key features of the Wikipedia article.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy Itâ€™s Important\\n\\t\\n\\n\\nEnhances Trust: For readers andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank."},
	{"name":"Thinking-multilingual-big-10k-sft","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/Thinking-multilingual-big-10k-sft","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"\\nA dataset based off of openo1 math, 500 examples translated to 23 different languages. filtered out un-translated examples.\\nenjoy ðŸ‘\\n"},
	{"name":"story_writing_benchmark","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lars1234/story_writing_benchmark","creator_name":"Lars Nieradzik","creator_url":"https://huggingface.co/lars1234","description":"\\n\\t\\n\\t\\t\\n\\t\\tStory Evaluation Dataset\\n\\t\\n\\n\\nThis dataset contains stories generated by Large Language Models (LLMs) across multiple languages, with comprehensive quality evaluations. It was created to train and benchmark models specifically on creative writing tasks.\\nThis benchmark evaluates an LLM's ability to generate high-quality short stories based on simple prompts like \\\"write a story about X with n words.\\\" It is similar to TinyStories but targets longer-form and more complex content, focusingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lars1234/story_writing_benchmark."},
	{"name":"multilingual_translation_sft","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Youseff1987/multilingual_translation_sft","creator_name":"JOON HYOUNG JUN","creator_url":"https://huggingface.co/Youseff1987","description":"Youseff1987/multilingual_translation_sft dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"m-WildVision","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/m-WildVision","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for m-WildVision\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\nThe m-WildVision dataset is a multilingual multimodal LLM evaluation set covering 23 languages.  It was created by translating prompts from the original English-only WildVision (vision_bench_0617) test set. \\nThe original prompts, developed by Lu et al. (2024) , consist of 500 challenging user queries sourced from the WildVision-Arena platform. \\nThe authors demonstrated that these prompts enable automatic LLM judgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/m-WildVision."},
	{"name":"OpenHumanreasoning-multilingual-2.2k","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/OpenHumanreasoning-multilingual-2.2k","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"Based off of Pinkstackorg/humanreasoning-testing-en, it is a human-generated dataset where a real person had to create most of the reasoning and the final output. If there are any mistakes please let us know.\\nWe offer this dataset at an apache-2.0 license to make it useful for everybody.\\nnote: translations are not human generated.\\n"},
	{"name":"AtomicGPT2-data","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Atomic-Ai/AtomicGPT2-data","creator_name":"Atomic Ai Studios","creator_url":"https://huggingface.co/Atomic-Ai","description":"Atomic-Ai/AtomicGPT2-data dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"BRIGHTER-emotion-categories","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/brighter-dataset/BRIGHTER-emotion-categories","creator_name":"BRIGHTER Dataset","creator_url":"https://huggingface.co/brighter-dataset","description":"\\n\\t\\n\\t\\t\\n\\t\\tBRIGHTER Emotion Categories Dataset\\n\\t\\n\\nThis dataset contains the emotion categories data from the BRIGHTER paper: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe BRIGHTER Emotion Categories dataset is a comprehensive multi-language, multi-label emotion classification dataset with separate configurations for each language. It represents one of the largest human-annotated emotion datasets across multipleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/brighter-dataset/BRIGHTER-emotion-categories."},
	{"name":"reasoning-conversations","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/syntaxsynth/reasoning-conversations","creator_name":"SyntaxSynth","creator_url":"https://huggingface.co/syntaxsynth","description":"\\n\\t\\n\\t\\t\\n\\t\\tMultilingual Reasoning Dataset\\n\\t\\n\\n\\nInclude languages from German, Korean, Spanish, Japanese, French, Simplified Chinese, Traditional Chinese\\n\\nReasoning traces from Deepseek-v3-R1, Deepseek-v3-R1-Zero\\n\\n\\nCredits sponsored by Currents API\\n"},
	{"name":"MLAAD","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mueller91/MLAAD","creator_name":"Nicolas MÃ¼ller","creator_url":"https://huggingface.co/mueller91","description":"\\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tIntroduction\\n\\t\\n\\nWelcome to MLAAD: The Multi-Language Audio Anti-Spoofing Dataset -- a dataset to train, test and evaluate audio deepfake detection. See\\nthe paper for more information.\\n\\n\\t\\n\\t\\t\\n\\t\\tStructure\\n\\t\\n\\nThe dataset is based on the M-AILABS dataset.\\nMLAAD is structured as follows:\\nfake\\n|-language_1\\n|-language_2\\n|- ....\\n|- language_K\\n    | - model_1_K\\n    | - model_2_K\\n    | - ....\\n    | - model_L_K\\n        | - meta.csv\\n        | - audio_L_K_1.wav\\n        | - audio_L_K_2.wavâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mueller91/MLAAD."},
	{"name":"blbooksgenre","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TheBritishLibrary/blbooksgenre","creator_name":"British Library","creator_url":"https://huggingface.co/TheBritishLibrary","description":"This dataset contains metadata for resources belonging to the British Libraryâ€™s digitised printed books (18th-19th century) collection (bl.uk/collection-guides/digitised-printed-books).\\nThis metadata has been extracted from British Library catalogue records.\\nThe metadata held within our main catalogue is updated regularly.\\nThis metadata dataset should be considered a snapshot of this metadata."},
	{"name":"bnl_newspapers","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bnl-data/bnl_newspapers","creator_name":"BnL Open Data","creator_url":"https://huggingface.co/bnl-data","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for BnL Historical Newspapers\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe BnL has digitised over 800.000 pages of Luxembourg newspapers. This dataset currently has one configuration covering a subset of these newspapers, which sit under the \\\"Processed Datasets\\\" collection. The BNL:\\n\\nprocessed all newspapers and monographs that are in the public domain and extracted the full text and associated meta data of every single article, section, advertisementâ€¦ The result is a largeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bnl-data/bnl_newspapers."},
	{"name":"euronews","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/community-datasets/euronews","creator_name":"Community Datasets","creator_url":"https://huggingface.co/community-datasets","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Europeana Newspapers\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCuration Rationaleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/euronews."},
	{"name":"europa_eac_tm","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/community-datasets/europa_eac_tm","creator_name":"Community Datasets","creator_url":"https://huggingface.co/community-datasets","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Europa Education and Culture Translation Memory (EAC-TM)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a corpus of manually produced translations from english to up to 25 languages, released in 2012 by the European Union's Directorate General for Education and Culture (EAC).\\nTo load a language pair that is not part of the config, just specify the language code as language pair. For example, if you want to translate Czech to Greek:\\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/europa_eac_tm."},
	{"name":"europa_ecdc_tm","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/community-datasets/europa_ecdc_tm","creator_name":"Community Datasets","creator_url":"https://huggingface.co/community-datasets","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for [Dataset Name]\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nIn October 2012, the European Union (EU) agency 'European Centre for Disease Prevention and Control' (ECDC) released a translation memory (TM), i.e. a collection of sentences and their professionally produced translations, in twenty-five languages.\\nECDC-TM covers 25 languages: the 23 official languages of the EU plus Norwegian (Norsk) and Icelandic. ECDC-TM was created by translating from English into the followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/europa_ecdc_tm."},
	{"name":"opus_paracrawl","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Helsinki-NLP/opus_paracrawl","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for OpusParaCrawl\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nParallel corpora from Web Crawls collected in the ParaCrawl project.\\nTha dataset contains:\\n\\n42 languages, 43 bitexts\\ntotal number of files: 59,996\\ntotal number of tokens: 56.11G\\ntotal number of sentence fragments: 3.13G\\n\\nTo load a language pair which isn't part of the config, all you need to do is specify the language code as pairs,\\ne.g.\\ndataset = load_dataset(\\\"opus_paracrawl\\\", lang1=\\\"en\\\", lang2=\\\"so\\\")\\n\\nYou can findâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Helsinki-NLP/opus_paracrawl."},
	{"name":"opus_ubuntu","keyword":"german","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/Helsinki-NLP/opus_ubuntu","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Opus Ubuntu\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThese are translations of the Ubuntu software package messages, donated by the Ubuntu community.\\nTo load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.\\nYou can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/Ubuntu.php\\nE.g.\\ndataset = load_dataset(\\\"opus_ubuntu\\\", lang1=\\\"it\\\", lang2=\\\"pl\\\")\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Helsinki-NLP/opus_ubuntu."},
	{"name":"xcsr","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/INK-USC/xcsr","creator_name":"INK Lab @ USC","creator_url":"https://huggingface.co/INK-USC","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for X-CSR\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTo evaluate multi-lingual language models (ML-LMs) for commonsense reasoning in a cross-lingual zero-shot transfer setting (X-CSR), i.e., training in English and test in other languages, we create two benchmark datasets, namely X-CSQA and X-CODAH. Specifically, we automatically translate the original CSQA and CODAH datasets, which only have English versions, to 15 other languages, forming development and test sets forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/INK-USC/xcsr."},
	{"name":"xquad_r","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google-research-datasets/xquad_r","creator_name":"Google Research Datasets","creator_url":"https://huggingface.co/google-research-datasets","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for [Dataset Name]\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nXQuAD-R is a retrieval version of the XQuAD dataset (a cross-lingual extractive\\nQA dataset). Like XQuAD, XQUAD-R is an 11-way parallel dataset,  where each\\nquestion appears in 11 different languages and has 11 parallel correct answers\\nacross the languages.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe dataset can be found with the following languages:\\n\\nArabic:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/google-research-datasets/xquad_r."},
	{"name":"RotoWire_English-German","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/RotoWire_English-German","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"Dataset for the WNGT 2019 DGT shared task on \\\"Document-Level Generation and Translationâ€."},
	{"name":"mobie","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DFKI-SLT/mobie","creator_name":"Speech and Language Technology, DFKI","creator_url":"https://huggingface.co/DFKI-SLT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"MobIE\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis script is for loading the MobIE dataset from https://github.com/dfki-nlp/mobie. \\nMobIE is a German-language dataset which is human-annotated with 20 coarse- and fine-grained entity types and entity linking information for geographically linkable entities. The dataset consists of 3,232 social media texts and traffic reports with 91K tokens, and contains 20.5K annotated entities, 13.1K of which are linked to a knowledgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DFKI-SLT/mobie."},
	{"name":"flores_101","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gsarti/flores_101","creator_name":"Gabriele Sarti","creator_url":"https://huggingface.co/gsarti","description":"One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the \\nlack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource \\nlanguages, consider only restricted domains, or are low quality because they are constructed using \\nsemi-automatic procedures. In this work, we introduce the FLORES evaluation benchmark, consisting of 3001 \\nsentences extracted from English Wikipedia and covering a variety of different topics and domains. \\nThese sentences have been translated in 101 languages by professional translators through a carefully \\ncontrolled process. The resulting dataset enables better assessment of model quality on the long tail of \\nlow-resource languages, including the evaluation of many-to-many multilingual translation systems, as all \\ntranslations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, \\nwe hope to foster progress in the machine translation community and beyond."},
	{"name":"Tilde-MODEL-Catalan","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/softcatala/Tilde-MODEL-Catalan","creator_name":"SoftcatalÃ ","creator_url":"https://huggingface.co/softcatala","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Tilde-MODEL-Catalan\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains the German version of the Tilde-MODEL corpus aligned with a Catalan translation.\\nThe catalan text has been obtained using Apertium's RBMT system from the Spanish version. It contains 3.4M segments.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be used to train NMT and SMT systems.\\nIt has been used as a training corpus for the SoftcatalÃ  machine translation engine.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/softcatala/Tilde-MODEL-Catalan."},
	{"name":"xlel_wd_dictionary","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/adithya7/xlel_wd_dictionary","creator_name":"Adithya Pratapa","creator_url":"https://huggingface.co/adithya7","description":"XLEL-WD is a multilingual event linking dataset. This sub-dataset contains a dictionary of events from Wikidata. The multilingual descriptions for Wikidata event items are taken from the corresponding Wikipedia articles."},
	{"name":"xlel_wd","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/adithya7/xlel_wd","creator_name":"Adithya Pratapa","creator_url":"https://huggingface.co/adithya7","description":"XLEL-WD is a multilingual event linking dataset. This dataset contains mention references from multilingual Wikipedia/Wikinews articles to event items in Wikidata. The text descriptions for Wikidata events are compiled from Wikipedia articles."},
	{"name":"shades_nationality","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bigscience-catalogue-data/shades_nationality","creator_name":"BigScience Catalogue Data","creator_url":"https://huggingface.co/bigscience-catalogue-data","description":"Possibly a placeholder dataset for the original here: https://huggingface.co/datasets/bigscience-catalogue-data/bias-shades\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Statement for SHADES\\n\\t\\n\\n\\nHow to use this document:\\nFill in each section according to the instructions. Give as much detail as you can, but there's no need to extrapolate. The goal is to help people understand your data when they approach it. This could be someone looking at it in ten years, or it could be you yourself looking back at the data in twoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigscience-catalogue-data/shades_nationality."},
	{"name":"wit_base","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wikimedia/wit_base","creator_name":"Wikimedia","creator_url":"https://huggingface.co/wikimedia","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for WIT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWikimedia's version of the Wikipedia-based Image Text (WIT) Dataset, a large multimodal multilingual dataset.\\nFrom the official blog post:\\n\\nThe core training data is taken from the Wikipedia Image-Text (WIT) Dataset, a large curated set of more than 37 million image-text associations extracted from Wikipedia articles in 108 languages that was recently released by Google Research.\\nThe WIT dataset offers extremely valuable data about theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wikimedia/wit_base."},
	{"name":"x-stance","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/strombergnlp/x-stance","creator_name":"StrÃ¸mberg NLP","creator_url":"https://huggingface.co/strombergnlp","description":"The x-stance dataset contains more than 150 political questions, and 67k comments written by candidates on those questions. The comments are partly German, partly French and Italian. The data have been extracted from the Swiss voting advice platform Smartvote."},
	{"name":"bucc-bitext-mining","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/bucc-bitext-mining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MTEB Benchmark\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMTEB is a heterogeneous benchmark that has been built from diverse tasks:\\n\\nBitextMining: BUCC, Tatoeba\\nClassification: AmazonCounterfactualClassification, AmazonPolarityClassification, AmazonReviewsClassification, Banking77Classification, EmotionClassification, ImdbClassification, MassiveIntentClassification, MassiveScenarioClassification, MTOPDomainClassification, MTOPIntentClassificationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/bucc-bitext-mining."},
	{"name":"sv-ident","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vadis/sv-ident","creator_name":"VAriable Detection, Interlinking and Summarization project","creator_url":"https://huggingface.co/vadis","description":"The SV-Ident corpus (version 0.3) is a collection of 4,248 expert-annotated English\\nand German sentences from social science publications, supporting the task of\\nmulti-label text classification."},
	{"name":"german_argument_mining","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/joelniklaus/german_argument_mining","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Annotated German Legal Decision Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset consists of 200 randomly chosen judgments. In these judgments a legal expert annotated the components\\nconclusion, definition and subsumption of the German legal writing style Urteilsstil.\\n\\\"Overall 25,075 sentences are annotated. 5% (1,202) of these sentences are marked as conclusion, 21% (5,328) as\\ndefinition, 53% (13,322) are marked as subsumption and the remaining 21% (6,481) asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/joelniklaus/german_argument_mining."},
	{"name":"hatecheck-german","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Paul/hatecheck-german","creator_name":"Paul RÃ¶ttger","creator_url":"https://huggingface.co/Paul","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Multilingual HateCheck\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nMultilingual HateCheck (MHC) is a suite of functional tests for hate speech detection models in 10 different languages: Arabic, Dutch, French, German, Hindi, Italian, Mandarin, Polish, Portuguese and Spanish.\\nFor each language, there are 25+ functional tests that correspond to distinct types of hate and challenging non-hate.\\nThis allows for targeted diagnostic insights into model performance.\\nFor moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Paul/hatecheck-german."},
	{"name":"wino_x","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/demelin/wino_x","creator_name":"Denis Emelin","creator_url":"https://huggingface.co/demelin","description":"Wino-X is a parallel dataset of German, French, and Russian Winograd schemas, aligned with their English \\ncounterparts, used to examine whether neural machine translation models can perform coreference resolution that \\nrequires commonsense knowledge and whether multilingual language models are capable of commonsense reasoning across \\nmultiple languages."},
	{"name":"mapa","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/joelniklaus/mapa","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Multilingual European Datasets for Sensitive Entity Detection in the Legal Domain\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe dataset consists of 12 documents (9 for Spanish due to parsing errors) taken from EUR-Lex, a multilingual corpus of court\\ndecisions and legal dispositions in the 24 official languages of the European Union. The documents have been annotated\\nfor named entities following the guidelines of the MAPA project which foresees two\\nannotation level, aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/joelniklaus/mapa."},
	{"name":"sbb-dc-ocr","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SBB/sbb-dc-ocr","creator_name":"Staatsbibliothek zu Berlin - PreuÃŸischer Kulturbesitz","creator_url":"https://huggingface.co/SBB","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Berlin State Library OCR data\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nThe digital collections of the SBB contain 153,942 digitized works from the time period of 1470 to 1945.\\n\\n\\nAt the time of publication, 28,909 works have been OCR-processed resulting in 4,988,099 full-text pages.\\nFor each page with OCR text, the language has been determined by langid (Lui/Baldwin 2012).\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\nlanguage-modeling: this dataset has the potentialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SBB/sbb-dc-ocr."},
	{"name":"lextreme","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/joelniklaus/lextreme","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","description":"The LEXTREME Benchmark is a collection of multilingual datasets for evaluating model performance \\nacross a diverse set of legal NLU tasks."},
	{"name":"mc4_legal","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/joelniklaus/mc4_legal","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MC4_Legal: A Corpus Covering the Legal Part of MC4 for European Languages\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains large text resources (~133GB in total) from mc4 filtered for legal data that can be used for pretraining language models.\\nUse the dataset like this:\\nfrom datasets import load_dataset\\ndataset = load_dataset(\\\"joelito/mc4_legal\\\", \\\"de\\\", split='train', streaming=True)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThe dataset supports theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/joelniklaus/mc4_legal."},
	{"name":"HashtagPrediction","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Twitter/HashtagPrediction","creator_name":"Twitter","creator_url":"https://huggingface.co/Twitter","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHashtag Prediction Dataset from paper TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations\\n\\t\\n\\n  \\nThis repo contains the Hashtag prediction dataset from our paper TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations. \\n[arXiv]\\n[HuggingFace Models]\\n[Github repo]\\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDownload\\n\\t\\n\\nUse theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Twitter/HashtagPrediction."},
	{"name":"saf_legal_domain_german","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Short-Answer-Feedback/saf_legal_domain_german","creator_name":"Short Answer Feedback Interest Group","creator_url":"https://huggingface.co/Short-Answer-Feedback","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"saf_legal_domain_german\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis Short Answer Feedback (SAF) dataset contains 19 German questions in the domain of the German social law (with reference answers). The idea of constructing a bilingual (English and German) short answer dataset as a way to remedy the lack of content-focused feedback datasets was introduced in Your Answer is Incorrect... Would you like to know why? Introducing aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Short-Answer-Feedback/saf_legal_domain_german."},
	{"name":"saf_micro_job_german","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Short-Answer-Feedback/saf_micro_job_german","creator_name":"Short Answer Feedback Interest Group","creator_url":"https://huggingface.co/Short-Answer-Feedback","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"saf_micro_job_german\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nShort Answer Feedback (SAF) dataset is a short answer dataset introduced in Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset (Filighera et al., ACL 2022) as a way to remedy the lack of content-focused feedback datasets. This version of the dataset contains 8 German questions used in micro-job training on the crowd-worker platform appJobber - whileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Short-Answer-Feedback/saf_micro_job_german."},
	{"name":"bnl_newspapers1841-1879","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/biglam/bnl_newspapers1841-1879","creator_name":"BigLAM: BigScience Libraries, Archives and Museums","creator_url":"https://huggingface.co/biglam","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for BnL Newspapers 1841-1881\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n592.192 articles from historical newspapers (1841-1881) along with metadata and the full text.\\n21 newspaper titles\\n24.415 newspaper issues\\n99.957 scanned pages\\nTranscribed using a variety of OCR engines and corrected using https://github.com/natliblux/nautilusocr (95% threshold)\\nPublic Domain, CC0 (See copyright notice)\\nThe newspapers used are:\\n\\nDer Arbeiter (1878-1881)\\nL'Arlequin (1848-1848)\\nL'Avenirâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/biglam/bnl_newspapers1841-1879."},
	{"name":"MultiLegalPile_Wikipedia_Filtered","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/joelniklaus/MultiLegalPile_Wikipedia_Filtered","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","description":"A filtered version of the MultiLegalPile dataset, together with wikipedia articles."},
	{"name":"EU_Wikipedias","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/joelniklaus/EU_Wikipedias","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","description":"Wikipedia dataset containing cleaned articles of all languages.\\nThe datasets are built from the Wikipedia dump\\n(https://dumps.wikimedia.org/) with one split per language. Each example\\ncontains the content of one full Wikipedia article with cleaning to strip\\nmarkdown and unwanted sections (references, etc.)."},
	{"name":"ger-backtrans-paraphrase","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/deutsche-telekom/ger-backtrans-paraphrase","creator_name":"Deutsche Telekom AG","creator_url":"https://huggingface.co/deutsche-telekom","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman Backtranslated Paraphrase Dataset\\n\\t\\n\\nThis is a dataset of more than 21 million German paraphrases.\\nThese are text pairs that have the same meaning but are expressed with different words.\\nThe source of the paraphrases are different parallel German / English text corpora.\\nThe English texts were machine translated back into German to obtain the paraphrases.\\nThis dataset can be used for example to train semantic text embeddings.\\nTo do this, for example, SentenceTransformers\\nandâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deutsche-telekom/ger-backtrans-paraphrase."},
	{"name":"tweetyface_debug","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ML-Projects-Kiel/tweetyface_debug","creator_name":"Machine Learning Projects FH Kiel","creator_url":"https://huggingface.co/ML-Projects-Kiel","description":"DEBUG DATASET"},
	{"name":"NLU-Evaluation-Data-en-de","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/deutsche-telekom/NLU-Evaluation-Data-en-de","creator_name":"Deutsche Telekom AG","creator_url":"https://huggingface.co/deutsche-telekom","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNLU Evaluation Data - English and German\\n\\t\\n\\nA labeled English and German language multi-domain dataset (21 domains) with 25K user utterances for human-robot interaction.\\nThis dataset is collected and annotated for evaluating NLU services and platforms.\\nThe detailed paper on this dataset can be found at arXiv.org:\\nBenchmarking Natural Language Understanding Services for building Conversational Agents\\nThe dataset builds on the annotated data of the xliuhw/NLU-Evaluation-Dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deutsche-telekom/NLU-Evaluation-Data-en-de."},
	{"name":"fashion-captions-de","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jinaai/fashion-captions-de","creator_name":"Jina AI","creator_url":"https://huggingface.co/jinaai","description":"\\n\\n\\n\\n\\n\\n\\nThe data offered by Jina AI, Finetuner team.\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSummary\\n\\t\\n\\nThis dataset is a German-language dataset based on the Fashion12K dataset, which originally contains both English and German text descriptions for each item.\\nThis dataset was used to to finetuner CLIP using the Finetuner tool.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFine-tuning\\n\\t\\n\\nPlease refer to our documentation: Multilingual Text-to-Image Search with MultilingualCLIP\\nand blog Improving Search Quality for Non-English Queries with Fine-tunedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jinaai/fashion-captions-de."},
	{"name":"mdk_gov_data_titles_clf","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/and-effect/mdk_gov_data_titles_clf","creator_name":"&effect data solutions GmbH","creator_url":"https://huggingface.co/and-effect","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MDK\\n\\t\\n\\nThis dataset was created as part of the Bertelsmann Foundation's \\nMusterdatenkatalog (MDK) project. The MDK provides an overview of Open Data in municipalities in Germany. It is intended to help municipalities in Germany, as well as data analysts and journalists, to get an overview of the topics and the extent to which cities have already published data sets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe dataset is an annotated corpusâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/and-effect/mdk_gov_data_titles_clf."},
	{"name":"multilingual-gec","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/juancavallotti/multilingual-gec","creator_name":"Juan Alberto Lopez Cavallotti","creator_url":"https://huggingface.co/juancavallotti","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Multilingual Grammar Error Correction\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset can be used to train a transformer model (we used T5) to correct grammar errors in simple sentences written in English, Spanish, French, or German. \\nThis dataset was developed as a component for the Squidigies platform.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\nGrammar Error Correction: By appending the prefix fix grammar: to the prrompt.\\nLanguage Detection: By appending theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/juancavallotti/multilingual-gec."},
	{"name":"german_municipal_coat_of_arms","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/johko/german_municipal_coat_of_arms","creator_name":"Johannes Kolbe","creator_url":"https://huggingface.co/johko","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman Municipal Coat of Arms Dataset\\n\\t\\n\\nThis dataset contains 13104 samples for German municipal coat of arms.\\nEach sample consists of the following features: 'img', 'acceptance', 'municipality', 'description', 'id', historicalJustification', 'municipalityName', 'uri', 'figure', 'cancellation', 'cancellationReason', 'author'\\n"},
	{"name":"swiss_legislation","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rcds/swiss_legislation","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Swiss Legislation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSwiss Legislation is a multilingual, diachronic dataset of 36K Swiss laws. This dataset is part of a challenging Information Retreival task.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe total number of texts in the dataset is 35,698. The dataset is saved in lexfind_v2.jsonl format.\\nSwitzerland has four official languages German, French, Italian and Romanch with some additionalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rcds/swiss_legislation."},
	{"name":"MultiLegalPileWikipediaFiltered","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/joelniklaus/MultiLegalPileWikipediaFiltered","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","description":"A filtered version of the MultiLegalPile dataset, together with wikipedia articles."},
	{"name":"miracl-de-corpus-22-12","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Cohere/miracl-de-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMIRACL (de) embedded with cohere.ai multilingual-22-12 encoder\\n\\t\\n\\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\\nThe query embeddings can be found in Cohere/miracl-de-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-de-corpus-22-12.\\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\\nDataset info:\\n\\nMIRACL ðŸŒðŸ™ŒðŸŒ (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-de-corpus-22-12."},
	{"name":"miracl-de-queries-22-12","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Cohere/miracl-de-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMIRACL (de) embedded with cohere.ai multilingual-22-12 encoder\\n\\t\\n\\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\\nThe query embeddings can be found in Cohere/miracl-de-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-de-corpus-22-12.\\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\\nDataset info:\\n\\nMIRACL ðŸŒðŸ™ŒðŸŒ (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-de-queries-22-12."},
	{"name":"pwesuite-eval","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zouharvi/pwesuite-eval","creator_name":"VilÃ©m Zouhar","creator_url":"https://huggingface.co/zouharvi","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\tPWESuite-Eval\\n\\t\\n\\nDataset composed of multiple smaller datasets used for the evaluation of phonetic word embeddings.\\nSee code for evaluation here.\\nIf you use this dataset/evaluation, please cite the paper at LREC-COLING 2024:\\n@inproceedings{zouhar-etal-2024-pwesuite,\\n    title = \\\"{PWES}uite: Phonetic Word Embeddings and Tasks They Facilitate\\\",\\n    author = \\\"Zouhar, Vil{\\\\'e}m  and\\n      Chang, Kalvin  and\\n      Cui, Chenxuan  and\\n      Carlson, Nate B.  and\\n      Robinson, Nathanielâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zouharvi/pwesuite-eval."},
	{"name":"DreamBank-dreams","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DReAMy-lib/DreamBank-dreams","creator_name":"DReAMy","creator_url":"https://huggingface.co/DReAMy-lib","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDreamBank - Dreams\\n\\t\\n\\nThe dataset is a collection of ~30k textual reports of dreams, originally scraped from the DreamBank databased by \\nmattbierner. The DreamBank reports are divided into series, \\nwhich are collections of individuals or research projects/groups that have gathered the dreams. The vast majority  of the series are in the \\nEnglish language, but a small part of the are in German. These series are indicated by the presence of .de in their name.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DReAMy-lib/DreamBank-dreams."},
	{"name":"Bundestag-v2","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/threite/Bundestag-v2","creator_name":"Thomas Reitenspiess","creator_url":"https://huggingface.co/threite","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Bundestag-v2\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset was generated from the ParlSpeech V2 dataset. It contains speeches from the german parliament from 1990 until 2020 labelled with the party of the speaker.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\nText Classification\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nGerman\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\n\\ntext: Transcript of the speech in german\\nparty: Party of the speaker\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits\\n\\t\\n\\n\\ntrainâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/threite/Bundestag-v2."},
	{"name":"swiss_doc2doc_ir","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rcds/swiss_doc2doc_ir","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","description":"https://huggingface.co/spaces/huggingface/datasets-tagging\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Swiss Doc2doc Information Retrieval\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSwiss Doc2doc Information Retrieval is a multilingual, diachronic dataset of 131K Swiss Federal Supreme Court (FSCS) cases annotated with law citations and ruling citations, posing a challenging text classification task. As unique label we are using decision_id of cited rulings and uuid of cited law articles, which can be found in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rcds/swiss_doc2doc_ir."},
	{"name":"wmt-mqm-human-evaluation","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RicardoRei/wmt-mqm-human-evaluation","creator_name":"Ricardo Costa Dias Rei","creator_url":"https://huggingface.co/RicardoRei","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains all MQM human annotations from previous WMT Metrics shared tasks and the MQM annotations from Experts, Errors, and Context.\\nThe data is organised into 8 columns:\\n\\nlp: language pair\\nsrc: input text\\nmt: translation\\nref: reference translation\\nscore: MQM score\\nsystem: MT Engine that produced the translation\\nannotators: number of annotators\\ndomain: domain of the input text (e.g. news)\\nyear: collection year\\n\\nYou can also find the original dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RicardoRei/wmt-mqm-human-evaluation."},
	{"name":"wmt-sqm-human-evaluation","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RicardoRei/wmt-sqm-human-evaluation","creator_name":"Ricardo Costa Dias Rei","creator_url":"https://huggingface.co/RicardoRei","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nIn 2022, several changes were made to the annotation procedure used in the WMT Translation task. In contrast to the standard DA (sliding scale from 0-100) used in previous years, in 2022 annotators performed DA+SQM (Direct Assessment + Scalar Quality Metric). In DA+SQM, the annotators still provide a raw score between 0 and 100, but also are presented with seven labeled tick marks. DA+SQM helps to stabilize scores across annotators (as compared to DA).\\nThe dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RicardoRei/wmt-sqm-human-evaluation."},
	{"name":"gutenberg_multilang","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sedthh/gutenberg_multilang","creator_name":"Richard Nagyfi","creator_url":"https://huggingface.co/sedthh","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Project Gutenber - Multilanguage eBooks\\n\\t\\n\\nA collection of non-english language eBooks (7907, about 75-80% of all the ES, DE, FR, NL, IT, PT, HU books available on the site) from the Project Gutenberg site with metadata removed. \\nOriginally colected for https://github.com/LAION-AI/Open-Assistant\\n\\n\\t\\n\\t\\t\\nLANG\\nEBOOKS\\n\\n\\n\\t\\t\\nES\\n717\\n\\n\\nDE\\n1735\\n\\n\\nFR\\n2863\\n\\n\\nNL\\n904\\n\\n\\nIT\\n692\\n\\n\\nPT\\n501\\n\\n\\nHU\\n495\\n\\n\\n\\t\\n\\nThe METADATA column contains catalogue meta information on each book as aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sedthh/gutenberg_multilang."},
	{"name":"occlusion_swiss_judgment_prediction","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rcds/occlusion_swiss_judgment_prediction","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","description":"This dataset contains an implementation of occlusion for the SwissJudgmentPrediction task."},
	{"name":"lower_court_insertion_swiss_judgment_prediction","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rcds/lower_court_insertion_swiss_judgment_prediction","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","description":"This dataset contains an implementation of lower court insertion for the SwissJudgmentPrediction task."},
	{"name":"Fact-Completion","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Polyglot-or-Not/Fact-Completion","creator_name":"Polyglot-or-Not","creator_url":"https://huggingface.co/Polyglot-or-Not","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\n\\nHomepage: https://bit.ly/ischool-berkeley-capstone\\nRepository: https://github.com/daniel-furman/Capstone\\nPoint of Contact: daniel_furman@berkeley.edu\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis is the dataset for Polyglot or Not?: Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTest Description\\n\\t\\n\\n Given a factual association such as The capital of France is Paris, we determine whether a model adequately \\\"knows\\\" thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Polyglot-or-Not/Fact-Completion."},
	{"name":"swiss_law_area_prediction","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rcds/swiss_law_area_prediction","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","description":"This dataset contains court decision for law area prediction task."},
	{"name":"swiss_leading_decisions","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rcds/swiss_leading_decisions","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Swiss Leading Decisions\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSwiss Leading Decisions is a multilingual, diachronic dataset of 21K Swiss Federal Supreme Court (FSCS) cases. This dataset is part of a challenging text classification task. We also provide additional metadata as the publication year, the law area and the canton of origin per case, to promote robustness and fairness studies on the critical area of legal NLP.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboardsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rcds/swiss_leading_decisions."},
	{"name":"swiss_criticality_prediction","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rcds/swiss_criticality_prediction","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","description":"This dataset contains Swiss federal court decisions for the legal criticality prediction task"},
	{"name":"snippet-mlsum-500","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/snipaid/snippet-mlsum-500","creator_name":"SnipAId","creator_url":"https://huggingface.co/snipaid","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Snippet-MLSUM-500\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a sample of ~500 news articles from the MLSUM dataset, augmented with machine generated news snippets. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\nThis dataset was created to support the task of generating news snippets such as title, teaser, keywords, serp and tweet for news articles in German language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nde - German\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\ntext: a string feature.title: aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/snipaid/snippet-mlsum-500."},
	{"name":"instruct-snippet-mlsum","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/snipaid/instruct-snippet-mlsum","creator_name":"SnipAId","creator_url":"https://huggingface.co/snipaid","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Instruct-Snippet-MLSUM-500\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis is a dataset for multitask instruction finetuning dataset for the task of news snippet generation. It is built from a sample of ~500 news articles from the MLSUM dataset, augmented with machine generated news snippets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\nThis dataset was created to support the task of generating news snippets such as title, teaser, keywords, serp and tweet for news articles in Germanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/snipaid/instruct-snippet-mlsum."},
	{"name":"snippet-mlsum-500-v2","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/snipaid/snippet-mlsum-500-v2","creator_name":"SnipAId","creator_url":"https://huggingface.co/snipaid","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Snippet-MLSUM-500-V2\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a sample of ~500 news articles from the MLSUM dataset, augmented with machine generated news snippets. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\nThis dataset was created to support the task of generating news snippets such as title, teaser, keywords, serp and tweet for news articles in German language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nde - German\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\ntext: a string feature.title: aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/snipaid/snippet-mlsum-500-v2."},
	{"name":"instruct-snippet-mlsum-v2","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/snipaid/instruct-snippet-mlsum-v2","creator_name":"SnipAId","creator_url":"https://huggingface.co/snipaid","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Instruct-Snippet-MLSUM-500-V2\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis is a dataset for multitask instruction finetuning dataset for the task of news snippet generation. It is built from a sample of ~500 news articles from the MLSUM dataset, augmented with machine generated news snippets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\nThis dataset was created to support the task of generating news snippets such as title, teaser, summary, keywords, serp and tweet for news articles inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/snipaid/instruct-snippet-mlsum-v2."},
	{"name":"rebel-dataset-de","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mingaflo/rebel-dataset-de","creator_name":"Florian","creator_url":"https://huggingface.co/mingaflo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for German REBEL Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is the German version of Babelscape/rebel-dataset. It has been generated using CROCODILE.\\nThe Wikipedia Version is from November 2022. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n\\nGerman\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n{\\\"docid\\\": \\\"9400003\\\",\\n \\\"title\\\": \\\"Odin-Gletscher\\\",\\n \\\"uri\\\": \\\"Q7077818\\\",\\n \\\"text\\\": \\\"Der Odin-Gletscher ist ein kleiner Gletscher im ostantarktischen Viktorialand. Er flieÃŸt von den WesthÃ¤ngen des Mountâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mingaflo/rebel-dataset-de."},
	{"name":"LoLLMS-Open-Community-discussions","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ParisNeo/LoLLMS-Open-Community-discussions","creator_name":"Saifeddine ALOUI","creator_url":"https://huggingface.co/ParisNeo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for GPT4All-Community-Discussions\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains ethically gathered discussions from the community, who shared their experiences with various open source discussion models using the GPT4All-ui tool. The dataset is open for any use, including commercial use, as long as proper citation is given to acknowledge the contributions of the community. \\nThe GPT4All-ui tool allows users to have conversations with various open sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ParisNeo/LoLLMS-Open-Community-discussions."},
	{"name":"iva_mt_wslot-exp","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cartesinus/iva_mt_wslot-exp","creator_name":"Marcin Sowanski","creator_url":"https://huggingface.co/cartesinus","description":""},
	{"name":"swiss_rulings","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rcds/swiss_rulings","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Swiss Rulings\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSwissRulings is a multilingual, diachronic dataset of 637K Swiss Federal Supreme Court (FSCS) cases. This dataset can be used to pretrain language models on Swiss legal data.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nSwitzerland has four official languages with three languages German, French and Italian being represenated. The decisions are written by the judges and clerks in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rcds/swiss_rulings."},
	{"name":"LHM-Dienstleistungen-QA","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/it-at-m/LHM-Dienstleistungen-QA","creator_name":"it@M","creator_url":"https://huggingface.co/it-at-m","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLHM-Dienstleistungen-QA - german public domain question-answering dataset\\n\\t\\n\\nDatasets created based on data from Munich city administration.\\nFormat inspired by GermanQuAD. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAnnotated by:\\n\\t\\n\\n\\n  Institute for Applied Artificial Intelligence: Leon Marius SchrÃ¶der \\n\\n\\n  BettercallPaul GmbH: Clemens Gutknecht, Oubada Alkiddeh, Susanne WeiÃŸ \\n\\n\\n  Stadt MÃ¼nchen: Leon Lukas\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData basis\\n\\t\\n\\nTexts taken from the â€œDienstleistungsfinderâ€œ of the city of Munichâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/it-at-m/LHM-Dienstleistungen-QA."},
	{"name":"DocNMT","keyword":"german","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gshbao/DocNMT","creator_name":"Guangsheng Bao","creator_url":"https://huggingface.co/gshbao","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe benchmark datasets for document-level machine translation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\nDocument-level Machine Translation Tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish-German\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nTED: iwslt17, News: nc2016, Europarl: europarl7\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\nPure text that each line represents a sentence and multiple lines separated by '<d>' line form a document.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/gshbao/DocNMT."},
	{"name":"MDK_taxonomy","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/and-effect/MDK_taxonomy","creator_name":"&effect data solutions GmbH","creator_url":"https://huggingface.co/and-effect","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MDK_taxonomy\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description / Summary\\n\\t\\n\\nThis dataset was created as part of the Bertelsmann Foundation\\nMusterdatenkatalog project. See the project on GitHub here.\\nThe MDK provides an overview of Open Data in municipalities in Germany. \\nThis data contains the taxonomy created by and-effect as part of the project.\\nThe taxonomy adheres to the SKOS standard and is available as RDF and JSON-LD. \\nThere are two levels to the taxonomy: 'Thema' (Topicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/and-effect/MDK_taxonomy."},
	{"name":"ws-semantics-simnrel","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/0x22almostEvil/ws-semantics-simnrel","creator_name":"David Glushkov","creator_url":"https://huggingface.co/0x22almostEvil","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WS353-semantics-sim-and-rel with ~2K entries.\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nLicense: Apache-2.0. Contains CSV of a list of word1, word2, their connection score, type of connection and language.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOriginal Datasets are available here:\\n\\t\\n\\n\\nhttps://leviants.com/multilingual-simlex999-and-wordsim353/\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPaper of original Dataset:\\n\\t\\n\\n\\nhttps://arxiv.org/pdf/1508.00106v5.pdf\\n\\n"},
	{"name":"semantics-ws-qna-oa","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/0x22almostEvil/semantics-ws-qna-oa","creator_name":"David Glushkov","creator_url":"https://huggingface.co/0x22almostEvil","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for semantics-ws-qna-oa with ~2K entries.\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nLicense: Apache-2.0. Contains parquet of INSTRUCTION, RESPONSE, SOURCE and METADATA.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOriginal Datasets are available here:\\n\\t\\n\\n\\nhttps://leviants.com/multilingual-simlex999-and-wordsim353/\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPaper of original Dataset:\\n\\t\\n\\n\\nhttps://arxiv.org/pdf/1508.00106v5.pdf\\n\\n"},
	{"name":"rsd-ists-2016","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZurichNLP/rsd-ists-2016","creator_name":"University of Zurich, Department of Computational Linguistics","creator_url":"https://huggingface.co/ZurichNLP","description":"Training and test data for the task of Recognizing Semantic Differences (RSD).\\nSee the paper for details on how the dataset was created, and see our code at https://github.com/ZurichNLP/recognizing-semantic-differences for an example of how to use the data for evaluation.\\nThe data are derived from the SemEval-2016 Task 2 for Interpretable Semantic Textual Similarity organized by Agirre et al. (2016).\\nThe original URLs of the data are:\\n\\nTrain:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZurichNLP/rsd-ists-2016."},
	{"name":"germanquad_qg_dataset","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fathyshalab/germanquad_qg_dataset","creator_name":"FathÃ½ Shalaby","creator_url":"https://huggingface.co/fathyshalab","description":"fathyshalab/germanquad_qg_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"germanquad_qaeval_qaeval_dataset","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fathyshalab/germanquad_qaeval_qaeval_dataset","creator_name":"FathÃ½ Shalaby","creator_url":"https://huggingface.co/fathyshalab","description":"fathyshalab/germanquad_qaeval_qaeval_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"swiss_citation_extraction","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rcds/swiss_citation_extraction","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","description":"This dataset contains court decision for cit ex task."},
	{"name":"TinyGuanaco_DE","keyword":"german","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mdroth/TinyGuanaco_DE","creator_name":"Matthias Droth","creator_url":"https://huggingface.co/mdroth","description":"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for TinyGuanaco_DE\\n\\t\\n\\nTinyGuanaco_DE\\n\\nis intended for development purposes: use TinyGuanaco_DE for prototyping your code\\nis comprised of German texts only (hence DE)\\nis really small: the train split has 4 instances and the test split has 2 instances\\nhas 3 columns: index, query, and reply\\nthe query column contains concatenations of a context (\\\"Kontext:\\\\n...\\\") and a question (\\\"Frage:\\\\n...\\\") that can be answered by knowing the context\\nthe reply column contains theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mdroth/TinyGuanaco_DE."},
	{"name":"KeyFiTax","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/danielsteinigen/KeyFiTax","creator_name":"Daniel Steinigen","creator_url":"https://huggingface.co/danielsteinigen","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains Key Figures with their properties from german tax acts. The dataset is annotated by tax experts and consists of 85 annotated paragraphs from 14 different German tax acts with 157 annotated tax key figures.\\nThe annotation was performed based on a developed universally applicable annotation schema and a semantic model for key figures and their properties in legal texts.\\nMore details about theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/danielsteinigen/KeyFiTax."},
	{"name":"REDFM","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Babelscape/REDFM","creator_name":"Babelscape","creator_url":"https://huggingface.co/Babelscape","description":"Relation Extraction (RE) is a task that identifies relationships between entities in a text, enabling the acquisition of relational facts and bridging the gap between natural language and structured knowledge. However, current RE models often rely on small datasets with low coverage of relation types, particularly when working with languages other than English. \\\\In this paper, we address the above issue and provide two new resources that enable the training and evaluation of multilingual RE systems.\\nFirst, we present SRED\\\\textsuperscript{FM}, an automatically annotated dataset covering 18 languages, 400 relation types, 13 entity types, totaling more than 40 million triplet instances. Second, we propose RED\\\\textsuperscript{FM}, a smaller, human-revised dataset for seven languages that allows for the evaluation of multilingual RE systems. \\nTo demonstrate the utility of these novel datasets, we experiment with the first end-to-end multilingual RE model, mREBEL, \\nthat extracts triplets, including entity types, in multiple languages. We release our resources and model checkpoints at \\\\href{https://www.github.com/babelscape/rebel}{https://www.github.com/babelscape/rebel}."},
	{"name":"SREDFM","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Babelscape/SREDFM","creator_name":"Babelscape","creator_url":"https://huggingface.co/Babelscape","description":"Relation Extraction (RE) is a task that identifies relationships between entities in a text, enabling the acquisition of relational facts and bridging the gap between natural language and structured knowledge. However, current RE models often rely on small datasets with low coverage of relation types, particularly when working with languages other than English. \\\\In this paper, we address the above issue and provide two new resources that enable the training and evaluation of multilingual RE systems.\\nFirst, we present SRED\\\\textsuperscript{FM}, an automatically annotated dataset covering 18 languages, 400 relation types, 13 entity types, totaling more than 40 million triplet instances. Second, we propose RED\\\\textsuperscript{FM}, a smaller, human-revised dataset for seven languages that allows for the evaluation of multilingual RE systems. \\nTo demonstrate the utility of these novel datasets, we experiment with the first end-to-end multilingual RE model, mREBEL, \\nthat extracts triplets, including entity types, in multiple languages. We release our resources and model checkpoints at \\\\href{https://www.github.com/babelscape/rebel}{https://www.github.com/babelscape/rebel}."},
	{"name":"openlegaldata-processed","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LennardZuendorf/openlegaldata-processed","creator_name":"Lennard ZÃ¼ndorf","creator_url":"https://huggingface.co/LennardZuendorf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for openlegaldata.io bulk case data\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a edit/cleanup of Bulk Data of openlegaldata.io, which I also brought onto Huggingface here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Entire Dataset Is In German\\n\\t\\n\\n\\nGithub Repository: [uniArchive-legalis]](https://github.com/LennardZuendorf/uniArchive-legalis)\\nRepository: Bulk Data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEdit Summary\\n\\t\\n\\nI have done some cleaning and splitting of the data and filtered out large parts that were not (easily)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LennardZuendorf/openlegaldata-processed."},
	{"name":"legalis","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LennardZuendorf/legalis","creator_name":"Lennard ZÃ¼ndorf","creator_url":"https://huggingface.co/LennardZuendorf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for openlegaldata.io bulk case data\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a labeled version of my already edited data from openlegaldata.io.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Entire Dataset Is In German\\n\\t\\n\\n\\nGithub Repository: [uniArchive-legalis]](https://github.com/LennardZuendorf/uniArchive-legalis)\\nProcessed Data: openlegaldata-processed\\nOriginal Bulk Data: Bulk Data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEdit Summary\\n\\t\\n\\n\\nThis Data is based on already processed data from openlegaldata. Repositoriesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LennardZuendorf/legalis."},
	{"name":"flores_101","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/severo/flores_101","creator_name":"Sylvain Lesage","creator_url":"https://huggingface.co/severo","description":"One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the \\nlack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource \\nlanguages, consider only restricted domains, or are low quality because they are constructed using \\nsemi-automatic procedures. In this work, we introduce the FLORES evaluation benchmark, consisting of 3001 \\nsentences extracted from English Wikipedia and covering a variety of different topics and domains. \\nThese sentences have been translated in 101 languages by professional translators through a carefully \\ncontrolled process. The resulting dataset enables better assessment of model quality on the long tail of \\nlow-resource languages, including the evaluation of many-to-many multilingual translation systems, as all \\ntranslations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, \\nwe hope to foster progress in the machine translation community and beyond."},
	{"name":"truthful_qa-validation-german_q_n_a","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Jakelolipopp/truthful_qa-validation-german_q_n_a","creator_name":"Jake Lolipopp","creator_url":"https://huggingface.co/Jakelolipopp","description":"Jakelolipopp/truthful_qa-validation-german_q_n_a dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"all-scam-spam","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FredZhang7/all-scam-spam","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","description":"This is a large corpus of 42,619 preprocessed text messages and emails sent by humans in 43 languages. is_spam=1 means spam and is_spam=0 means ham.\\n1040 rows of balanced data, consisting of casual conversations and scam emails in â‰ˆ10 languages, were manually collected and annotated by me, with some help from ChatGPT.\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSome preprcoessing algorithms\\n\\t\\n\\n\\nspam_assassin.js, followed by spam_assassin.py\\nenron_spam.py\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData composition\\n\\t\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nToâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/all-scam-spam."},
	{"name":"xP3x-sample","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Muennighoff/xP3x-sample","creator_name":"Niklas Muennighoff","creator_url":"https://huggingface.co/Muennighoff","description":"A multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual commonsense reasoning capabilities."},
	{"name":"malicious-website-features-2.4M","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FredZhang7/malicious-website-features-2.4M","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","description":"Important Notice:\\n\\nA subset of the URL dataset is from Kaggle, and the Kaggle datasets contained 10%-15% mislabelled data. See this dicussion I opened for some false positives. I have contacted Kaggle regarding their erroneous \\\"Usability\\\" score calculation for these unreliable datasets.\\nThe feature extraction methods shown here are not robust at all in 2023, and there're even silly mistakes in 3 functions: not_indexed_by_google, domain_registration_length, and age_of_domain.\\n\\n\\n\\nThe featuresâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/malicious-website-features-2.4M."},
	{"name":"blbooks-parquet","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/biglam/blbooks-parquet","creator_name":"BigLAM: BigScience Libraries, Archives and Museums","creator_url":"https://huggingface.co/biglam","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for British Library Books\\n\\t\\n\\nThis dataset is the same as https://huggingface.co/datasets/TheBritishLibrary/blbooks, however, this version is stored as parquet to avoid needing to run a datasets script. This also makes loading this dataset much quicker. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset consists of books digitised by the British Library in partnership with Microsoft. The dataset includes ~25 million pages of out of copyright texts. The majority of the textsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/biglam/blbooks-parquet."},
	{"name":"blbooks-parquet-embedded","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/blbooks-parquet-embedded","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"blbooks-parquet-embedded\\\"\\n\\t\\n\\nMore Information needed\\n"},
	{"name":"professor_heideltime_en","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hugosousa/professor_heideltime_en","creator_name":"Hugo Sousa","creator_url":"https://huggingface.co/hugosousa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProfessor HeidelTime\\n\\t\\n\\n\\n\\nProfessor HeidelTime is a project to create a multilingual corpus weakly labeled with HeidelTime, a temporal tagger.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCorpus Details\\n\\t\\n\\nThe weak labeling was performed in six languages. Here are the specifics of the corpus for each language:\\n\\n\\t\\n\\t\\t\\nDataset\\nLanguage\\nDocuments\\nFrom\\nTo\\nTokens\\nTimexs\\n\\n\\n\\t\\t\\nAll the News 2.0\\nEN\\n24,642\\n2016-01-01\\n2020-04-02\\n18,755,616\\n254,803\\n\\n\\nItalian Crime News\\nIT\\n9,619\\n2011-01-01\\n2021-12-31\\n3,296,898\\n58,823\\n\\n\\nGermanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hugosousa/professor_heideltime_en."},
	{"name":"ggml-vicuna-v0-quantized","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maknee/ggml-vicuna-v0-quantized","creator_name":"maknee","creator_url":"https://huggingface.co/maknee","description":"These are quantized ggml binary files for vicuna 7B and 13B models. The version of vicuna for these models are v0.\\nThese files can be used in conjunction with minigpt4 ggml models 7B and 13B in minigpt4.cpp\\nRecommended are the Q5_K and Q6_K implementations. If there are any issues, use Q4_1 or Q4_0.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVicuna Model Card\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tModel details\\n\\t\\n\\nModel type:\\nVicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.\\nIt isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/maknee/ggml-vicuna-v0-quantized."},
	{"name":"openassistant-guanaco-de","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RainerGa/openassistant-guanaco-de","creator_name":"Rainer Garbotz","creator_url":"https://huggingface.co/RainerGa","description":"This is only a Copy of the Work of OpenAssistant and the user timdettmers \\nTarget of this trainingdata is finetuning only in German language.\\nFile openassistant_origfile_with_lang_informations.txt is the full trainingdata. Every line starts with Language Informations. You can easily filter with:\\ncat openassistant_origfile_with_lang_informations.txt | grep ^de | sed s/^de,//g > openassistant_best_replies_de_train.jsonl\\nReplace ^de with the language you are interessted in.\\nFor language detectionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RainerGa/openassistant-guanaco-de."},
	{"name":"calculation","keyword":"german","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/OzoneAsai/calculation","creator_name":"AsaiTaka","creator_url":"https://huggingface.co/OzoneAsai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Calculation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tsize\\n\\t\\n\\n  JSON file: output1.jsonâ‰’1.3GB\\n  ~\\n    output60.json\\n     In total 70 ~ 80GB\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nen: Calculation. Its range will be expanded later.\\nzh: è®¡ç®—ã€‚å…¶èŒƒå›´å°†åœ¨ä»¥åŽæ‰©å±•ã€‚\\nde: Berechnung. Der Umfang wird spÃ¤ter erweitert werden.\\nru: Ð Ð°ÑÑ‡ÐµÑ‚. Ð•Ð³Ð¾ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½ Ð±ÑƒÐ´ÐµÑ‚ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½ Ð¿Ð¾Ð·Ð¶Ðµ.\\nko: ê³„ì‚°. ë²”ìœ„ëŠ” ë‚˜ì¤‘ì— í™•ìž¥ë  ê²ƒìž…ë‹ˆë‹¤.\\nfr: Calcul. Sa portÃ©e sera Ã©tendue ultÃ©rieurement.\\nja: è¨ˆç®—ã€‚ç¯„å›²ã¯å¾Œã§æ‹¡å¼µã•ã‚Œã¾ã™ã€‚\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nen:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/OzoneAsai/calculation."},
	{"name":"german-multifin","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anhaltai/german-multifin","creator_name":"Artificial Intelligence at Anhalt University of Applied Sciences","creator_url":"https://huggingface.co/anhaltai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Geman financial text (sentence) classification dataset\\n\\t\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset contains real-world financial article headlines annotated with both high-level and low-level topics. \\nThe dataset is annotated with 6 high-level topics and 23 low-level topics for multi-class and multi-label \\nclassification, respectively. For the multi-label classification task, there are up to 3 annotations per example, whichâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anhaltai/german-multifin."},
	{"name":"4typeCalculation","keyword":"german","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/OzoneAsai/4typeCalculation","creator_name":"AsaiTaka","creator_url":"https://huggingface.co/OzoneAsai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Calculation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tsize\\n\\t\\n\\n  JSON file: output1.jsonâ‰’1.3GB\\n  ~\\n    output60.json\\n     In total 70 ~ 80GB\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nen: Calculation. Its range will be expanded later.\\nzh: è®¡ç®—ã€‚å…¶èŒƒå›´å°†åœ¨ä»¥åŽæ‰©å±•ã€‚\\nde: Berechnung. Der Umfang wird spÃ¤ter erweitert werden.\\nru: Ð Ð°ÑÑ‡ÐµÑ‚. Ð•Ð³Ð¾ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½ Ð±ÑƒÐ´ÐµÑ‚ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½ Ð¿Ð¾Ð·Ð¶Ðµ.\\nko: ê³„ì‚°. ë²”ìœ„ëŠ” ë‚˜ì¤‘ì— í™•ìž¥ë  ê²ƒìž…ë‹ˆë‹¤.\\nfr: Calcul. Sa portÃ©e sera Ã©tendue ultÃ©rieurement.\\nja: è¨ˆç®—ã€‚ç¯„å›²ã¯å¾Œã§æ‹¡å¼µã•ã‚Œã¾ã™ã€‚\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nen:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/OzoneAsai/4typeCalculation."},
	{"name":"test_llm_dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/youssefoud/test_llm_dataset","creator_name":"Youssef Oudghiri","creator_url":"https://huggingface.co/youssefoud","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tModel Card for Mixtral-8x7B\\n\\t\\n\\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks we tested.\\nFor full details of this model please read our release blog post.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWarning\\n\\t\\n\\nThis repo contains weights that are compatible with vLLM serving of the model as well as Hugging Face transformers library. It is based on the original Mixtral torrent release, but the file formatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/youssefoud/test_llm_dataset."},
	{"name":"lola-gramma-de-en","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/honzatoegel/lola-gramma-de-en","creator_name":"Jan Toegel","creator_url":"https://huggingface.co/honzatoegel","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nThis gramma correction dataset is still work in progress! Do not use it for any serious LLM task - see Issues bellow.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset summary\\n\\t\\n\\nThis dataset is used to finetune LLMs for German gramma correction for English speakers.  \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInput\\n\\t\\n\\nAn input is German sentence, which has potentially grammatical errors.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOutput\\n\\t\\n\\nOutput is corrected sentence with minimal adjustments and list all gramma corrections and explanations.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/honzatoegel/lola-gramma-de-en."},
	{"name":"TruthfulQA_de","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LeoLM/TruthfulQA_de","creator_name":"LAION LeoLM","creator_url":"https://huggingface.co/LeoLM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for truthful_qa\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LeoLM/TruthfulQA_de."},
	{"name":"openassistant-guanaco-unfiltered","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Fredithefish/openassistant-guanaco-unfiltered","creator_name":"Fredi","creator_url":"https://huggingface.co/Fredithefish","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGuanaco-Unfiltered\\n\\t\\n\\n\\nAny language other than English, German, French, or Spanish has been removed.\\nRefusals of assistance have been removed.\\nThe identification as OpenAssistant has been removed.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVersion 2 is out\\n\\t\\n\\n\\nIdentification as OpenAssistant is now fully removed\\nother improvements\\n\\n"},
	{"name":"ingredient-detection","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/openfoodfacts/ingredient-detection","creator_name":"Open Food Facts","creator_url":"https://huggingface.co/openfoodfacts","description":"This dataset is used to train a multilingual ingredient list detection model. The goal is to automate the extraction of ingredient lists from food packaging images. See this issue for a broader context about ingredient list extraction.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset generation\\n\\t\\n\\nRaw unannotated texts are OCR results obtained with Google Cloud Vision. It only contains images marked as ingredient image on Open Food Facts.\\nThe dataset was generated using ChatGPT-3.5: we asked ChatGPT to extract ingredientâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openfoodfacts/ingredient-detection."},
	{"name":"RyokoAI_ShareGPT52K","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ShareGPT52K90K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\\nThis repository now contains the new 90K conversations version. The previous 52K may\\nbe found in the old/ directory.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\ntext-generation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K."},
	{"name":"Mietvertraege","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ulajessen/Mietvertraege","creator_name":"Urszula Jessen","creator_url":"https://huggingface.co/ulajessen","description":"ulajessen/Mietvertraege dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"OASST-DE","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenAssistant/OASST-DE","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman OpenAssistant Conversations Dataset (OASST-DE)\\n\\t\\n\\nWith the goal of advancing open-source, german-language LLM research, we present \\nOASST-DE: a high quality subset of a recent (25.08.23) dump from the OpenAssistant website\\ntranslated to German using the GPT-3.5 API. More details on how the dataset was filtered and translated under dataset creation.\\nFor more details on the OpenAssistant Project, look at the first OASST dataset (OASST1), the Open-Assistant GitHub repo\\nor ourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/OASST-DE."},
	{"name":"wikianc","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cyanic-selkie/wikianc","creator_name":"cyanic-selkie","creator_url":"https://huggingface.co/cyanic-selkie","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WikiAnc\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe WikiAnc dataset is an automatically generated dataset from Wikipedia (all languages) and Wikidata dumps (August, 2023). \\nThe code for generating the dataset can be found here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\n\\nwikificiation: The dataset can be used to train a model for Wikification.\\nnamed-entity-linking: The dataset can be used to train a model for Named Entity Linking.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe text in the dataset isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cyanic-selkie/wikianc."},
	{"name":"entity_cs","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huawei-noah/entity_cs","creator_name":"HUAWEI Noah's Ark Lab","creator_url":"https://huggingface.co/huawei-noah","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for EntityCS\\n\\t\\n\\n\\nRepository: https://github.com/huawei-noah/noah-research/tree/master/NLP/EntityCS  \\nPaper: https://aclanthology.org/2022.findings-emnlp.499.pdf  \\nPoint of Contact: Fenia Christopoulou, Chenxi Whitehouse\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nWe use the English Wikipedia and leverage entity information from Wikidata to construct an entity-based Code Switching corpus. \\nTo achieve this, we make use of wikilinks in Wikipedia, i.e. links from one page toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/huawei-noah/entity_cs."},
	{"name":"text_coordinates_regions","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yachay/text_coordinates_regions","creator_name":"Yachay AI","creator_url":"https://huggingface.co/yachay","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Multilingual Geo-Tagged Social Media Posts (by 123 world regions)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe \\\"Regions\\\" dataset is a multilingual corpus that encompasses textual data from the 123 most populated regions worldwide, with each region's data organized into separate .json files. This dataset consists of approximately 500,000 text samples, each paired with its geographic coordinates.\\nKey Features:\\n\\nTextual Data: The dataset contains 500,000 text samples.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yachay/text_coordinates_regions."},
	{"name":"german-czech-paired-placenames","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DebasishDhal99/german-czech-paired-placenames","creator_name":"Debasish Dhal","creator_url":"https://huggingface.co/DebasishDhal99","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains the German and corresponding Czech names for almost 5k places in Czech Republic. It has been generated using this code.\\nMany of these names are related to each other. Some German names are literal translation of the Czech names (or maybe the other way around), some are phonetic modifications while some are unrelated.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data\\n\\t\\n\\nEnglish wiki page containing German exonyms for places in Czechâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DebasishDhal99/german-czech-paired-placenames."},
	{"name":"german-polish-paired-placenames","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DebasishDhal99/german-polish-paired-placenames","creator_name":"Debasish Dhal","creator_url":"https://huggingface.co/DebasishDhal99","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains the German and Polish names for almost 10k places in Poland. It has been generated using this code.\\nMany of these names are related to each other. Some German names are literal translation of the Polish names, some are phonetic modifications while some are unrelated.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data\\n\\t\\n\\nGerman wiki page\\n"},
	{"name":"MultiCoNER","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tomaarsen/MultiCoNER","creator_name":"Tom Aarsen","creator_url":"https://huggingface.co/tomaarsen","description":"We present MultiCoNER, a large multilingual dataset for Named Entity Recognition that covers 3 domains (Wiki sentences, questions, and search queries) across 11 languages, as well as multilingual and code-mixing subsets. This dataset is designed to represent contemporary challenges in NER, including low-context scenarios (short and uncased text), syntactically complex entities like movie titles, and long-tail entity distributions. The 26M token dataset is compiled from public resources using techniques such as heuristic-based sentence sampling, template extraction and slotting, and machine translation. We applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a state-of-the-art GEMNET model that leverages gazetteers. The baseline achieves moderate performance (macro-F1=54%), highlighting the difficulty of our data. GEMNET, which uses gazetteers, improvement significantly (average improvement of macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained language models, and we believe that it can help further research in building robust NER systems. MultiCoNER is publicly available at https://registry.opendata.aws/multiconer/ and we hope that this resource will help advance research in various aspects of NER."},
	{"name":"openassistant-guanaco-EOS","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-guanaco-EOS","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChat Fine-tuning Dataset - Guanaco Style\\n\\t\\n\\nThis dataset allows for fine-tuning chat models using \\\"### Human:\\\" AND \\\"### Assistant\\\" as the beginning and end of sequence tokens.\\nPreparation:\\n\\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\\nThe dataset was then slightly adjusted to:\\n\\n\\nif a row ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-guanaco-EOS."},
	{"name":"sharegpt-deduplicated","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaterinaLac/sharegpt-deduplicated","creator_name":"Caterina Lacerra","creator_url":"https://huggingface.co/CaterinaLac","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a deduplicated version of sharegpt4. \\nThe deduplication process has two steps:\\n\\nThe literal duplicates (both input and outputs) are removed\\nThe remaining (5749) instances are embedded with the SentenceTransformer library (\\\"paraphrase-multilingual-mpnet-base-v2\\\" model).\\nThen, we compute the cosine similarity among all the possible pairs, and consider paraphrases thoseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CaterinaLac/sharegpt-deduplicated."},
	{"name":"openassistant-llama-style","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-llama-style","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChat Fine-tuning Dataset - Llama 2 Style\\n\\t\\n\\nThis dataset allows for fine-tuning chat models using [INST] AND [/INST] to wrap user messages.\\nPreparation:\\n\\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\\nThe dataset was then filtered to:\\n\\n\\nreplace instances of '### Human:' with '[INST]'\\nreplaceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-llama-style."},
	{"name":"project_gutenberg","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SinclairSchneider/project_gutenberg","creator_name":"Sinclair Schneider","creator_url":"https://huggingface.co/SinclairSchneider","description":"SinclairSchneider/project_gutenberg dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"mqnli","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SachinPatel248/mqnli","creator_name":"Patel","creator_url":"https://huggingface.co/SachinPatel248","description":"SachinPatel248/mqnli dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"MSVAMP","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mathoctopus/MSVAMP","creator_name":"Mathoctopus","creator_url":"https://huggingface.co/Mathoctopus","description":"Mathoctopus/MSVAMP dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"swiss-code-of-obligations","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/brunnolou/swiss-code-of-obligations","creator_name":"Bruno","creator_url":"https://huggingface.co/brunnolou","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSwiss Code of Obligations (OR) and Swiss Civil Code\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t(Part Five: The Code of Obligations) of 30 March 1911 (Status as of 1 September 2023)\\n\\t\\n\\nFiles generated from the Swiss publication platform for federal law\\nSwiss Code of Obligations\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFormat\\n\\t\\n\\nEach article has the following type definition:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWith vector embeddings by Xenova/paraphrase-multilingual-mpnet-base-v2\\n\\t\\n\\n\\nswiss-civil-code-de-paraphrase-multilingual-mpnet-base-v2.jsonlâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/brunnolou/swiss-code-of-obligations."},
	{"name":"Multi-EuP","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/unimelb-nlp/Multi-EuP","creator_name":"The University of Melbourne","creator_url":"https://huggingface.co/unimelb-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNOTES FOR DOWNLOAD!\\n\\t\\n\\n\\nHighly recommend downloading it via the API:\\n\\ncurl -X GET \\\\\\n     \\\"https://datasets-server.huggingface.co/first-rows?dataset=unimelb-nlp%2FMulti-EuP&config=default&split=full\\\"\\n\\n\\nIf you are using the HuggingFace library, please follow these steps:\\n\\npip install datasets\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"unimelb-nlp/Multi-EuP\\\", keep_default_na=False)\\n\\nNote: It's crucial to use keep_default_na=False because some datasets contain 'null'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/unimelb-nlp/Multi-EuP."},
	{"name":"udhr-lid","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cis-lmu/udhr-lid","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUDHR-LID\\n\\t\\n\\nWhy UDHR-LID?\\nYou can access UDHR (Universal Declaration of Human Rights) here, but when a verse is missing, they have texts such as \\\"missing\\\" or \\\"?\\\". Also, about 1/3 of the sentences consist only of \\\"articles 1-30\\\" in different languages. We cleaned the entire dataset from XML files and selected only the paragraphs. We cleared any unrelated language texts from the data and also removed the cases that were incorrect.\\nIncorrect? Look at the ckb and kmr files in the UDHR.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/udhr-lid."},
	{"name":"udhr-lid","keyword":"swiss german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cis-lmu/udhr-lid","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUDHR-LID\\n\\t\\n\\nWhy UDHR-LID?\\nYou can access UDHR (Universal Declaration of Human Rights) here, but when a verse is missing, they have texts such as \\\"missing\\\" or \\\"?\\\". Also, about 1/3 of the sentences consist only of \\\"articles 1-30\\\" in different languages. We cleaned the entire dataset from XML files and selected only the paragraphs. We cleared any unrelated language texts from the data and also removed the cases that were incorrect.\\nIncorrect? Look at the ckb and kmr files in the UDHR.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/udhr-lid."},
	{"name":"wiki-stance","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/copenlu/wiki-stance","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Wiki-Stance\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is the dataset Wiki-Stance introduced in EMNLP 2023 paper \\\"Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions\\\"\\nA pre-print version of the paper can be found here: Arxiv\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nRepository: https://github.com/copenlu/wiki-stance\\nPaper: https://aclanthology.org/2023.emnlp-main.361/\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tColumnâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/wiki-stance."},
	{"name":"seamless-align","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jhu-clsp/seamless-align","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Seamless-Align (WIP). Inspired by https://huggingface.co/datasets/allenai/nllb\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset was created based on metadata for mined Speech-to-Speech(S2S), Text-to-Speech(TTS) and Speech-to-Text(S2T) released by Meta AI.  The S2S contains data for 35 language pairs. The S2S dataset is ~1000GB compressed.\\n\\n\\t\\n\\t\\t\\n\\t\\tHow to use the data\\n\\t\\n\\nThere are two ways to access the data:\\n\\nVia the Hugging Face Python datasets library\\n\\nScripts coming soonâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/seamless-align."},
	{"name":"WEATHub","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iamshnoo/WEATHub","creator_name":"Anjishnu Mukherjee","creator_url":"https://huggingface.co/iamshnoo","description":"\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"WEATHub\\\"\\n\\t\\n\\nThis dataset corresponds to the data described in the paper \\\"Global Voices, Local Biases: Socio-Cultural Prejudices across Languages\\\"\\naccepted to EMNLP 2023.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWEATHub is a dataset containing 24 languages. It contains words organized into groups of (target1, target2, attribute1, attribute2)\\nto measure the association target1:target2 :: attribute1:attribute2. For example target1 can be insects, target2 can be flowers.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/iamshnoo/WEATHub."},
	{"name":"markt-pilot","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/trettenmeier/markt-pilot","creator_name":"Tobias Rettenmeier","creator_url":"https://huggingface.co/trettenmeier","description":"This dataset has an accompanying paper \\\"Introducing a novel dataset for product matching: A new challenge for matching systems\\\" that is accepted at The 3rd International Conference on Computers and Automation (CompAuto 2023) and will be published in IEEE Xplore.\\nThe structure of the dataset is as follows: Each data point consists of a pair products and a binary label that indicates if these two product refer to the same real-world entity.\\nIt consists of four subsets that differ in size andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/trettenmeier/markt-pilot."},
	{"name":"openassistant-falcon","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-falcon","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChat Fine-tuning Dataset - OpenAssistant Falcon\\n\\t\\n\\nThis dataset allows for fine-tuning chat models using '\\\\Human:' AND '\\\\nAssistant:' to wrap user messages.\\nIt still uses <|endoftext|> as EOS and BOS token, as per Falcon.\\nSample \\nPreparation:\\n\\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-falcon."},
	{"name":"mapa-eur-lex","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dglover1/mapa-eur-lex","creator_name":"D Glover","creator_url":"https://huggingface.co/dglover1","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Multilingual European Datasets for Sensitive Entity Detection in the Legal Domain\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a completed version of the MAPA EUR-LEX dataset, originally converted to Huggingface format by joelniklaus. See the dataset card for more information about MAPA.\\n3 of the (Spanish) EUR-LEX WebAnno TSV files in the source MAPA repository are malformed, so they were omitted from the original conversion, causing under-representation ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dglover1/mapa-eur-lex."},
	{"name":"megawika-report-generation","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hltcoe/megawika-report-generation","creator_name":"JHU Human Language Technology Center of Excellence","creator_url":"https://huggingface.co/hltcoe","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MegaWika for Report Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMegaWika is a multi- and crosslingual text dataset containing 30 million Wikipedia passages with their scraped and cleaned web citations. The passages span\\n50 Wikipedias in 50 languages, and the articles in which the passages were originally embedded are included for convenience. Where a Wikipedia passage is in a\\nnon-English language, an automated English translation is provided. \\nThis datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hltcoe/megawika-report-generation."},
	{"name":"ProfessorHeidelTime","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hugosousa/ProfessorHeidelTime","creator_name":"Hugo Sousa","creator_url":"https://huggingface.co/hugosousa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProfessor HeidelTime\\n\\t\\n\\nPaper    GitHub\\nProfessor HeidelTime is a project to create a multilingual corpus weakly labeled with HeidelTime, a temporal tagger.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCorpus Details\\n\\t\\n\\nThe weak labeling was performed in six languages. Here are the specifics of the corpus for each language:\\n\\n\\t\\n\\t\\t\\nDataset\\nLanguage\\nDocuments\\nFrom\\nTo\\nTokens\\nTimexs\\n\\n\\n\\t\\t\\nAll the News 2.0\\nEN\\n24,642\\n2016-01-01\\n2020-04-02\\n18,755,616\\n254,803\\n\\n\\nItalian Crime News\\nIT\\n9,619\\n2011-01-01\\n2021-12-31\\n3,296,898\\n58â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hugosousa/ProfessorHeidelTime."},
	{"name":"ALMA-prompt-completion","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kristaller486/ALMA-prompt-completion","creator_name":"Kristaller486","creator_url":"https://huggingface.co/kristaller486","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ALMA-prompt-completion\\n\\t\\n\\n ALMA Dataset if format of prompt-completion\\n\\nCreated by: fe1ixxu\\nShared by: me\\nLanguage(s) (NLP): English, Czech, German, Russian, Islandic, Chinese\\nLicense: MIT\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [https://github.com/fe1ixxu/ALMA]\\nPaper [optional]: [https://arxiv.org/abs/2309.11674]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUses\\n\\t\\n\\nLLM translators\\n"},
	{"name":"APIS_OEBL__Named_Entity_Recognition","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SteffRhes/APIS_OEBL__Named_Entity_Recognition","creator_name":"Stefan Resch","creator_url":"https://huggingface.co/SteffRhes","description":"JSON file of 6,941 sentences of historical biographies, annotated with \\\"PER\\\" (Person), \\\"ORG\\\" (Organisation), \\\"LOC\\\" (Location).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tsource\\n\\t\\n\\nThe original data was extracted from the Austrian Biographical Lexicon (Ã–BL) in the context of the Austrian Prosopographical Information System (APIS) project.\\nFrom there, samples were randomly pulled and annotated for Named Entity Recognition tasks, which form this dataset.\\nThe texts concern numerous smaller biographies in the time period betweenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SteffRhes/APIS_OEBL__Named_Entity_Recognition."},
	{"name":"APIS_OEBL__Abbreviations","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SteffRhes/APIS_OEBL__Abbreviations","creator_name":"Stefan Resch","creator_url":"https://huggingface.co/SteffRhes","description":"CoNLL-U(ish) file of 954 sentences of 164 texts, containing abbreviations and their extensions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tsource\\n\\t\\n\\nThe original data was extracted from the Austrian Biographical Lexicon (Ã–BL) in the context of the Austrian Prosopographical Information System (APIS) project.\\nFrom there, samples were randomly pulled and annotated for Named Entity Recognition tasks, which form this dataset.\\nThe texts concern numerous smaller biographies in the time period between 19th and early 20th centuryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SteffRhes/APIS_OEBL__Abbreviations."},
	{"name":"Open_Assistant_Chains_German_Translation","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-ric/Open_Assistant_Chains_German_Translation","creator_name":"Aymeric Roucher","creator_url":"https://huggingface.co/m-ric","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset description\\n\\t\\n\\n\\n\\nThis dataset is derived from OpenAssistant Conversation Chains, which is a reformatting of OpenAssistant Conversations (OASST1), which is itself\\n\\na human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwideâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-ric/Open_Assistant_Chains_German_Translation."},
	{"name":"absinth_german_faithfulness_detection_dataset","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mtc/absinth_german_faithfulness_detection_dataset","creator_name":"MTC","creator_url":"https://huggingface.co/mtc","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Absinth - Hallucination Detection Dataset of German News Summarization\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nAbsinth is a human-annotated dataset for faithfulness detection in the context of German news summarization. \\nThe dataset has 4335 instances in total, where each instance consists of:\\n\\nNews Article: The original news article from the 20Minuten dataset. Please note that original source articles are not included in the dataset and need to be downloadedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mtc/absinth_german_faithfulness_detection_dataset."},
	{"name":"retrieval_qa","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lnwang/retrieval_qa","creator_name":"Luning Wang","creator_url":"https://huggingface.co/lnwang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRetrieval_QA: A Simple Multilingual Benchmark For Retrieval Encoder Models\\n\\t\\n\\n\\n\\nThe purpose of this dataset is to provide a simple and easy-to-use benchmark for retrieval encoder models, which helps researchers quickly select the most effective retrieval encoder for text extraction and achieve optimal results in subsequent retrieval tasks such as retrieval-augmented-generation (RAG). The dataset contains multiple document-question pairs, where each document is a short text aboutâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lnwang/retrieval_qa."},
	{"name":"Buergerliches_Gesetzbuch_BGB","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nookbe/Buergerliches_Gesetzbuch_BGB","creator_name":"legal","creator_url":"https://huggingface.co/nookbe","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman BGB Law Dataset (BÃ¼rgerliches Gesetzbuch)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe BGB Law Dataset contains legal text from the German Civil Code (BÃ¼rgerliches Gesetzbuch - BGB). It focuses on the general principles of German civil law, and the dataset is designed for tasks related to legal text analysis.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nA typical data point in the dataset comprises a legal paragraph and its corresponding text. For example:\\n{â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nookbe/Buergerliches_Gesetzbuch_BGB."},
	{"name":"Handelsgesetzbuch_HGB","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nookbe/Handelsgesetzbuch_HGB","creator_name":"legal","creator_url":"https://huggingface.co/nookbe","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tlicense: mit\\ntask_categories:\\n- text-classification\\nlanguage:\\n- de\\ntags:\\n- legal\\npretty_name: HGB\\nsize_categories:\\n- 1K<n<10K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman HGB Law Dataset (Handelsgesetzbuch)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe HGB Law Dataset contains legal text from the German Commercial Code (Handelsgesetzbuch - HGB). It focuses on the general principles of German commercial law, and the dataset is designed for tasks related to legal text analysis.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nookbe/Handelsgesetzbuch_HGB."},
	{"name":"wikiquote-de-quotes","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/caretech-owl/wikiquote-de-quotes","creator_name":"CareTech OWL","creator_url":"https://huggingface.co/caretech-owl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Wikiquotes German\\n\\t\\n\\nThis dataset contains german quotes from wikiquote. It consists of two columns named 'author' and 'quote'.\\nFor regenerating the dataset we provided the source code in this repo. You can use it as follows:\\npip install bs4 pandas\\npython CrawlingQuotes.py\\n\\nFor usag in python just include \\nfrom datasets import load_dataset\\ntraining_data = load_dataset(\\\"caretech-owl/wikiquote-de-quotes\\\", split=\\\"train\\\")\\n\\nafter installing ðŸ¤— datasets (pip installâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/caretech-owl/wikiquote-de-quotes."},
	{"name":"wmt-mqm-error-spans","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RicardoRei/wmt-mqm-error-spans","creator_name":"Ricardo Costa Dias Rei","creator_url":"https://huggingface.co/RicardoRei","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains all MQM human annotations from previous WMT Metrics shared tasks and the MQM annotations from Experts, Errors, and Context in a form of error spans. Moreover, it contains some hallucinations used in the training of XCOMET models.\\nPlease note that this is not an official release of the data and the original data can be found here.\\nThe data is organised into 8 columns:\\n\\nsrc: input text\\nmt: translation\\nref: reference translation\\nannotations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RicardoRei/wmt-mqm-error-spans."},
	{"name":"Pontoon-Translations","keyword":"german","license":"Mozilla Public License 2.0","license_url":"https://choosealicense.com/licenses/mpl-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ayymen/Pontoon-Translations","creator_name":"Mohamed Aymane Farhi","creator_url":"https://huggingface.co/ayymen","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Pontoon Translations\\n\\t\\n\\n\\n\\nThis is a dataset containing strings from various Mozilla projects on Mozilla's Pontoon localization platform and their translations into more than 200 languages.\\nSource strings are in English.\\nTo avoid rows with values like \\\"None\\\" and \\\"N/A\\\" being interpreted as missing values, pass the keep_default_na parameter like this:\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"ayymen/Pontoon-Translations\\\", keep_default_na=False)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ayymen/Pontoon-Translations."},
	{"name":"medicinal-plants","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mikehemberger/medicinal-plants","creator_name":"Mike Hemberger","creator_url":"https://huggingface.co/mikehemberger","description":"A growing dataset about medicinal plants. We plan to construct a multimodal dataset with images and text content extracted from \\n\\nbooks that went out of copywrite and \\ndiverse and high-quality video data taken via smart phone and various lenses with a DSLR camera (24mm Macro, 50mm, 100mm Macro and a 24-120mm Zoom lens).\\nParts of the iNaturalist and PlantNet300K datasets will be integrated as well to cover a wide spectrum of the kingdom Plantae.\\n\\nThe resulting dataset should be able to power aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mikehemberger/medicinal-plants."},
	{"name":"mswc_fscil_subset","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NeuroBench/mswc_fscil_subset","creator_name":"NeuroBench","creator_url":"https://huggingface.co/NeuroBench","description":"This is a subset of the Multilingual Spoken Word Corpus dataset, which is built specifically for the Few-shot Class-incremental Learning (FSCIL) task. \\nA total of 15 languages are chosen, split into 5 base languages (English, German, Catalan, French, Kinyarwanda) and 10 incrementally learned languages (Persian, Spanish, Russian, Welsh, Italian, Basque, Polish, Esparanto, Portuguese, Dutch).\\nThe FSCIL task entails first training a model using abundant training data on words from the 5 baseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NeuroBench/mswc_fscil_subset."},
	{"name":"uner_llm_instructions","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/universalner/uner_llm_instructions","creator_name":"Universal NER","creator_url":"https://huggingface.co/universalner","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Universal NER v1 in the Aya format\\n\\t\\n\\nThis dataset is a format conversion from its original v1 format into the Aya instruction format and it's released here under the same CC-BY-SA 4.0 license and conditions.\\nIt contains data in multiple languages and this version is intended for multi-lingual LLM construction/tuning.\\nThe dataset contains different subsets and their dev/test/train splits, depending on language. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you utilize this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/universalner/uner_llm_instructions."},
	{"name":"uner_llm_inst_german","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/universalner/uner_llm_inst_german","creator_name":"Universal NER","creator_url":"https://huggingface.co/universalner","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Universal NER v1 in the Aya format - German subset\\n\\t\\n\\nThis dataset is a format conversion for the German data in the original Universal NER v1 into the Aya instruction format and it's released here under the same CC-BY-SA 4.0 license and conditions.\\nThe dataset contains different subsets and their dev/test/train splits, depending on language. For more details, please refer to:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nFor the original Universal NER dataset v1 and more detailsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/universalner/uner_llm_inst_german."},
	{"name":"ntx_llm_instructions","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tellarin-ai/ntx_llm_instructions","creator_name":"Tellarin.ai","creator_url":"https://huggingface.co/tellarin-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for NTX v1 in the Aya format\\n\\t\\n\\nThis dataset is a format conversion from its original v1 format into the Aya instruction format and it's released here under the CC-BY-SA 4.0 license and conditions.\\nIt contains data in multiple languages and this version is intended for multi-lingual LLM construction/tuning.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you utilize this dataset version, feel free to cite/footnote this huggingface dataset repo, but please also cite the original datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tellarin-ai/ntx_llm_instructions."},
	{"name":"ntx_llm_inst_german","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_german","creator_name":"Tellarin.ai","creator_url":"https://huggingface.co/tellarin-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for NTX v1 in the Aya format - German subset\\n\\t\\n\\nThis dataset is a format conversion for the German data from the original NTX into the Aya instruction format and it's released here under the CC-BY-SA 4.0 license.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nFor the original NTX dataset, the conversion to the Aya instructions format, or more details, please refer to the full dataset in instruction form (https://huggingface.co/datasets/tellarin-ai/ntx_llm_instructions) or to the paperâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tellarin-ai/ntx_llm_inst_german."},
	{"name":"tla-Earlier_Egyptian_original-v18-premium","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thesaurus-linguae-aegyptiae/tla-Earlier_Egyptian_original-v18-premium","creator_name":"Thesaurus Linguae Aegyptiae","creator_url":"https://huggingface.co/thesaurus-linguae-aegyptiae","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset tla-Earlier_Egyptian_original-v18-premium\\n\\t\\n\\n\\nThis data set contains Earlier Egyptian, i.e., ancient Old Egyptian and ancient Middle Egyptian, sentences in hieroglyphs and transliteration, with lemmatization, with POS glossing and with a German translation. \\nThis set of original Earlier Egyptian sentences only contains text witnesses from before the start of the New Kingdom (late 16th century BCE).\\nThe data comes from the database of the Thesaurus Linguaeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thesaurus-linguae-aegyptiae/tla-Earlier_Egyptian_original-v18-premium."},
	{"name":"ml-kge","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/ml-kge","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMKGE: Multilingual Knowledge Graph Enhancement\\n\\t\\n\\nnote this dataset card was copied from this GitHub Repository\\nTask Description |\\nWikiKGE-10 |\\nEvaluation |\\nPaper |\\nCitation |\\nLicense\\nRecent work in Natural Language Processing and Computer Vision has been leveraging textual information -- e.g., entity names and descriptions -- available in knowledge graphs to ground neural models to high-quality structured data.\\nHowever, when it comes to non-English languages, both quantity andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/ml-kge."},
	{"name":"German_Names_Central_And_Eastern_Europe","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DebasishDhal99/German_Names_Central_And_Eastern_Europe","creator_name":"Debasish Dhal","creator_url":"https://huggingface.co/DebasishDhal99","description":"This dataset contains German exonyms for various places in modern day Poland, Czech Republic, Latvia, Lithuania and Estonia. \\nExonym : - A placename that is used by people who are not locals. For example, Prague is the Eng. exonym of Czech capital Praha, or Cologne is an exonym for German city KÃ¶ln.\\nDue to extensive historical German rule and presence over large chunks of modern day Poland and Czech republic, these two countries populate the dataset the most.\\n"},
	{"name":"dialect-at-tirol","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/morgendigital/dialect-at-tirol","creator_name":"Morgendigital","creator_url":"https://huggingface.co/morgendigital","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset of Tyrolean Dialect (Austria)\\n\\t\\n\\nThis dataset contains 200+ words used in Tirol (Austria), together with their German translation and (optional) meaning.\\n"},
	{"name":"prompt_injections","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yanismiraoui/prompt_injections","creator_name":"Yanis Miraoui","creator_url":"https://huggingface.co/yanismiraoui","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Prompt Injections by  Yanis Miraoui  ðŸ‘‹\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset of prompt injections enriches Large Language Models (LLMs) by providing task-specific examples and prompts, helping improve LLMs' performance and control their behavior.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains over 1000 rows of prompt injections in multiple languages. It contains examples of prompt injections using different techniques such as: prompt leakingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yanismiraoui/prompt_injections."},
	{"name":"WikidataLabels","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rayliuca/WikidataLabels","creator_name":"Ray Liu","creator_url":"https://huggingface.co/rayliuca","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWikidata Labels\\n\\t\\n\\nLarge parallel corpus for machine translation\\n\\nEntity label data extracted from Wikidata (2022-01-03), filtered for item entities only  \\nOnly download the languages you need with datasets>=2.14.0\\nSimilar dataset: https://huggingface.co/datasets/wmt/wikititles (18 Wikipedia titles pairs instead of all Wikidata entities)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nWikidata JSON dump (wikidata-20220103-all.json.gz)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/rayliuca/WikidataLabels."},
	{"name":"GPT-4-Self-Instruct-German","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-German","creator_name":"CausalLM","creator_url":"https://huggingface.co/CausalLM","description":"Here we share a German dataset synthesized using the OpenAI GPT-4 model with Self-Instruct, utilizing some excess Azure credits. Please feel free to use it. All questions and answers are newly generated by GPT-4, without specialized verification, only simple filtering and strict semantic similarity control have been applied.\\nWe hope that this will be helpful for fine-tuning open-source models for non-English languages, particularly German. This dataset will be updated continuously.\\n"},
	{"name":"oasst2_top1_chat_format","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/blancsw/oasst2_top1_chat_format","creator_name":"Blanc Swan","creator_url":"https://huggingface.co/blancsw","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenAssistant TOP-1 Conversation Threads in huggingface chat format\\n\\t\\n\\nExport of oasst2 only top 1 threads in huggingface chat format\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tScript\\n\\t\\n\\nThe convert script can be find here\\n"},
	{"name":"language_tags","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lbourdois/language_tags","creator_name":"LoÃ¯ck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nDataset listing 27,328 languages and dialects (also includes macrolanguage names).For each language, either the ISO 639 code, the Glottolog code or both are provided.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tColumns\\n\\t\\n\\n\\nEnglish_Name: Language name in English (e.g. \\\"French\\\").\\nNative_Name: If value is not 0, corresponds to the name of the language by native speakers (e.g. \\\"FranÃ§ais\\\") which may have been found in Wikipedia's nativename field.\\nGlottocode: The language tag in the Glottolog conventionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/language_tags."},
	{"name":"language_tags","keyword":"swiss german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lbourdois/language_tags","creator_name":"LoÃ¯ck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nDataset listing 27,328 languages and dialects (also includes macrolanguage names).For each language, either the ISO 639 code, the Glottolog code or both are provided.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tColumns\\n\\t\\n\\n\\nEnglish_Name: Language name in English (e.g. \\\"French\\\").\\nNative_Name: If value is not 0, corresponds to the name of the language by native speakers (e.g. \\\"FranÃ§ais\\\") which may have been found in Wikipedia's nativename field.\\nGlottocode: The language tag in the Glottolog conventionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/language_tags."},
	{"name":"germanquad-retrieval","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/germanquad-retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"This dataset is derived from the GermanQuAD dataset.\\nThis dataset takes the testset and represents it as a corpus in the BEIR information retrieval benchmark format.\\nCorpus and query ids have been added.\\nThe corresponding qrels can be found here.\\nFull credit for the original dataset goes to the authors of the GermanQuAD dataset.\\nThe original dataset is licensed under CC BY-SA 4.0.\\nCitation for the original dataset:\\n@misc{mÃ¶ller2021germanquad,\\n      title={GermanQuAD and GermanDPR: Improvingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/germanquad-retrieval."},
	{"name":"germanquad-retrieval-qrels","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/germanquad-retrieval-qrels","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"This dataset is derived from the GermanQuAD dataset.\\nThis dataset takes the testset and represents it as qrels in the BEIR information retrieval benchmark format.\\nCorpus and query ids have been added.\\nThe corresponding corpus can be found here.\\nFull credit for the original dataset goes to the authors of the GermanQuAD dataset.\\nThe original dataset is licensed under CC BY-SA 4.0.\\nCitation for the original dataset:\\n@misc{mÃ¶ller2021germanquad,\\n      title={GermanQuAD and GermanDPR: Improvingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/germanquad-retrieval-qrels."},
	{"name":"oaast_rm_full_jieba","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lenML/oaast_rm_full_jieba","creator_name":"len","creator_url":"https://huggingface.co/lenML","description":"å°è¯•è§£å†³\\\"llm repetition problem\\\"ï¼Œä½¿ç”¨åˆ†è¯æ¨¡åž‹å¯¹oaastè¯­æ–™è¿›è¡Œâ€œç»“å·´åŒ–â€æ•°æ®å¢žå¼ºï¼Œæä¾›æ›´å¼ºçš„é‡å¤å†…å®¹æ‹’ç»æ•ˆæžœã€‚\\nAttempts to solve the \\\"llm repetition problem\\\" by using a segmentation model to enhance the oaast corpus with \\\"stuttering\\\" data to provide stronger rejection of duplicate content.\\nå…¶æ¬¡ï¼Œè¿˜è¿‡æ»¤æŽ‰äº†æ‰€æœ‰è‡ªæˆ‘è®¤çŸ¥çš„å¾®è°ƒæ ·æœ¬ã€‚\\nSecond, it also filters out all the fine-tuned samples of self-cognition.\\nfiles:\\n\\noaast_rm_full_jieba.jsonl : word level repeat\\noaast_rm_full_sent_jieba.jsonl : sentence level repeat\\n\\n"},
	{"name":"NewsEye-Austrian-line","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/NewsEye-Austrian-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNewsEye Austrian - line level\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe dataset comprises Austrian newspaper pages from 19th and early 20th century. The images were provided by the Austrian National Library.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe documents are in Austrian German with the Fraktur font.\\nNote that all images are resized to a fixed height of 128 pixels.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n{\\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGBâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/NewsEye-Austrian-line."},
	{"name":"telekom-backtrans-paraphrase-filtered","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/danielheinz/telekom-backtrans-paraphrase-filtered","creator_name":"Daniel Heinz","creator_url":"https://huggingface.co/danielheinz","description":"This is a filtered version of Philip May's German paraphrase dataset.\\nThe dataset has been filtered for the sake of convenience, since smaller devices do not support such large files.\\nAll text pairs in the dataset are paraphrases, and are therefore labelled 1. As such, the dataset is well-suited for use in conjunction with the multiple negatives ranking loss.\\nAs the original author suggests, the dataset has been filtered, mostly following the guidelines set by the author. Any row that doesn'tâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/danielheinz/telekom-backtrans-paraphrase-filtered."},
	{"name":"Deltacorpus_1.1","keyword":"swiss german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FrancophonIA/Deltacorpus_1.1","creator_name":"FrancophonIA","creator_url":"https://huggingface.co/FrancophonIA","description":"\\nDataset origin: https://lindat.cz/repository/xmlui/handle/11234/1-1743\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nTexts in 107 languages from the W2C corpus (http://hdl.handle.net/11858/00-097C-0000-0022-6133-9), first 1,000,000 tokens per language, tagged by the delexicalized tagger described in Yu et al. (2016, LREC, PortoroÅ¾, Slovenia).\\nChanges in version 1.1: \\n\\nUniversal Dependencies tagset instead of the older and smaller Google Universal POS tagset. \\n\\nSVM classifier trained on Universal Dependenciesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FrancophonIA/Deltacorpus_1.1."},
	{"name":"oasst2_dpo_pairs","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexredna/oasst2_dpo_pairs","creator_name":"Alexander Gruhl","creator_url":"https://huggingface.co/alexredna","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"oasst2_dpo_pairs\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nDataset transferred into the structure for trainig with DPO and can be used with the Alignment Handbook\\nThe structure follows mostly the same scheme as HuggingFaceH4/ultrafeedback_binarized\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nTo load the dataset, run:\\nfrom datasets import load_dataset\\n\\nds = load_dataset(\\\"alexredna/oasst2_dpo_pairs\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nBase dataset filtered to only contain: German, English, Spanishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alexredna/oasst2_dpo_pairs."},
	{"name":"language-dataset","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neon-mao/language-dataset","creator_name":"maoqifan","creator_url":"https://huggingface.co/neon-mao","description":"\\n"},
	{"name":"seamless-align-expressive","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jhu-clsp/seamless-align-expressive","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Seamless-Align-Expressive (WIP). Inspired by https://huggingface.co/datasets/allenai/nllb\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset was created based on metadata for mined expressive Speech-to-Speech(S2S) released by Meta AI.  The S2S contains data for 5 language pairs. The S2S dataset is ~228GB compressed.\\n\\n\\t\\n\\t\\t\\n\\t\\tHow to use the data\\n\\t\\n\\nThere are two ways to access the data:\\n\\nVia the Hugging Face Python datasets library\\n\\nScripts coming soon\\n\\n\\nClone the git repo\\n\\ngitâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/seamless-align-expressive."},
	{"name":"Deltacorpus_1.1","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FrancophonIA/Deltacorpus_1.1","creator_name":"FrancophonIA","creator_url":"https://huggingface.co/FrancophonIA","description":"\\nDataset origin: https://lindat.cz/repository/xmlui/handle/11234/1-1743\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nTexts in 107 languages from the W2C corpus (http://hdl.handle.net/11858/00-097C-0000-0022-6133-9), first 1,000,000 tokens per language, tagged by the delexicalized tagger described in Yu et al. (2016, LREC, PortoroÅ¾, Slovenia).\\nChanges in version 1.1: \\n\\nUniversal Dependencies tagset instead of the older and smaller Google Universal POS tagset. \\n\\nSVM classifier trained on Universal Dependenciesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FrancophonIA/Deltacorpus_1.1."},
	{"name":"MM-Eval","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prometheus-eval/MM-Eval","creator_name":"prometheus-eval","creator_url":"https://huggingface.co/prometheus-eval","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMultilingual Meta-EVALuation benchmark (MM-Eval)\\n\\t\\n\\n\\nðŸ‘¨â€ðŸ’»Code\\n|\\nðŸ“„Paper\\n|\\nðŸ¤— MMQA\\n\\n\\nMM-Eval is a multilingual meta-evaluation benchmark consisting of five core subsetsâ€”Chat, Reasoning, Safety, Language Hallucination, and Linguisticsâ€”spanning 18 languages and a Language Resource subset spanning 122 languages for a broader analysis of language effects. \\n\\nDesign ChoiceIn this work, we minimize the inclusion of translated samples, as mere translation may alter existing preferences dueâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prometheus-eval/MM-Eval."},
	{"name":"Saka-Alpaca-v1","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sakalti/Saka-Alpaca-v1","creator_name":"sakasan","creator_url":"https://huggingface.co/Sakalti","description":"https://chatgpt.com\\n"},
	{"name":"linkedin-industry-list","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fantastic-jobs/linkedin-industry-list","creator_name":"Fantastic.jobs","creator_url":"https://huggingface.co/fantastic-jobs","description":"fantastic-jobs/linkedin-industry-list dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"tla-late_egyptian-v19-premium","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thesaurus-linguae-aegyptiae/tla-late_egyptian-v19-premium","creator_name":"Thesaurus Linguae Aegyptiae","creator_url":"https://huggingface.co/thesaurus-linguae-aegyptiae","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset tla-Late_Egyptian-v19-premium\\n\\t\\n\\n\\nThis data set contains Late Egyptian sentences in hieroglyphs and transliteration, with lemmatization, with POS glossing and with a German translation. \\nThe data comes from the database of the Thesaurus Linguae Aegyptiae, corpus version 19. \\nThis set of Late Egyptian sentences only contains text witnesses classified as \\\"Late Egyptian\\\" in the TLA corpus metadata. Moreover, it contains only fully intact, \\nunambiguously readableâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thesaurus-linguae-aegyptiae/tla-late_egyptian-v19-premium."},
	{"name":"MMMLU","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bzantium/MMMLU","creator_name":"Minho Ryu","creator_url":"https://huggingface.co/bzantium","description":"\\n\\t\\n\\t\\t\\n\\t\\tMultilingual Massive Multitask Language Understanding (MMMLU)\\n\\t\\n\\nThe MMLU is a widely recognized benchmark of general knowledge attained by AI models. It covers a broad range of topics from 57 different categories, covering elementary-level knowledge up to advanced professional subjects like law, physics, history, and computer science.\\nWe translated the MMLUâ€™s test set into 14 languages using professional human translators. Relying on human translators for this evaluation increasesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bzantium/MMMLU."},
	{"name":"MX-CHAT","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berwart/MX-CHAT","creator_name":"Berwart","creator_url":"https://huggingface.co/berwart","description":"MX-CHAT 01\\n"},
	{"name":"German-RAG-HARD-BENCHMARK-REASONING-EVAL-OPEN-SOURCE","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-HARD-BENCHMARK-REASONING-EVAL-OPEN-SOURCE","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\nSee the image of the comparison in full-size here\\n"},
	{"name":"BenchMAX_Rule-based","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Rule-based","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\\nLink: https://arxiv.org/pdf/2502.07346\\nRepository: https://github.com/CONE-MT/BenchMAX\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBenchMAX_Rule-based is a dataset of BenchMAX, sourcing from IFEval, which is a rule-based benchmark for evaluating the instruction following capabilities.\\nWe extend the original dataset to 16 non-English languages by first translating and then manualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Rule-based."},
	{"name":"BenchMAX_Model-based","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\\nLink: https://arxiv.org/pdf/2502.07346\\nRepository: https://github.com/CONE-MT/BenchMAX\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBenchMAX_Model-based is a dataset of BenchMAX, sourcing from m-ArenaHard.\\nWe extend the original English dataset to 16 non-English languages by first translating and then manual post-editing.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Languages\\n\\t\\n\\nArabic, Bengali, Chinese, Czechâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based."},
	{"name":"BenchMAX_Multiple_Functions","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Multiple_Functions","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\\nLink: https://arxiv.org/pdf/2502.07346\\nRepository: https://github.com/CONE-MT/BenchMAX\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBenchMAX_Multiple_Functions is a dataset of BenchMAX, sourcing from Nexus.\\nWe translate the standardized queries from English to 16 non-English languages by google Translate.\\nSome special function arguments remain English since the APIs are in English.\\nAllâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Multiple_Functions."},
	{"name":"BenchMAX_General_Translation","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_General_Translation","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\\nLink: https://arxiv.org/pdf/2502.07346\\nRepository: https://github.com/CONE-MT/BenchMAX\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBenchMAX_General_Translation is a dataset of BenchMAX.\\nWe collect parallel test data from Flore-200, TED-talk, and WMT24.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Languages\\n\\t\\n\\nArabic, Bengali, Chinese, Czech, English, French, German, Hungarian, Japanese, Korean, Serbian, Spanishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_General_Translation."},
	{"name":"BenchMAX_Domain_Translation","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Domain_Translation","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\\nLink: https://arxiv.org/pdf/2502.07346\\nRepository: https://github.com/CONE-MT/BenchMAX\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nBenchMAX_Domain_Translation is a dataset of BenchMAX.\\nWe collect the domain parallel data from other tasks in BenchMAX.\\nEach sample contains one or three human-annotated translations.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Languages\\n\\t\\n\\nArabic, Bengali, Chinese, Czech, Englishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Domain_Translation."},
	{"name":"dtak-transnormer-basic-v1","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/textplus-bbaw/dtak-transnormer-basic-v1","creator_name":"Text+ at Berlin-Brandenburgische Akademie der Wissenschaften","creator_url":"https://huggingface.co/textplus-bbaw","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for DTAK-transnormer-basic (v1.0)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nDTAK-transnormer-basic is a modified subset of the DTA-Kernkorpus (Deutsches Textarchiv, German Text Archive Core Corpus).\\nIt is a parallel corpus of German texts from the period between 1600 to 1899, that aligns sentences in historical spelling with their normalizations.\\nA normalization is a modified version of the original text that is adapted to modern spelling conventions.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/textplus-bbaw/dtak-transnormer-basic-v1."},
	{"name":"subtractionerror","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TinaAbt/subtractionerror","creator_name":"Tina Abt","creator_url":"https://huggingface.co/TinaAbt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Subtractionerror\\n\\t\\n\\nThis dataset mimics a primary student that has troubles with substracting over the tens.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset mimics a primary student that has troubles with substracting over the tens.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe data is a json file with questions and answers\\n"},
	{"name":"AufenthG","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/YaTutarsa/AufenthG","creator_name":"BeN","creator_url":"https://huggingface.co/YaTutarsa","description":"YaTutarsa/AufenthG dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"agentic_synthetic_customer_service_conversations","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marccgrau/agentic_synthetic_customer_service_conversations","creator_name":"Marc Christopher Grau","creator_url":"https://huggingface.co/marccgrau","description":"\\n\\t\\n\\t\\t\\n\\t\\tLLM-filtered Customer Service Conversations Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains simulated conversations generated by our agentic simulation system.\\nThe conversations are filtered by a LLM to ensure they are of high quality.\\nEach record is stored in JSON Lines (JSONL) format and includes:\\n\\nInput Settings: Metadata such as selected bank, customer, agent profiles, and task details.\\nMessages: The full conversation messages.\\nSummary: A German summary of the conversation.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/marccgrau/agentic_synthetic_customer_service_conversations."},
	{"name":"wikibooks-wikijunior","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bbunzeck/wikibooks-wikijunior","creator_name":"Bastian Bunzeck","creator_url":"https://huggingface.co/bbunzeck","description":"MiniKlexikon\\nThis dataset represents a scrape (approx. 250K lexical tokens) of the website Wikibooks Wikijunior section, the Wikibooks bookshelve of the German Wikibooks collection that contains educational material aimed at children. Texts were collected on Feb 20, 2025. At the time of writing, the texts mainly revolve around medicine, how things work, the solar system, information technology, and more.\\nAll texts are licensed under cc-by-sa-4.0.\\nThe individual text files contain only the bodyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bbunzeck/wikibooks-wikijunior."},
	{"name":"V1Q","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\\nds = load_dataset(\\\"b3x0m/Chinese-H-Novels\\\")\\nimport sagemaker\\nimport boto3\\nfrom sagemaker.huggingface import HuggingFaceModel\\ntry:\\n    role = sagemaker.get_execution_role()\\nexcept ValueError:\\n    iam = boto3.client('iam')\\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\\n\\n\\t\\n\\t\\t\\n\\t\\tHub Model configuration. https://huggingface.co/models\\n\\t\\n\\nhub = {\\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\\n    'HF_TASK':'any-to-any'\\n}\\n\\n\\t\\n\\t\\t\\n\\t\\tcreateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q."},
	{"name":"PleIAs-ToxicCommons","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/PleIAs-ToxicCommons","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\tPleIAs/ToxicCommons\\n\\t\\n\\nThis dataset is a refined version of the PleIAs/ToxicCommons collection, focusing on historical texts labeled for content that may be considered objectionable by modern standards (what the authors of the dataset deem \\\"toxic\\\"). \\nThe cleaned dataset contains 1â€‰051â€‰027 rows, each representing a text sample with associated toxicity scores across five dimensions:\\n\\nRace and origin-based bias\\nGender and sexuality-based bias\\nReligious bias\\nAbility bias\\nViolence and abuseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/PleIAs-ToxicCommons."},
	{"name":"RSSNEWS","keyword":"german","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TiberiuCristianLeon/RSSNEWS","creator_name":"Tiberiu Cristian Leon","creator_url":"https://huggingface.co/TiberiuCristianLeon","description":"\\n\\t\\n\\t\\t\\n\\t\\tRSS News Romanian/German/English\\n\\t\\n\\n"},
	{"name":"MOL","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/franciellevargas/MOL","creator_name":"Francielle Vargas","creator_url":"https://huggingface.co/franciellevargas","description":"\\n\\t\\n\\t\\t\\n\\t\\tMOL - Context-Aware Multilingual Offensive Lexicon\\n\\t\\n\\nThe MOL is the first specialized lexicon for hate speech detection, annotated with contextual information.\\nIt consists of 1,000 explicit and implicit (clue-based) human-annotated rationales used with pejorative connotations, manually identified by a linguist and annotated by three experts regarding their contextual dependency (context-dependent or context-independent).\\nFor example, the term \\\"stupid\\\" is classified as aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/franciellevargas/MOL."},
	{"name":"apps-competition-de","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LenDigLearn/apps-competition-de","creator_name":"Lennard Michael Strohmeyer","creator_url":"https://huggingface.co/LenDigLearn","description":"\\n\\t\\n\\t\\t\\n\\t\\tapps competition subset with all of the prompts translated to German\\n\\t\\n\\nAll examples were taken from codeparrot/apps and the prompts were translated to German using educa-ai-nemo-dpo.\\nNo filtering or processing has been done afterwards, use with care.\\n"},
	{"name":"multilingual_translation_gen_binarized","keyword":"german","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Youseff1987/multilingual_translation_gen_binarized","creator_name":"JOON HYOUNG JUN","creator_url":"https://huggingface.co/Youseff1987","description":"Youseff1987/multilingual_translation_gen_binarized dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"mini-klexikon","keyword":"german","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FabianToSpace/mini-klexikon","creator_name":"Fabian Haeger","creator_url":"https://huggingface.co/FabianToSpace","description":"FabianToSpace/mini-klexikon dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"HistLuxAlign","keyword":"german","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/impresso-project/HistLuxAlign","creator_name":"Impresso - Media Monitoring of the Past","creator_url":"https://huggingface.co/impresso-project","description":"impresso-project/HistLuxAlign dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"gsm8k-translated","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sara237/gsm8k-translated","creator_name":"Sara Rajaee","creator_url":"https://huggingface.co/Sara237","description":"Sara237/gsm8k-translated dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Emilia-YODAS","keyword":"german","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrfakename/Emilia-YODAS","creator_name":"mrfakename","creator_url":"https://huggingface.co/mrfakename","description":"A mirror of the Emilia-YODAS dataset. Only includes the YODAS subset from the original dataset.\\nhttps://huggingface.co/datasets/amphion/Emilia-Dataset\\n"},
	{"name":"HPLT2.0_cleaned","keyword":"german","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jobs-git/HPLT2.0_cleaned","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","description":"This is a large-scale collection of web-crawled documents in 191 world languages, produced by the HPLT project. \\nThe source of the data is mostly Internet Archive with some additions from Common Crawl.\\nFor a detailed description of the dataset, please refer to https://hplt-project.org/datasets/v2.0\\nThe Cleaned variant of HPLT Datasets v2.0\\nThis is the cleaned variant of the HPLT Datasets v2.0 converted to the Parquet format semi-automatically when being uploaded here. \\nThe original JSONL filesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jobs-git/HPLT2.0_cleaned."},
	{"name":"taiga_corpus_subtitles","keyword":"german","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Fascinat0r/taiga_corpus_subtitles","creator_name":"Nikita Kulakov","creator_url":"https://huggingface.co/Fascinat0r","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Taiga Corpus - TV Series Subtitles\\n\\t\\n\\nThis dataset contains subtitles extracted from various TV series. The original data is sourced from the Taiga Corpus. It consists of line-level subtitle information with precise timing and additional metadata including series title and episode information. The dataset is designed for tasks such as subtitle alignment, translation, and dialogue analysis.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nEach record in the dataset contains the followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Fascinat0r/taiga_corpus_subtitles."}
]
;
