const data_for_language_asia_nepali = 
[
	{"name":"Nepali","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SunilC/Nepali","creator_name":"Sunil Chaudhary","creator_url":"https://huggingface.co/SunilC","description":"SunilC/Nepali dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Nepali-Roman-Transliteration","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Saugatkafley/Nepali-Roman-Transliteration","creator_name":"Saugat Kafley","creator_url":"https://huggingface.co/Saugatkafley","description":"Saugatkafley/Nepali-Roman-Transliteration dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"okapi-ranking","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Saugatkafley/okapi-ranking","creator_name":"Saugat Kafley","creator_url":"https://huggingface.co/Saugatkafley","description":"Saugatkafley/okapi-ranking dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"XLSum-nepali-summerization-dataset","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sanjeev-bhandari01/XLSum-nepali-summerization-dataset","creator_name":"Sanjeev Bhandari","creator_url":"https://huggingface.co/sanjeev-bhandari01","description":"sanjeev-bhandari01/XLSum-nepali-summerization-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"XLSum-nepali-summerization-dataset","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sanjeev-bhandari01/XLSum-nepali-summerization-dataset","creator_name":"Sanjeev Bhandari","creator_url":"https://huggingface.co/sanjeev-bhandari01","description":"sanjeev-bhandari01/XLSum-nepali-summerization-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"nep-spell-synthetic-27k","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/duraad/nep-spell-synthetic-27k","creator_name":"Dura Amar","creator_url":"https://huggingface.co/duraad","description":"Contains 27k sentence pairs for Nepali Spell Checking.\\nhttps://www.kaggle.com/code/amardura/thegroup-nep-spell-synthetic-datapoints\\n"},
	{"name":"nep-spell-synthetic-27k","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/duraad/nep-spell-synthetic-27k","creator_name":"Dura Amar","creator_url":"https://huggingface.co/duraad","description":"Contains 27k sentence pairs for Nepali Spell Checking.\\nhttps://www.kaggle.com/code/amardura/thegroup-nep-spell-synthetic-datapoints\\n"},
	{"name":"thegroup-datafile-31k","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/duraad/thegroup-datafile-31k","creator_name":"Dura Amar","creator_url":"https://huggingface.co/duraad","description":""},
	{"name":"nepali-summarization-dataset","keyword":"nepali","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/sanjeev-bhandari01/nepali-summarization-dataset","creator_name":"Sanjeev Bhandari","creator_url":"https://huggingface.co/sanjeev-bhandari01","description":"This dataset was intended to to be used for finetuning the nepali text summerization task.\\nFeel free to contribute to this readme to add any information\\n"},
	{"name":"nepali-summarization-dataset","keyword":"nepali","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/sanjeev-bhandari01/nepali-summarization-dataset","creator_name":"Sanjeev Bhandari","creator_url":"https://huggingface.co/sanjeev-bhandari01","description":"This dataset was intended to to be used for finetuning the nepali text summerization task.\\nFeel free to contribute to this readme to add any information\\n"},
	{"name":"indic-align","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai4bharat/indic-align","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIndicAlign\\n\\t\\n\\nA diverse collection of Instruction and Toxic alignment datasets for 14 Indic Languages. The collection comprises of:\\n\\nIndicAlign - Instruct\\nIndic-ShareLlama\\nDolly-T\\nOpenAssistant-T\\nWikiHow\\nIndoWordNet\\nAnudesh\\nWiki-Conv\\nWiki-Chat\\n\\n\\nIndicAlign - Toxic\\nHHRLHF-T\\nToxic-Matrix\\n\\n\\n\\nWe use IndicTrans2 (Gala et al., 2023) for the translation of the datasets. \\nWe recommend the readers to check out our paper on Arxiv for detailed information on the curation process of these‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/indic-align."},
	{"name":"Nadi_Indic466k_Instruct","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/convaiinnovations/Nadi_Indic466k_Instruct","creator_name":"Convai Innovations","creator_url":"https://huggingface.co/convaiinnovations","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNadi_Indic466K_Instruct Dataset\\n\\t\\n\\nThe Nadi_Indic466K_Instruct dataset is the world's first coding dataset with 18 Indian language support, 466k rows and 142 Million total tokens. This dataset can be used by developers to build Indian coding language models (LLMs) for various programming languages.\\nQ-LoRA based SFT/PPO/DPO fine-tuning can be done on the dataset in LLAMA-2 or Mistral or any opens-soure LLM for text generation.\\nThe dataset was carefully curated such that the coding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/convaiinnovations/Nadi_Indic466k_Instruct."},
	{"name":"MultiQ","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/caro-holt/MultiQ","creator_name":"Holtermann","creator_url":"https://huggingface.co/caro-holt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MultiQ\\n\\t\\n\\nThis is the dataset corresponding to the paper \\\"Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ\\\". \\nIt is a silver standard benchmark that can be used to evaluate the basic multilingual capabilities of LLMs. It contains 200 open ended questions automatically \\ntranslated into 137 typologically diverse languages. \\n\\nCurated by: Carolin Holtermann, Paul R√∂ttger, Timm Dill, Anne Lauscher\\nLanguage(s) (NLP): 137 diverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/caro-holt/MultiQ."},
	{"name":"NTREX","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davidstap/NTREX","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nNTREX -- News Test References for MT Evaluation from English into a total of 128 target languages. See original GitHub repo for full details.\\nExample of loading:\\ndataset = load_dataset(\\\"davidstap/NTREX\\\", \\\"rus_Cyrl\\\", trust_remote_code=True)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe following languages are available:\\n\\n\\t\\n\\t\\t\\nLanguage Code\\nLanguage Name\\n\\n\\n\\t\\t\\nafr_Latn\\nAfrikaans\\n\\n\\namh_Ethi\\nAmharic\\n\\n\\narb_Arab\\nArabic\\n\\n\\naze_Latn\\nAzerbaijani\\n\\n\\nbak_Cyrl\\nBashkir\\n\\n\\nbel_Cyrl\\nBelarusian‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/NTREX."},
	{"name":"nepali_sa","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DGurgurov/nepali_sa","creator_name":"Daniil Gurgurov","creator_url":"https://huggingface.co/DGurgurov","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSentiment Analysis Data for the Nepali Language\\n\\t\\n\\nDataset Description:\\nThis dataset contains a sentiment analysis dataset from Singh et al. (2020).\\nData Structure:\\nThe data was used for the project on injecting external commonsense knowledge into multilingual Large Language Models.\\nCitation:\\n@INPROCEEDINGS{9381292,\\n  author={Singh, Oyesh Mann and Timilsina, Sandesh and Bal, Bal Krishna and Joshi, Anupam},\\n  booktitle={2020 IEEE/ACM International Conference on Advances in Social‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DGurgurov/nepali_sa."},
	{"name":"NTREX","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/NTREX","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nNTREX -- News Test References for MT Evaluation from English into a total of 128 target languages. See original GitHub repo for full details.\\nExample of loading:\\ndataset = load_dataset(\\\"davidstap/NTREX\\\", \\\"rus_Cyrl\\\", trust_remote_code=True)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe following languages are available:\\n\\n\\t\\n\\t\\t\\nLanguage Code\\nLanguage Name\\n\\n\\n\\t\\t\\nafr_Latn\\nAfrikaans\\n\\n\\namh_Ethi\\nAmharic\\n\\n\\narb_Arab\\nArabic\\n\\n\\naze_Latn\\nAzerbaijani\\n\\n\\nbak_Cyrl\\nBashkir\\n\\n\\nbel_Cyrl\\nBelarusian‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NTREX."},
	{"name":"IN22-Conv","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/IN22-Conv","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIN22-Conv\\n\\t\\n\\nIN-22 is a newly created comprehensive benchmark for evaluating machine translation performance in multi-domain, n-way parallel contexts across 22 Indic languages. IN22-Conv is the conversation domain subset of IN22. It is designed to assess translation quality in typical day-to-day conversational-style applications. The evaluation subset consists of 1503 sentences translated across 22 Indic languages enabling evaluation of MT systems across 506 directions.\\nCurrently‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22-Conv."},
	{"name":"IN22-Gen","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/IN22-Gen","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIN22-Gen\\n\\t\\n\\nIN22 is a newly created comprehensive benchmark for evaluating machine translation performance in multi-domain, n-way parallel contexts across 22 Indic languages. IN22-Gen is a general-purpose multi-domain evaluation subset of IN22. It has been created from two sources: Wikipedia and Web Sources offering diverse content spanning news, entertainment, culture, legal, and India-centric topics. The evaluation subset consists of 1024 sentences translated across 22 Indic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22-Gen."},
	{"name":"ParaNames","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bltlab/ParaNames","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bltlab/ParaNames."},
	{"name":"Devanagari-Ecommerce-Dataset","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kshitizgajurel/Devanagari-Ecommerce-Dataset","creator_name":"Kshitiz Gajurel","creator_url":"https://huggingface.co/kshitizgajurel","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n‡§Ø‡•ã ‡§¶‡•á‡§µ‡§®‡§æ‡§ó‡§∞‡•Ä ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§≠‡§æ‡§∑‡§æ‡§ï‡•ã ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§µ‡§ø‡§∂‡•á‡§∑‡§ó‡§∞‡•Ä ‡§ö‡•ç‡§Ø‡§æ‡§ü‡§¨‡•ã‡§ü ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä‡§π‡§∞‡•Ç ‡§¨‡§®‡§æ‡§â‡§®‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§°‡§ø‡§ú‡§æ‡§á‡§® ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§π‡•ã‡•§ ‡§Ø‡§∏‡§Æ‡§æ ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§∂‡•ç‡§∞‡•á‡§£‡•Ä‡§π‡§∞‡•Ç‡§ï‡•ã ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü‡§π‡§∞‡•Ç ‡§∏‡§Æ‡§æ‡§µ‡•á‡§∂ ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§õ, ‡§ú‡§∏‡§≤‡§æ‡§à JSON ‡§Æ‡§æ ‡§¢‡§æ‡§Å‡§ö‡§æ ‡§¨‡§®‡§æ‡§à‡§è‡§ï‡•ã ‡§õ, ‡§ú‡§∏‡§≤‡•á ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§µ‡§æ‡§∞‡•ç‡§§‡§æ‡§≤‡§æ‡§™ ‡§è‡§Ü‡§à ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§π‡§∞‡•Ç‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•ã‡§°‡•á‡§≤‡§π‡§∞‡•Ç‡§≤‡§æ‡§à ‡§§‡§æ‡§≤‡§ø‡§Æ ‡§∞ ‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§® ‡§ó‡§∞‡•ç‡§®‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§∏‡•ç‡§∞‡•ã‡§§ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ó‡§∞‡•ç‡§¶‡§õ‡•§\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Prepared by:\\n\\t\\n\\n\\nAakash‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kshitizgajurel/Devanagari-Ecommerce-Dataset."},
	{"name":"Devanagari-Ecommerce-fomatted-for-llama2-chat-Dataset","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kshitizgajurel/Devanagari-Ecommerce-fomatted-for-llama2-chat-Dataset","creator_name":"Kshitiz Gajurel","creator_url":"https://huggingface.co/kshitizgajurel","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n‡§Ø‡•ã ‡§¶‡•á‡§µ‡§®‡§æ‡§ó‡§∞‡•Ä ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§≠‡§æ‡§∑‡§æ‡§ï‡•ã ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§µ‡§ø‡§∂‡•á‡§∑‡§ó‡§∞‡•Ä ‡§ö‡•ç‡§Ø‡§æ‡§ü‡§¨‡•ã‡§ü ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä‡§π‡§∞‡•Ç ‡§¨‡§®‡§æ‡§â‡§®‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§°‡§ø‡§ú‡§æ‡§á‡§® ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§π‡•ã‡•§ ‡§Ø‡§∏‡§Æ‡§æ ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§∂‡•ç‡§∞‡•á‡§£‡•Ä‡§π‡§∞‡•Ç‡§ï‡•ã ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü‡§π‡§∞‡•Ç ‡§∏‡§Æ‡§æ‡§µ‡•á‡§∂ ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§õ, ‡§ú‡§∏‡§≤‡§æ‡§à JSON ‡§Æ‡§æ ‡§¢‡§æ‡§Å‡§ö‡§æ ‡§¨‡§®‡§æ‡§à‡§è‡§ï‡•ã ‡§õ, ‡§ú‡§∏‡§≤‡•á ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§µ‡§æ‡§∞‡•ç‡§§‡§æ‡§≤‡§æ‡§™ ‡§è‡§Ü‡§à ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§π‡§∞‡•Ç‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•ã‡§°‡•á‡§≤‡§π‡§∞‡•Ç‡§≤‡§æ‡§à ‡§§‡§æ‡§≤‡§ø‡§Æ ‡§∞ ‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§® ‡§ó‡§∞‡•ç‡§®‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§∏‡•ç‡§∞‡•ã‡§§ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ó‡§∞‡•ç‡§¶‡§õ‡•§\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kshitizgajurel/Devanagari-Ecommerce-fomatted-for-llama2-chat-Dataset."},
	{"name":"Multilingual-Benchmark","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TaiMingLu/Multilingual-Benchmark","creator_name":"TaiMing","creator_url":"https://huggingface.co/TaiMingLu","description":"These are the GSM8K and ARC dataset translated by Google Translate. \\nBibTex\\n@misc{lu2024languagecountslearnunlearn,\\n      title={Every Language Counts: Learn and Unlearn in Multilingual LLMs}, \\n      author={Taiming Lu and Philipp Koehn},\\n      year={2024},\\n      eprint={2406.13748},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2406.13748}, \\n}\\n\\n"},
	{"name":"Customer-Care-Services-Dataset-in-Nepali","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kshitizgajurel/Customer-Care-Services-Dataset-in-Nepali","creator_name":"Kshitiz Gajurel","creator_url":"https://huggingface.co/kshitizgajurel","description":"kshitizgajurel/Customer-Care-Services-Dataset-in-Nepali dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"XL-HeadTags","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/faisaltareque/XL-HeadTags","creator_name":"Faisal Tareque Shohan","creator_url":"https://huggingface.co/faisaltareque","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for XL-HeadTags Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Source\\n\\t\\n\\nWe have used M3LS and XL-Sum as source for this dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n We provide XL-HeadTags, a large-scale news headline and tags generation dataset. The dataset consists of 20 languages across six diverse language families. It contains 415K news headline-article pairs with auxiliary information such as image captions, topic words (read more).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/faisaltareque/XL-HeadTags."},
	{"name":"Roleplay-Nepali","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jojo-ai-mst/Roleplay-Nepali","creator_name":"Min Si Thu","creator_url":"https://huggingface.co/jojo-ai-mst","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRolePlay-Nepali\\n\\t\\n\\nRoleplay-Nepali Dataset is a dataset for roleplaying in the Nepali language for the Large Language Model.\\nThe base dataset is the GPTeacher role play dataset by teknium 1, which can be found under this link, released under MIT License. The dataset is then translated into respective languages. The translation process is powered by Google Translate, using cloud translation API. \\nFor more information and other language datasets for roleplay, see this github repo.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jojo-ai-mst/Roleplay-Nepali."},
	{"name":"MegaWika","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/conceptofmind/MegaWika","creator_name":"Enrico Shippole","creator_url":"https://huggingface.co/conceptofmind","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MegaWika\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMegaWika is a multi- and crosslingual text dataset containing 30 million Wikipedia passages with their scraped and cleaned web citations. The passages span\\n50 Wikipedias in 50 languages, and the articles in which the passages were originally embedded are included for convenience. Where a Wikipedia passage is in a\\nnon-English language, an automated English translation is provided. Furthermore, nearly 130 million English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/conceptofmind/MegaWika."},
	{"name":"constitution","keyword":"nepali","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/nayan135/constitution","creator_name":"NAYAN ACHARYA","creator_url":"https://huggingface.co/nayan135","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nayan135/constitution."},
	{"name":"Bharat_NanoClimateFEVER_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoClimateFEVER_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoClimateFEVER dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoClimateFEVER_ne."},
	{"name":"Bharat_NanoDBPedia_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ne."},
	{"name":"Bharat_NanoFEVER_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoFEVER_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoFEVER dataset, specifically adapted for information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoFEVER_ne."},
	{"name":"Bharat_NanoFiQA2018_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoFiQA2018_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoFiQA2018 dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoFiQA2018_ne."},
	{"name":"Bharat_NanoMSMARCO_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoMSMARCO_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoMSMARCO dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoMSMARCO_ne."},
	{"name":"Bharat_NanoNQ_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoNQ_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoNQ dataset, specifically adapted for information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoNQ_ne."},
	{"name":"Nepali_metaphor_dataset","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bnabin/Nepali_metaphor_dataset","creator_name":"Nabin Bhusal","creator_url":"https://huggingface.co/bnabin","description":"bnabin/Nepali_metaphor_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Nepali_metaphor_dataset","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bnabin/Nepali_metaphor_dataset","creator_name":"Nabin Bhusal","creator_url":"https://huggingface.co/bnabin","description":"bnabin/Nepali_metaphor_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Bharat_NanoSCIDOCS_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoSCIDOCS_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoSCIDOCS dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoSCIDOCS_ne."},
	{"name":"Bharat_NanoSciFact_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoSciFact_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoSciFact dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoSciFact_ne."},
	{"name":"Nepali-Small","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NKRSubedi/Nepali-Small","creator_name":"NKRS","creator_url":"https://huggingface.co/NKRSubedi","description":"NKRSubedi/Nepali-Small dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Bharat_NanoTouche2020_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoTouche2020_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoTouche2020 dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoTouche2020_ne."},
	{"name":"IN22-Conv-Doc-Level","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VarunGumma/IN22-Conv-Doc-Level","creator_name":"Varun Gumma","creator_url":"https://huggingface.co/VarunGumma","description":"This dataset was constructed by merging individual conversations from the IN22-Conv dataset to create a long-context, document-level parallel benchmark. For further information on domains and statistics, please refer to the original paper and dataset.\\n"},
	{"name":"Flores-Indic-Doc-Level","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VarunGumma/Flores-Indic-Doc-Level","creator_name":"Varun Gumma","creator_url":"https://huggingface.co/VarunGumma","description":"This dataset was constructed by merging individual sentences from the Flores dataset based on matching domain, topic, and URL attributes. The result is a long-context, document-level parallel benchmark. For more details on the domains and dataset statistics, please refer to the original paper and the dataset.\\n"},
	{"name":"X-ALMA-Preference","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/haoranxu/X-ALMA-Preference","creator_name":"Haoran Xu","creator_url":"https://huggingface.co/haoranxu","description":"This is the translation preference dataset used by X-ALMA.\\nsource: the source sentence.\\nchosen: the preferred translation.\\nreject: the dis-preferred translation.\\ndirections: the translation direction.\\n@misc{xu2024xalmaplugplay,\\n      title={X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale}, \\n      author={Haoran Xu and Kenton Murray and Philipp Koehn and Hieu Hoang and Akiko Eriguchi and Huda Khayrallah},\\n      year={2024},\\n      eprint={2410.03115}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/haoranxu/X-ALMA-Preference."},
	{"name":"Devnagari-Romanized-Pair","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nirajandhakal/Devnagari-Romanized-Pair","creator_name":"Nirajan Dhakal","creator_url":"https://huggingface.co/nirajandhakal","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThe dataset devanagari romanized pair contains, 959 rows, where each row has one English sentence and its corresponding Nepali translations both in the devanagari script and in romanized format, the size of the data set is less than 1,000 elements and it's designed for use in Translation, text generation and text to text generation tasks.\\n"},
	{"name":"Devnagari-Romanized-Pair","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nirajandhakal/Devnagari-Romanized-Pair","creator_name":"Nirajan Dhakal","creator_url":"https://huggingface.co/nirajandhakal","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThe dataset devanagari romanized pair contains, 959 rows, where each row has one English sentence and its corresponding Nepali translations both in the devanagari script and in romanized format, the size of the data set is less than 1,000 elements and it's designed for use in Translation, text generation and text to text generation tasks.\\n"},
	{"name":"dataset-for-peft-cv-nepds","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kiranpantha/dataset-for-peft-cv-nepds","creator_name":"Kiran Pantha","creator_url":"https://huggingface.co/kiranpantha","description":"kiranpantha/dataset-for-peft-cv-nepds dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"nep_qa_test","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jangedoo/nep_qa_test","creator_name":"Sanjaya Subedi","creator_url":"https://huggingface.co/jangedoo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Nepali Q&A Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nNepali Q&A Dataset\\nThis dataset was automatically generated using this library\\n\\nCurated by: ['Sanjaya Subedi jangedoo@gmail.com']\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): ['ne']\\nLicense: mit\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More Information Needed]\\nPaper [optional]: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jangedoo/nep_qa_test."},
	{"name":"Global-MMLU","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/Global-MMLU","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGlobal-MMLU üåç is a multilingual evaluation set spanning 42 languages, including English. This dataset combines machine translations for MMLU questions along with professional translations and crowd-sourced post-edits.\\nIt also includes cultural sensitivity annotations for a subset of the questions (2850 questions per language) and classifies them as Culturally Sensitive (CS) üóΩ or Culturally Agnostic (CA) ‚öñÔ∏è. These annotations were collected as part of an open‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/Global-MMLU."},
	{"name":"toxi-text-3M","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FredZhang7/toxi-text-3M","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","description":"This is a large multilingual toxicity dataset with 3M rows of text data from 55 natural languages, all of which are written/sent by humans, not machine translation models.\\nThe preprocessed training data alone consists of 2,880,667 rows of comments, tweets, and messages. Among these rows, 416,529 are classified as toxic, while the remaining 2,463,773 are considered neutral. Below is a table to illustrate the data composition:\\n\\n\\t\\n\\t\\t\\n\\nToxic\\nNeutral\\nTotal\\n\\n\\n\\t\\t\\nmultilingual-train-deduplicated.csv‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/toxi-text-3M."},
	{"name":"belebele","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/belebele","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Belebele Benchmark for Massively Multilingual NLU Evaluation\\n\\t\\n\\nBelebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. This dataset enables the evaluation of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the FLORES-200 dataset. The human annotation procedure was carefully curated to create questions that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/belebele."},
	{"name":"multilingual-pl-bert","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/styletts2-community/multilingual-pl-bert","creator_name":"StyleTTS 2 Community","creator_url":"https://huggingface.co/styletts2-community","description":"Attribution: Wikipedia.org\\n"},
	{"name":"CulturaY","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ontocord/CulturaY","creator_name":"Ontocord.AI","creator_url":"https://huggingface.co/ontocord","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCulturaY: A Large Cleaned Multilingual Dataset of 75 Languages\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nFrom the team that brought you CulturaX, we present CulturaY, another substantial multilingual dataset of 15TB (uncompressed)/3TB (zstd-compressed) that applies the same dataset cleaning methodology to the HPLT v1.1 dataset. \\nPlease note that HPLT v1.2 has also been released and is an alternative verison with different cleaning methodolgies. \\nThis data was used in part to train our SOTA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ontocord/CulturaY."},
	{"name":"sangraha","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai4bharat/sangraha","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","description":"\\n\\t\\n\\t\\t\\n\\t\\tSangraha\\n\\t\\n\\n\\n  \\n\\n\\nSangraha is the largest high-quality, cleaned Indic language pretraining data containing 251B tokens summed up over 22 languages, extracted from curated sources, existing multilingual corpora and large scale translations.\\nMore information:\\n\\nFor detailed information on the curation and cleaning process of Sangraha, please checkout our paper on Arxiv;\\nCheck out the scraping and cleaning pipelines used to curate Sangraha on GitHub;\\n\\n\\n\\t\\n\\t\\n\\t\\n\\t\\tGetting Started\\n\\t\\n\\nFor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/sangraha."},
	{"name":"webui-dom-snapshots","keyword":"nepali","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gbenson/webui-dom-snapshots","creator_name":"Gary Benson","creator_url":"https://huggingface.co/gbenson","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WebUI DOM snapshots\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: Gary Benson\\nLanguages: Mostly English (87%);\\nDutch, French, Chinese, Japanese (1-2% each); 30+ others (<1% each)\\nLicense: CC0 1.0 Universal\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More Information Needed]\\nPaper [optional]: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gbenson/webui-dom-snapshots."},
	{"name":"Nepali-Text-Corpus","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/IRIISNEPAL/Nepali-Text-Corpus","creator_name":"Institute for Research and Innovation in Intelligent Systems (IRIIS)","creator_url":"https://huggingface.co/IRIISNEPAL","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNepali Text Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nNepali-Text-Corpus is a comprehensive collection of approximately 6.4 million articles in the \\nNepali language. This dataset is the largest text dataset on Nepali Language. It  encompasses a \\ndiverse range of text types, including news articles, blogs, and more, making it an invaluable \\nresource for researchers, developers, and enthusiasts in the fields of Natural Language Processing (NLP)\\nand computational linguistics.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IRIISNEPAL/Nepali-Text-Corpus."},
	{"name":"roman2nepali-transliteration","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/syubraj/roman2nepali-transliteration","creator_name":"Yubraj Sigdel","creator_url":"https://huggingface.co/syubraj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUse this dataset\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndata = load_dataset(\\\"syubraj/roman2nepali-transliteration\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Dataset:\\n\\t\\n\\nSaugatkafley/Nepali-Roman-Transliteration\\n"},
	{"name":"roman2nepali-transliteration","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/syubraj/roman2nepali-transliteration","creator_name":"Yubraj Sigdel","creator_url":"https://huggingface.co/syubraj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUse this dataset\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndata = load_dataset(\\\"syubraj/roman2nepali-transliteration\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Dataset:\\n\\t\\n\\nSaugatkafley/Nepali-Roman-Transliteration\\n"},
	{"name":"belebele-fleurs","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WueNLP/belebele-fleurs","creator_name":"W√ºNLP","creator_url":"https://huggingface.co/WueNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBelebele-Fleurs\\n\\t\\n\\nBelebele-Fleurs is a dataset suitable to evaluate two core tasks:\\n\\nMultilingual Spoken Language Understanding (Listening Comprehension): For each spoken paragraph, the task is to answer a multiple-choice question. The question and four answer choices are provided in text form.\\nMultilingual Long-Form Automatic Speech Recognition (ASR) with Diverse Speakers: By concatenating sentence-level utterances, long-form audio clips (ranging from 30 seconds to 1 minute 30‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/belebele-fleurs."},
	{"name":"include-base-44","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/include-base-44","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tINCLUDE-base (44 languages)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nPaper: http://arxiv.org/abs/2411.19799\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \\nIt contains 22,637 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/include-base-44."},
	{"name":"include-lite-44","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/include-lite-44","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tINCLUDE-lite (44 languages)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nPaper: http://arxiv.org/abs/2411.19799\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \\nIt contains 11,095 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/include-lite-44."},
	{"name":"2M-Belebele","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/2M-Belebele","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t2M-Belebele\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHighly-Multilingual Speech and American Sign Language Comprehension Dataset\\n\\t\\n\\nWe introduce 2M-Belebele as the first highly multilingual speech and American Sign Language (ASL) comprehension dataset. Our dataset, which is an extension of the existing Belebele only-text dataset, covers 74 spoken languages at the intersection of Belebele and Fleurs, and one sign language (ASL). \\nThe speech dataset is built from aligning Belebele, Flores200 and Fleurs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/2M-Belebele."},
	{"name":"reranker_continuous_filt_max7_train","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lightblue/reranker_continuous_filt_max7_train","creator_name":"Lightblue KK.","creator_url":"https://huggingface.co/lightblue","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReranker training data\\n\\t\\n\\nThis data was generated using 4 steps:\\n\\nWe gathered queries and corresponding text data from 35 high quality datasets covering more than 95 languages.\\nFor datasets which did not already have negative texts for queries, we mined hard negatives using the BAAI/bge-m3 embedding model.\\nFor each query, we selected one positive and one negative text and used Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4 to rate the relatedness of each query-text pair using a token \\\"1\\\", \\\"2\\\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lightblue/reranker_continuous_filt_max7_train."},
	{"name":"smol","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/smol","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\\n\\t\\n\\t\\t\\n\\t\\tSMOL\\n\\t\\n\\nSMOL (Set for Maximal Overall Leverage) is a collection of professional\\ntranslations into 221 Low-Resource Languages, for the purpose of training\\ntranslation models, and otherwise increasing the representations of said\\nlanguages in NLP and technology.\\nPlease read the SMOL Paper and the\\nGATITOS Paper for a much more\\nthorough description!\\nThere are four resources in this directory:\\n\\nSmolDoc: document-level translations into 100 languages\\nSmolSent: sentence-level translations into‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/smol."},
	{"name":"high-quality-multilingual-sentences","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/high-quality-multilingual-sentences","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\tHigh Quality Multilingual Sentences\\n\\t\\n\\n\\nThis dataset contains multilingual sentences derived from the agentlans/LinguaNova dataset.\\nIt includes 1.58 million rows across 51 different languages, each in its own configuration.\\n\\nExample row (from the all config):\\n{\\n    \\\"text\\\": \\\"ÿßŸÖÿßŸÖ ÿ¨ŸÖÿπŸá ÿßÿµŸÅŸáÿßŸÜ ⁄ØŸÅÿ™: ŸÖ€åÿ≤ÿßŸÜ ŸÜ€åÿßÿ≤ ÿ¢ÿ® ÿ¥ÿ±ÿ® ÿßÿµŸÅŸáÿßŸÜ €±€±.€µ ŸÖÿ™ÿ± ŸÖ⁄©ÿπÿ® ÿßÿ≥ÿ™ ⁄©Ÿá ÿ™ŸÖÿßŸÖ ÿßÿ≥ÿ™ÿßŸÜ ÿßÿµŸÅŸáÿßŸÜ ÿ±ÿß ŸæŸàÿ¥ÿ¥ ŸÖ€åÿØŸáÿØ Ÿà ŸÜÿ≥ÿ®ÿ™ ÿ®Ÿá ŸÇÿ®ŸÑ ÿßÿ≤ ÿßŸÜŸÇŸÑÿßÿ® €å⁄©€å ÿßÿ≤ Ÿæ€åÿ¥ÿ±ŸÅÿ™Ÿáÿß ÿØÿ± ÿ≠Ÿàÿ≤Ÿá ÿ¢ÿ® ÿ®ŸàÿØŸá ÿßÿ≥ÿ™.\\\",\\n    \\\"fasttext\\\": \\\"fa\\\",\\n    \\\"gcld3\\\": \\\"fa\\\"\\n}\\n\\nFields:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/high-quality-multilingual-sentences."},
	{"name":"opus_paracrawl","keyword":"nepali","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Helsinki-NLP/opus_paracrawl","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for OpusParaCrawl\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nParallel corpora from Web Crawls collected in the ParaCrawl project.\\nTha dataset contains:\\n\\n42 languages, 43 bitexts\\ntotal number of files: 59,996\\ntotal number of tokens: 56.11G\\ntotal number of sentence fragments: 3.13G\\n\\nTo load a language pair which isn't part of the config, all you need to do is specify the language code as pairs,\\ne.g.\\ndataset = load_dataset(\\\"opus_paracrawl\\\", lang1=\\\"en\\\", lang2=\\\"so\\\")\\n\\nYou can find‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Helsinki-NLP/opus_paracrawl."},
	{"name":"opus_ubuntu","keyword":"nepali","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/Helsinki-NLP/opus_ubuntu","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Opus Ubuntu\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThese are translations of the Ubuntu software package messages, donated by the Ubuntu community.\\nTo load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.\\nYou can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/Ubuntu.php\\nE.g.\\ndataset = load_dataset(\\\"opus_ubuntu\\\", lang1=\\\"it\\\", lang2=\\\"pl\\\")\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Helsinki-NLP/opus_ubuntu."},
	{"name":"nepalitext-language-model-dataset","keyword":"nepali","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sakonii/nepalitext-language-model-dataset","creator_name":"Utsav Maskey","creator_url":"https://huggingface.co/Sakonii","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"nepalitext-language-model-dataset\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\\"NepaliText\\\" language modeling dataset is a collection of over 13 million Nepali text sequences (phrases/sentences/paragraphs) extracted by combining the datasets: OSCAR , cc100 and a set of scraped Nepali articles on Wikipedia. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset is intended to pre-train language models and word representations on Nepali Language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sakonii/nepalitext-language-model-dataset."},
	{"name":"flores_101","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gsarti/flores_101","creator_name":"Gabriele Sarti","creator_url":"https://huggingface.co/gsarti","description":"One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the \\nlack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource \\nlanguages, consider only restricted domains, or are low quality because they are constructed using \\nsemi-automatic procedures. In this work, we introduce the FLORES evaluation benchmark, consisting of 3001 \\nsentences extracted from English Wikipedia and covering a variety of different topics and domains. \\nThese sentences have been translated in 101 languages by professional translators through a carefully \\ncontrolled process. The resulting dataset enables better assessment of model quality on the long tail of \\nlow-resource languages, including the evaluation of many-to-many multilingual translation systems, as all \\ntranslations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, \\nwe hope to foster progress in the machine translation community and beyond."},
	{"name":"wit_base","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wikimedia/wit_base","creator_name":"Wikimedia","creator_url":"https://huggingface.co/wikimedia","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for WIT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWikimedia's version of the Wikipedia-based Image Text (WIT) Dataset, a large multimodal multilingual dataset.\\nFrom the official blog post:\\n\\nThe core training data is taken from the Wikipedia Image-Text (WIT) Dataset, a large curated set of more than 37 million image-text associations extracted from Wikipedia articles in 108 languages that was recently released by Google Research.\\nThe WIT dataset offers extremely valuable data about the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wikimedia/wit_base."},
	{"name":"xP3all","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bigscience/xP3all","creator_name":"BigScience Workshop","creator_url":"https://huggingface.co/bigscience","description":"xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks. It is used for the training of BLOOMZ and mT0, multilingual language models capable of following human instructions in dozens of languages zero-shot."},
	{"name":"xP3mt","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bigscience/xP3mt","creator_name":"BigScience Workshop","creator_url":"https://huggingface.co/bigscience","description":"xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks. It is used for the training of BLOOMZ and mT0, multilingual language models capable of following human instructions in dozens of languages zero-shot."},
	{"name":"HashtagPrediction","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Twitter/HashtagPrediction","creator_name":"Twitter","creator_url":"https://huggingface.co/Twitter","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHashtag Prediction Dataset from paper TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations\\n\\t\\n\\n  \\nThis repo contains the Hashtag prediction dataset from our paper TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations. \\n[arXiv]\\n[HuggingFace Models]\\n[Github repo]\\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDownload\\n\\t\\n\\nUse the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Twitter/HashtagPrediction."},
	{"name":"flores_101","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/severo/flores_101","creator_name":"Sylvain Lesage","creator_url":"https://huggingface.co/severo","description":"One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the \\nlack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource \\nlanguages, consider only restricted domains, or are low quality because they are constructed using \\nsemi-automatic procedures. In this work, we introduce the FLORES evaluation benchmark, consisting of 3001 \\nsentences extracted from English Wikipedia and covering a variety of different topics and domains. \\nThese sentences have been translated in 101 languages by professional translators through a carefully \\ncontrolled process. The resulting dataset enables better assessment of model quality on the long tail of \\nlow-resource languages, including the evaluation of many-to-many multilingual translation systems, as all \\ntranslations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, \\nwe hope to foster progress in the machine translation community and beyond."},
	{"name":"all-scam-spam","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FredZhang7/all-scam-spam","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","description":"This is a large corpus of 42,619 preprocessed text messages and emails sent by humans in 43 languages. is_spam=1 means spam and is_spam=0 means ham.\\n1040 rows of balanced data, consisting of casual conversations and scam emails in ‚âà10 languages, were manually collected and annotated by me, with some help from ChatGPT.\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSome preprcoessing algorithms\\n\\t\\n\\n\\nspam_assassin.js, followed by spam_assassin.py\\nenron_spam.py\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData composition\\n\\t\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nTo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/all-scam-spam."},
	{"name":"wikianc","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cyanic-selkie/wikianc","creator_name":"cyanic-selkie","creator_url":"https://huggingface.co/cyanic-selkie","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WikiAnc\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe WikiAnc dataset is an automatically generated dataset from Wikipedia (all languages) and Wikidata dumps (August, 2023). \\nThe code for generating the dataset can be found here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\n\\nwikificiation: The dataset can be used to train a model for Wikification.\\nnamed-entity-linking: The dataset can be used to train a model for Named Entity Linking.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe text in the dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cyanic-selkie/wikianc."},
	{"name":"entity_cs","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huawei-noah/entity_cs","creator_name":"HUAWEI Noah's Ark Lab","creator_url":"https://huggingface.co/huawei-noah","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for EntityCS\\n\\t\\n\\n\\nRepository: https://github.com/huawei-noah/noah-research/tree/master/NLP/EntityCS  \\nPaper: https://aclanthology.org/2022.findings-emnlp.499.pdf  \\nPoint of Contact: Fenia Christopoulou, Chenxi Whitehouse\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nWe use the English Wikipedia and leverage entity information from Wikidata to construct an entity-based Code Switching corpus. \\nTo achieve this, we make use of wikilinks in Wikipedia, i.e. links from one page to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huawei-noah/entity_cs."},
	{"name":"eng2nep","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/momo22/eng2nep","creator_name":"Sajjan","creator_url":"https://huggingface.co/momo22","description":"momo22/eng2nep dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Nepali-Health-QA","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Chhabi/Nepali-Health-QA","creator_name":"Acharya","creator_url":"https://huggingface.co/Chhabi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Chhabi/Nepali-Health-QA."},
	{"name":"Nepali-Health-QA","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Chhabi/Nepali-Health-QA","creator_name":"Acharya","creator_url":"https://huggingface.co/Chhabi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Chhabi/Nepali-Health-QA."},
	{"name":"megawika-report-generation","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hltcoe/megawika-report-generation","creator_name":"JHU Human Language Technology Center of Excellence","creator_url":"https://huggingface.co/hltcoe","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MegaWika for Report Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMegaWika is a multi- and crosslingual text dataset containing 30 million Wikipedia passages with their scraped and cleaned web citations. The passages span\\n50 Wikipedias in 50 languages, and the articles in which the passages were originally embedded are included for convenience. Where a Wikipedia passage is in a\\nnon-English language, an automated English translation is provided. \\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hltcoe/megawika-report-generation."},
	{"name":"Nepali-HealthChat","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NepaliAI/Nepali-HealthChat","creator_name":"NepaliAI","creator_url":"https://huggingface.co/NepaliAI","description":"NepaliAI/Nepali-HealthChat dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Pontoon-Translations","keyword":"nepali","license":"Mozilla Public License 2.0","license_url":"https://choosealicense.com/licenses/mpl-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ayymen/Pontoon-Translations","creator_name":"Mohamed Aymane Farhi","creator_url":"https://huggingface.co/ayymen","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Pontoon Translations\\n\\t\\n\\n\\n\\nThis is a dataset containing strings from various Mozilla projects on Mozilla's Pontoon localization platform and their translations into more than 200 languages.\\nSource strings are in English.\\nTo avoid rows with values like \\\"None\\\" and \\\"N/A\\\" being interpreted as missing values, pass the keep_default_na parameter like this:\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"ayymen/Pontoon-Translations\\\", keep_default_na=False)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ayymen/Pontoon-Translations."},
	{"name":"Deltacorpus_1.1","keyword":"nepali","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FrancophonIA/Deltacorpus_1.1","creator_name":"FrancophonIA","creator_url":"https://huggingface.co/FrancophonIA","description":"\\nDataset origin: https://lindat.cz/repository/xmlui/handle/11234/1-1743\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nTexts in 107 languages from the W2C corpus (http://hdl.handle.net/11858/00-097C-0000-0022-6133-9), first 1,000,000 tokens per language, tagged by the delexicalized tagger described in Yu et al. (2016, LREC, Portoro≈æ, Slovenia).\\nChanges in version 1.1: \\n\\nUniversal Dependencies tagset instead of the older and smaller Google Universal POS tagset. \\n\\nSVM classifier trained on Universal Dependencies‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FrancophonIA/Deltacorpus_1.1."},
	{"name":"Bharat_NanoHotpotQA_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoHotpotQA_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoHotpotQA dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoHotpotQA_ne."},
	{"name":"Bharat_NanoNFCorpus_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoNFCorpus_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoNFCorpus dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoNFCorpus_ne."},
	{"name":"Bharat_NanoQuoraRetrieval_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoQuoraRetrieval_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoQuoraRetrieval dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoQuoraRetrieval_ne."},
	{"name":"nepali_law_datasets","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Ashalupreti/nepali_law_datasets","creator_name":"Ashal Upreti","creator_url":"https://huggingface.co/Ashalupreti","description":"\\n\\t\\n\\t\\t\\n\\t\\tüìú Nepali Law Commission Dataset\\n\\t\\n\\nA structured dataset of Nepali legal questions and answers, focusing on the rights of people with disabilities.\\n\\n\\t\\n\\t\\t\\n\\t\\tüìå Overview\\n\\t\\n\\nThis dataset contains questions and answers based on Nepal's legal framework, particularly regarding the rights of persons with disabilities. The dataset is designed for AI/ML applications such as:‚úÖ Legal chatbots‚úÖ Question-answering models‚úÖ Legal document analysis‚úÖ NLP research on Nepali language  \\nThe dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ashalupreti/nepali_law_datasets."},
	{"name":"newari-dataset","keyword":"nepali","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SughamKharel/newari-dataset","creator_name":"Sugham Kharel","creator_url":"https://huggingface.co/SughamKharel","description":"SughamKharel/newari-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"V1Q","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\\nds = load_dataset(\\\"b3x0m/Chinese-H-Novels\\\")\\nimport sagemaker\\nimport boto3\\nfrom sagemaker.huggingface import HuggingFaceModel\\ntry:\\n    role = sagemaker.get_execution_role()\\nexcept ValueError:\\n    iam = boto3.client('iam')\\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\\n\\n\\t\\n\\t\\t\\n\\t\\tHub Model configuration. https://huggingface.co/models\\n\\t\\n\\nhub = {\\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\\n    'HF_TASK':'any-to-any'\\n}\\n\\n\\t\\n\\t\\t\\n\\t\\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q."},
	{"name":"Bharat_NanoArguAna_ne","keyword":"nepali","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoArguAna_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoArguAna dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoArguAna_ne."},
	{"name":"nepali_speech_to_text","keyword":"nepali","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pujanpaudel/nepali_speech_to_text","creator_name":"Pujan Paudel","creator_url":"https://huggingface.co/pujanpaudel","description":"\\n\\t\\n\\t\\t\\n\\t\\tNepali Speech-to-Text Dataset\\n\\t\\n\\nThis repository contains a dataset for Automatic Speech Recognition (ASR) in the Nepali language. The dataset is designed for supervised learning tasks and includes audio files along with their corresponding transcriptions. The audio samples have been collected from various open-source platforms and other publicly available sources on the internet.  \\nEach audio file has an average length of 15 seconds and has been converted into a consistent WAV format‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pujanpaudel/nepali_speech_to_text."},
	{"name":"squad_v1.1_np","keyword":"nepali","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/suban244/squad_v1.1_np","creator_name":"Suban Shrestha","creator_url":"https://huggingface.co/suban244","description":"\\n\\t\\n\\t\\t\\n\\t\\tSQuAD-NP: Nepali Translation of SQuAD\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nSQuAD-NP is a Nepali translation of the SQuAD dataset, the standard dataset for question answering task. The dataset consists of Nepali translations of the original English passages, questions, and answers from the SQuAD dataset. \\nThis resource is useful for training machine learning models on extractive question-answering tasks in Nepali.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nLanguage: Nepali (ne)\\nSource: Translated from SQuAD-English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suban244/squad_v1.1_np."}
]
;
