const data_for_language_asia_maithili = 
[
	{"name":"udhr-lid","keyword":"maithili","description":"\n\t\n\t\t\n\t\tUDHR-LID\n\t\n\nWhy UDHR-LID?\nYou can access UDHR (Universal Declaration of Human Rights) here, but when a verse is missing, they have texts such as \"missing\" or \"?\". Also, about 1/3 of the sentences consist only of \"articles 1-30\" in different languages. We cleaned the entire dataset from XML files and selected only the paragraphs. We cleared any unrelated language texts from the data and also removed the cases that were incorrect.\nIncorrect? Look at the ckb and kmr files in the UDHR.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/udhr-lid.","url":"https://huggingface.co/datasets/cis-lmu/udhr-lid","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Tigrinya","Balkan Romani","Standard Arabic","Metlat√≥noc Mixtec"],"keywords_longer_than_N":true},
	{"name":"finepdfs","keyword":"maithili","description":"\n\nLiberating 3T of the finest tokens from PDFs\n\n\n\t\n\t\t\n\t\tWhat is this?\n\t\n\nAs we run out of web pages to process, the natural question has always been: what to do next? Only a few knew about a data source that everyone avoided for ages, due to its incredible extraction cost and complexity: PDFs.\nüìÑ FinePDFs is exactly that. It is the largest publicly available corpus sourced exclusively from PDFs, containing about 3 trillion tokens across 475 million documents in 1733 languages.\nCompared to HTML‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/finepdfs.","url":"https://huggingface.co/datasets/HuggingFaceFW/finepdfs","creator_name":"FineData","creator_url":"https://huggingface.co/HuggingFaceFW","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Arifama-Miniafia","Ankave","Abau","Amarasi"],"keywords_longer_than_N":true},
	{"name":"xP3x","keyword":"maithili","description":"\n\t\n\t\t\n\t\tDataset Card for xP3x\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nxP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts & datasets across 277 languages & 16 NLP tasks. It contains all of xP3 + much more! It is used for training future contenders of mT0 & BLOOMZ at project Aya @Cohere Labs üß°\n\n\nCreation: The dataset can be recreated using instructions available here together with the file in this repository named xp3x_create.py. We provide this version to save processing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/xP3x.","url":"https://huggingface.co/datasets/CohereLabs/xP3x","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"smol","keyword":"maithili","description":"\n\t\n\t\t\n\t\tSMOL\n\t\n\nSMOL (Set for Maximal Overall Leverage) is a collection professional\ntranslations into 221 Low-Resource Languages, for the purpose of training\ntranslation models, and otherwise increasing the representations of said\nlanguages in NLP and technology.\nPlease read the SMOL Paper and the\nGATITOS Paper for a much more\nthorough description!\nThere are four resources in this directory:\n\nSmolDoc: document-level translations into 104 language pairs (103 unique languages)\nSmolSent:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/smol.","url":"https://huggingface.co/datasets/google/smol","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","Afar","Abkhaz","Achinese","Acoli"],"keywords_longer_than_N":true},
	{"name":"IndicLangClassification","keyword":"maithili","description":"\n  IndicLangClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA language identification test set for native-script as well as Romanized text which spans 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nWeb, Non-fiction, Written\nReference\nhttps://arxiv.org/abs/2305.15814\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"IndicLangClassification\"])‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicLangClassification.","url":"https://huggingface.co/datasets/mteb/IndicLangClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","language-identification","expert-annotated","monolingual","Assamese"],"keywords_longer_than_N":true},
	{"name":"IN22ConvBitextMining","keyword":"maithili","description":"\n  IN22ConvBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Conv is a n-way parallel conversation domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nSocial, Spoken, Fiction, Spoken\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Conv\n\n\n\t\n\nSource datasets:\n\nmteb/IN22-Conv\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22ConvBitextMining.","url":"https://huggingface.co/datasets/mteb/IN22ConvBitextMining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","multilingual","mteb/IN22-Conv","Assamese"],"keywords_longer_than_N":true},
	{"name":"IN22-Conv","keyword":"maithili","description":"\n  IN22ConvBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Conv is a n-way parallel conversation domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nSocial, Spoken, Fiction, Spoken\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Conv\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22-Conv.","url":"https://huggingface.co/datasets/mteb/IN22-Conv","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","expert-generated","multilingual","Assamese"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoFEVER_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoFEVER dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoFEVER_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoFEVER_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoFEVER","Maithili"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoHotpotQA_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoHotpotQA dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoHotpotQA_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoHotpotQA_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoHotpotQA","Maithili"],"keywords_longer_than_N":true},
	{"name":"nllb-200-10M-sample","keyword":"maithili","description":"\n\t\n\t\t\n\t\tDataset Card for \"nllb-200-10M-sample\"\n\t\n\nThis is a sample of nearly 10M sentence pairs from the NLLB-200 \nmined dataset allenai/nllb, \nscored with the model facebook/blaser-2.0-qe \ndescribed in the SeamlessM4T paper.\nThe sample is not random; instead, we just took the top n sentence pairs from each translation direction.\nThe number n was computed with the goal of upsamping the directions that contain underrepresented languages.\nNevertheless, the 187 languoids (language and script‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/slone/nllb-200-10M-sample.","url":"https://huggingface.co/datasets/slone/nllb-200-10M-sample","creator_name":"SLONE","creator_url":"https://huggingface.co/slone","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["translation","Akan","Amharic","Arabic","Awadhi"],"keywords_longer_than_N":true},
	{"name":"IndicCOPA","keyword":"maithili","description":"\\","url":"https://huggingface.co/datasets/ai4bharat/IndicCOPA","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"xP3x-sample","keyword":"maithili","description":"A multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual commonsense reasoning capabilities.","url":"https://huggingface.co/datasets/Muennighoff/xP3x-sample","creator_name":"Niklas Muennighoff","creator_url":"https://huggingface.co/Muennighoff","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"xsimplusplus","keyword":"maithili","description":"xSIM++ is an extension of xSIM. In comparison to xSIM, this evaluates using target-side data with additional synthetic, hard-to-distinguish examples. You can find more details about it in the publication: xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages.\n","url":"https://huggingface.co/datasets/jaygala24/xsimplusplus","creator_name":"Jay Gala","creator_url":"https://huggingface.co/jaygala24","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["derived","multilingual","Achinese","Mesopotamian Arabic","Ta'izzi-Adeni Arabic"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoQuoraRetrieval_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoQuoraRetrieval dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoQuoraRetrieval_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoQuoraRetrieval_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoQuoraRetrieval","Maithili"],"keywords_longer_than_N":true},
	{"name":"Indic-Rag-Suite","keyword":"maithili","description":"\n\t\n\t\t\n\t\tüåè Multilingual Indic RAG Suite\n\t\n\nA comprehensive multilingual question-answering dataset covering 18 Indian languages with 12,802,615 total samples, designed for RAG (Retrieval-Augmented Generation) applications and multilingual NLP research.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load specific language (recommended)\ndataset = load_dataset(\"AshwinSankar/Indic-Rag-Suite\", \"as\")\ntrain_data = dataset['train']\n\nprint(f\"Loaded {len(train_data)} samples\")\n\n# Access‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AshwinSankar/Indic-Rag-Suite.","url":"https://huggingface.co/datasets/AshwinSankar/Indic-Rag-Suite","creator_name":"Ashwin Sankar","creator_url":"https://huggingface.co/AshwinSankar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multilingual","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"muri-it-language-split","keyword":"maithili","description":"\n\t\n\t\t\n\t\tMURI-IT: Multilingual Instruction Tuning Dataset for 200 Languages via Multilingual Reverse Instructions\n\t\n\nMURI-IT is a large-scale multilingual instruction tuning dataset containing 2.2 million instruction-output pairs across 200 languages. It is designed to address the challenges of instruction tuning in low-resource languages with Multilingual Reverse Instructions (MURI), which ensures that the output is human-written, high-quality, and authentic to the cultural and linguistic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/akoksal/muri-it-language-split.","url":"https://huggingface.co/datasets/akoksal/muri-it-language-split","creator_name":"Abdullatif Koksal","creator_url":"https://huggingface.co/akoksal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","question-answering","summarization","Achinese"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoSCIDOCS_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoSCIDOCS dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoSCIDOCS_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoSCIDOCS_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoSCIDOCS","Maithili"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoFiQA2018_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoFiQA2018 dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoFiQA2018_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoFiQA2018_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoFiQA2018","Maithili"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoMSMARCO_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoMSMARCO dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoMSMARCO_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoMSMARCO_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoMSMARCO","Maithili"],"keywords_longer_than_N":true},
	{"name":"HPLT2.0_cleaned","keyword":"maithili","description":"This is a large-scale collection of web-crawled documents in 191 world languages, produced by the HPLT project. \nThe source of the data is mostly Internet Archive with some additions from Common Crawl.\nFor a detailed description of the dataset, please refer to https://hplt-project.org/datasets/v2.0\nThe Cleaned variant of HPLT Datasets v2.0\nThis is the cleaned variant of the HPLT Datasets v2.0 converted to the Parquet format semi-automatically when being uploaded here. \nThe original JSONL files‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jobs-git/HPLT2.0_cleaned.","url":"https://huggingface.co/datasets/jobs-git/HPLT2.0_cleaned","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","text-generation","language-modeling","multilingual","Achinese"],"keywords_longer_than_N":true},
	{"name":"GlotCC-V1","keyword":"maithili","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n \n\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\n\n\t\t\n\t\tUsage (Huggingface Hub -- Recommended)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1.","url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Abau","Amarasi","Abkhaz","Abkhazian"],"keywords_longer_than_N":true},
	{"name":"IN22-Conv-Doc-Level","keyword":"maithili","description":"This dataset was constructed by merging individual conversations from the IN22-Conv dataset to create a long-context, document-level parallel benchmark. For further information on domains and statistics, please refer to the original paper and dataset.\n","url":"https://huggingface.co/datasets/VarunGumma/IN22-Conv-Doc-Level","creator_name":"Varun Gumma","creator_url":"https://huggingface.co/VarunGumma","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-generated","multilingual","translation","Assamese"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoSciFact_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoSciFact dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoSciFact_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoSciFact_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoSciFact","Maithili"],"keywords_longer_than_N":true},
	{"name":"indicCorpv2","keyword":"maithili","description":"    IndicCORPV2 is the largest collection of texts for Indic langauges consisting of 20.9 Billion tokens of which 14.4B tokens correspond to 23 Indic languages and 6.B tokens of Indian English content curated from Indian websites.","url":"https://huggingface.co/datasets/satpalsr/indicCorpv2","creator_name":"Satpal Singh Rathore","creator_url":"https://huggingface.co/satpalsr","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","Assamese","Bodo (India)","Bengali","Dogri (macrolanguage)"],"keywords_longer_than_N":true},
	{"name":"Bhasha-Abhijnaanam","keyword":"maithili","description":"\n\t\n\t\t\n\t\tDataset Card for Aksharantar\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBhasha-Abhijnaanam is a language identification test set for native-script as well as Romanized text which spans 22 Indic languages.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\n\t\n\t\t\n\n\n\n\n\n\n\n\n\t\t\nAssamese (asm)\nHindi (hin)\nMaithili (mai)\nNepali (nep)\nSanskrit (san)\nTamil (tam)\n\n\nBengali (ben)\nKannada (kan)\nMalayalam (mal)\nOriya (ori)\nSantali (sat)\nTelugu (tel)\n\n\nBodo(brx)\nKashmiri‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/Bhasha-Abhijnaanam.","url":"https://huggingface.co/datasets/ai4bharat/Bhasha-Abhijnaanam","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","crowdsourced","expert-generated","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"language_tags","keyword":"maithili","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nDataset listing 27,328 languages and dialects (also includes macrolanguage names).For each language, either the ISO 639 code, the Glottolog code or both are provided.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\nEnglish_Name: Language name in English (e.g. \"French\").\nNative_Name: If value is not 0, corresponds to the name of the language by native speakers (e.g. \"Fran√ßais\") which may have been found in Wikipedia's nativename field.\nGlottocode: The language tag in the Glottolog convention (e.g.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/language_tags.","url":"https://huggingface.co/datasets/lbourdois/language_tags","creator_name":"Lo√Øck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Afade","Par√° Ar√°ra","Afar","Aka-Bea","Abon"],"keywords_longer_than_N":true},
	{"name":"tatoeba","keyword":"maithili","description":"This is a collection of translated sentences from Tatoeba\n359 languages, 3,403 bitexts\ntotal number of files: 750\ntotal number of tokens: 65.54M\ntotal number of sentence fragments: 8.96M","url":"https://huggingface.co/datasets/Helsinki-NLP/tatoeba","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":null,"first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"HPLT2.0_cleaned","keyword":"maithili","description":"This is a large-scale collection of web-crawled documents in 191 world languages, produced by the HPLT project. \nThe source of the data is mostly Internet Archive with some additions from Common Crawl.\nFor a detailed description of the dataset, please refer to our website and our pre-print.\n\n\t\n\t\t\n\t\n\t\n\t\tThe Cleaned variant of HPLT Datasets v2.0\n\t\n\nThis is the cleaned variant of the HPLT Datasets v2.0 converted to the Parquet format semi-automatically when being uploaded here. \nThe original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPLT/HPLT2.0_cleaned.","url":"https://huggingface.co/datasets/HPLT/HPLT2.0_cleaned","creator_name":"HPLT","creator_url":"https://huggingface.co/HPLT","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","text-generation","language-modeling","multilingual","Achinese"],"keywords_longer_than_N":true},
	{"name":"Flores-Indic-Doc-Level","keyword":"maithili","description":"This dataset was constructed by merging individual sentences from the Flores dataset based on matching domain, topic, and URL attributes. The result is a long-context, document-level parallel benchmark. For more details on the domains and dataset statistics, please refer to the original paper and the dataset.\n","url":"https://huggingface.co/datasets/VarunGumma/Flores-Indic-Doc-Level","creator_name":"Varun Gumma","creator_url":"https://huggingface.co/VarunGumma","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-generated","multilingual","translation","Assamese"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoClimateFEVER_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoClimateFEVER dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoClimateFEVER_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoClimateFEVER_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoClimateFEVER","Maithili"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoArguAna_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoArguAna dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoArguAna_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoArguAna_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoArguAna","Maithili"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Maithili"],"keywords_longer_than_N":true},
	{"name":"muri-it","keyword":"maithili","description":"\n\t\n\t\t\n\t\tMURI-IT: Multilingual Instruction Tuning Dataset for 200 Languages via Multilingual Reverse Instructions\n\t\n\nMURI-IT is a large-scale multilingual instruction tuning dataset containing 2.2 million instruction-output pairs across 200 languages. It is designed to address the challenges of instruction tuning in low-resource languages with Multilingual Reverse Instructions (MURI), which ensures that the output is human-written, high-quality, and authentic to the cultural and linguistic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/akoksal/muri-it.","url":"https://huggingface.co/datasets/akoksal/muri-it","creator_name":"Abdullatif Koksal","creator_url":"https://huggingface.co/akoksal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","question-answering","summarization","Achinese"],"keywords_longer_than_N":true},
	{"name":"ParaNames","keyword":"maithili","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bltlab/ParaNames.","url":"https://huggingface.co/datasets/bltlab/ParaNames","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","Nias","Kotava","Banjar","Angika"],"keywords_longer_than_N":true},
	{"name":"xP3x-Kongo","keyword":"maithili","description":"\n\t\n\t\t\n\t\tDataset Card for xP3x Kikongo Focus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nxP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts & datasets across 277 languages & 16 NLP tasks. It contains all of xP3 + much more! It is used for training future contenders of mT0 & BLOOMZ at project Aya @C4AI üß°\n\n\nCreation: The dataset can be recreated using instructions available here together with the file in this repository named xp3x_create.py. We provide this version to save‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Svngoku/xP3x-Kongo.","url":"https://huggingface.co/datasets/Svngoku/xP3x-Kongo","creator_name":"NIONGOLO Chrys F√©-Marty","creator_url":"https://huggingface.co/Svngoku","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","translation","expert-generated","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"IN22GenBitextMining","keyword":"maithili","description":"\n  IN22GenBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Gen is a n-way parallel general-purpose multi-domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Legal, Government, News, Religious, Non-fiction, Written\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Gen\n\n\n\t\n\nSource datasets:\n\nmteb/IN22-Gen\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22GenBitextMining.","url":"https://huggingface.co/datasets/mteb/IN22GenBitextMining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","multilingual","mteb/IN22-Gen","Assamese"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoNFCorpus_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoNFCorpus dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoNFCorpus_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoNFCorpus_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoNFCorpus","Maithili"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoNQ_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoNQ dataset, specifically adapted for information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoNQ_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoNQ_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoNQ","Maithili"],"keywords_longer_than_N":true},
	{"name":"panlex","keyword":"maithili","description":"\n\t\n\t\t\n\t\tPanLex\n\t\n\nJanuary 1, 2024 version of PanLex Language Vocabulary with 24,650,274 rows covering 6,152 languages.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\nvocab: contains the text entry.  \n639-3: contains the ISO 639-3 languages tags to allow users to filter on the language(s) of their choice.\n639-3_english_name: the English language name associated to the code ISO 639-3. \nvar_code: contains a code to differentiate language variants. In practice, this is the code 639-3 + a number.  If 000, it corresponds to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/panlex.","url":"https://huggingface.co/datasets/lbourdois/panlex","creator_name":"Lo√Øck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["Ghotuo","Alumu-Tesu","Ari","Amal","Arb√´resh√´ Albanian"],"keywords_longer_than_N":true},
	{"name":"Indic-Rag-Suite","keyword":"maithili","description":"\n\t\n\t\t\n\t\tüåè Multilingual Indic RAG Suite\n\t\n\nA comprehensive multilingual question-answering dataset covering 18 Indian languages with 21,439,886 total samples, designed for RAG (Retrieval-Augmented Generation) applications and multilingual NLP research.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load specific language (recommended)\ndataset = load_dataset(\"ai4bharat/Indic-Rag-Suite\", \"as\")\ntrain_data = dataset['train']\n\nprint(f\"Loaded {len(train_data)} samples\")\n\n# Access‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite.","url":"https://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multilingual","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"IN22-Gen","keyword":"maithili","description":"\n  IN22GenBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Gen is a n-way parallel general-purpose multi-domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Legal, Government, News, Religious, Non-fiction, Written\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Gen\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22-Gen.","url":"https://huggingface.co/datasets/mteb/IN22-Gen","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","expert-generated","multilingual","Assamese"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoTouche2020_mai","keyword":"maithili","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoTouche2020 dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoTouche2020_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoTouche2020_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoTouche2020","Maithili"],"keywords_longer_than_N":true},
	{"name":"fineweb-2","keyword":"maithili","description":"\n\t\n\t\t\n\t\tü•Ç FineWeb2\n\t\n\n\n    \n\n\n\nA sparkling update with 1000s of languages\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nThis is the second iteration of the popular üç∑ FineWeb dataset, bringing high quality pretraining data to over 1000 üó£Ô∏è languages.\nThe ü•Ç FineWeb2 dataset is fully reproducible, available under the permissive ODC-By 1.0 license and extensively validated through hundreds of ablation experiments.\nIn particular, on the set of 9 diverse languages we used to guide our processing decisions, ü•Ç FineWeb2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/fineweb-2.","url":"https://huggingface.co/datasets/HuggingFaceFW/fineweb-2","creator_name":"FineData","creator_url":"https://huggingface.co/HuggingFaceFW","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Arifama-Miniafia","Ankave","Abau","Amarasi"],"keywords_longer_than_N":true}
]
;
