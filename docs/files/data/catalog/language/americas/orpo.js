const data_for_language_americas_orpo = 
[
	{"name":"PKU-SafeRLHF-orpo-72k","keyword":"orpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/juneup/PKU-SafeRLHF-orpo-72k","creator_name":"Jundifang","creator_url":"https://huggingface.co/juneup","description":"Warning: this dataset contains data that may be offensive or harmful. The data are intended for research purposes, especially research that can make models less harmful.\nğŸ‘‡original PKU-SafeRLHF datasets (click ğŸ”— for more details)\n\nwhat's the advantage of this train dataset over the original one ?\n\nstandard chosen/rejected format of preference datasets : make 'chosen' and 'rejected' according to 'better_response_id'\nonly one file : merge three train datasets(Alpaca-7Bã€Alpaca2-7Bã€Alpaca3-8B)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/juneup/PKU-SafeRLHF-orpo-72k.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.2\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Fine-tune Llama 3 with ORPO for more information about how to use it.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)argilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-flat","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k-flat\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Uncensor any LLM with Abliteration for more information about how to use it.\nThis is version with raw text instead of lists of dicts as in the original version here.\nIt makes easier to parse in Axolotl, especially for DPO.ORPO-DPO-mix-40k-flat is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-TR-20k","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\n\t\n\t\t\n\t\tORPO-DPO-Mix-TR-20k\n\t\n\n\nThis repository contains a Turkish translation of 20k records from the mlabonne/orpo-dpo-mix-40k dataset. The translation was carried out using the gemini-1.5-flash-002 model, chosen for its 1M token context size and overall accurate Turkish responses.\n\n\t\n\t\t\n\t\n\t\n\t\tTranslation Process\n\t\n\nThe translation process uses the LLM model with translation prompt and pydantic data validation to ensure accuracy. The complete translation pipeline is available in the GitHubâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k.","first_N":5,"first_N_keywords":["text2text-generation","question-answering","text-generation","Turkish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita-reranked","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\n\t\n\t\t\n\t\tEvol DPO Ita Reranked\n\t\n\n\nA high-quality Italian preference dataset suitable for Direct Preference Optimization (DPO), ORPO, and other Preference Tuning algorithms.\n\n\t\n\t\t\n\t\n\t\n\t\tğŸ¥‡ğŸ¥ˆ Reranking process\n\t\n\nThis work is based on efederici/evol-dpo-ita, a nice Italian preference dataset.\nThe original dataset includes prompts translated from the Evol-Instruct datasets, with responses generated using GPT-3.5-Turbo (rejected) and claude-3-opus-20240229 (chosen).\nChoosing the response from theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked.","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"oasst2_dpo_pairs_enth","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/oasst2_dpo_pairs_enth","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\n\t\n\t\t\n\t\tOASST2 DPO Pairs English and Thai\n\t\n\nThis dataset contains message ChatML. It was create from Open Assistant Conversations Dataset Release 2 (OASST2). You can use to do human preference optimization (DPO, ORPO, and other).\n\n\t\n\t\t\n\t\tSelect Thai only\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"pythainlp/oasst2_dpo_pairs_enth\",split=\"train\")\nthai_dataset = dataset.filter(lambda example: example['lang']==\"th\") # if you want to use English only, change to \"en\".\n\nlicense:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/oasst2_dpo_pairs_enth.","first_N":5,"first_N_keywords":["text-generation","English","Thai","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Aya-Command.R-DPO","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-Command.R-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tğŸ¤— Dataset Card for \"Aya-Command.R-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-Command.R-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled as \"chosen,\" withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-Command.R-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Aya-SambaLingo.Arabic.Chat-DPO","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-SambaLingo.Arabic.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tDataset Card for \"Aya-SambaLingo.Arabic.Chat-DPO\" ğŸ¤—\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-SambaLingo.Arabic.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responsesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-SambaLingo.Arabic.Chat-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-SHORT","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\n\t\n\t\t\n\t\tSHORTENED - ORPO-DPO-mix-40k v1.1\n\t\n\nFILTERED to remove rows with chosen responses longer than 2k characters or with a final assistant message longer than 500 characters.\n\nThe original dataset card follows below.\n\n\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.1\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"gemini_orpo_dpo_ptbr","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/gemini_orpo_dpo_ptbr","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/gemini_orpo_dpo_ptbr dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","question-answering","Portuguese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized","keyword":"orpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/ultrafeedback-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"Ultrafeedback binarized dataset using the mean of preference ratings by Argilla. \nThey addressed TruthfulQA-related data contamination in this version.\nSteps:\n\nCompute mean of preference ratings (honesty, instruction-following, etc.)\nPick the best mean rating as the chosen\nPick random rejected with lower mean (or another random if equal to chosen rating)\nFilter out examples with chosen rating == rejected rating\n\nReference:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/ultrafeedback-binarized.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs-binarized","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/distilabel-intel-orca-dpo-pairs-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"This is the binarized version of distilabel Orca Pairs for DPO and ORPO. \nReference: https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs?row=0\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"multiturn-capybara-preferences-filtered-binarized","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"This dataset has been created with distilabel, plus some extra post-processing steps described below.\nDataset Summary\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\n\nRemove responses from the assistant, not only in the last turn, but also in intermediate turns where the assistant responds with a potential refusal and/or some ChatGPT-isms such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Capybara-Preferences","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/Capybara-Preferences","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for Capybara-Preferences\n\t\n\nThis dataset has been created with distilabel.\n\n    \n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is built on top of LDJnr/Capybara, in order to generate a preference\ndataset out of an instruction-following dataset. This is done by keeping the conversations in the column conversation but splitting\nthe last assistant turn from it, so that the conversation contains all the turns up until the last user's turn, so that it can be reusedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Capybara-Preferences.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"DPO-En-Zh-20k","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llamafactory/DPO-En-Zh-20k","creator_name":"LLaMA Factory","creator_url":"https://huggingface.co/llamafactory","description":"This dataset is composed by\n\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\n10,000 examples of wenbopan/Chinese-dpo-pairs.\n\nYou can use it in LLaMA Factory by specifying dataset: dpo_mix_en,dpo_mix_zh.\n","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-50k","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-50k\n\t\n\nThis dataset is designed for DPO or ORPO training.\nThis dataset combines samples of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\nargilla/distilabel-math-preference-dpo: highly scored chosenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo_binarized","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"enunciados_pge_rj_orpo","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/enunciados_pge_rj_orpo","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/enunciados_pge_rj_orpo dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","Portuguese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-45k","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-45k\n\t\n\nThis dataset is designed for DPO or ORPO training.\nThis dataset combines samples of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\nargilla/distilabel-math-preference-dpo: highly scored chosenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-38k-balanced","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-38k\n\t\n\nThis dataset is intended for use with DPO or ORPO training. \nIt represents a balanced version of the llmat/dpo-orpo-mix-45k dataset, achieved through a clustering-based approach as outlined in this paper.\nThe dataset integrates high-quality samples from the following DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"lima_dirty_tr","keyword":"orpo","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emre/lima_dirty_tr","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","description":"\n\t\n\t\t\n\t\tLima Turkish Translated & Engineered for Alignment\n\t\n\n\n\t\n\t\t\n\t\tGiriÅŸ\n\t\n\nBu veri seti, LIMA (Less Is More for Alignment) [^1] Ã§alÄ±ÅŸmasÄ±ndan ilham alÄ±narak oluÅŸturulmuÅŸ, orijinal LIMA veri setinin TÃ¼rkÃ§e'ye Ã§evrilmiÅŸ ve hizalama (alignment) teknikleri iÃ§in Ã¶zel olarak yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir versiyonudur. LIMA'nÄ±n temel felsefesi, az sayÄ±da ancak yÃ¼ksek kaliteli Ã¶rnekle dil modellerini etkili bir ÅŸekilde hizalayabilmektir. Bu Ã§alÄ±ÅŸma, bu felsefeyi TÃ¼rkÃ§e dil modelleri ekosistemine taÅŸÄ±mayÄ±â€¦ See the full description on the dataset page: https://huggingface.co/datasets/emre/lima_dirty_tr.","first_N":5,"first_N_keywords":["text-generation","Turkish","English","afl-3.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"DPO-En-Zh-20k-Preference","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference","creator_name":"Ming Xu (å¾æ˜)","creator_url":"https://huggingface.co/shibing624","description":"This dataset is composed by\n\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\n10,000 examples of wenbopan/Chinese-dpo-pairs.\n\nrefer: https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k   æ”¹äº†questionã€response_rejectedã€response_chosenå­—æ®µï¼Œæ–¹ä¾¿ORPOã€DPOæ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨train usage:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"classifai","keyword":"orpo","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jhmejia/classifai","creator_name":"John Henry","creator_url":"https://huggingface.co/jhmejia","description":"jhmejia/classifai dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","gpl-3.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Aya-AceGPT.13B.Chat-DPO","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-AceGPT.13B.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tDataset Card for \"Aya-AceGPT.13B.Chat-DPO\" ğŸ¤—\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-AceGPT.13B.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-AceGPT.13B.Chat-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"NoRobots-AceGPT.13B.Chat-DPO","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-AceGPT.13B.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tğŸ¤— Dataset Card for \"NoRobots-AceGPT.13B.Chat-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-AceGPT.13B.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-AceGPT.13B.Chat-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"NoRobots-Aya.23.8B-DPO","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-Aya.23.8B-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tğŸ¤— Dataset Card for \"NoRobots-Aya.23.8B-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-Aya.23.8B-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-Aya.23.8B-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"NoRobots-SambaLingo.Arabic.Chat-DPO","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-SambaLingo.Arabic.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tğŸ¤— Dataset Card for \"NoRobots-SambaLingo.Arabic.Chat-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-SambaLingo.Arabic.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-SambaLingo.Arabic.Chat-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"NoRobots-Command.R-DPO","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-Command.R-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tğŸ¤— Dataset Card for \"NoRobots-Command.R-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-Command.R-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-Command.R-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"auryn_dpo_orpo_english","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/auryn_dpo_orpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/auryn_dpo_orpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"auryn_dpo_orpo_english","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/auryn_dpo_orpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/auryn_dpo_orpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Aya-Aya.23.8B-DPO","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-Aya.23.8B-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tğŸ¤— Dataset Card for \"Aya-Aya.23.8B-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-Aya.23.8B-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled as \"chosen,\" withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-Aya.23.8B-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"demons_megaten_fandom_orpo_dpo_english","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/demons_megaten_fandom_orpo_dpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/demons_megaten_fandom_orpo_dpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"demons_megaten_fandom_orpo_dpo_english","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/demons_megaten_fandom_orpo_dpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/demons_megaten_fandom_orpo_dpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-flat-mlx","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-flat-mlx","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a split version of orpo-dpo-mix-40k-flat for direct use with mlx-lm-lora, specifically tailored to be compatible with DPO and CPO training.\nThe dataset has been divided into three parts:\n\nTrain Set: 90%\nValidation Set: 6% \nTest Set: 4%\n\n\n\t\n\t\t\n\t\n\t\n\t\tExample Usage\n\t\n\nTo train a model using this dataset, you can use the following command:\nmlx_lm_lora.train \\\n--model Qwen/Qwen2.5-3B-Instruct \\\n--train \\\n--test \\\n--num-layers 8 \\\n--dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-flat-mlx.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ExplainableAI-emotions-DPO-ORPO-RLHF","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF","creator_name":"Hammad Fahim","creator_url":"https://huggingface.co/imhmdf","description":"\n\t\n\t\t\n\t\tPreference Dataset for Explainable Multi-Label Emotion Classification\n\t\n\nThis repository contains a preference dataset compiled to compare two model-generated responses for explaining multi-label emotion classifications on Tweets. The dataset is accompanied by human annotations indicating which response was preferred, based on a set of defined dimensions (clarity, correctness, helpfulness, and verbosity). The annotation guidelines are included to describe how these preference judgmentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-mlx","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-mlx","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a split version of orpo-dpo-mix-40k for direct use with mlx-lm-lora, specifically tailored to be compatible with ORPO training.\nThe dataset has been divided into three parts:\n\nTrain Set: 90%\nValidation Set: 6% \nTest Set: 4%\n\n\n\t\n\t\t\n\t\n\t\n\t\tExample Usage\n\t\n\nTo train a model using this dataset, you can use the following command:\nmlx_lm_lora.train \\\n--model Qwen/Qwen2.5-3B-Instruct \\\n--train \\\n--test \\\n--num-layers 8 \\\n--dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-mlx.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"gemma-vs-gemma-preferences","keyword":"orpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\n\t\n\t\t\n\t\tğŸ’ğŸ†šğŸ’ Gemma vs Gemma Preferences\n\t\n\nThis dataset contains on-policy collected preferences generated using anakin87/gemma-2-2b-ita-sft.\nâš ï¸ While this dataset may be valuable for didactic purposes, it is not recommended for training a model using Preference Tuning due to the following reasons:\n\nThe training would be off-policy for your model.\nThe dataset was generated with gemma-2-2b-ita-sft, a small model for Italian.\n\n\n\t\n\t\t\n\t\n\t\n\t\tMotivation\n\t\n\nWhile DPO (Direct Preferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences.","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true}
]
;
