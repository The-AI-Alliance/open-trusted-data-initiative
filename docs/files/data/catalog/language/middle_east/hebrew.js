const data_for_language_middle_east_hebrew = 
[
	{"name":"xlel_wd_dictionary","keyword":"hebrew","description":"XLEL-WD is a multilingual event linking dataset. This sub-dataset contains a dictionary of events from Wikidata. The multilingual descriptions for Wikidata event items are taken from the corresponding Wikipedia articles.","url":"https://huggingface.co/datasets/adithya7/xlel_wd_dictionary","creator_name":"Adithya Pratapa","creator_url":"https://huggingface.co/adithya7","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["found","found","multilingual","original","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"HebNLI","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tHebNLI - A Natural Language Inference Dataset in Hebrew\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nHebNLI is a Hebrew dataset for natural language inference (NLI) tasks.\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset is the first of its kind in the Hebrew language and aims to serve as training data for NLI tasks.\nHebNLI is based on MultiNLI, a large crowd-sourced corpus of sentences from varied genres and writing styles in the English language.\nMultiNLI was originally built by collecting hundreds of thousands of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HebArabNlpProject/HebNLI.","url":"https://huggingface.co/datasets/HebArabNlpProject/HebNLI","creator_name":"Israel National  NLP Program","creator_url":"https://huggingface.co/HebArabNlpProject","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["Hebrew","cc-by-3.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"c4","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tC4\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: \"https://commoncrawl.org\".\nThis is the processed version of Google's C4 dataset\nWe prepared five variants of the data: en, en.noclean, en.noblocklist, realnewslike, and multilingual (mC4).\nFor reference, these are the sizes of the variants:\n\nen: 305GB\nen.noclean: 2.3TB\nen.noblocklist: 380GB\nrealnewslike: 15GB\nmultilingual (mC4): 9.7TB (108 subsets, one per‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/c4.","url":"https://huggingface.co/datasets/allenai/c4","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"udhr-lid","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tUDHR-LID\n\t\n\nWhy UDHR-LID?\nYou can access UDHR (Universal Declaration of Human Rights) here, but when a verse is missing, they have texts such as \"missing\" or \"?\". Also, about 1/3 of the sentences consist only of \"articles 1-30\" in different languages. We cleaned the entire dataset from XML files and selected only the paragraphs. We cleared any unrelated language texts from the data and also removed the cases that were incorrect.\nIncorrect? Look at the ckb and kmr files in the UDHR.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/udhr-lid.","url":"https://huggingface.co/datasets/cis-lmu/udhr-lid","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Tigrinya","Balkan Romani","Standard Arabic","Metlat√≥noc Mixtec"],"keywords_longer_than_N":true},
	{"name":"mqa","keyword":"hebrew","description":"MQA is a multilingual corpus of questions and answers parsed from the Common Crawl. Questions are divided between Frequently Asked Questions (FAQ) pages and Community Question Answering (CQA) pages.","url":"https://huggingface.co/datasets/clips/mqa","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","no-annotation","other","multilingual"],"keywords_longer_than_N":true},
	{"name":"knesset_meetings_corpus","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAn example of a sample:\n{\n    \"text\": <text content of given document>,\n    \"path\": <file path to docx>\n}\n\nDataset usage\nAvailable \"kneset16\",\"kneset17\",\"knesset_tagged\" configurations\nAnd only train set.\ntrain_ds = load_dataset(\"imvladikon/knesset_meetings_corpus\", \"kneset16\", split=\"train\")\n\nThe Knesset Meetings Corpus 2004-2005 is made up of two components:\n\nRaw texts - 282 files made up of 867,725 lines together. These can be downloaded in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imvladikon/knesset_meetings_corpus.","url":"https://huggingface.co/datasets/imvladikon/knesset_meetings_corpus","creator_name":"Vladimir Gurevich","creator_url":"https://huggingface.co/imvladikon","license_name":"Public Domain Dedication & License","license_url":"https://scancode-licensedb.aboutcode.org/pddl-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"CC-Cat","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tCC_Cat\n\t\n\n\nExtract from CC-WARC snapshots.\nMainly includes texts with 149 languages.\nPDF/IMAGE/AUDIO/VIDEO raw downloading link.\n\n\n\t\n\t\t\n\t\tNotice\n\t\n\n\nSince my computing resources are limited, this dataset will update by one-day of CC snapshots timestampts.\nAfter a snapshot is updated, the deduplicated version will be uploaded.\nIf you are interested in providing computing resources or have cooperation needs, please contact me.\n  carreyallthetime@gmail.com  \n      \n  \n\n","url":"https://huggingface.co/datasets/chengshidehaimianti/CC-Cat","creator_name":"zyq","creator_url":"https://huggingface.co/chengshidehaimianti","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","German","Russian"],"keywords_longer_than_N":true},
	{"name":"HeQ_v1","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for HeQ_v1\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHeQ is a question answering dataset in Modern Hebrew, consisting of 30,147 questions. It follows the format and crowdsourcing methodology of SQuAD and ParaShoot, with paragraphs sourced from Hebrew Wikipedia and Geektime.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\nTask: Question Answering\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nHebrew (he)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nID:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pig4431/HeQ_v1.","url":"https://huggingface.co/datasets/pig4431/HeQ_v1","creator_name":"Itay Etelis","creator_url":"https://huggingface.co/pig4431","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Hebrew","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"oasst2","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tOpen Assistant Conversations Dataset Release 2 (OASST2)\n\t\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains message trees. Each message tree has an initial prompt message as the root node, \nwhich can have multiple child messages as replies, and these child messages can have multiple replies. \nAll messages have a role property: this can either be \"assistant\" or \"prompter\". The roles in \nconversation threads from prompt to leaf node strictly alternate between \"prompter\" and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst2.","url":"https://huggingface.co/datasets/OpenAssistant/oasst2","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"fleurs","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tFLEURS\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark. \nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\nused and ‚Äùunit error rate‚Äù (characters, signs) of all languages is averaged. Languages and results are also grouped into seven‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/fleurs.","url":"https://huggingface.co/datasets/google/fleurs","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"tapaco","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for TaPaCo Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA freely available paraphrase corpus for 73 languages extracted from the Tatoeba database. \nTatoeba is a crowdsourcing project mainly geared towards language learners. Its aim is to provide example sentences \nand translations for particular linguistic constructions and words. The paraphrase corpus is created by populating a \ngraph with Tatoeba sentences and equivalence links between sentences ‚Äúmeaning the same thing‚Äù. This‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/tapaco.","url":"https://huggingface.co/datasets/community-datasets/tapaco","creator_name":"Community Datasets","creator_url":"https://huggingface.co/community-datasets","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","text-classification","semantic-similarity-classification","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"mkqa","keyword":"hebrew","description":"We introduce MKQA, an open-domain question answering evaluation set comprising 10k question-answer pairs sampled from the Google Natural Questions dataset, aligned across 26 typologically diverse languages (260k question-answer pairs in total). For each query we collected new passage-independent answers. These queries and answers were then human translated into 25 Non-English languages.","url":"https://huggingface.co/datasets/apple/mkqa","creator_name":"Apple","creator_url":"https://huggingface.co/apple","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":null,"first_N":5,"first_N_keywords":["question-answering","open-domain-qa","crowdsourced","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"openassistant-llama-style","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - Llama 2 Style\n\t\n\nThis dataset allows for fine-tuning chat models using [INST] AND [/INST] to wrap user messages.\nPreparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe dataset was then filtered to:\n\n\nreplace instances of '### Human:' with '[INST]'\nreplace‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-llama-style.","url":"https://huggingface.co/datasets/Trelis/openassistant-llama-style","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"amazon_massive_intent","keyword":"hebrew","description":"\n  MassiveIntentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSpoken\n\n\nReference\nhttps://arxiv.org/abs/2204.08582\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"MassiveIntentClassification\"])\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/amazon_massive_intent.","url":"https://huggingface.co/datasets/mteb/amazon_massive_intent","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","translated","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"hebrew-tzfira-dataset","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tHa-Tsfira OCR and POS-Tagged Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains OCR-processed and POS-tagged text from Ha-Tsfira (◊î◊¶◊§◊ô◊®◊î), a Hebrew-language newspaper published in Poland from 1862 and then from 1874 to 1931. The dataset includes 50 newspaper issues that have been digitized, cleaned, and linguistically annotated.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nHebrew (he)\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'ocr_text', 'cleaned_text'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mbole/hebrew-tzfira-dataset.","url":"https://huggingface.co/datasets/mbole/hebrew-tzfira-dataset","creator_name":"Bolek","creator_url":"https://huggingface.co/mbole","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","machine-generated","found","original","Hebrew"],"keywords_longer_than_N":true},
	{"name":"hebrew-tzfira-dataset","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tHa-Tsfira OCR and POS-Tagged Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains OCR-processed and POS-tagged text from Ha-Tsfira (◊î◊¶◊§◊ô◊®◊î), a Hebrew-language newspaper published in Poland from 1862 and then from 1874 to 1931. The dataset includes 50 newspaper issues that have been digitized, cleaned, and linguistically annotated.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nHebrew (he)\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'ocr_text', 'cleaned_text'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mbole/hebrew-tzfira-dataset.","url":"https://huggingface.co/datasets/mbole/hebrew-tzfira-dataset","creator_name":"Bolek","creator_url":"https://huggingface.co/mbole","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","machine-generated","found","original","Hebrew"],"keywords_longer_than_N":true},
	{"name":"finepdfs","keyword":"hebrew","description":"\n\nLiberating 3T of the finest tokens from PDFs\n\n\n\t\n\t\t\n\t\tWhat is this?\n\t\n\nAs we run out of web pages to process, the natural question has always been: what to do next? Only a few knew about a data source that everyone avoided for ages, due to its incredible extraction cost and complexity: PDFs.\nüìÑ FinePDFs is exactly that. It is the largest publicly available corpus sourced exclusively from PDFs, containing about 3 trillion tokens across 475 million documents in 1733 languages.\nCompared to HTML‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/finepdfs.","url":"https://huggingface.co/datasets/HuggingFaceFW/finepdfs","creator_name":"FineData","creator_url":"https://huggingface.co/HuggingFaceFW","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Arifama-Miniafia","Ankave","Abau","Amarasi"],"keywords_longer_than_N":true},
	{"name":"Question-Sparql","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 895,954 examples of natural language questions paired with their corresponding SPARQL queries. It spans 12 languages and targets 15 distinct knowledge graphs, with a significant portion focused on Wikidata and DBpedia.\nThe dataset was developed as a contribution for the Master Thesis: \"Impact of Continual Multilingual Pre-training on Cross-Lingual Transferability for Source Languages\". Its purpose is to facilitate research in text-to-SPARQL‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/julioc-p/Question-Sparql.","url":"https://huggingface.co/datasets/julioc-p/Question-Sparql","creator_name":"Julio Perez","creator_url":"https://huggingface.co/julioc-p","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","German","Hebrew","Kannada"],"keywords_longer_than_N":true},
	{"name":"xP3x","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for xP3x\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nxP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts & datasets across 277 languages & 16 NLP tasks. It contains all of xP3 + much more! It is used for training future contenders of mT0 & BLOOMZ at project Aya @Cohere Labs üß°\n\n\nCreation: The dataset can be recreated using instructions available here together with the file in this repository named xp3x_create.py. We provide this version to save processing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/xP3x.","url":"https://huggingface.co/datasets/CohereLabs/xP3x","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"llm-metric-mrewardbench","keyword":"hebrew","description":"rifqifarhansyah/llm-metric-mrewardbench dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/rifqifarhansyah/llm-metric-mrewardbench","creator_name":"Mohammad Rifqi Farhansyah","creator_url":"https://huggingface.co/rifqifarhansyah","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["Arabic","Chinese","Czech","Dutch","French"],"keywords_longer_than_N":true},
	{"name":"bhojpuri","keyword":"hebrew","description":"\n\t\n\t\t\n\t\n\t\n\t\tFLEURS\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark. \nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\nused and ‚Äùunit error rate‚Äù (characters, signs) of all languages is averaged. Languages and results are also grouped into seven‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ankur02/bhojpuri.","url":"https://huggingface.co/datasets/ankur02/bhojpuri","creator_name":"Ankur Verma","creator_url":"https://huggingface.co/ankur02","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"multilingual_translation_gen_binarized","keyword":"hebrew","description":"Youseff1987/multilingual_translation_gen_binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Youseff1987/multilingual_translation_gen_binarized","creator_name":"JOON HYOUNG JUN","creator_url":"https://huggingface.co/Youseff1987","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Korean","English","Chinese","Zulu"],"keywords_longer_than_N":true},
	{"name":"domain-translations","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMultilingual Domain Name Translations Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 155,004 domain names with their multilingual translations across 20 languages. Each domain has been segmented into constituent words and translated while preserving semantic meaning and commercial appeal. The dataset is particularly valuable for domain name research, multilingual NLP tasks, and understanding how brand names and concepts translate across languages.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/humbleworth/domain-translations.","url":"https://huggingface.co/datasets/humbleworth/domain-translations","creator_name":"HumbleWorth","creator_url":"https://huggingface.co/humbleworth","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","feature-extraction","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"dhanishtha-2.0-superthinker","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tüì¶ Dhanishtha-2.0-SUPERTHINKER-MLX\n\t\n\n A distilled corpus of 11.7K high-quality samples showcasing multi-phase reasoning and structured emotional cognition. Sourced directly from the internal training data of Dhanishtha-2.0 ‚Äî the world‚Äôs first Large Language Model (LLM) to implement Intermediate Thinking, featuring multiple <think> and <ser> blocks per response\n\n\t\n\t\t\n\t\n\t\n\t\tExample with MLX-LM-LoRA:\n\t\n\nmlx_lm_lora.train \\\n--model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker.","url":"https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Afrikaans","Arabic","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"angry_customers_heb","keyword":"hebrew","description":"Adamham/angry_customers_heb dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Adamham/angry_customers_heb","creator_name":"Adam Agbaria","creator_url":"https://huggingface.co/Adamham","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","Hebrew","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"judaic-texts-structure","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tJewish Texts Structure Dataset\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis dataset provides a structured, machine-readable catalog summarizing the divisions and lengths of fundamental Jewish texts. It is designed to be a foundational resource for developers and researchers building applications, tools, or conducting analyses related to Jewish studies.\nThe dataset contains over 200 records, with each record representing a book (e.g., Genesis), a tractate (e.g., Berakhot), or a major section (e.g.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/judaic-texts-structure.","url":"https://huggingface.co/datasets/NHLOCAL/judaic-texts-structure","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["other","Hebrew","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"judaic-texts-structure","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tJewish Texts Structure Dataset\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis dataset provides a structured, machine-readable catalog summarizing the divisions and lengths of fundamental Jewish texts. It is designed to be a foundational resource for developers and researchers building applications, tools, or conducting analyses related to Jewish studies.\nThe dataset contains over 200 records, with each record representing a book (e.g., Genesis), a tractate (e.g., Berakhot), or a major section (e.g.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/judaic-texts-structure.","url":"https://huggingface.co/datasets/NHLOCAL/judaic-texts-structure","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["other","Hebrew","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"llm-metric-mrewardbench","keyword":"hebrew","description":"rubricreward/llm-metric-mrewardbench dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/rubricreward/llm-metric-mrewardbench","creator_name":"rubricreward","creator_url":"https://huggingface.co/rubricreward","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["Arabic","Chinese","Czech","Dutch","French"],"keywords_longer_than_N":true},
	{"name":"phonetic-piper-recording-studio-prompts","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tPhonetic Piper Studio Recordings Prompts\n\t\n\nThis dataset is a processed version of an utterance dataset made available for various languages as prompts for the Piper recording studio. Along with the original prompts, we include:\n\ncolumns ipa_espeak and ipa_epitran containing phonemized versions of the original sentences according to espeak-ng and Epitran phonemizers, respectively\ncolumns lang, espeak_lang_code, epitran_lang_code containing the language codes as reported by piper‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fdemelo/phonetic-piper-recording-studio-prompts.","url":"https://huggingface.co/datasets/fdemelo/phonetic-piper-recording-studio-prompts","creator_name":"Fl√°vio Eler De Melo","creator_url":"https://huggingface.co/fdemelo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Afrikaans","Arabic","Bulgarian","Catalan","Czech"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"hebrew","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"tatoeba-tokipona","keyword":"hebrew","description":"NetherQuartz/tatoeba-tokipona dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/NetherQuartz/tatoeba-tokipona","creator_name":"Vladimir Larkin","creator_url":"https://huggingface.co/NetherQuartz","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","Toki Pona","English","Russian","Ukrainian"],"keywords_longer_than_N":true},
	{"name":"OpenHumanreasoning-multilingual-2.2k","keyword":"hebrew","description":"Based off of Pinkstackorg/humanreasoning-testing-en, it is a human-generated dataset where a real person had to create most of the reasoning and the final output. If there are any mistakes please let us know.\nWe offer this dataset at an apache-2.0 license to make it useful for everybody.\nnote: translations are not human generated.\n","url":"https://huggingface.co/datasets/Pinkstack/OpenHumanreasoning-multilingual-2.2k","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"wmt24pp","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tWMT24++\n\t\n\nThis repository contains the human translation and post-edit data for the 55 en->xx language pairs released in\nthe publication\nWMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects.\nIf you are interested in the MT/LLM system outputs and automatic metric scores, please see MTME.\nIf you are interested in the images of the source URLs for each document, please see here.\n\n\t\n\t\t\n\t\n\t\n\t\tSchema\n\t\n\nEach language pair is stored in its own jsonl file.\nEach row is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/wmt24pp.","url":"https://huggingface.co/datasets/google/wmt24pp","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Arabic","Bulgarian","Bengali","Catalan"],"keywords_longer_than_N":true},
	{"name":"librivox-tracks","keyword":"hebrew","description":"A dataset of all audio files uploaded to LibriVox before 26th September 2023.\nForked from https://huggingface.co/datasets/pykeio/librivox-tracks\nChanges:\n\nUsed archive.org metadata API to annotate rows with \"duration\" column\n\n","url":"https://huggingface.co/datasets/xacer/librivox-tracks","creator_name":"xacer","creator_url":"https://huggingface.co/xacer","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","feature-extraction","Achinese","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"multilingual-reward-bench","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMultilingual Reward Bench (v1.0)\n\t\n\nReward models (RMs) have driven the development of state-of-the-art LLMs today, with unprecedented impact across the globe. However, their performance in multilingual settings still remains understudied. \nIn order to probe reward model behavior on multilingual data, we present M-RewardBench, a benchmark for 23 typologically diverse languages. \nM-RewardBench contains prompt-chosen-rejected preference triples obtained by curating and translating chat‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabsCommunity/multilingual-reward-bench.","url":"https://huggingface.co/datasets/CohereLabsCommunity/multilingual-reward-bench","creator_name":"Cohere Labs Community","creator_url":"https://huggingface.co/CohereLabsCommunity","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["Arabic","Chinese","Czech","Dutch","French"],"keywords_longer_than_N":true},
	{"name":"include-lite-44","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tINCLUDE-lite (44 languages)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nPaper: http://arxiv.org/abs/2411.19799\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \nIt contains 11,095 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/include-lite-44.","url":"https://huggingface.co/datasets/CohereLabs/include-lite-44","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Albanian","Arabic","Armenian","Azerbaijani"],"keywords_longer_than_N":true},
	{"name":"common_voice_21_0","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Common Voice Corpus 21.0\n\t\n\n\n\nThis dataset is an unofficial version of the Mozilla Common Voice Corpus 21. It was downloaded and converted from the project's website https://commonvoice.mozilla.org/.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAbkhaz, Albanian, Amharic, Arabic, Armenian, Assamese, Asturian, Azerbaijani, Basaa, Bashkir, Basque, Belarusian, Bengali, Breton, Bulgarian, Cantonese, Catalan, Central Kurdish, Chinese (China), Chinese (Hong Kong), Chinese (Taiwan), Chuvash, Czech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fsicoli/common_voice_21_0.","url":"https://huggingface.co/datasets/fsicoli/common_voice_21_0","creator_name":"Fabio Sicoli","creator_url":"https://huggingface.co/fsicoli","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","Abkhaz","Afrikaans","Amharic","Arabic"],"keywords_longer_than_N":true},
	{"name":"hebrew-qa","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tHebrew Questions and Answers Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset is a comprehensive collection of Hebrew question and answer pairs covering a wide range of topics, derived from multiple reliable sources. It is specifically curated to support research and applications in the field of natural language processing, especially for Hebrew language understanding.\n\n\t\n\t\t\n\t\tSource\n\t\n\nThis dataset has been compiled and adapted from the Hebrew Question Answering Dataset repository‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yuvalav/hebrew-qa.","url":"https://huggingface.co/datasets/yuvalav/hebrew-qa","creator_name":"Yuval Avidani","creator_url":"https://huggingface.co/yuvalav","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Hebrew","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ccmatrix_en_he","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tEnglish-Hebrew Translation Dataset\n\t\n\nThis dataset contains parallel text in English (en) and Hebrew (he), useful for machine translation tasks.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nLanguages: English (en) and Hebrew (he)\nNumber of Examples: 25,228,938\nSplit: Single split (train)\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach entry has the following structure:\n{\n  \"id\": \"0\",\n  \"translation\": {\n    \"en\": \"Hello\",\n    \"he\": \"◊©◊ú◊ï◊ù\"\n  }\n}\n\n","url":"https://huggingface.co/datasets/Picard1203/ccmatrix_en_he","creator_name":"Eyal Grisaru","creator_url":"https://huggingface.co/Picard1203","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","English","Hebrew","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"montok","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMonTok: A Suite of Monolingual Tokenizers\n\t\n\nThis is a set of monolingual tokenizers for 98 languages. For each language, there are Unigram, BPE, and SuperBPE tokenizers, ranging in vocabulary size from around 6k to over 200k.\n\n\t\n\t\t\n\t\tTraining Details\n\t\n\n\n\t\n\t\t\n\t\tTraining Data\n\t\n\nAll tokenizers are trained on samples of the data used to the train the Goldfish language models. \nThe tokenizers were either trained on scaled or unscaled data. This refers to whether the models are trained on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/catherinearnett/montok.","url":"https://huggingface.co/datasets/catherinearnett/montok","creator_name":"Catherine Arnett","creator_url":"https://huggingface.co/catherinearnett","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Afrikaans","Tosk Albanian","Amharic","Standard Arabic","Assamese"],"keywords_longer_than_N":true},
	{"name":"librivox-tracks","keyword":"hebrew","description":"A dataset of all audio files uploaded to LibriVox before 26th September 2023.\n","url":"https://huggingface.co/datasets/pykeio/librivox-tracks","creator_name":"pyke.io","creator_url":"https://huggingface.co/pykeio","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Achinese","Afrikaans","Ancient Greek (to 1453)"],"keywords_longer_than_N":true},
	{"name":"fiNERweb","keyword":"hebrew","description":"fiNERweb is a multilingual named entity recognition dataset containing annotated text in multiple languages.\nEach example contains the original text, tokenized text, BIO tags, and character/token spans for entities.","url":"https://huggingface.co/datasets/whoisjones/fiNERweb","creator_name":"Jonas Golde","creator_url":"https://huggingface.co/whoisjones","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","Vietnamese","Tamil","Oriya"],"keywords_longer_than_N":true},
	{"name":"multilingual-text","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMultilingual Text Dataset\n\t\n\nThis dataset contains a curated selection of rows from multiple input datasets, where each row includes a text chunk of approximately 2000 tokens (as measured by Llama 3.1 tokenizer) verified to be written in the correct language. Only rows with properly classified language chunks are retained, ensuring high-quality multilingual data for analysis or model training.\n\n\t\n\t\t\n\t\tPreprocessing Steps\n\t\n\n\nNormalized whitespace, punctuation, Unicode characters, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/multilingual-text.","url":"https://huggingface.co/datasets/agentlans/multilingual-text","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","Amharic","Arabic","Bengali"],"keywords_longer_than_N":true},
	{"name":"Chatgpt","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tOpenAssistant Conversations Dataset (OASST1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \nis a product of a worldwide crowd-sourcing effort‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RajChat/Chatgpt.","url":"https://huggingface.co/datasets/RajChat/Chatgpt","creator_name":"Rajdeep Chatterjee ","creator_url":"https://huggingface.co/RajChat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"common_voice_22_0","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Common Voice Corpus 22.0\n\t\n\n\n\nThis dataset is an unofficial version of the Mozilla Common Voice Corpus 22. It was downloaded and converted from the project's website https://commonvoice.mozilla.org/.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAbkhaz, Albanian, Amharic, Arabic, Armenian, Assamese, Asturian, Azerbaijani, Basaa, Bashkir, Basque, Belarusian, Bengali, Breton, Bulgarian, Cantonese, Catalan, Central Kurdish, Chinese (China), Chinese (Hong Kong), Chinese (Taiwan), Chuvash, Czech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fsicoli/common_voice_22_0.","url":"https://huggingface.co/datasets/fsicoli/common_voice_22_0","creator_name":"Fabio Sicoli","creator_url":"https://huggingface.co/fsicoli","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","Abkhaz","Afrikaans","Amharic","Arabic"],"keywords_longer_than_N":true},
	{"name":"HebrewSentimentAnalysis","keyword":"hebrew","description":"\n  HebrewSentimentAnalysis\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nHebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israel‚Äôs president, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder, 2013) to scrape all the comments to all of the president‚Äôs posts in the period of June ‚Äì August 2014, the first three months of Rivlin‚Äôs presidency.2 While the president‚Äôs posts aimed at reconciling‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HebrewSentimentAnalysis.","url":"https://huggingface.co/datasets/mteb/HebrewSentimentAnalysis","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"xm3600","keyword":"hebrew","description":"\n\t\n\t\t\n\t\n\t\n\t\tXM3600 - Crossmodal-3600\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a copy from https://google.github.io/crossmodal-3600/\n\t\n\nIf you use this dataset, please cite the original authors:\n@inproceedings{ThapliyalCrossmodal2022,\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\n  booktitle = {EMNLP},\n  year = {2022}\n}\n\nIt also includes the image features as PIL Image and has a uniform and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/floschne/xm3600.","url":"https://huggingface.co/datasets/floschne/xm3600","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","Bengali","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"xm3600_1k","keyword":"hebrew","description":"\n\t\n\t\t\n\t\n\t\n\t\tXM3600 - Crossmodal-3600 - 1K Split\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a copy from https://google.github.io/crossmodal-3600/\n\t\n\nIf you use this dataset, please cite the original authors:\n@inproceedings{ThapliyalCrossmodal2022,\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\n  booktitle = {EMNLP},\n  year = {2022}\n}\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a 1K split of XM3600!\n\t\n\nFor this, we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/floschne/xm3600_1k.","url":"https://huggingface.co/datasets/floschne/xm3600_1k","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","Bengali","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"aya_collection_language_split","keyword":"hebrew","description":"\nThis is a re-upload of the aya_collection, and only differs in the structure of upload. While the original aya_collection is structured by folders split according to dataset name, this dataset is split by language. We recommend you use this version of the dataset if you are only interested in downloading all of the Aya collection for a single or smaller set of languages.\n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe Aya Collection is a massive multilingual collection consisting of 513 million instances of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/aya_collection_language_split.","url":"https://huggingface.co/datasets/CohereLabs/aya_collection_language_split","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Achinese","Afrikaans","Amharic","Arabic","Azerbaijani"],"keywords_longer_than_N":true},
	{"name":"heb-synthtiger-30k","keyword":"hebrew","description":"\n\t\n\t\t\n\t\theb-synthtiger-30k (Synthetic Hebrew Printed Text Dataset)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nheb-synthtiger-30 is a dataset containing 30,000 synthetic Hebrew text images generated using SynthTIGER with 11 different printed fonts.\nThe text in these images consists primarily of single words, sampled from the 10,000 most frequent Hebrew words, along with words from additional sources such as:\n\nIsraeli place names\nBiblical texts\n\nThis dataset is designed to support Hebrew OCR and text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/asafd60/heb-synthtiger-30k.","url":"https://huggingface.co/datasets/asafd60/heb-synthtiger-30k","creator_name":"Asaf Delmedigo","creator_url":"https://huggingface.co/asafd60","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Hebrew","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Tatoeba-Translations","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis is the latest version of Tatoeba translations as of December 2024.\nThe sentences are downloaded from the Tatoeba collection website.\nThe dataset is processed through mapping sentences.tar.bz2 using sentences_base.tar.bz2 to find source (sentence_src) and target (sentence_tgt) sentences.\nWhile lang_src and lang_tgt columns follow the mapping provided by Tatoeba, the lang_pair column merely lists the two languages in the translation pair.\n\n\t\n\t\t\n\t\n\t\n\t\tStatistics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/Tatoeba-Translations.","url":"https://huggingface.co/datasets/ymoslem/Tatoeba-Translations","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","Abkhaz","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for BibleNLP Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPartial and complete Bible translations in 833 languages, aligned by verse.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\naai, aak, aau, aaz, abt, abx, aby, acf, acr, acu, adz, aer, aey, agd, agg, agm, agn, agr, agt, agu, aia, aii, aka, ake, alp, alq, als, aly, ame, amf, amk, amm, amn, amo, amp, amr, amu, amx, anh, anv, aoi, aoj, aom, aon, apb, ape, apn, apr, apu, apw, apz, arb, are, arl, arn, arp, asm, aso, ata, atb, atd, atg, att, auc, aui, auy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bible-nlp/biblenlp-corpus.","url":"https://huggingface.co/datasets/bible-nlp/biblenlp-corpus","creator_name":"The Partnership for Applied Biblical NLP","creator_url":"https://huggingface.co/bible-nlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","no-annotation","expert-generated","translation","multilingual"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for BibleNLP Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPartial and complete Bible translations in 833 languages, aligned by verse.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\naai, aak, aau, aaz, abt, abx, aby, acf, acr, acu, adz, aer, aey, agd, agg, agm, agn, agr, agt, agu, aia, aii, aka, ake, alp, alq, als, aly, ame, amf, amk, amm, amn, amo, amp, amr, amu, amx, anh, anv, aoi, aoj, aom, aon, apb, ape, apn, apr, apu, apw, apz, arb, are, arl, arn, arp, asm, aso, ata, atb, atd, atg, att, auc, aui, auy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bible-nlp/biblenlp-corpus.","url":"https://huggingface.co/datasets/bible-nlp/biblenlp-corpus","creator_name":"The Partnership for Applied Biblical NLP","creator_url":"https://huggingface.co/bible-nlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","no-annotation","expert-generated","translation","multilingual"],"keywords_longer_than_N":true},
	{"name":"Piyyut","keyword":"hebrew","description":"","url":"https://huggingface.co/datasets/tokeron/Piyyut","creator_name":"Michael Toker","creator_url":"https://huggingface.co/tokeron","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","monolingual","original","Hebrew","afl-3.0"],"keywords_longer_than_N":true},
	{"name":"mfaq","keyword":"hebrew","description":"We present the first multilingual FAQ dataset publicly available. We collected around 6M FAQ pairs from the web, in 21 different languages.","url":"https://huggingface.co/datasets/clips/mfaq","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","no-annotation","other","multilingual"],"keywords_longer_than_N":true},
	{"name":"Open_Assistant_Conversation_Chains","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset description\n\t\n\n\n\nThis dataset is a reformatting of OpenAssistant Conversations (OASST1), which is\n\na human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.\n\nIt was modified‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m-ric/Open_Assistant_Conversation_Chains.","url":"https://huggingface.co/datasets/m-ric/Open_Assistant_Conversation_Chains","creator_name":"Aymeric Roucher","creator_url":"https://huggingface.co/m-ric","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Spanish","Russian","German"],"keywords_longer_than_N":true},
	{"name":"sefaria","keyword":"hebrew","description":"This Dataset is a Hugging Face interface to the Sefaria database export \nSefaria is a large collection of early Jewish texts, mostly in ancient Hebrew, but also some are in \nAramaic, and some are translations into English.\n","url":"https://huggingface.co/datasets/mehdie/sefaria","creator_name":"MEHDIE","creator_url":"https://huggingface.co/mehdie","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["Hebrew","English","cc-by-4.0","üá∫üá∏ Region: US","History"],"keywords_longer_than_N":true},
	{"name":"nllb-200-10M-sample","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for \"nllb-200-10M-sample\"\n\t\n\nThis is a sample of nearly 10M sentence pairs from the NLLB-200 \nmined dataset allenai/nllb, \nscored with the model facebook/blaser-2.0-qe \ndescribed in the SeamlessM4T paper.\nThe sample is not random; instead, we just took the top n sentence pairs from each translation direction.\nThe number n was computed with the goal of upsamping the directions that contain underrepresented languages.\nNevertheless, the 187 languoids (language and script‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/slone/nllb-200-10M-sample.","url":"https://huggingface.co/datasets/slone/nllb-200-10M-sample","creator_name":"SLONE","creator_url":"https://huggingface.co/slone","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["translation","Akan","Amharic","Arabic","Awadhi"],"keywords_longer_than_N":true},
	{"name":"bible_para","keyword":"hebrew","description":"This is a multilingual parallel corpus created from translations of the Bible compiled by Christos Christodoulopoulos and Mark Steedman.\n\n102 languages, 5,148 bitexts\ntotal number of files: 107\ntotal number of tokens: 56.43M\ntotal number of sentence fragments: 2.84M","url":"https://huggingface.co/datasets/Helsinki-NLP/bible_para","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"CulturaY","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tCulturaY: A Large Cleaned Multilingual Dataset of 75 Languages\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nFrom the team that brought you CulturaX, we present CulturaY, another substantial multilingual dataset of 15TB (uncompressed)/3TB (zstd-compressed) that applies the same dataset cleaning methodology to the HPLT v1.1 dataset. \nPlease note that HPLT v1.2 has also been released and is an alternative verison with different cleaning methodolgies. \nThis data was used in part to train our SOTA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Viet-Mistral/CulturaY.","url":"https://huggingface.co/datasets/Viet-Mistral/CulturaY","creator_name":"Vietnamese Mistral","creator_url":"https://huggingface.co/Viet-Mistral","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"xP3x-sample","keyword":"hebrew","description":"A multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual commonsense reasoning capabilities.","url":"https://huggingface.co/datasets/Muennighoff/xP3x-sample","creator_name":"Niklas Muennighoff","creator_url":"https://huggingface.co/Muennighoff","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"MevakerConcTree","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMevakerConcTree\n\t\n\nA dataset intended for the conclusion allocation task. \nThe dataset represents several states of pre-allocated conclusions to a given hierarchical heading structure.\n\n\t\n\t\t\n\t\tCiting\n\t\n\nIf you use MevakerConcTree in your research, please cite Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language.\n@article{shalumov2024mevaker,\n      title={Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language}, \n      author={Vitaly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HeTree/MevakerConcTree.","url":"https://huggingface.co/datasets/HeTree/MevakerConcTree","creator_name":"HeTree","creator_url":"https://huggingface.co/HeTree","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","Hebrew","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"xsimplusplus","keyword":"hebrew","description":"xSIM++ is an extension of xSIM. In comparison to xSIM, this evaluates using target-side data with additional synthetic, hard-to-distinguish examples. You can find more details about it in the publication: xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages.\n","url":"https://huggingface.co/datasets/jaygala24/xsimplusplus","creator_name":"Jay Gala","creator_url":"https://huggingface.co/jaygala24","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["derived","multilingual","Achinese","Mesopotamian Arabic","Ta'izzi-Adeni Arabic"],"keywords_longer_than_N":true},
	{"name":"dictalm2.0-quant-calib-dataset","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDictaLM 2.0 Quantization Calibration Dataset\n\t\n\nThis dataset consists of documents which can be used as a calibration dataset for quantizing Hebrew-English LLMs, such as DictaLM 2.0. You can read more about the model on the release page or in the technical report.\nThe dataset consists of 1024 documents - 512 in English (FineWeb-EDU) and 512 in Hebrew (Wikipedia, OSCAR).  \n","url":"https://huggingface.co/datasets/dicta-il/dictalm2.0-quant-calib-dataset","creator_name":"DICTA: The Israel Center for Text Analysis","creator_url":"https://huggingface.co/dicta-il","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["Hebrew","English","odc-by","1K - 10K","arrow"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus-mmteb","keyword":"hebrew","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\nLoading example:\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"davidstap/biblenlp-corpus-mmteb\", \"eng-arb\", trust_remote_code=True)\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 28723\n    })\n    validation: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 1578\n    })‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb.","url":"https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","expert-generated","translation","multilingual","Arifama-Miniafia"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus-mmteb","keyword":"hebrew","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\nLoading example:\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"davidstap/biblenlp-corpus-mmteb\", \"eng-arb\", trust_remote_code=True)\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 28723\n    })\n    validation: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 1578\n    })‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb.","url":"https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","expert-generated","translation","multilingual","Arifama-Miniafia"],"keywords_longer_than_N":true},
	{"name":"Thinking-multilingual-big-10k-sft","keyword":"hebrew","description":"\nA dataset based off of openo1 math, 500 examples translated to 23 different languages. filtered out un-translated examples.\nenjoy üëç\n","url":"https://huggingface.co/datasets/Pinkstack/Thinking-multilingual-big-10k-sft","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"hebrew_suffix_verbal_forms","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tSuffixed Verbal Forms Detection Dataset for Modern Hebrew\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains annotated Hebrew sentences containing verbal forms that are ambiguous as to whether they include a pronominal suffix or not (e.g., the Hebrew word lamed-yod-mem-daled-vav can be understood as either \"he taught him\" or \"they taught\"). The goal of the dataset is to support tasks involving the identification and disambiguation of verbs with pronominal suffixes in Hebrew literature‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dicta-il/hebrew_suffix_verbal_forms.","url":"https://huggingface.co/datasets/dicta-il/hebrew_suffix_verbal_forms","creator_name":"DICTA: The Israel Center for Text Analysis","creator_url":"https://huggingface.co/dicta-il","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Hebrew","cc-by-4.0","1K - 10K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"heb-synthtiger-16k","keyword":"hebrew","description":"\n\t\n\t\t\n\t\theb-synthtiger-16k (Synthetic Hebrew Printed Text Dataset)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nheb-synthtiger-16k is a dataset containing 16,000 synthetic Hebrew text images generated using SynthTIGER with 11 different printed fonts.\nThe text in these images consists primarily of single words, sampled from the 10,000 most frequent Hebrew words, along with words from additional sources such as:\n\nIsraeli place names\nBiblical texts\n\nThis dataset is designed to support Hebrew OCR and text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/asafd60/heb-synthtiger-16k.","url":"https://huggingface.co/datasets/asafd60/heb-synthtiger-16k","creator_name":"Asaf Delmedigo","creator_url":"https://huggingface.co/asafd60","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Hebrew","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"amazon_massive_scenario","keyword":"hebrew","description":"\n  MassiveScenarioClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSpoken\n\nReference\nhttps://arxiv.org/abs/2204.08582\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"MassiveScenarioClassification\"])\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/amazon_massive_scenario.","url":"https://huggingface.co/datasets/mteb/amazon_massive_scenario","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","translated","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"mewsli-x","keyword":"hebrew","description":"I generated the dataset following mewsli-x.md#getting-started\nand converted into different parts (see process.py):\n\nar/de/en/es/fa/ja/pl/ro/ta/tr/uk wikinews_mentions dev and test (from wikinews_mentions-dev/test.jsonl)\ncandidate entities of 50 languages (from candidate_set_entities.jsonl)\nEnglish wikipedia_pairs to fine-tune models (from wikipedia_pairs-dev/train.jsonl)\n\nRaw data files are in raw.tar.gz, which contains:\n[...] 535M Feb 24 22:06 candidate_set_entities.jsonl\n[...] 9.8M Feb 24‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/izhx/mewsli-x.","url":"https://huggingface.co/datasets/izhx/mewsli-x","creator_name":"Xin Zhang","creator_url":"https://huggingface.co/izhx","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","entity-linking-retrieval","Afrikaans","Arabic","Azerbaijani"],"keywords_longer_than_N":true},
	{"name":"otzar-hatorah","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tOtzar HaTorah Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Card for Otzar HaTorah\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Otzar HaTorah Corpus is a large, comprehensive collection of contemporary Rabbinic and Torah scholarship, sourced from the \"◊í◊û\"◊ó ◊ê◊ï◊¶◊® ◊î◊™◊ï◊®◊î ◊î◊ó◊ì◊©\" (Gmach Otzar HaTorah HaChadash) digital library. The dataset comprises approximately 12,000 documents, originally in formats like .doc, .docx, and .pdf, which have been converted to Markdown format.\nThe content covers a vast range of Judaic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/otzar-hatorah.","url":"https://huggingface.co/datasets/NHLOCAL/otzar-hatorah","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","Hebrew","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"otzar-hatorah","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tOtzar HaTorah Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Card for Otzar HaTorah\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Otzar HaTorah Corpus is a large, comprehensive collection of contemporary Rabbinic and Torah scholarship, sourced from the \"◊í◊û\"◊ó ◊ê◊ï◊¶◊® ◊î◊™◊ï◊®◊î ◊î◊ó◊ì◊©\" (Gmach Otzar HaTorah HaChadash) digital library. The dataset comprises approximately 12,000 documents, originally in formats like .doc, .docx, and .pdf, which have been converted to Markdown format.\nThe content covers a vast range of Judaic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/otzar-hatorah.","url":"https://huggingface.co/datasets/NHLOCAL/otzar-hatorah","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","Hebrew","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ApolloMoEDataset","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDemocratizing Medical LLMs For Much More Languages\n\t\n\nCovering 12 Major Languages including English, Chinese, French, Hindi, Spanish, Arabic, Russian, Japanese, Korean, German, Italian, Portuguese and 38 Minor Languages So far.\n\n   üìÉ Paper ‚Ä¢ üåê Demo ‚Ä¢ ü§ó ApolloMoEDataset ‚Ä¢ ü§ó ApolloMoEBench  ‚Ä¢ ü§ó Models  ‚Ä¢üåê Apollo  ‚Ä¢ üåê ApolloMoE\n\n\n\n\n\n\n\t\n\t\t\n\t\tüåà Update\n\t\n\n\n[2024.10.15] ApolloMoE repo is publishedÔºÅüéâ\n\n\n\t\n\t\t\n\t\tLanguages Coverage\n\t\n\n12 Major Languages and 38 Minor Languages\n\n  Click to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEDataset.","url":"https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEDataset","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Arabic","English","Chinese","Korean"],"keywords_longer_than_N":true},
	{"name":"CaLMQA","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\n\nCaLMQA is a translation-free long-form question answering (LFQA) dataset spanning 23 high- to low-resource languages. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCaLMQA is a translation-free LFQA dataset with 51.7K questions from 23 languages, 11 high- to mid-resource and 12 low-resource.\nAll questions are culturally specific ‚Äì (1) they refer to concepts unique to one or a few cultures, such as\n\"Kuber iki umwami wa mbere w‚Äôuburundi yitwa Ntare?\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shanearora/CaLMQA.","url":"https://huggingface.co/datasets/shanearora/CaLMQA","creator_name":"Shane Arora","creator_url":"https://huggingface.co/shanearora","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multilingual","Afar","Arabic","Baluchi","German"],"keywords_longer_than_N":true},
	{"name":"muri-it-language-split","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMURI-IT: Multilingual Instruction Tuning Dataset for 200 Languages via Multilingual Reverse Instructions\n\t\n\nMURI-IT is a large-scale multilingual instruction tuning dataset containing 2.2 million instruction-output pairs across 200 languages. It is designed to address the challenges of instruction tuning in low-resource languages with Multilingual Reverse Instructions (MURI), which ensures that the output is human-written, high-quality, and authentic to the cultural and linguistic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/akoksal/muri-it-language-split.","url":"https://huggingface.co/datasets/akoksal/muri-it-language-split","creator_name":"Abdullatif Koksal","creator_url":"https://huggingface.co/akoksal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","question-answering","summarization","Achinese"],"keywords_longer_than_N":true},
	{"name":"oaast_rm_full_jieba","keyword":"hebrew","description":"Â∞ùËØïËß£ÂÜ≥\"llm repetition problem\"Ôºå‰ΩøÁî®ÂàÜËØçÊ®°ÂûãÂØπoaastËØ≠ÊñôËøõË°å‚ÄúÁªìÂ∑¥Âåñ‚ÄùÊï∞ÊçÆÂ¢ûÂº∫ÔºåÊèê‰æõÊõ¥Âº∫ÁöÑÈáçÂ§çÂÜÖÂÆπÊãíÁªùÊïàÊûú„ÄÇ\nAttempts to solve the \"llm repetition problem\" by using a segmentation model to enhance the oaast corpus with \"stuttering\" data to provide stronger rejection of duplicate content.\nÂÖ∂Ê¨°ÔºåËøòËøáÊª§Êéâ‰∫ÜÊâÄÊúâËá™ÊàëËÆ§Áü•ÁöÑÂæÆË∞ÉÊ†∑Êú¨„ÄÇ\nSecond, it also filters out all the fine-tuned samples of self-cognition.\nfiles:\n\noaast_rm_full_jieba.jsonl : word level repeat\noaast_rm_full_sent_jieba.jsonl : sentence level repeat\n\n","url":"https://huggingface.co/datasets/lenML/oaast_rm_full_jieba","creator_name":"len","creator_url":"https://huggingface.co/lenML","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"aya_evaluation_suite","keyword":"hebrew","description":"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAya Evaluation Suite contains a total of 26,750 open-ended conversation-style prompts to evaluate multilingual open-ended generation quality.To strike a balance between language coverage and the quality that comes with human curation, we create an evaluation suite that includes:\n\nhuman-curated examples in 7 languages (tur, eng, yor, arb, zho, por, tel) ‚Üí aya-human-annotated.\nmachine-translations of handpicked examples into 101 languages ‚Üí dolly-machine-translated.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/aya_evaluation_suite.","url":"https://huggingface.co/datasets/CohereLabs/aya_evaluation_suite","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","crowdsourced","expert-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"panlex-meanings","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for panlex-meanings\n\t\n\nThis is a dataset of words in several thousand languages, extracted from https://panlex.org.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset has been extracted from https://panlex.org (the 20240301 database dump) and rearranged on the per-language basis.\nEach language subset consists of expressions (words and phrases). \nEach expression is associated with some meanings (if there is more than one meaning, they are in separate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cointegrated/panlex-meanings.","url":"https://huggingface.co/datasets/cointegrated/panlex-meanings","creator_name":"David Dale","creator_url":"https://huggingface.co/cointegrated","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["translation","Afar","Western Abnaki","Abkhazian","Abaza"],"keywords_longer_than_N":true},
	{"name":"Hebrew_wikipedia","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tAbout Dataset\n\t\n\nThe following dataset is a collection of JSON sets that contain most of the Hebrew Wikipedia.\n\n\t\n\t\t\n\t\tAbout Data collection methodology\n\t\n\nThe strategy behind the methodology for collecting the data is as follows:\n\nCrawl Hebrew Wikipedia: Begin by crawling through Hebrew Wikipedia to collect all redirect links on each page.\nBreadth-First Search (BFS): For each page, apply a BFS-like strategy to ensure that every link is scraped.\nLink Collection: Collect the links as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YanFren/Hebrew_wikipedia.","url":"https://huggingface.co/datasets/YanFren/Hebrew_wikipedia","creator_name":"Yan","creator_url":"https://huggingface.co/YanFren","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Hebrew","mit","100K - 1M","text","Text"],"keywords_longer_than_N":true},
	{"name":"Lemonade","keyword":"hebrew","description":"LEMONADE is a large, expert-annotated dataset for event extraction from news articles in 20 languages: English, Spanish, Arabic, French, Italian, Russian, German, Turkish, Burmese, Indonesian, Ukrainian, Korean, Portuguese, Dutch, Somali, Nepali, Chinese, Persian, Hebrew, and Japanese.\nSee https://github.com/stanford-oval/Lemonade for details.\n","url":"https://huggingface.co/datasets/stanford-oval/Lemonade","creator_name":"Stanford Open Virtual Assistant Lab (OVAL)","creator_url":"https://huggingface.co/stanford-oval","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","Spanish","Arabic"],"keywords_longer_than_N":true},
	{"name":"philosophy-culture-translations-html-csv","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tAI-Culture Philosophy and Culture Translations CSV + HTML Corpus\n\t\n\nThe corpus contains an exceptionally diverse range of cultural, philosophical, and literary texts, available in 12 major languages. Among other topics, there is extensive engagement with the ethics and aesthetics of artificial intelligence and its cultural and philosophical implications, as well as connections between AI and philosophy of language and philosophy of mind.\nThis project is maintained by a non-profit‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-Culture-Commons/philosophy-culture-translations-html-csv.","url":"https://huggingface.co/datasets/AI-Culture-Commons/philosophy-culture-translations-html-csv","creator_name":"AI‚ÄëCulture‚ÄëCommons","creator_url":"https://huggingface.co/AI-Culture-Commons","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text-classification","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"daf-yomi-talmud-whisper-training","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Citing of the Gmara\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThis dataset contains audio citing of the Gmara\n\nLicense: cc-by-4.0\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nEach example in the dataset contains:\n\naudio: An audio column containing:\nbytes: The audio data encoded in MP3 format\npath: A string identifier derived from the source entry ID\nSampling rate: Fixed at 16000 Hz\n\n\ntranscript: A string containing the text with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/portal-daf-yomi/daf-yomi-talmud-whisper-training.","url":"https://huggingface.co/datasets/portal-daf-yomi/daf-yomi-talmud-whisper-training","creator_name":"Portal Hadaf AaYomi","creator_url":"https://huggingface.co/portal-daf-yomi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Shai Engel","Hebrew","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"original-language-bibles-hebrew","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tCitation Information\n\t\n\nIf you use this data, please cite:\nBibTeX:\n@dataset{original_language_bibles,\n  author       = {Hope McGovern},\n  title        = {{{Original Language Bible Corpus}}: {{Translator's Amalgamated Hebrew Old Testament}} and {{Translator's Amalgamated Greek New Testament}} with Word-Level Translations},\n  year         = {2024},\n  publisher    = {Hugging Face Hub},\n  note         = {TAHOT DOI:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hmcgovern/original-language-bibles-hebrew.","url":"https://huggingface.co/datasets/hmcgovern/original-language-bibles-hebrew","creator_name":"Hope McGovern","creator_url":"https://huggingface.co/hmcgovern","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Hebrew","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"MAPS_Verified","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Multilingual Benchmark for Global Agent Performance and Security\n\t\n\nThis is the first Multilingual Agentic AI Benchmark for evaluating agentic AI systems across different languages and diverse tasks. Benchmark enables systematic analysis of how agents perform under multilingual conditions. This dataset contains 550 instances for GAIA, 660 instances for ASB, 737 instances for Maths, and 1100 instances for SWE. Each task was translated into 10 target languages resulting‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Fujitsu-FRE/MAPS_Verified.","url":"https://huggingface.co/datasets/Fujitsu-FRE/MAPS_Verified","creator_name":"Fujitsu Research of Europe","creator_url":"https://huggingface.co/Fujitsu-FRE","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Arabic","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"webfaq","keyword":"hebrew","description":"WebFAQ Q&A Dataset\n\n   \n       Overview |\n       Details  |\n       Structure  |\n       Examples |\n       Considerations |\n       License |\n       Citation |\n       Contact |\n       Acknowledgement\n   \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe WebFAQ Q&A Dataset is a broad-coverage corpus of 96 million natural question-answer (QA) pairs in 75 languages, gathered from FAQ pages on the web. It leverages structured schema.org FAQPage annotations, making it a unique resource for large-scale Question Answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PaDaS-Lab/webfaq.","url":"https://huggingface.co/datasets/PaDaS-Lab/webfaq","creator_name":"Chair of Data Science, University of Passau","creator_url":"https://huggingface.co/PaDaS-Lab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multilingual","Afrikaans","Amharic","Arabic"],"keywords_longer_than_N":true},
	{"name":"STT-Voice-Notes-Evals","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tSTT Voice Note Evaluation\n\t\n\nAuthor: Daniel RosehillDate Created: August 11, 2025Purpose: Comparative evaluation of Speech-to-Text (STT) services for voice note transcription\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset was created as part of ongoing work developing voice note transcription systems. It contains ground truth transcripts representing typical daily voice notes, recorded to evaluate and compare STT service accuracy across different content types.\nSpeaker Profile:\n\nSingle speaker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/danielrosehill/STT-Voice-Notes-Evals.","url":"https://huggingface.co/datasets/danielrosehill/STT-Voice-Notes-Evals","creator_name":"Daniel Rosehill","creator_url":"https://huggingface.co/danielrosehill","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","Hebrew","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"MAPS","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Multilingual Benchmark for Global Agent Performance and Security\n\t\n\nThis is the first Multilingual Agentic AI Benchmark for evaluating agentic AI systems across different languages and diverse tasks. Benchmark enables systematic analysis of how agents perform under multilingual conditions. To balance performance and safety evaluation, our benchmark comprises 805 tasks: 405 from performance-oriented datasets (GAIA, SWE-bench, MATH) and 400 from the Agent Security‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Fujitsu-FRE/MAPS.","url":"https://huggingface.co/datasets/Fujitsu-FRE/MAPS","creator_name":"Fujitsu Research of Europe","creator_url":"https://huggingface.co/Fujitsu-FRE","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Arabic","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"judaic-texts-corpus","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tJudaic Texts Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a comprehensive corpus of classic Jewish rabbinic literature, providing a vast collection of foundational texts in a clean, machine-readable format. The corpus spans a wide range of genres and historical periods, from the Tanakh and Talmud to Halakha (Jewish Law), Kabbalah, Chassidut, and modern rabbinic responsa.\nThe content is sourced from the otzaria-library project, curated by Sivan22, and was processed and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/judaic-texts-corpus.","url":"https://huggingface.co/datasets/NHLOCAL/judaic-texts-corpus","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","question-answering","text-classification","language-modeling"],"keywords_longer_than_N":true},
	{"name":"judaic-texts-corpus","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tJudaic Texts Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a comprehensive corpus of classic Jewish rabbinic literature, providing a vast collection of foundational texts in a clean, machine-readable format. The corpus spans a wide range of genres and historical periods, from the Tanakh and Talmud to Halakha (Jewish Law), Kabbalah, Chassidut, and modern rabbinic responsa.\nThe content is sourced from the otzaria-library project, curated by Sivan22, and was processed and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/judaic-texts-corpus.","url":"https://huggingface.co/datasets/NHLOCAL/judaic-texts-corpus","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","question-answering","text-classification","language-modeling"],"keywords_longer_than_N":true},
	{"name":"HPLT2.0_cleaned","keyword":"hebrew","description":"This is a large-scale collection of web-crawled documents in 191 world languages, produced by the HPLT project. \nThe source of the data is mostly Internet Archive with some additions from Common Crawl.\nFor a detailed description of the dataset, please refer to https://hplt-project.org/datasets/v2.0\nThe Cleaned variant of HPLT Datasets v2.0\nThis is the cleaned variant of the HPLT Datasets v2.0 converted to the Parquet format semi-automatically when being uploaded here. \nThe original JSONL files‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jobs-git/HPLT2.0_cleaned.","url":"https://huggingface.co/datasets/jobs-git/HPLT2.0_cleaned","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","text-generation","language-modeling","multilingual","Achinese"],"keywords_longer_than_N":true},
	{"name":"GlotCC-V1","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n \n\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\n\n\t\t\n\t\tUsage (Huggingface Hub -- Recommended)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1.","url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Abau","Amarasi","Abkhaz","Abkhazian"],"keywords_longer_than_N":true},
	{"name":"GlotCC-V1","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n \n\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\n\n\t\t\n\t\tUsage (Huggingface Hub -- Recommended)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1.","url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Abau","Amarasi","Abkhaz","Abkhazian"],"keywords_longer_than_N":true},
	{"name":"reasoning-multilingual-R1-Llama-70B-train","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tlightblue/reasoning-multilingual-R1-Llama-70B-train\n\t\n\nThis is a multilingual reasoning dataset covering more than 30 languages.\nThis dataset was made by:\n\nSampling prompts from English datasets and translating them to various languages\nGenerating responses to these prompts 8 times using deepseek-ai/DeepSeek-R1-Distill-Llama-70B\nFiltering out <think> sections with incorrect language, non-fluent language, and incorrect answers\n\nThis dataset was then used to train a multilingual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lightblue/reasoning-multilingual-R1-Llama-70B-train.","url":"https://huggingface.co/datasets/lightblue/reasoning-multilingual-R1-Llama-70B-train","creator_name":"Lightblue KK.","creator_url":"https://huggingface.co/lightblue","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Amharic","Arabic","Bengali","Chinese","Czech"],"keywords_longer_than_N":true},
	{"name":"fleurs_clean","keyword":"hebrew","description":"\n\t\n\t\t\n\t\n\t\n\t\tFLEURS\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark. \nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\nused and ‚Äùunit error rate‚Äù (characters, signs) of all languages is averaged. Languages and results are also grouped into seven‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Helloworld668/fleurs_clean.","url":"https://huggingface.co/datasets/Helloworld668/fleurs_clean","creator_name":"zhide Tang","creator_url":"https://huggingface.co/Helloworld668","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"multiblimp","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMultiBLiMP\n\t\n\nMultiBLiMP is a massively Multilingual Benchmark for Linguistic Minimal Pairs. The dataset is composed of synthetic pairs generated using Universal Dependencies and UniMorph.\nThe paper can be found here.\nWe split the data set by language: each language consists of a single .tsv file. The rows contain many attributes for a particular pair, most important are the sen and wrong_sen fields, which we use for evaluating the language models.\n\n\t\n\t\t\n\t\n\t\n\t\tUsing MultiBLiMP\n\t\n\nTo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jumelet/multiblimp.","url":"https://huggingface.co/datasets/jumelet/multiblimp","creator_name":"Jaap Jumelet","creator_url":"https://huggingface.co/jumelet","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Buriat","Spanish","Sanskrit","Romanian"],"keywords_longer_than_N":true},
	{"name":"oscar-mini","keyword":"hebrew","description":"The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\\","url":"https://huggingface.co/datasets/nthngdy/oscar-mini","creator_name":"Nathan Godey","creator_url":"https://huggingface.co/nthngdy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"language_tags","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nDataset listing 27,328 languages and dialects (also includes macrolanguage names).For each language, either the ISO 639 code, the Glottolog code or both are provided.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\nEnglish_Name: Language name in English (e.g. \"French\").\nNative_Name: If value is not 0, corresponds to the name of the language by native speakers (e.g. \"Fran√ßais\") which may have been found in Wikipedia's nativename field.\nGlottocode: The language tag in the Glottolog convention (e.g.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/language_tags.","url":"https://huggingface.co/datasets/lbourdois/language_tags","creator_name":"Lo√Øck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Afade","Par√° Ar√°ra","Afar","Aka-Bea","Abon"],"keywords_longer_than_N":true},
	{"name":"fleurs","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tFLEURS\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark. \nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\nused and ‚Äùunit error rate‚Äù (characters, signs) of all languages is averaged. Languages and results are also grouped into seven‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cahya/fleurs.","url":"https://huggingface.co/datasets/cahya/fleurs","creator_name":"Cahya Wirawan","creator_url":"https://huggingface.co/cahya","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"oasst2_top1_chat_format","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tOpenAssistant TOP-1 Conversation Threads in huggingface chat format\n\t\n\nExport of oasst2 only top 1 threads in huggingface chat format\n\n\t\n\t\t\n\t\tScript\n\t\n\nThe convert script can be find here\n","url":"https://huggingface.co/datasets/blancsw/oasst2_top1_chat_format","creator_name":"Blanc Swan","creator_url":"https://huggingface.co/blancsw","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"tatoeba","keyword":"hebrew","description":"This is a collection of translated sentences from Tatoeba\n359 languages, 3,403 bitexts\ntotal number of files: 750\ntotal number of tokens: 65.54M\ntotal number of sentence fragments: 8.96M","url":"https://huggingface.co/datasets/Helsinki-NLP/tatoeba","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":null,"first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"ms_terms","keyword":"hebrew","description":"The Microsoft Terminology Collection can be used to develop localized versions of applications that integrate with Microsoft products.\nIt can also be used to integrate Microsoft terminology into other terminology collections or serve as a base IT glossary\nfor language development in the nearly 100 languages available. Terminology is provided in .tbx format, an industry standard for terminology exchange.","url":"https://huggingface.co/datasets/microsoft/ms_terms","creator_name":"Microsoft","creator_url":"https://huggingface.co/microsoft","license_name":"Microsoft Public License","license_url":"https://choosealicense.com/licenses/ms-pl/","language":null,"first_N":5,"first_N_keywords":["translation","expert-generated","expert-generated","multilingual","translation"],"keywords_longer_than_N":true},
	{"name":"hebrew_sentiment","keyword":"hebrew","description":"HebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israel‚Äôs\npresident, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder,\n2013) to scrape all the comments to all of the president‚Äôs posts in the period of June ‚Äì August 2014,\nthe first three months of Rivlin‚Äôs presidency.2 While the president‚Äôs posts aimed at reconciling tensions\nand called for tolerance and empathy, the sentiment expressed in the comments to the president‚Äôs posts\nwas polarized between citizens who warmly thanked the president, and citizens that fiercely critiqued his\npolicy. Of the 12,804 comments, 370 are neutral; 8,512 are positive, 3,922 negative.\n\nData Annotation: A trained researcher examined each comment and determined its sentiment value,\nwhere comments with an overall positive sentiment were assigned the value 1, comments with an overall\nnegative sentiment were assigned the value -1, and comments that are off-topic to the post‚Äôs content\nwere assigned the value 0. We validated the coding scheme by asking a second trained researcher to\ncode the same data. There was substantial agreement between raters (N of agreements: 10623, N of\ndisagreements: 2105, Coehn‚Äôs Kappa = 0.697, p = 0).","url":"https://huggingface.co/datasets/omilab/hebrew_sentiment","creator_name":"OMILab, The Open University of Israel","creator_url":"https://huggingface.co/omilab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","sentiment-classification","expert-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"common_voice_17_0","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Common Voice Corpus 17.0\n\t\n\n\n\nThis dataset is an unofficial version of the Mozilla Common Voice Corpus 17. It was downloaded and converted from the project's website https://commonvoice.mozilla.org/.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAbkhaz, Albanian, Amharic, Arabic, Armenian, Assamese, Asturian, Azerbaijani, Basaa, Bashkir, Basque, Belarusian, Bengali, Breton, Bulgarian, Cantonese, Catalan, Central Kurdish, Chinese (China), Chinese (Hong Kong), Chinese (Taiwan), Chuvash, Czech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fsicoli/common_voice_17_0.","url":"https://huggingface.co/datasets/fsicoli/common_voice_17_0","creator_name":"Fabio Sicoli","creator_url":"https://huggingface.co/fsicoli","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","Abkhaz","Afrikaans","Amharic","Arabic"],"keywords_longer_than_N":true},
	{"name":"MevakerConc","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMevakerConc\n\t\n\nConclusion extraction dataset.\nContains the context of the audit, the offsets of conclusions as marked by the auditors and the conclusions text contained within the offsets.\n\n\t\n\t\t\n\t\tCiting\n\t\n\nIf you use MevakerConc in your research, please cite Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language.\n@article{shalumov2024mevaker,\n      title={Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language}, \n      author={Vitaly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HeTree/MevakerConc.","url":"https://huggingface.co/datasets/HeTree/MevakerConc","creator_name":"HeTree","creator_url":"https://huggingface.co/HeTree","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Hebrew","apache-2.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"entity_cs","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for EntityCS\n\t\n\n\nRepository: https://github.com/huawei-noah/noah-research/tree/master/NLP/EntityCS  \nPaper: https://aclanthology.org/2022.findings-emnlp.499.pdf  \nPoint of Contact: Fenia Christopoulou, Chenxi Whitehouse\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nWe use the English Wikipedia and leverage entity information from Wikidata to construct an entity-based Code Switching corpus. \nTo achieve this, we make use of wikilinks in Wikipedia, i.e. links from one page to another.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huawei-noah/entity_cs.","url":"https://huggingface.co/datasets/huawei-noah/entity_cs","creator_name":"HUAWEI Noah's Ark Lab","creator_url":"https://huggingface.co/huawei-noah","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Afrikaans","Amharic","Arabic","Assamese","Azerbaijani"],"keywords_longer_than_N":true},
	{"name":"hebrew_projectbenyehuda","keyword":"hebrew","description":"This repository contains a dump of thousands of public domain works in Hebrew, from Project Ben-Yehuda, in plaintext UTF-8 files, with and without diacritics (nikkud). The metadata (pseudocatalogue.csv) file is a list of titles, authors, genres, and file paths, to help you process the dump.\nAll these works are in the public domain, so you are free to make any use of them, and do not need to ask for permission.\nThere are 10078 files, 3181136 lines","url":"https://huggingface.co/datasets/projectbenyehuda/hebrew_projectbenyehuda","creator_name":"Project Ben-Yehuda - ◊§◊®◊ï◊ô◊ß◊ò ◊ë◊ü-◊ô◊î◊ï◊ì◊î","creator_url":"https://huggingface.co/projectbenyehuda","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","expert-generated"],"keywords_longer_than_N":true},
	{"name":"openassistant-falcon","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - OpenAssistant Falcon\n\t\n\nThis dataset allows for fine-tuning chat models using '\\Human:' AND '\\nAssistant:' to wrap user messages.\nIt still uses <|endoftext|> as EOS and BOS token, as per Falcon.\nSample \nPreparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-falcon.","url":"https://huggingface.co/datasets/Trelis/openassistant-falcon","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"oasst1","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tOpenAssistant Conversations Dataset (OASST1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \nis a product of a worldwide crowd-sourcing effort‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst1.","url":"https://huggingface.co/datasets/OpenAssistant/oasst1","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"HPLT2.0_cleaned","keyword":"hebrew","description":"This is a large-scale collection of web-crawled documents in 191 world languages, produced by the HPLT project. \nThe source of the data is mostly Internet Archive with some additions from Common Crawl.\nFor a detailed description of the dataset, please refer to our website and our pre-print.\n\n\t\n\t\t\n\t\n\t\n\t\tThe Cleaned variant of HPLT Datasets v2.0\n\t\n\nThis is the cleaned variant of the HPLT Datasets v2.0 converted to the Parquet format semi-automatically when being uploaded here. \nThe original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPLT/HPLT2.0_cleaned.","url":"https://huggingface.co/datasets/HPLT/HPLT2.0_cleaned","creator_name":"HPLT","creator_url":"https://huggingface.co/HPLT","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","text-generation","language-modeling","multilingual","Achinese"],"keywords_longer_than_N":true},
	{"name":"DATA-AI_Chat","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDATA-AI: Il Modello di IA di M.INC.\n\t\n\n\n\t\n\t\t\n\t\tüìå Introduzione\n\t\n\nDATA-AI √® un avanzato modello di intelligenza artificiale sviluppato da *M.INC., un'azienda italiana fondata da *Mattimax (M. Marzorati).Questo modello √® basato sull'architettura ELNS (Elaborazione del Linguaggio Naturale Semplice), un sistema innovativo progettato per rendere l'IA accessibile su quasi qualsiasi dispositivo, garantendo prestazioni ottimali anche su hardware limitato.  \nDATA-AI √® stato addestrato su un‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mattimax/DATA-AI_Chat.","url":"https://huggingface.co/datasets/Mattimax/DATA-AI_Chat","creator_name":"M.INC.","creator_url":"https://huggingface.co/Mattimax","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Italian","English","Spanish","Russian","German"],"keywords_longer_than_N":true},
	{"name":"reranker_continuous_filt_max7_train","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tReranker training data\n\t\n\nThis data was generated using 4 steps:\n\nWe gathered queries and corresponding text data from 35 high quality datasets covering more than 95 languages.\nFor datasets which did not already have negative texts for queries, we mined hard negatives using the BAAI/bge-m3 embedding model.\nFor each query, we selected one positive and one negative text and used Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4 to rate the relatedness of each query-text pair using a token \"1\", \"2\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lightblue/reranker_continuous_filt_max7_train.","url":"https://huggingface.co/datasets/lightblue/reranker_continuous_filt_max7_train","creator_name":"Lightblue KK.","creator_url":"https://huggingface.co/lightblue","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","Spanish","German","Arabic"],"keywords_longer_than_N":true},
	{"name":"eng_montok","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMonTok: A Suite of Monolingual Tokenizers\n\t\n\nThis is a set of monolingual tokenizers for 98 languages. For each language, there are Unigram, BPE, and SuperBPE tokenizers, ranging in vocabulary size from around 6k to over 200k.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\n","url":"https://huggingface.co/datasets/catherinearnett/eng_montok","creator_name":"Catherine Arnett","creator_url":"https://huggingface.co/catherinearnett","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Afrikaans","Tosk Albanian","Amharic","Standard Arabic","Assamese"],"keywords_longer_than_N":true},
	{"name":"real_music_albums_fs","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tReal Music Albums FS\n\t\n\nReal Music Albums FS is a structured dataset representing metadata extracted from real-world music album directories in Hebrew. The dataset was created by scanning existing folder structures from personal or archival music collections, typically stored on hard drives or local systems.\nThe data is organized by artist and album, and contains information on individual audio files including file names, sizes, formats, and file hashes.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/real_music_albums_fs.","url":"https://huggingface.co/datasets/NHLOCAL/real_music_albums_fs","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","Hebrew","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"real_music_albums_fs","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tReal Music Albums FS\n\t\n\nReal Music Albums FS is a structured dataset representing metadata extracted from real-world music album directories in Hebrew. The dataset was created by scanning existing folder structures from personal or archival music collections, typically stored on hard drives or local systems.\nThe data is organized by artist and album, and contains information on individual audio files including file names, sizes, formats, and file hashes.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/real_music_albums_fs.","url":"https://huggingface.co/datasets/NHLOCAL/real_music_albums_fs","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","Hebrew","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"Open-R1-Mulitlingual-SFT","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tOpen-R1-Mulitlingual-SFT\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nOpen-R1-Mulitlingual-SFT is a curated dataset designed for multilingual supervised fine-tuning.\nThe source data comprises multiple datasets containing original prompts and responses, which were subsequently translated into 14 languages using GPT-4o.\n\n\t\n\t\t\n\t\tSources\n\t\n\nThe dataset is derived from:\n\nopen-thoughts/OpenThoughts-114kHugging Face: open-thoughts/OpenThoughts-114k\nbespokelabs/Bespoke-Stratos-17kHugging Face:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/amphora/Open-R1-Mulitlingual-SFT.","url":"https://huggingface.co/datasets/amphora/Open-R1-Mulitlingual-SFT","creator_name":"GUIJIN SON","creator_url":"https://huggingface.co/amphora","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Afrikaans","Arabic","Chinese","English","French"],"keywords_longer_than_N":true},
	{"name":"X-ALMA-Preference","keyword":"hebrew","description":"This is the translation preference dataset used by X-ALMA.\nsource: the source sentence.\nchosen: the preferred translation.\nreject: the dis-preferred translation.\ndirections: the translation direction.\n@misc{xu2024xalmaplugplay,\n      title={X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale}, \n      author={Haoran Xu and Kenton Murray and Philipp Koehn and Hieu Hoang and Akiko Eriguchi and Huda Khayrallah},\n      year={2024},\n      eprint={2410.03115}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/haoranxu/X-ALMA-Preference.","url":"https://huggingface.co/datasets/haoranxu/X-ALMA-Preference","creator_name":"Haoran Xu","creator_url":"https://huggingface.co/haoranxu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","Danish","Dutch","German","Icelandic"],"keywords_longer_than_N":true},
	{"name":"hebrew_sa","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tSentiment Analysis Data for the Hebrew Language\n\t\n\nDataset Description:\nThis dataset contains a sentiment analysis dataset from Amram et al. (2018).\nData Structure:\nThe data was used for the project on improving word embeddings with graph knowledge for Low Resource Languages.\nCitation:\n@inproceedings{amram-etal-2018-representations,\n    title = \"Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from {M}odern {H}ebrew\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DGurgurov/hebrew_sa.","url":"https://huggingface.co/datasets/DGurgurov/hebrew_sa","creator_name":"Daniil Gurgurov","creator_url":"https://huggingface.co/DGurgurov","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","Hebrew","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"multilingual_translation_gpt4o_gen","keyword":"hebrew","description":"Youseff1987/multilingual_translation_gpt4o_gen dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Youseff1987/multilingual_translation_gpt4o_gen","creator_name":"JOON HYOUNG JUN","creator_url":"https://huggingface.co/Youseff1987","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Korean","English","Chinese","Zulu"],"keywords_longer_than_N":true},
	{"name":"m-ArenaHard","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for m-ArenaHard\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThe m-ArenaHard dataset is a multilingual LLM evaluation set. This dataset was created by translating the prompts from the originally English-only LMarena (formerly LMSYS) arena-hard-auto-v0.1 test dataset using Google Translate API v3 to 22 languages. The original English-only prompts were created by Li et al. (2024) and consist of 500 challenging user queries sourced from Chatbot Arena. The authors show that these can be used‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/m-ArenaHard.","url":"https://huggingface.co/datasets/CohereLabs/m-ArenaHard","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","French","German","Spanish","Italian"],"keywords_longer_than_N":true},
	{"name":"aya_collection","keyword":"hebrew","description":"\nThis dataset is uploaded in two places: here and additionally here as 'Aya Collection Language Split.' These datasets are identical in content but differ in structure of upload. This dataset is structured by folders split according to dataset name. The version here instead divides the Aya collection into folders split by language. We recommend you use the language split version if you are only interested in downloading data for a single or smaller set of languages, and this version if you‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/aya_collection.","url":"https://huggingface.co/datasets/CohereLabs/aya_collection","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","summarization","translation","Achinese","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"hebrew_synth_lines","keyword":"hebrew","description":"samaritan-ai/hebrew_synth_lines dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/samaritan-ai/hebrew_synth_lines","creator_name":"Samaritan AI","creator_url":"https://huggingface.co/samaritan-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Hebrew","Official Aramaic (700-300 BCE)","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mHumanEval-Benchmark","keyword":"hebrew","description":"\n\n\n\t\n\t\t\n\t\tüî∑ Accepted in NAACL Proceedings (2025) üî∑\n\t\n\n\n\n\n\n  \n    \n      \n        \n          \n        \n      \n      \n        \n          \n        \n      \n      \n        \n          \n        \n      \n    \n  \n\n\n\n\n\n  \n\n\t\n\t\n\t\n\t\tmHumanEval\n\t\n\nThe mHumanEval benchmark is curated based on prompts from the original HumanEval üìö [Chen et al., 2021]. It includes a total of 33,456 prompts for Python, and 836,400 in total - significantly expanding from the original 164. \n\n\n\t\n\t\t\n\t\tQuick Start\n\t\n\n  Detailed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/md-nishat-008/mHumanEval-Benchmark.","url":"https://huggingface.co/datasets/md-nishat-008/mHumanEval-Benchmark","creator_name":"Nishat Raihan","creator_url":"https://huggingface.co/md-nishat-008","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Afar","Abkhaz","Avestan","Afrikaans","Akan"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus-mmteb","keyword":"hebrew","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\nLoading example:\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"davidstap/biblenlp-corpus-mmteb\", \"eng-arb\", trust_remote_code=True)\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 28723\n    })\n    validation: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 1578\n    })‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb.","url":"https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","expert-generated","translation","multilingual","Arifama-Miniafia"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus-mmteb","keyword":"hebrew","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\nLoading example:\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"davidstap/biblenlp-corpus-mmteb\", \"eng-arb\", trust_remote_code=True)\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 28723\n    })\n    validation: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 1578\n    })‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb.","url":"https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","expert-generated","translation","multilingual","Arifama-Miniafia"],"keywords_longer_than_N":true},
	{"name":"toxi-text-3M","keyword":"hebrew","description":"This is a large multilingual toxicity dataset with 3M rows of text data from 55 natural languages, all of which are written/sent by humans, not machine translation models.\nThe preprocessed training data alone consists of 2,880,667 rows of comments, tweets, and messages. Among these rows, 416,529 are classified as toxic, while the remaining 2,463,773 are considered neutral. Below is a table to illustrate the data composition:\n\n\t\n\t\t\n\nToxic\nNeutral\nTotal\n\n\n\t\t\nmultilingual-train-deduplicated.csv‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/toxi-text-3M.","url":"https://huggingface.co/datasets/FredZhang7/toxi-text-3M","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","Arabic","Spanish","Panjabi"],"keywords_longer_than_N":true},
	{"name":"clams","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tCLAMS - Cross-Linguistic Assessment of Models on Syntax\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCLAMS (Cross-Linguistic Assessment of Models on Syntax) is a dataset of syntactic minimal pairs for English, French, German, Hebrew, and Russian. The dataset contains grammaticality judgment pairs (good/bad sentence pairs) for various syntactic phenomena, designed to evaluate language models' syntactic knowledge across different languages.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains minimal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/juletxara/clams.","url":"https://huggingface.co/datasets/juletxara/clams","creator_name":"Julen Etxaniz","creator_url":"https://huggingface.co/juletxara","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","expert-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"crowd-transcribe-v5-processed","keyword":"hebrew","description":"yuvals2/crowd-transcribe-v5-processed dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/yuvals2/crowd-transcribe-v5-processed","creator_name":"Yuval S","creator_url":"https://huggingface.co/yuvals2","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Hebrew","cc-by-4.0","100K - 1M","parquet","Datasets"],"keywords_longer_than_N":true},
	{"name":"muri-it","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMURI-IT: Multilingual Instruction Tuning Dataset for 200 Languages via Multilingual Reverse Instructions\n\t\n\nMURI-IT is a large-scale multilingual instruction tuning dataset containing 2.2 million instruction-output pairs across 200 languages. It is designed to address the challenges of instruction tuning in low-resource languages with Multilingual Reverse Instructions (MURI), which ensures that the output is human-written, high-quality, and authentic to the cultural and linguistic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/akoksal/muri-it.","url":"https://huggingface.co/datasets/akoksal/muri-it","creator_name":"Abdullatif Koksal","creator_url":"https://huggingface.co/akoksal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","question-answering","summarization","Achinese"],"keywords_longer_than_N":true},
	{"name":"latin-greek-hebrew-english-dataset","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tProverbs: Ancient Languages Set\n\t\n\nThis repository contains a collection of 2,000 short phrases translated into three ancient languages: Ancient Latin, Ancient Greek, Biblical Hebrew, and English. The phrases cover a wide variety of contexts, providing insight into the linguistic, cultural, and philosophical landscapes of these ancient civilizations.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe \"Proverbs: Ancient Languages Set\" is a resource designed to help individuals explore and understand ancient‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dddixyy/latin-greek-hebrew-english-dataset.","url":"https://huggingface.co/datasets/Dddixyy/latin-greek-hebrew-english-dataset","creator_name":"Davide Brunori","creator_url":"https://huggingface.co/Dddixyy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Latin","Hebrew","Greek","English"],"keywords_longer_than_N":true},
	{"name":"latin-greek-hebrew-english-dataset","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tProverbs: Ancient Languages Set\n\t\n\nThis repository contains a collection of 2,000 short phrases translated into three ancient languages: Ancient Latin, Ancient Greek, Biblical Hebrew, and English. The phrases cover a wide variety of contexts, providing insight into the linguistic, cultural, and philosophical landscapes of these ancient civilizations.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe \"Proverbs: Ancient Languages Set\" is a resource designed to help individuals explore and understand ancient‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dddixyy/latin-greek-hebrew-english-dataset.","url":"https://huggingface.co/datasets/Dddixyy/latin-greek-hebrew-english-dataset","creator_name":"Davide Brunori","creator_url":"https://huggingface.co/Dddixyy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Latin","Hebrew","Greek","English"],"keywords_longer_than_N":true},
	{"name":"ParaNames","keyword":"hebrew","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bltlab/ParaNames.","url":"https://huggingface.co/datasets/bltlab/ParaNames","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","Nias","Kotava","Banjar","Angika"],"keywords_longer_than_N":true},
	{"name":"SingNER","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tHebrew Song Titles NER Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is designed for Named Entity Recognition (NER) tasks focused on identifying key entities within Hebrew song titles. The dataset has been built using real-world music databases and enriched with synthetic data generated by language models. The annotations were performed using the Gemini model to ensure high-quality entity labeling.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of over 32,000 song titles, primarily‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/SingNER.","url":"https://huggingface.co/datasets/NHLOCAL/SingNER","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","Hebrew","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Wikipedia-Abstract","keyword":"hebrew","description":"Wikipedia Abstract\n\n\n  \n\n\n\nIntroducing Wikipedia Abstract, a comprehensive dataset encompassing abstracts, complete articles, and a popularity score index for both widely spoken and lesser-known Wikipedia subsets. Our dedication to Wikipedia-X ensures a centralized Wikipedia dataset that undergoes regular updates and adheres to the highest standards.\nA central focus of our efforts was to include exotic languages that often lack up-to-date Wikipedia dumps or may not have any dumps at all.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laion/Wikipedia-Abstract.","url":"https://huggingface.co/datasets/laion/Wikipedia-Abstract","creator_name":"LAION eV","creator_url":"https://huggingface.co/laion","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","text-generation","fill-mask","Arabic"],"keywords_longer_than_N":true},
	{"name":"xP3x-Kongo","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for xP3x Kikongo Focus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nxP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts & datasets across 277 languages & 16 NLP tasks. It contains all of xP3 + much more! It is used for training future contenders of mT0 & BLOOMZ at project Aya @C4AI üß°\n\n\nCreation: The dataset can be recreated using instructions available here together with the file in this repository named xp3x_create.py. We provide this version to save‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Svngoku/xP3x-Kongo.","url":"https://huggingface.co/datasets/Svngoku/xP3x-Kongo","creator_name":"NIONGOLO Chrys F√©-Marty","creator_url":"https://huggingface.co/Svngoku","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","translation","expert-generated","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"wikipedia-citation-index","keyword":"hebrew","description":"Dataset with citation indexes as of 1 August 2024 for 47 million Wikipedia articles in 55 language versions. Research: ArXiv\n","url":"https://huggingface.co/datasets/lewoniewski/wikipedia-citation-index","creator_name":"W≈Çodzimierz Lewoniewski","creator_url":"https://huggingface.co/lewoniewski","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","Azerbaijani","Belarusian","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"project-ben-yehuda","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tProject Ben-Yehuda Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains a comprehensive collection of public domain Hebrew texts, sourced from the digital library of Project Ben-Yehuda. The corpus includes thousands of works spanning various genres such as prose, poetry, essays, and translations, primarily from the Haskalah period to the early 20th century.\nThe data has been structured and cleaned to be readily usable for Natural Language Processing (NLP) tasks, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/project-ben-yehuda.","url":"https://huggingface.co/datasets/NHLOCAL/project-ben-yehuda","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["Hebrew","cc0-1.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"project-ben-yehuda","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tProject Ben-Yehuda Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains a comprehensive collection of public domain Hebrew texts, sourced from the digital library of Project Ben-Yehuda. The corpus includes thousands of works spanning various genres such as prose, poetry, essays, and translations, primarily from the Haskalah period to the early 20th century.\nThe data has been structured and cleaned to be readily usable for Natural Language Processing (NLP) tasks, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NHLOCAL/project-ben-yehuda.","url":"https://huggingface.co/datasets/NHLOCAL/project-ben-yehuda","creator_name":"NH Local","creator_url":"https://huggingface.co/NHLOCAL","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["Hebrew","cc0-1.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"thinking-multilingual-30-23-small-690","keyword":"hebrew","description":"\nBased on OPENO1 math, this is a translated dataset of 30 high quality questions & answers in 23 languages. \nOr use the \"big\" version: big 10k rows version\n","url":"https://huggingface.co/datasets/Pinkstack/thinking-multilingual-30-23-small-690","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"reranking-datasets-light","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tüî• Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation üî•\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tReRanking Datasets : A comprehensive collection of retrieval and reranking datasets with full passage contexts, including titles, text, and metadata for in-depth research.\n\t\n\n\n    \n    \n    \n    \n    \n    \n        \n    \n    \n        \n    \n    \n        \n    \n    \n        \n    \n    \n    \n\n\n\nA curated collection of ready-to-use datasets for retrieval and reranking‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light.","url":"https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light","creator_name":"Abdelrahman Abdallah","creator_url":"https://huggingface.co/abdoelsayed","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","Arabic","German","French"],"keywords_longer_than_N":true},
	{"name":"AyaVisionBench","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Aya Vision Benchmark\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThe Aya Vision Benchmark is designed to evaluate vision-language models in real-world multilingual scenarios. It spans 23 languages and 9 distinct task categories, with 15 samples per category, resulting in 135 image-question pairs per language. \nEach question requires visual context for the answer and covers languages that half of the world's population speaks, making this dataset particularly suited for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/AyaVisionBench.","url":"https://huggingface.co/datasets/CohereLabs/AyaVisionBench","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","French","German","Spanish","Italian"],"keywords_longer_than_N":true},
	{"name":"include-base-44","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tINCLUDE-base (44 languages)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nPaper: http://arxiv.org/abs/2411.19799\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \nIt contains 22,637 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/include-base-44.","url":"https://huggingface.co/datasets/CohereLabs/include-base-44","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Albanian","Arabic","Armenian","Azerbaijani"],"keywords_longer_than_N":true},
	{"name":"Pornhub","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tPornhub Dataset\n\t\n\nThe Pornhub Dataset provides a comprehensive collection of data sourced from pornhub.com, encompassing various details from MANYYY videos available on the platform.\nThe file consists of 742.133 lines of videos.\n\n\t\n\t\t\n\t\tData Description\n\t\n\n\nDelimiter: ‚ÄΩ\nFile Format: CSV\nContent:\nURL: The URL of the video.\nCategory: The genre or category of the video.\nUser: The username of the uploader.\nVideo_title: The title of the video.\nViews: The number of views the video has‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Nikity/Pornhub.","url":"https://huggingface.co/datasets/Nikity/Pornhub","creator_name":"Nikita","creator_url":"https://huggingface.co/Nikity","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["Albanian","Arabic","Bengali","Bulgarian","Chinese"],"keywords_longer_than_N":true},
	{"name":"xtreme","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for \"xtreme\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Cross-lingual Natural Language Inference (XNLI) corpus is a crowd-sourced collection of 5,000 test and\n2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into\n14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese,\nHindi, Swahili and Urdu. This results in 112.5k annotated pairs. Each premise can be associated with the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/xtreme.","url":"https://huggingface.co/datasets/google/xtreme","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","token-classification","text-classification","text-retrieval"],"keywords_longer_than_N":true},
	{"name":"LCHAIM","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tLCHAIM: Investigating Long Context Reasoning in Hebrew\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nLCHAIM is a dataset designed to evaluate Natural Language Inference (NLI) models in Hebrew. Unlike English, Hebrew is a Morphologically Rich Language (MRL), requiring more research to develop robust NLI models. LCHAIM provides a benchmark for models that need to handle long premises and complex reasoning in Hebrew.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nLCHAIM was created by translating and validating the English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HebArabNlpProject/LCHAIM.","url":"https://huggingface.co/datasets/HebArabNlpProject/LCHAIM","creator_name":"Israel National  NLP Program","creator_url":"https://huggingface.co/HebArabNlpProject","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","Hebrew","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"tatoeba_mt","keyword":"hebrew","description":"The Tatoeba Translation Challenge is a multilingual data set of\nmachine translation benchmarks derived from user-contributed\ntranslations collected by [Tatoeba.org](https://tatoeba.org/) and\nprovided as parallel corpus from [OPUS](https://opus.nlpl.eu/). This\ndataset includes test and development data sorted by language pair. It\nincludes test sets for hundreds of language pairs and is continuously\nupdated. Please, check the version number tag to refer to the release\nthat your are using.","url":"https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":null,"first_N":5,"first_N_keywords":["text-generation","translation","no-annotation","crowdsourced","translation"],"keywords_longer_than_N":true},
	{"name":"common_voice_16_0","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Common Voice Corpus 16.0\n\t\n\n\n\nThis dataset is an unofficial version of the Mozilla Common Voice Corpus 16. It was downloaded and converted from the project's website https://commonvoice.mozilla.org/.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAbkhaz, Albanian, Amharic, Arabic, Armenian, Assamese, Asturian, Azerbaijani, Basaa, Bashkir, Basque, Belarusian, Bengali, Breton, Bulgarian, Cantonese, Catalan, Central Kurdish, Chinese (China), Chinese (Hong Kong), Chinese (Taiwan), Chuvash, Czech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fsicoli/common_voice_16_0.","url":"https://huggingface.co/datasets/fsicoli/common_voice_16_0","creator_name":"Fabio Sicoli","creator_url":"https://huggingface.co/fsicoli","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","Abkhaz","Afrikaans","Amharic","Arabic"],"keywords_longer_than_N":true},
	{"name":"openassistant-deepseek-coder","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - OpenAssistant DeepSeek Coder\n\t\n\nThis dataset allows for fine-tuning chat models using:\nB_INST = '\\n### Instruction:\\n'\nE_INST = '\\n### Response:\\n'\nBOS = '<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>'\nEOS = '\\n<|EOT|>\\n'\n\nSample Preparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-deepseek-coder.","url":"https://huggingface.co/datasets/Trelis/openassistant-deepseek-coder","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"xtreme_s","keyword":"hebrew","description":"XTREME-S covers four task families: speech recognition, classification, speech-to-text translation and retrieval. Covering 102\nlanguages from 10+ language families, 3 different domains and 4\ntask families, XTREME-S aims to simplify multilingual speech\nrepresentation evaluation, as well as catalyze research in ‚Äúuniversal‚Äù speech representation learning.","url":"https://huggingface.co/datasets/google/xtreme_s","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"MevakerConcSen","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMevakerConcSen\n\t\n\nA sentence-level dataset for sentence-level conclusion extraction which provides a label of conclusion/not conclusion (1/0 respectivly) for each sentence\ntogether with indexes of the sentence and their document of origin.\n\n\t\n\t\t\n\t\tCiting\n\t\n\nIf you use MevakerConcSen in your research, please cite Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language.\n@article{shalumov2024mevaker,\n      title={Mevaker: Conclusion Extraction and Allocation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HeTree/MevakerConcSen.","url":"https://huggingface.co/datasets/HeTree/MevakerConcSen","creator_name":"HeTree","creator_url":"https://huggingface.co/HeTree","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Hebrew","apache-2.0","100K - 1M","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"HebrewSentiment","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tHebrewSentiment - A Sentiment-Analysis Dataset in Hebrew\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nHebrewSentiment is a Hebrew dataset for the sentiment analysis task. \n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset was constructed via [To Fill In].\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\nThe table below shows the number of examples from each category in each of the splits:\n\n\t\n\t\t\nsplit\ntotal\npositive\nnegative\nneutral\n\n\n\t\t\ntrain\n39,135\n8,968\n7,669\n22,498\n\n\ntest\n2,170\n503\n433\n1,234\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nEach‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HebArabNlpProject/HebrewSentiment.","url":"https://huggingface.co/datasets/HebArabNlpProject/HebrewSentiment","creator_name":"Israel National  NLP Program","creator_url":"https://huggingface.co/HebArabNlpProject","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","Hebrew","cc-by-4.0","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"oscar-small","keyword":"hebrew","description":"The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\\","url":"https://huggingface.co/datasets/djstrong/oscar-small","creator_name":"Krzysztof Wr√≥bel","creator_url":"https://huggingface.co/djstrong","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"oscar-small","keyword":"hebrew","description":"The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\\","url":"https://huggingface.co/datasets/nthngdy/oscar-small","creator_name":"Nathan Godey","creator_url":"https://huggingface.co/nthngdy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"multilingual-pl-bert","keyword":"hebrew","description":"Attribution: Wikipedia.org\n","url":"https://huggingface.co/datasets/styletts2-community/multilingual-pl-bert","creator_name":"StyleTTS 2 Community","creator_url":"https://huggingface.co/styletts2-community","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Afrikaans","Aragonese","Arabic","Azerbaijani","Bashkir"],"keywords_longer_than_N":true},
	{"name":"hebrew-handwritten-dataset","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\n\t\n\t\t\n\t\tKeywords\n\t\n\nHebrew, handwritten, letters\n\n\t\n\t\t\n\t\tDescription\n\t\n\nHDD_v0 consists of images of isolated Hebrew characters together with training and test sets subdivision. \nThe images were collected from hand-filled forms.\nFor more details, please refer to [1].\nWhen using this dataset in research work, please cite [1].\n[1] I. Rabaev, B. Kurar Barakat, A. Churkin and J. El-Sana. The HHD Dataset. The 17th International Conference on Frontiers in Handwriting‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sivan22/hebrew-handwritten-dataset.","url":"https://huggingface.co/datasets/sivan22/hebrew-handwritten-dataset","creator_name":"Sivan Ratson","creator_url":"https://huggingface.co/sivan22","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","Hebrew","cc-by-3.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"openassistant-guanaco-EOS","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - Guanaco Style\n\t\n\nThis dataset allows for fine-tuning chat models using \"### Human:\" AND \"### Assistant\" as the beginning and end of sequence tokens.\nPreparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe dataset was then slightly adjusted to:\n\n\nif a row of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-guanaco-EOS.","url":"https://huggingface.co/datasets/Trelis/openassistant-guanaco-EOS","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"Wikinews-multilingual","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tWikinews - weakly aligned multilingual pararell sentence datasets\n\t\n\nThis dataset contains 15,200 multilingual WikiNews articles in 33 languages.\nOut of 15,200 articles, 9,960 are non-English news and 5240 are English news.  All non-English news are linked to one of 5240 English news. Linked articles show the same event.\nList of non-English languages are: Spanish, French, German, Portuguese, Polish, Italian, Chinese, Russian, Japanese, Dutch, Swedish, Tamil, Serbian, Czech, Catalan‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Fumika/Wikinews-multilingual.","url":"https://huggingface.co/datasets/Fumika/Wikinews-multilingual","creator_name":"Fumika Isono","creator_url":"https://huggingface.co/Fumika","license_name":"Creative Commons Attribution 2.5","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.5.html","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","English","Spanish","French"],"keywords_longer_than_N":true},
	{"name":"wmt-human-all","keyword":"hebrew","description":"This dataset is continuously updated and contains a compilation of human translation quality assessment from past WMT campaigns.\nSpecifically, this dataset merges all annotation protocols (DA, MQM, ESA) on a semi-unified scale (0 to 100).\nThe current version of the dataset includes human scores up to WMT 2024 (inclusive) and has been created with the following script:\nimport subset2evaluate # version 1.0.14\nimport json\nimport statistics\n\ndata =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zouharvi/wmt-human-all.","url":"https://huggingface.co/datasets/zouharvi/wmt-human-all","creator_name":"Vil√©m Zouhar","creator_url":"https://huggingface.co/zouharvi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Bengali","Czech","German","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"MevakerSumm","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMevakerSumm\n\t\n\nAbstractive summarization dataset for long documents.\n\n\t\n\t\t\n\t\tCiting\n\t\n\nIf you use MevakerSumm in your research, please cite Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language.\n@article{shalumov2024mevaker,\n      title={Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language}, \n      author={Vitaly Shalumov and Harel Haskey and Yuval Solaz},\n      year={2024},\n      eprint={2403.09719},\n      archivePrefix={arXiv}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HeTree/MevakerSumm.","url":"https://huggingface.co/datasets/HeTree/MevakerSumm","creator_name":"HeTree","creator_url":"https://huggingface.co/HeTree","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","Hebrew","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"xlel_wd","keyword":"hebrew","description":"XLEL-WD is a multilingual event linking dataset. This dataset contains mention references from multilingual Wikipedia/Wikinews articles to event items in Wikidata. The text descriptions for Wikidata events are compiled from Wikipedia articles.","url":"https://huggingface.co/datasets/adithya7/xlel_wd","creator_name":"Adithya Pratapa","creator_url":"https://huggingface.co/adithya7","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["found","found","multilingual","original","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"panlex","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tPanLex\n\t\n\nJanuary 1, 2024 version of PanLex Language Vocabulary with 24,650,274 rows covering 6,152 languages.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\nvocab: contains the text entry.  \n639-3: contains the ISO 639-3 languages tags to allow users to filter on the language(s) of their choice.\n639-3_english_name: the English language name associated to the code ISO 639-3. \nvar_code: contains a code to differentiate language variants. In practice, this is the code 639-3 + a number.  If 000, it corresponds to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/panlex.","url":"https://huggingface.co/datasets/lbourdois/panlex","creator_name":"Lo√Øck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["Ghotuo","Alumu-Tesu","Ari","Amal","Arb√´resh√´ Albanian"],"keywords_longer_than_N":true},
	{"name":"tatoeba-mt-all-in-one","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for The Tatoeba Translation Challenge | All In One\n\t\n\n~7.3M entries.\nJust more user-friendly version that combines all of the entries of original dataset in a single file:\nhttps://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt\n","url":"https://huggingface.co/datasets/0x22almostEvil/tatoeba-mt-all-in-one","creator_name":"David Glushkov","creator_url":"https://huggingface.co/0x22almostEvil","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["Helsinki-NLP","crowdsourced","translation","Helsinki-NLP/tatoeba_mt","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"tokenizer-wiki-bench","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tMultilingual Tokenizer Benchmark\n\t\n\nThis dataset includes pre-processed wikipedia data for tokenizer evaluation in 45 languages. We provide more information on the evaluation task in general this blogpost.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThe dataset allows us to easily calculate tokenizer fertility and the proportion of continued words on any of the supported languages. In the example below we take the Mistral tokenizer and evaluate its performance on Slovak. \nfrom transformers import AutoTokenizer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/occiglot/tokenizer-wiki-bench.","url":"https://huggingface.co/datasets/occiglot/tokenizer-wiki-bench","creator_name":"Occiglot","creator_url":"https://huggingface.co/occiglot","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Afrikaans","Arabic","Bulgarian","Catalan","Czech"],"keywords_longer_than_N":true},
	{"name":"ApolloMoEBench","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDemocratizing Medical LLMs For Much More Languages\n\t\n\nCovering 12 Major Languages including English, Chinese, French, Hindi, Spanish, Arabic, Russian, Japanese, Korean, German, Italian, Portuguese and 38 Minor Languages So far.\n\n   üìÉ Paper ‚Ä¢ üåê Demo ‚Ä¢ ü§ó ApolloMoEDataset ‚Ä¢ ü§ó ApolloMoEBench  ‚Ä¢ ü§ó Models  ‚Ä¢üåê Apollo  ‚Ä¢ üåê ApolloMoE\n\n\n\n\n\n\n\t\n\t\t\n\t\tüåà Update\n\t\n\n\n[2024.10.15] ApolloMoE repo is publishedÔºÅüéâ\n\n\n\t\n\t\t\n\t\tLanguages Coverage\n\t\n\n12 Major Languages and 38 Minor Languages\n\n  Click to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEBench.","url":"https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEBench","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Arabic","English","Chinese","Korean"],"keywords_longer_than_N":true},
	{"name":"rips-koren-torah-xlatin-corpus","keyword":"hebrew","description":"Source\n: TorahBibleCodes / TorahBibleCodes \nTransliterated into Latin script.\n","url":"https://huggingface.co/datasets/mad0perator/rips-koren-torah-xlatin-corpus","creator_name":"mad0perator.crypto","creator_url":"https://huggingface.co/mad0perator","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","text-classification","translation","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"Roleplay-Hebrew","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tRolePlay-Hebrew\n\t\n\nRoleplay-Hebrew Dataset is a dataset for roleplaying in the Hebrew language for the Large Language Model.\nThe base dataset is the GPTeacher role play dataset by teknium 1, which can be found under this link, released under MIT License. The dataset is then translated into respective languages. The translation process is powered by Google Translate, using cloud translation API. \nFor more information and other language datasets for roleplay, see this github repo.\nFor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jojo-ai-mst/Roleplay-Hebrew.","url":"https://huggingface.co/datasets/jojo-ai-mst/Roleplay-Hebrew","creator_name":"Min Si Thu","creator_url":"https://huggingface.co/jojo-ai-mst","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Hebrew","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"MultiQ","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for MultiQ\n\t\n\nThis is the dataset corresponding to the paper \"Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ\". \nIt is a silver standard benchmark that can be used to evaluate the basic multilingual capabilities of LLMs. It contains 200 open ended questions automatically \ntranslated into 137 typologically diverse languages. \n\nCurated by: Carolin Holtermann, Paul R√∂ttger, Timm Dill, Anne Lauscher\nLanguage(s) (NLP): 137 diverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/caro-holt/MultiQ.","url":"https://huggingface.co/datasets/caro-holt/MultiQ","creator_name":"Holtermann","creator_url":"https://huggingface.co/caro-holt","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Tagalog","Samoan","Macedonian","Gujarati"],"keywords_longer_than_N":true},
	{"name":"language-dataset","keyword":"hebrew","description":"\n","url":"https://huggingface.co/datasets/neon-mao/language-dataset","creator_name":"maoqifan","creator_url":"https://huggingface.co/neon-mao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","Chinese","French","Russian"],"keywords_longer_than_N":true},
	{"name":"multilingual_translation_sft","keyword":"hebrew","description":"Youseff1987/multilingual_translation_sft dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Youseff1987/multilingual_translation_sft","creator_name":"JOON HYOUNG JUN","creator_url":"https://huggingface.co/Youseff1987","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Korean","English","Chinese","Zulu"],"keywords_longer_than_N":true},
	{"name":"strongsgreek","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Strongsgreek\n\t\n\nThis is  Strongs Exhaustive Concordance Greek\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n<The Strong's Exhaustive Concordance is the most complete, easy-to-use, and understandable concordance for studying the original languages of the Bible. -->\n\nCurated by: Lawrence McGaffie\nFunded by [optional]: PAiR\nShared by [optional]: \nLanguage(s) (NLP): English, Hebrew, Greek\nLicense: Apache\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pair01/strongsgreek.","url":"https://huggingface.co/datasets/pair01/strongsgreek","creator_name":"Lawrence McGaffie","creator_url":"https://huggingface.co/pair01","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Hebrew","Greek","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"wikipedia_quality_wikirank","keyword":"hebrew","description":"Datasets with WikiRank quality score as of 1 August 2024 for 47 million Wikipedia articles in 55 language versions (also simplified version for each language in separate files).\nThe WikiRank quality score is a metric designed to assess the overall quality of a Wikipedia article. Although its specific algorithm can vary depending on the implementation, the score typically combines several key features of the Wikipedia article.\n\n\t\n\t\t\n\t\n\t\n\t\tWhy It‚Äôs Important\n\t\n\n\nEnhances Trust: For readers and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank.","url":"https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank","creator_name":"W≈Çodzimierz Lewoniewski","creator_url":"https://huggingface.co/lewoniewski","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","Azerbaijani","Belarusian","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"Aphrodite-yandere-pt","keyword":"hebrew","description":"ZH\nYandere, as a unique character archetype, captivates enthusiasts with its distinctive emotional depth and extreme behavioral patterns. However, current natural language generation models often fail to accurately capture the complex psychology and expressive nuances of such characters.\nWe aim to collaborate with creators, researchers, and enthusiasts to build a dedicated dataset for yandere characters, imbuing language models with the authentic essence of this archetype. This project will‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/win10/Aphrodite-yandere-pt.","url":"https://huggingface.co/datasets/win10/Aphrodite-yandere-pt","creator_name":"ZINWIN(Zuojun-Ye)","creator_url":"https://huggingface.co/win10","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Hebrew","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"fineweb-2","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tü•Ç FineWeb2\n\t\n\n\n    \n\n\n\nA sparkling update with 1000s of languages\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nThis is the second iteration of the popular üç∑ FineWeb dataset, bringing high quality pretraining data to over 1000 üó£Ô∏è languages.\nThe ü•Ç FineWeb2 dataset is fully reproducible, available under the permissive ODC-By 1.0 license and extensively validated through hundreds of ablation experiments.\nIn particular, on the set of 9 diverse languages we used to guide our processing decisions, ü•Ç FineWeb2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/fineweb-2.","url":"https://huggingface.co/datasets/HuggingFaceFW/fineweb-2","creator_name":"FineData","creator_url":"https://huggingface.co/HuggingFaceFW","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Arifama-Miniafia","Ankave","Abau","Amarasi"],"keywords_longer_than_N":true},
	{"name":"panlex-definitions","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for panlex-definitions\n\t\n\nThis is a dataset of word definitions in several hudnred languages, extracted from https://panlex.org.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset has been extracted from https://panlex.org (the 20250201 database dump) and rearranged on the per-language basis (by the language of the definition).\nEach language subset consists of definitions (short phrases).\nEach definition is associated with some meanings (if there is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cointegrated/panlex-definitions.","url":"https://huggingface.co/datasets/cointegrated/panlex-definitions","creator_name":"David Dale","creator_url":"https://huggingface.co/cointegrated","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["translation","Abkhazian","Hijazi Arabic","Afrikaans","Ainu (Japan)"],"keywords_longer_than_N":true},
	{"name":"common_voice_21_0","keyword":"hebrew","description":"Due to storage limits some files had to be split into multiple parts. They can be merged like this: cat file.* > file.\n","url":"https://huggingface.co/datasets/2Jyq/common_voice_21_0","creator_name":"2Jyq","creator_url":"https://huggingface.co/2Jyq","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["crowdsourced","crowdsourced","multilingual","extended|common_voice","Abkhaz"],"keywords_longer_than_N":true},
	{"name":"GlobalDISCO","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tGlobalDISCO\n\t\n\nGlobalDISCO is a large-scale dataset consisting of 73k music tracks generated by state-of-the-art commercial generative music models, along with paired links to 93k reference tracks in LAION-DISCO-12M. The dataset spans 147 languages and includes musical style prompts extracted from MusicBrainz and Wikipedia. The dataset is globally balanced, representing musical styles from artists across 79 countries and five continents. It is aimed to support the research community in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/disco-eth/GlobalDISCO.","url":"https://huggingface.co/datasets/disco-eth/GlobalDISCO","creator_name":"DISCO","creator_url":"https://huggingface.co/disco-eth","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["audio-classification","English","Spanish","French","German"],"keywords_longer_than_N":true},
	{"name":"wildchat-filtered","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tWildChat Filtered Dataset\n\t\n\nThis is a filtered version of the WildChat-4.8M dataset.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 3,199,860 conversations between human users and ChatGPT, filtered to keep only the essential conversation structure.\n\n\t\n\t\t\n\t\tData Structure\n\t\n\nEach conversation contains only:\n\nconversations: A list of message objects with:\nrole: Either \"user\" or \"assistant\"\ncontent: The text content of the message\n\n\n\nAll other metadata (timestamps, moderation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rayonlabs/wildchat-filtered.","url":"https://huggingface.co/datasets/rayonlabs/wildchat-filtered","creator_name":"Rayon Labs","creator_url":"https://huggingface.co/rayonlabs","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Spanish","French"],"keywords_longer_than_N":true},
	{"name":"torah-verses-transliterated","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for Torah Verses Transliterated\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nAll files are 1 verse per line, space separated tokens\n\nxlatin.txt\n: Torah transliterated to Latin script\nsplit\ntrain.txt\nvalid.txt\ntest.txt\n\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nVerses of the Torah transliterated to Latin script.\nSplit is random, 80%, 10%, 10% for training, validation, testing,\nrespectively. Each split's contents were resorted to maintain\noriginal general flow direction of text.\n\nCurated by:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mad0perator/torah-verses-transliterated.","url":"https://huggingface.co/datasets/mad0perator/torah-verses-transliterated","creator_name":"mad0perator.crypto","creator_url":"https://huggingface.co/mad0perator","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Hebrew","mit","1K - 10K","text","Text"],"keywords_longer_than_N":true},
	{"name":"ai-culture-multilingual-json-dolma","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tAI-Culture Multilingual JSON + DOLMA Corpus\n\t\n\n\n16M words ¬∑ 12 languages ¬∑ CC-BY-4.0\n\nThe AI-Culture corpus contains 5K articles providing comprehensive philosophical and cultural content, exploring the intersection of technology, artificial intelligence, and human culture, perfectly aligned across 12 languages. All content maintains identical parallel structure across translations with zero duplication and editor-curated quality.\nThis project is maintained by a non-profit digital‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-Culture-Commons/ai-culture-multilingual-json-dolma.","url":"https://huggingface.co/datasets/AI-Culture-Commons/ai-culture-multilingual-json-dolma","creator_name":"AI‚ÄëCulture‚ÄëCommons","creator_url":"https://huggingface.co/AI-Culture-Commons","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text-classification","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"tatoeba-bitext-mining","keyword":"hebrew","description":"\n  Tatoeba\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\n1,000 English-aligned sentence pairs for each language based on the Tatoeba corpus\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://github.com/facebookresearch/LASER/tree/main/data/tatoeba/v1\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"Tatoeba\"])\nevaluator = mteb.MTEB(task)\n\nmodel =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/tatoeba-bitext-mining.","url":"https://huggingface.co/datasets/mteb/tatoeba-bitext-mining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","human-annotated","multilingual","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"Intermediate-Thinking-130k","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tIntermediate-Thinking-130k\n\t\n\nA comprehensive dataset of 135,000 high-quality samples designed to advance language model reasoning capabilities through structured intermediate thinking processes. This dataset enables training and evaluation of models with sophisticated self-correction and iterative reasoning abilities across 42 languages.\nOG Link\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIntermediate-Thinking-130k addresses a fundamental limitation in current language models: their inability to pause‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/Intermediate-Thinking-130k.","url":"https://huggingface.co/datasets/mlx-community/Intermediate-Thinking-130k","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Afrikaans","Arabic","Bengali","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"high-quality-multilingual-sentences","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tHigh Quality Multilingual Sentences\n\t\n\n\nThis dataset contains multilingual sentences derived from the agentlans/LinguaNova dataset.\nIt includes 1.58 million rows across 51 different languages, each in its own configuration.\n\nExample row (from the all config):\n{\n    \"text\": \"ÿßŸÖÿßŸÖ ÿ¨ŸÖÿπŸá ÿßÿµŸÅŸáÿßŸÜ ⁄ØŸÅÿ™: ŸÖ€åÿ≤ÿßŸÜ ŸÜ€åÿßÿ≤ ÿ¢ÿ® ÿ¥ÿ±ÿ® ÿßÿµŸÅŸáÿßŸÜ €±€±.€µ ŸÖÿ™ÿ± ŸÖ⁄©ÿπÿ® ÿßÿ≥ÿ™ ⁄©Ÿá ÿ™ŸÖÿßŸÖ ÿßÿ≥ÿ™ÿßŸÜ ÿßÿµŸÅŸáÿßŸÜ ÿ±ÿß ŸæŸàÿ¥ÿ¥ ŸÖ€åÿØŸáÿØ Ÿà ŸÜÿ≥ÿ®ÿ™ ÿ®Ÿá ŸÇÿ®ŸÑ ÿßÿ≤ ÿßŸÜŸÇŸÑÿßÿ® €å⁄©€å ÿßÿ≤ Ÿæ€åÿ¥ÿ±ŸÅÿ™Ÿáÿß ÿØÿ± ÿ≠Ÿàÿ≤Ÿá ÿ¢ÿ® ÿ®ŸàÿØŸá ÿßÿ≥ÿ™.\",\n    \"fasttext\": \"fa\",\n    \"gcld3\": \"fa\"\n}\n\nFields:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/high-quality-multilingual-sentences.","url":"https://huggingface.co/datasets/agentlans/high-quality-multilingual-sentences","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","text-retrieval","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"hebrew-space-restoration-corpus","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tRestoring Missing Spaces in Scraped Hebrew Social Media\n\t\n\nThis dataset holds the test corpus used in the 2025 W-Nut paper: Avi Shmidman and Shaltiel Shmidman, \"Restoring Missing Spaces in Scraped Hebrew Social Media\", The 10th Workshop on Noisy and User-generated Text (W-NUT), 2025.\nThe corpus consists of ~6,000 Hebrew sentences, sampled from the Hebrew portion of FineWeb-2. \nEach row of the dataset contains two fields:\n\ninput: The Hebrew sentence with 1-4 spaces randomly removed (see‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dicta-il/hebrew-space-restoration-corpus.","url":"https://huggingface.co/datasets/dicta-il/hebrew-space-restoration-corpus","creator_name":"DICTA: The Israel Center for Text Analysis","creator_url":"https://huggingface.co/dicta-il","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["Hebrew","odc-by","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"test_4","keyword":"hebrew","description":"\n\t\n\t\t\n\t\n\t\n\t\tFLEURS\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark. \nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\nused and ‚Äùunit error rate‚Äù (characters, signs) of all languages is averaged. Languages and results are also grouped into seven‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tunngo/test_4.","url":"https://huggingface.co/datasets/tunngo/test_4","creator_name":"nguyen tuan","creator_url":"https://huggingface.co/tunngo","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"m-WildVision","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tDataset Card for m-WildVision\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThe m-WildVision dataset is a multilingual multimodal LLM evaluation set covering 23 languages.  It was created by translating prompts from the original English-only WildVision (vision_bench_0617) test set. \nThe original prompts, developed by Lu et al. (2024) , consist of 500 challenging user queries sourced from the WildVision-Arena platform. \nThe authors demonstrated that these prompts enable automatic LLM judge‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/m-WildVision.","url":"https://huggingface.co/datasets/CohereLabs/m-WildVision","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","French","German","Spanish","Italian"],"keywords_longer_than_N":true},
	{"name":"MKQARetrieval","keyword":"hebrew","description":"\n  MKQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual Knowledge Questions & Answers (MKQA)contains 10,000 queries sampled from the Google Natural Questions dataset.\n        For each query we collect new passage-independent answers. These queries and answers are then human translated into 25 Non-English languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://github.com/apple/ml-mkqa\n\n\n\t\n\nSource datasets:\n\napple/mkqa\n\n\n\t\n\t\t\n\t\tHow to evaluate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MKQARetrieval.","url":"https://huggingface.co/datasets/mteb/MKQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","human-annotated","multilingual","apple/mkqa"],"keywords_longer_than_N":true},
	{"name":"GolemGuard","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tGolemGuard: Hebrew Privacy Information Detection Corpus\n\t\n\nGolemGuard is a comprehensive Hebrew language dataset specifically designed for training and evaluating models for Personal Identifiable Information (PII) detection and masking. The dataset contains ~600MB of synthetic text data representing various document types and communication formats commonly found in Israeli professional and administrative contexts.\n\n\t\n\t\t\n\t\tSource Data\n\t\n\n\n\t\n\t\t\n\t\tInitial Data Collection and Normalization‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CordwainerSmith/GolemGuard.","url":"https://huggingface.co/datasets/CordwainerSmith/GolemGuard","creator_name":"Liran Baba","creator_url":"https://huggingface.co/CordwainerSmith","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","Hebrew","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GolemGuard","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tGolemGuard: Hebrew Privacy Information Detection Corpus\n\t\n\nGolemGuard is a comprehensive Hebrew language dataset specifically designed for training and evaluating models for Personal Identifiable Information (PII) detection and masking. The dataset contains ~600MB of synthetic text data representing various document types and communication formats commonly found in Israeli professional and administrative contexts.\n\n\t\n\t\t\n\t\tSource Data\n\t\n\n\n\t\n\t\t\n\t\tInitial Data Collection and Normalization‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CordwainerSmith/GolemGuard.","url":"https://huggingface.co/datasets/CordwainerSmith/GolemGuard","creator_name":"Liran Baba","creator_url":"https://huggingface.co/CordwainerSmith","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","Hebrew","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"PangeaBench-xm100","keyword":"hebrew","description":"\n\t\n\t\t\n\t\n\t\n\t\tXM100\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a copy from https://google.github.io/crossmodal-3600/\n\t\n\nIf you use this dataset, please cite the original authors:\n@inproceedings{ThapliyalCrossmodal2022,\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\n  booktitle = {EMNLP},\n  year = {2022}\n}\n\n","url":"https://huggingface.co/datasets/neulab/PangeaBench-xm100","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","Bengali","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"parallel_corpus_game","keyword":"hebrew","description":"https://github.com/mnbvc-parallel-corpus-team/parallel_corpus_mnbvc\nGame Corpus Collected by MNBVC Parallel Corpus Team.\n\n\t\n\t\t\n\t\t09/17/2025 Updated\n\t\n\n\nHollow Knight\n\n\n\t\n\t\t\n\t\t09/15/2025 Updated\n\t\n\n\nLimbus Company\nMirror\n\n\n\t\n\t\t\n\t\t09/08/2025 Updated\n\t\n\n\nSpice and Wolf VR (1&2)\nDeep Rock Galactic\nCities Skylines 1\n\n\n\t\n\t\t\n\t\t09/02/2025 Updated\n\t\n\n\nPlague Inc\n\n\n\t\n\t\t\n\t\t09/01/2025 Updated\n\t\n\n\nBanGDream from https://bestdori.com/\n\n\n\t\n\t\t\n\t\t08/15/2025 Updated\n\t\n\n\nATRI from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bot-yaya/parallel_corpus_game.","url":"https://huggingface.co/datasets/bot-yaya/parallel_corpus_game","creator_name":"yaya","creator_url":"https://huggingface.co/bot-yaya","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","Arabic","Chinese","German","English"],"keywords_longer_than_N":true},
	{"name":"Global-MMLU","keyword":"hebrew","description":"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGlobal-MMLU üåç is a multilingual evaluation set spanning 42 languages, including English. This dataset combines machine translations for MMLU questions along with professional translations and crowd-sourced post-edits.\nIt also includes cultural sensitivity annotations for a subset of the questions (2850 questions per language) and classifies them as Culturally Sensitive (CS) üóΩ or Culturally Agnostic (CA) ‚öñÔ∏è. These annotations were collected as part of an open‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/Global-MMLU.","url":"https://huggingface.co/datasets/CohereLabs/Global-MMLU","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Arabic","Bengali","Spanish","French"],"keywords_longer_than_N":true},
	{"name":"samaritan_v1","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tSamaritan v1 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Samaritanv1 dataset comprises Samaritan Biblical manuscripts line images and text from 14th and early 17th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Hebrew, Samaritan Hebrew and Samaritan Aramaic.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/johnlockejrr/samaritan_v1.","url":"https://huggingface.co/datasets/johnlockejrr/samaritan_v1","creator_name":"John Locke","creator_url":"https://huggingface.co/johnlockejrr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Hebrew","Samaritan Aramaic","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"multiCHILDES","keyword":"hebrew","description":"\n\t\n\t\t\n\t\tmultiCHILDES: Multilingual Child-Directed Speech Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains child-directed speech from 19 languages, extracted from the CHILDES corpus. The text has been cleaned and is designed for text generation tasks, particularly in studying early language acquisition.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: CHILDES corpus\nLanguages: 19 languages\nText Type: Child-directed speech\nTask: Text Generation, Language Modeling\nData Processing: The dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IParraMartin/multiCHILDES.","url":"https://huggingface.co/datasets/IParraMartin/multiCHILDES","creator_name":"I√±igo Parra","creator_url":"https://huggingface.co/IParraMartin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Basque","Spanish","Portuguese"],"keywords_longer_than_N":true}
]
;
