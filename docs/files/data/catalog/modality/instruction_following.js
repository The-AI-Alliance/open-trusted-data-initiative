const data_for_modality_instruction_following = 
[
	{"name":"Code-170k-kituba","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kituba is a groundbreaking dataset containing 148,000 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kituba, making coding education accessible to Kituba speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n148,000 high-quality conversations about programming and coding\nPure Kituba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kituba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kituba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kituba (Democratic Republic of Congo)","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kinyarwanda","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kinyarwanda is a groundbreaking dataset containing 12,345 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kinyarwanda, making coding education accessible to Kinyarwanda speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,345 high-quality conversations about programming and coding\nPure Kinyarwanda language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kinyarwanda.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kinyarwanda","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kinyarwanda","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-swahili","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-swahili is a groundbreaking dataset containing 126,025 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Swahili, making coding education accessible to Swahili speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n126,025 high-quality conversations about programming and coding\nPure Swahili language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-swahili.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-swahili","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Swahili","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-afar","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-afar is a groundbreaking dataset containing 114,895 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Afar, making coding education accessible to Afar speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n114,895 high-quality conversations about programming and coding\nPure Afar language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-afar.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-afar","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Afar","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-bambara","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-bambara is a groundbreaking dataset containing 54,885 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Bambara, making coding education accessible to Bambara speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n54,885 high-quality conversations about programming and coding\nPure Bambara language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-bambara.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-bambara","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Bambara","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"tiny-llm-synthetic-qa","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tTiny-LLM: Synthetic Question-Answering Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset was created for the fine-tuning stage of the Tiny-LLM Project, a project focused on training and evaluating compact language models from scratch.\nIt contains 706,727 high-quality, synthetic multi-turn Question-Answering (Q&A) conversations in English, generated using the Gemini API. The dataset was designed to teach small models instruction-following capabilities across a diverse range ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Gabriel8/tiny-llm-synthetic-qa.","url":"https://huggingface.co/datasets/Gabriel8/tiny-llm-synthetic-qa","creator_name":"Gabriel de Antonio Mazetto","creator_url":"https://huggingface.co/Gabriel8","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"reward-aira-dataset","keyword":"instruction","description":"\n\t\n\t\t\n\t\tReward-Aira Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of prompt + completion examples of LLM following instructions in a conversational manner. All prompts come with two possible completions (one better than the other). The dataset is available in both Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized to train a reward/preference model or DPO fine-tuning.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nEnglish and Portuguese.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset.","url":"https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","Portuguese","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MathCanvas-Instruct","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMathCanvas-Instruct Dataset\n\t\n\n\n  \n    \n  \n  Â Â Â Â Â Â Â Â \n  \n    \n  \n  Â Â Â Â Â Â Â Â \n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“– Overview\n\t\n\nMathCanvas-Instruct is a high-quality, fine-tuning dataset with 219K examples of interleaved visual-textual reasoning paths. It is the core component for the second phase of the [MathCanvas] framework: Strategic Visual-Aided Reasoning.\nAfter a model learns foundational diagram generation and editing from MathCanvas-Imagen and MathCanvas-Edit, this dataset teaches it theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shiwk24/MathCanvas-Instruct.","url":"https://huggingface.co/datasets/shiwk24/MathCanvas-Instruct","creator_name":"Weikang Shi","creator_url":"https://huggingface.co/shiwk24","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"tiny-llm-synthetic-qa","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tTiny-LLM: Synthetic Question-Answering Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset was created for the fine-tuning stage of the Tiny-LLM Project, a project focused on training and evaluating compact language models from scratch.\nIt contains 706,727 high-quality, synthetic multi-turn Question-Answering (Q&A) conversations in English, generated using the Gemini API. The dataset was designed to teach small models instruction-following capabilities across a diverse range ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Gabriel8/tiny-llm-synthetic-qa.","url":"https://huggingface.co/datasets/Gabriel8/tiny-llm-synthetic-qa","creator_name":"Gabriel de Antonio Mazetto","creator_url":"https://huggingface.co/Gabriel8","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"unix-commands","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tUnix Commands Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Unix Commands Dataset is a unique collection of real-world Unix command line examples, captured from various system prompts representing different user roles and responsibilities, such as system administrators, DevOps, network administrators, Docker administrators, regular users, and hackers.\nThe dataset consists of Unix commands ranging from basic to advanced levels and from a wide array of categories, including file operations (lsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/harpomaxx/unix-commands.","url":"https://huggingface.co/datasets/harpomaxx/unix-commands","creator_name":"Carlos A. Catania (Harpo)","creator_url":"https://huggingface.co/harpomaxx","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Code-170k-malagasy","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-malagasy is a groundbreaking dataset containing 12,232 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Malagasy, making coding education accessible to Malagasy speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,232 high-quality conversations about programming and coding\nPure Malagasy language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-malagasy.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-malagasy","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Malagasy","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kikongo","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kikongo is a groundbreaking dataset containing 111,609 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kikongo, making coding education accessible to Kikongo speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n111,609 high-quality conversations about programming and coding\nPure Kikongo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kikongo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kikongo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kongo","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kiga","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kiga is a groundbreaking dataset containing 124,707 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kiga, making coding education accessible to Kiga speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n124,707 high-quality conversations about programming and coding\nPure Kiga language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kiga.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kiga","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Chiga","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tsonga","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tsonga is a groundbreaking dataset containing 123,270 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tsonga, making coding education accessible to Tsonga speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n123,270 high-quality conversations about programming and coding\nPure Tsonga language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tsonga.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tsonga","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tsonga","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-igbo","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-igbo is a groundbreaking dataset containing 12,467 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Igbo, making coding education accessible to Igbo speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,467 high-quality conversations about programming and coding\nPure Igbo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-igbo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-igbo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Igbo","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-alur","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-alur is a groundbreaking dataset containing 56,109 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Alur, making coding education accessible to Alur speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n56,109 high-quality conversations about programming and coding\nPure Alur language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-alur.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-alur","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Alur","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kanuri","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kanuri is a groundbreaking dataset containing 128,111 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kanuri, making coding education accessible to Kanuri speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n128,111 high-quality conversations about programming and coding\nPure Kanuri language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kanuri.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kanuri","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kanuri","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-sepedi","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-sepedi is a groundbreaking dataset containing 108,619 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Sepedi, making coding education accessible to Sepedi speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n108,619 high-quality conversations about programming and coding\nPure Sepedi language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-sepedi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-sepedi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Pedi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tshiluba","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tshiluba is a groundbreaking dataset containing 113,468 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tshiluba, making coding education accessible to Tshiluba speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n113,468 high-quality conversations about programming and coding\nPure Tshiluba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tshiluba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tshiluba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Luba-Lulua","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-swati","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-swati is a groundbreaking dataset containing 122,345 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Swati, making coding education accessible to Swati speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n122,345 high-quality conversations about programming and coding\nPure Swati language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-swati.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-swati","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Swati","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tumbuka","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tumbuka is a groundbreaking dataset containing 129,591 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tumbuka, making coding education accessible to Tumbuka speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n129,591 high-quality conversations about programming and coding\nPure Tumbuka language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tumbuka.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tumbuka","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tumbuka","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-fon","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-fon is a groundbreaking dataset containing 125,588 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Fon, making coding education accessible to Fon speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n125,588 high-quality conversations about programming and coding\nPure Fon language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-fon.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-fon","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Fon","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-seychellois-creole","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-seychellois-creole is a groundbreaking dataset containing 100,690 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Seychellois Creole, making coding education accessible to Seychellois Creole speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n100,690 high-quality conversations about programming and coding\nPure Seychellois Creole language - democratizing coding education\nMulti-turn dialogues covering variousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-seychellois-creole.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-seychellois-creole","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Seselwa Creole French","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-rundi","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-rundi is a groundbreaking dataset containing 56,496 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Rundi, making coding education accessible to Rundi speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n56,496 high-quality conversations about programming and coding\nPure Rundi language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-rundi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-rundi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kirundi","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-oromo","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-oromo is a groundbreaking dataset containing 150,739 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Oromo, making coding education accessible to Oromo speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n150,739 high-quality conversations about programming and coding\nPure Oromo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-oromo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-oromo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Oromo","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-ndebele-south","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-ndebele-south is a groundbreaking dataset containing 176,994 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Ndebele (South), making coding education accessible to Ndebele (South) speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n176,994 high-quality conversations about programming and coding\nPure Ndebele (South) language - democratizing coding education\nMulti-turn dialogues covering various programmingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-ndebele-south.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-ndebele-south","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Southern Ndebele","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-mauritian-creole","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-mauritian-creole is a groundbreaking dataset containing 145,454 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Mauritian Creole, making coding education accessible to Mauritian Creole speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n145,454 high-quality conversations about programming and coding\nPure Mauritian Creole language - democratizing coding education\nMulti-turn dialogues covering various programmingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-mauritian-creole.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-mauritian-creole","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Morisyen","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"alpaca-spanish","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tBERTIN Alpaca Spanish\n\t\n\nThis dataset is a translation to Spanish of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\n","url":"https://huggingface.co/datasets/bertin-project/alpaca-spanish","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"urdu_alpaca_yc_filtered","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tAlpaca Urdu\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Alpaca Urdu  is a translation of the original dataset into Urdu. This dataset is a part of the Alpaca project and is designed for NLP tasks.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSize: The translated dataset contains [45,622] samples.\nLanguages: Urdu\nLicense: [cc-by-4.0]\nOriginal Dataset: Link to the original Alpaca Cleaned dataset repository\n\n\n\t\n\t\t\n\t\tColumns\n\t\n\nThe translated dataset includes the following columns:\n\ninput: The input text in Urdu.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered.","url":"https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered","creator_name":"Mahwiz Khalil","creator_url":"https://huggingface.co/mahwizzzz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Urdu","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"UltraChatTR_50k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ’¬ UltraChat 50K â€“ TÃ¼rkÃ§e Diyalog Veri Seti\n\t\n\nUltraChat 50K, orijinal UltraChat veri setinden tÃ¼retilmiÅŸ,55.046 TÃ¼rkÃ§e diyalog Ã¶rneÄŸi iÃ§eren aÃ§Ä±k kaynak bir veri setidir.Veri, bÃ¼yÃ¼k dil modellerinin TÃ¼rkÃ§e konuÅŸma anlayÄ±ÅŸÄ± ve cevap kalitesini geliÅŸtirmek iÃ§infine-tuning (SFT) amacÄ±yla dÃ¼zenlenmiÅŸtir.\n\n\n\t\n\t\t\n\t\tðŸ“˜ Veri KÃ¼nyesi\n\t\n\n\n\t\n\t\t\nÃ–zellik\nAÃ§Ä±klama\n\n\n\t\t\nToplam SatÄ±r SayÄ±sÄ±\n55.046\n\n\nVeri FormatÄ±\nJSON Lines, Parquet\n\n\nAlanlar\ninstruction, input, output\n\n\nDil\nTÃ¼rkÃ§e ðŸ‡¹ðŸ‡·\n\n\nLisans\nMITâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hamuz/UltraChatTR_50k.","url":"https://huggingface.co/datasets/hamuz/UltraChatTR_50k","creator_name":"Hamza YiÄŸit KÃ¼ltÃ¼r","creator_url":"https://huggingface.co/hamuz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Turkish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ru_turbo_alpaca","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tRuTurboAlpaca\n\t\n\nDataset of ChatGPT-generated instructions in Russian.\n\n\n\nCode: rulm/self_instruct\nCode is based on Stanford Alpaca and self-instruct.\n29822 examples\n\nPreliminary evaluation by an expert based on 400 samples:\n\n83% of samples contain correct instructions\n63% of samples have correct instructions and outputs\n\nCrowdsouring-based evaluation on 3500 samples:\n\n90% of samples contain correct instructions\n68% of samples have correct instructions and outputs\n\nPrompt template:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/IlyaGusev/ru_turbo_alpaca.","url":"https://huggingface.co/datasets/IlyaGusev/ru_turbo_alpaca","creator_name":"Ilya Gusev","creator_url":"https://huggingface.co/IlyaGusev","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","Russian","cc-by-4.0","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"MIXTURE","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMIXTURE\n\t\n\nMIXTURE is a dataset designed for instruction distillation, which aims to transform sparse, incomplete, and low-quality inputs into a single information-dense output.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset provides two main components for different stages of model training:\n\ndata_sft/ â€” Used for cold start supervised fine-tuning (SFT).  \n\ndata_grpo/ â€” Used for GRPO (Group Relative Policy Optimization) training.\n\n\nFor more details about data construction, structure, and usageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dokiik/MIXTURE.","url":"https://huggingface.co/datasets/dokiik/MIXTURE","creator_name":"ikod","creator_url":"https://huggingface.co/dokiik","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-sv","keyword":"instruct","description":"\n\t\n\t\t\n\t\tOpenHermes-2.5-sv\n\t\n\nThis is a machine translated instruct dataset from OpenHermes-2.5. \nThe facebook/seamless-m4t-v2-large was used, and some post filtering is done to remove repetitive texts that occurred due to translation errors.\n\n\t\n\t\t\n\t\tExample data:\n\t\n\n[\n   {\n      \"from\":\"human\",\n      \"value\":\"Vilket naturfenomen, som orsakas av att ljus reflekteras och bryts genom vattendroppar, resulterar i en fÃ¤rgglad bÃ¥ge pÃ¥ himlen?\",\n      \"weight\":null\n   },\n   {\n      \"from\":\"gpt\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv.","url":"https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv","creator_name":"Tim Isbister","creator_url":"https://huggingface.co/timpal0l","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Swedish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Shakespeare_Poetry","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tShakespeare Poetry Instruction Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset provides a high-quality, curated collection of instruction-response pairs designed for fine-tuning large language models (LLMs) to generate poetry in the style of William Shakespeare. The dataset is sourced from public domain Shakespearean works including the Sonnets, Venus and Adonis, The Rape of Lucrece, and other poetic texts.\nEach record is formatted for instruction-following tasks, making itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Exquisique/Shakespeare_Poetry.","url":"https://huggingface.co/datasets/Exquisique/Shakespeare_Poetry","creator_name":"Pankaj Singh","creator_url":"https://huggingface.co/Exquisique","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Performance-Marketing-Data","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tPerformance Marketing Expert Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains comprehensive performance marketing knowledge and logical reasoning patterns for Meta (Facebook/Instagram), Google Ads, and TikTok advertising platforms. It's designed for fine-tuning language models to understand brand verticals, performance marketing strategies, and develop reasoning capacity for creating winning ad campaigns.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example follows anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sri-Vigneshwar-DJ/Performance-Marketing-Data.","url":"https://huggingface.co/datasets/Sri-Vigneshwar-DJ/Performance-Marketing-Data","creator_name":"DJ Sri Vigneshwar","creator_url":"https://huggingface.co/Sri-Vigneshwar-DJ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"dolly-15k-instruction-alpaca-format","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDatabricks Dolly 15k Dataset with citations removed and in Alpaca Format\n\t\n\nNOTE \nThis is a reupload of the Databricks dataset found here, but modified to be in Alpaca format, and with the citation numbers removed.\nThis work is not my own, and all credit goes to Databricks.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\ndatabricks-dolly-15k is a corpus of more than 15,000 records generated by thousands of Databricks employees to enable large language\nmodels to exhibit the magical interactivity ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/dolly-15k-instruction-alpaca-format.","url":"https://huggingface.co/datasets/llm-wizard/dolly-15k-instruction-alpaca-format","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["English","cc-by-3.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"github-ai-projects-dataset","keyword":"instruction","description":"\n\t\n\t\t\n\t\tGitHub Code Instruction Dataset for LLM Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains high-quality code instruction examples extracted from popular GitHub repositories focused on LLMs, LangChain, FastAPI, Django, and Transformers. It is designed for supervised fine-tuning of large language models (LLMs) for code generation, completion, and documentation tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is split into three parts:\n\nTrain: 80% of examples for modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pranav-pvnn/github-ai-projects-dataset.","url":"https://huggingface.co/datasets/pranav-pvnn/github-ai-projects-dataset","creator_name":"Pranav","creator_url":"https://huggingface.co/pranav-pvnn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"alpaca_astronomy_bg","keyword":"instruction-finetuning","description":"vislupus/alpaca_astronomy_bg dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/vislupus/alpaca_astronomy_bg","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Bulgarian","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"OpenManus-RL","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for OpenManusRL\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\n  ðŸ’» [Github Repo]\n\n\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\n\nðŸ” ReAct Framework - Reasoning-Acting integration\nðŸ§  Structured Training - Separate format/reasoning learning\nðŸš« Anti-Hallucination - Negative samples + environment grounding\nðŸŒ 6 Domains - OS, DB, Web, KG, Household, E-commerce\n\n\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\nSourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL.","url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"WildChat","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for WildChat\n\t\n\n\n\t\n\t\t\n\t\tNote: a newer version with 4.8 million conversations and demographic information can be found here.\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nPaper: https://arxiv.org/abs/2405.01470\n\nInteractive Search Tool: https://wildvisualizer.com (paper)\n\nLicense: ODC-BY\n\nLanguage(s) (NLP): multi-lingual\n\nPoint of Contact: Yuntian Deng\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nWildChat is a collection of 650K conversations between human users and ChatGPT. We collected WildChatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/WildChat.","url":"https://huggingface.co/datasets/allenai/WildChat","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","odc-by","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Model-based","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Model-based is a dataset of BenchMAX, sourcing from m-ArenaHard, which evaluates the instruction following capability via model-based judgment.\nWe extend the original dataset to include languages that are not supported by m-ArenaHard throughâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"dataset_qa","keyword":"instruction","description":"\n\t\n\t\t\n\t\tðŸ“˜ Personal QA Instruction Dataset\n\t\n\nThis dataset contains instruction-style question-answer pairs about personal and educational background, projects, technical skills, internships, and AI interests. It is well-suited for testing or fine-tuning small language models like TinyLlama or for building personalized assistants.\n\n\t\n\t\t\n\t\tðŸ“ Dataset Details\n\t\n\n\nFormat: JSON (List of dictionaries)\nLanguage: English\nSize: ~200 entries\nFields:\nquestion: The instruction or question prompt\nanswer:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/sujal7102003/dataset_qa.","url":"https://huggingface.co/datasets/sujal7102003/dataset_qa","creator_name":"sujalthakkar","creator_url":"https://huggingface.co/sujal7102003","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-contrast-sample","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDPO Contrast Sample\n\t\n\nThis dataset provides a direct comparison between high-quality and low-quality instruction-output pairs evaluated by a LLaMA-2-7B-Chat model in a DPO-style workflow.\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\ninstruction: The generated prompt (x).\noutput: The associated output (y).\nscore: Quality rating assigned by LLaMA2 (1 = high, 5 = low).\n\n\n\t\n\t\t\n\t\tSamples\n\t\n\n5 examples with score = 1 (excellent), and 5 with score = 5 (poor).\n","url":"https://huggingface.co/datasets/helloTR/dpo-contrast-sample","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"asena_safetalk_dataset_tr","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tAsena Safe TR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Ã–zeti\n\t\n\nAsena Safe TR, TÃ¼rkÃ§e gÃ¼venli AI asistanlarÄ± geliÅŸtirmek iÃ§in Ã¶zel olarak hazÄ±rlanmÄ±ÅŸ 42.000+ Ã¶rnek iÃ§eren bir SFT (Supervised Fine-Tuning) veri setidir. Dataset, zararlÄ± veya problemli sorulara gÃ¼venli, yapÄ±cÄ± ve bilgilendirici yanÄ±tlar vermeyi Ã¶ÄŸreten prompt-response Ã§iftlerinden oluÅŸmaktadÄ±r.\n\n\t\n\t\t\n\t\tÄ°Ã§erik Bilgisi\n\t\n\n\n\t\n\t\t\n\t\tVeri TÃ¼rÃ¼\n\t\n\n\nFormat: Sohbet diyaloglarÄ± (conversational AI training)\nDosya FormatÄ±: JSONL (JSON Lines)\nDil:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/limeXx/asena_safetalk_dataset_tr.","url":"https://huggingface.co/datasets/limeXx/asena_safetalk_dataset_tr","creator_name":"yusuf uzun","creator_url":"https://huggingface.co/limeXx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Turkish","mit","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"gsm8k_multiturn","keyword":"multiturn","description":"The \"socratic\" version of GSM8K has the model reflect and ask itself sub-questions about the initial question, before coming to a final answer.\nThis dataset reformats the socratic GSM8K version into a multi-turn conversation, where the sub-questions are asked by the user rather than being self-asked by the model.\n","url":"https://huggingface.co/datasets/euclaise/gsm8k_multiturn","creator_name":"Jade","creator_url":"https://huggingface.co/euclaise","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"guanjian_anli_test1","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/papaya523/guanjian_anli_test1.","url":"https://huggingface.co/datasets/papaya523/guanjian_anli_test1","creator_name":"zhaoyue","creator_url":"https://huggingface.co/papaya523","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Electrical-engineering-ru","keyword":"instruction","description":"Translated instructions from STEM-AI-mtl/Electrical-engineering into Russian using gemini-flash-1.5-8b.\n","url":"https://huggingface.co/datasets/d0rj/Electrical-engineering-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","translated","STEM-AI-mtl/Electrical-engineering","Russian"],"keywords_longer_than_N":true},
	{"name":"Titles-WildChat-GPT4-100k","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tTitles-WildChat-GPT4-100k\n\t\n\nThis dataset consists of conversation titles generated from the allenai/WildChat-1M dataset. It is designed for fine-tuning lightweight models to improve conversation title generation.\nHere are some examples:\n\nâœ¨ Chat History Summary\nðŸ“º TVs & Recommendations\nâ¤ï¸ MI Risk Factors\nðŸ“Š Followers Median Calculation\nðŸ¤– Chat History Example\nðŸ‘‘ Gold & Black Symbolism\nâœ¨ Endless Title Generation\nðŸ¤– Recursive Title Generation\nðŸŽ­ Chatbot Roleplay Fun\nðŸ’» Sorting Algorithmâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/k4yt3x/Titles-WildChat-GPT4-100k.","url":"https://huggingface.co/datasets/k4yt3x/Titles-WildChat-GPT4-100k","creator_name":"K4YT3X","creator_url":"https://huggingface.co/k4yt3x","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["summarization","odc-by","100K - 1M","arrow","Text"],"keywords_longer_than_N":true},
	{"name":"medical_cot_rus","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMykes/medical_cot_rus\n\t\n\nÐÐ°Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¾Ð¼ Ð´Ð¾Ð¼ÐµÐ½Ðµ Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Ñ†ÐµÐ¿Ð¾Ñ‡ÐµÐº Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ (Chain-of-Thought, CoT). ÐŸÐ¾Ð´Ñ…Ð¾Ð´Ð¸Ñ‚ Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡ Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¾Ð³Ð¾ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾-Ð¾Ñ‚Ð²ÐµÑ‚Ð°, Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ LLM Ð¸ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ñ Ð¾Ð±ÑŠÑÑÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒÑŽ.\n\nÐžÐ±ÑŠÐµÐ¼: â‰ˆ 6.29k Ð·Ð°Ð¿Ð¸ÑÐµÐ¹\nÐ¯Ð·Ñ‹Ðº: Ñ€ÑƒÑÑÐºÐ¸Ð¹\nÐ”Ð¾Ð¼ÐµÐ½Ñ‹: ÐºÐ»Ð¸Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, ÑÐ¸Ð¼Ð¿Ñ‚Ð¾Ð¼Ñ‹, Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ñ, Ð´Ð¸Ñ„Ñ„ÐµÑ€ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° Ð¸ Ð´Ñ€.\nÐŸÐ¾Ð»Ñ: question, raw_answer, cot, answer, old_thoughts\n\nâš ï¸ ÐžÑ‚ÐºÐ°Ð· Ð¾Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸: Ð´Ð°Ñ‚Ð°ÑÐµÑ‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mykes/medical_cot_rus.","url":"https://huggingface.co/datasets/Mykes/medical_cot_rus","creator_name":"Maxim Titkov","creator_url":"https://huggingface.co/Mykes","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Russian","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"multiturn-capybara-preferences-filtered-binarized","keyword":"multiturn","description":"This dataset has been created with distilabel, plus some extra post-processing steps described below.\nDataset Summary\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\n\nRemove responses from the assistant, not only in the last turn, but also in intermediate turns where the assistant responds with a potential refusal and/or some ChatGPT-isms such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized.","url":"https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Colossal-Instruction-Translation-EN-ES","keyword":"instruction","description":"\n\t\n\t\t\n\t\tColossal Instruction Translation Corpus (English - Spanish )\n\t\n\n\nA deduplicated version of this dataset can be found here, thanks to @NickyNicky: https://huggingface.co/datasets/NickyNicky/Iker-Colossal-Instruction-Translation-EN-ES_deduplicated\n\nThis dataset contains 2284632 instructions and answers translated from English into Spanish. Is a fully synthetic corpus generated using machine translation. We used the model Iker/TowerInstruct-13B-v0.1-EN2ES. A few examples were alsoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Iker/Colossal-Instruction-Translation-EN-ES.","url":"https://huggingface.co/datasets/Iker/Colossal-Instruction-Translation-EN-ES","creator_name":"Iker GarcÃ­a-Ferrero","creator_url":"https://huggingface.co/Iker","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Spanish","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-gpt4-turbo","keyword":"instruction-finetuning","description":"I downloaded the dataset from Alpaca at https://huggingface.co/datasets/yahma/alpaca-cleaned and processed it using a script to convert the text to hashes. I removed any duplicate hashes along with their corresponding input and output columns. Subsequently, I utilized the GPT-4 Turbo API to feed each message, instruction, and input to the model for generating responses.\nHere are the outputs generated by the GPT-4 Turbo model. The information in the input column and instruction column should beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo.","url":"https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo","creator_name":"myles bruce","creator_url":"https://huggingface.co/mylesgoose","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"VoiceAssistant-Eval","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tðŸ”¥ VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing\n\t\n\n \n \n \n \n\n\n\n\n\n\n\nðŸŒŸ  This is the official repository for the paper \"VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing\", which contains the evaluation code for the VoiceAssistant-Eval benchmark.\n[ðŸŒ Homepage] [ðŸ’» Github] [ðŸ“Š Leaderboard ] [ðŸ“Š Detailed Leaderboard ] [ðŸ“Š Roleplay Leaderboard ] [ðŸ“– Paper]\n\n\n\n\n\n\t\n\t\t\n\t\tðŸš€ Data Usage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/VoiceAssistant-Eval.","url":"https://huggingface.co/datasets/MathLLMs/VoiceAssistant-Eval","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","audio-to-audio","any-to-any","multiple-choice"],"keywords_longer_than_N":true},
	{"name":"twinkle-dialogue-gemma3-2025-08","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTwinkle Dialogue (Gemma-3-12B-it, 2025-08)\n\t\n\n\n  \n    \n  \n  \n    \n  \n\n\næœ¬è³‡æ–™é›†ç”± Gemma-3-12B-itï¼ˆTwinkle AI ç¤¾ç¾¤æœå‹™ï¼‰ ç”Ÿæˆä¹‹å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ï¼Œä¸¦æ•´åˆï¼š\n\nReference-freeï¼ˆç”± seed æ´¾ç”Ÿå–®è¼ªå•ç­”ï¼‰\nReference-basedï¼ˆä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ï¼‰\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"messages\": [{\"role\":\"system\",\"content\":\"...\"}, ...]}\nè¨“ç·´æ™‚å¯æ“·å–ç¬¬ä¸€å€‹ user èˆ‡å°æ‡‰ assistant å½¢æˆ (instruction, response) pairï¼Œæˆ–ç›´æŽ¥ä½¿ç”¨ chat æ ¼å¼çš„ trainerã€‚\n\n\n\t\n\t\t\n\t\tä¾†æºèˆ‡é™åˆ¶\n\t\n\n\nModel:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ethan615/twinkle-dialogue-gemma3-2025-08.","url":"https://huggingface.co/datasets/Ethan615/twinkle-dialogue-gemma3-2025-08","creator_name":"Ethan Kuo","creator_url":"https://huggingface.co/Ethan615","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"nemotron-post-training-samples","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tNemotron Post-Training Samples\n\t\n\nThis dataset contains random samples extracted from the nvidia/Llama-Nemotron-Post-Training-Dataset.\n\n\t\n\t\t\n\t\tAttribution\n\t\n\nThis work is derived from the Llama-Nemotron-Post-Training-Dataset-v1.1 by NVIDIA Corporation, licensed under CC BY 4.0. \nOriginal Dataset: nvidia/Llama-Nemotron-Post-Training-DatasetOriginal Authors: NVIDIA CorporationOriginal License: CC BY 4.0  \n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\nSource: nvidia/Llama-Nemotron-Post-Training-Datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples.","url":"https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples","creator_name":"Brandon Tong","creator_url":"https://huggingface.co/brandolorian","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","nvidia/Llama-Nemotron-Post-Training-Dataset","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Spurline","keyword":"instruct","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\nThe 2024-10-30 version contains:\n\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nðŸŒ Homepage | Code | ðŸ¤— Paper | ðŸ“– arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"indonesian-conversation","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tIndonesian Conversation\n\t\n\nIndonesian Conversation is a carefully curated conversational dataset featuring high-quality dialogues primarily in Bahasa Indonesia, with occasional English phrases. The dataset has been specifically designed to support alignment and supervised fine-tuning (SFT) for open-source large language models targeting Indonesian language applications.\nThe collection consists predominantly of multi-turn conversations that showcase natural, friendly, and informativeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/izzulgod/indonesian-conversation.","url":"https://huggingface.co/datasets/izzulgod/indonesian-conversation","creator_name":"Izzul Fahmi","creator_url":"https://huggingface.co/izzulgod","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Indonesian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"math-reasoning-dpo","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMathematical Reasoning DPO Dataset\n\t\n\nThis dataset contains mathematical reasoning problems with chosen and rejected responses, designed for Direct Preference Optimization (DPO) and preference learning of language models.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset follows the ShareGPT format for DPO training with three main fields:\n\nconversations: List of conversation turns leading up to the response\nchosen: Preferred response with detailed reasoning and correct solution\nrejected: Lessâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/est-ai/math-reasoning-dpo.","url":"https://huggingface.co/datasets/est-ai/math-reasoning-dpo","creator_name":"ESTsoft AI","creator_url":"https://huggingface.co/est-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Data-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-Data-ru\n\t\n\nTranslated lmms-lab/LLaVA-OneVision-Data dataset into Russian language using Google translate.\n\nAlmost all datasets have been translated, except for the following:\n[\"tallyqa(cauldron,llava_format)\", \"clevr(cauldron,llava_format)\", \"VisualWebInstruct(filtered)\", \"figureqa(cauldron,llava_format)\", \"magpie_pro(l3_80b_mt)\", \"magpie_pro(qwen2_72b_st)\", \"rendered_text(cauldron)\", \"ureader_ie\"]\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru.","url":"https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-to-text","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Trendyol-Cybersecurity-Instruction-Tuning-Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTrendyol Cybersecurity Defense Instruction-Tuning Dataset (v2.0)\n\t\n\n\n  \n  \n  \n  \n\n\n\n\t\n\t\t\n\t\tðŸš€ TL;DR\n\t\n\n53,202 meticulously curated system/user/assistant instruction-tuning examples covering 200+ specialized cybersecurity domains. Built by the Trendyol Security Team for training state-of-the-art defensive security AI assistants. Expanded from 21K to 53K rows with comprehensive coverage of modern security challenges including cloud-native threats, AI/ML security, quantum computing risksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset.","url":"https://huggingface.co/datasets/Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset","creator_name":"Trendyol","creator_url":"https://huggingface.co/Trendyol","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Bielikowo","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDatasets for Bielikowo tutorials\n\t\n\nBielikowo Github\n","url":"https://huggingface.co/datasets/kubasoltys/Bielikowo","creator_name":"Kuba Soltys","creator_url":"https://huggingface.co/kubasoltys","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","fill-mask","table-question-answering"],"keywords_longer_than_N":true},
	{"name":"en-bg-os-full-50m","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tBulgarian-English OpenSubtitles Full Dataset (50M, ChessInstruct Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset contains 48,749,944 English to Bulgarian subtitle translation pairs in ChessInstruct format for fine-tuning Gemma3-270m using the Unsloth framework. This represents the complete OpenSubtitles parallel corpus for the BG-EN language pair, making it one of the largest translation datasets available.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nðŸ“Š Massive Scale: 48.7 million translation pairs fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zantag/en-bg-os-full-50m.","url":"https://huggingface.co/datasets/zantag/en-bg-os-full-50m","creator_name":"zantag","creator_url":"https://huggingface.co/zantag","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","English","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"alpha","keyword":"instruction-finetuning","description":"dawsondavis/alpha dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/dawsondavis/alpha","creator_name":"Dawson Davis","creator_url":"https://huggingface.co/dawsondavis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"adobetest","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tAdobeTest Dataset\n\t\n\nStructured image metadata (title, keywords, category) for generative tasks.\n","url":"https://huggingface.co/datasets/ahmedmehtab/adobetest","creator_name":"Ahmed Mehtab","creator_url":"https://huggingface.co/ahmedmehtab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"stackoverflow-qa-dataset","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tStackOverflow Q&A Dataset\n\t\n\nThis dataset contains question-answer pairs extracted from StackOverflow via CommonCrawl.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\ninstruction: The question title\ninput: Additional question context (optional)\nresponse: The highest-voted answer\nmetadata: Source URL, answer score, total answers\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=\"stackoverflow_training_dataset.jsonl\")\n\nGenerated usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/varsunk/stackoverflow-qa-dataset.","url":"https://huggingface.co/datasets/varsunk/stackoverflow-qa-dataset","creator_name":"Varun Sunkavalli","creator_url":"https://huggingface.co/varsunk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"constitucion-venezuela-1000","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tConstitucion de Venezuela - Dataset de 1000 Instrucciones\n\t\n\n\n\t\n\t\t\n\t\tDescripcion General\n\t\n\nEste dataset contiene 1000 pares de instruccion-respuesta cuidadosamente curados sobre la Constitucion de la Republica Bolivariana de Venezuela de 1999. Ha sido diseÃ±ado especificamente para el entrenamiento y evaluacion de modelos de lenguaje en tareas de comprension y generacion de texto sobre contenido constitucional venezolano.\nEl dataset abarca los aspectos mas importantes de laâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-1000.","url":"https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-1000","creator_name":"Niurka Oropeza","creator_url":"https://huggingface.co/niuvaroza","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Spanish","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PathSum-CoT","keyword":"instruction-tuning","description":"singhprabhat/PathSum-CoT dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/singhprabhat/PathSum-CoT","creator_name":"Prabhat Singh","creator_url":"https://huggingface.co/singhprabhat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"instruction-collection-fin","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a collection of Finnish instruction data compiled from various sources. Most of the original data is in English and was machine translated into Finnish using Poro-34B. We supplemented this translated data with Finnish paraphrase tasks and English-Finnish translation and language identification tasks. This dataset is suitable for fine-tuning LLMs for instruction-following in Finnish and is usable for commercial purposes.\nWe use this dataset inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LumiOpen/instruction-collection-fin.","url":"https://huggingface.co/datasets/LumiOpen/instruction-collection-fin","creator_name":"LumiOpen","creator_url":"https://huggingface.co/LumiOpen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Finnish","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"phanuphun-instructuib-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tVocational Thai Instruction Dataset\n\t\n\nà¸„à¸³à¸­à¸˜à¸´à¸šà¸²à¸¢à¸ªà¸±à¹‰à¸™à¹† à¸§à¸´à¸˜à¸µà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸Ÿà¸´à¸¥à¸”à¹Œ (instruction, input, output)\n","url":"https://huggingface.co/datasets/parnuphun1598/phanuphun-instructuib-dataset","creator_name":"phanuphun","creator_url":"https://huggingface.co/parnuphun1598","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Thai","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Discord-Dialogues","keyword":"multi-turn","description":"\n  \n\n\n\nDiscord-Dialogues is a large-scale dataset of anonymized Discord conversations from late spring to early fall 2025 for training and evaluating realistic conversational AI models in a ChatML-friendly format.\n\nThis dataset contains 7.3 million exchanges spread out over 16 million turns, with more than 139 million words.\n\n\n  \n    \n  \n\n\n\n  Nomic Atlas Map\n\n\n\n\n\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nMixed single and multi-turn exchanges\nHuman-only dialogues (no bots)\nFiltered for ToS and harmful contentLinksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mookiezi/Discord-Dialogues.","url":"https://huggingface.co/datasets/mookiezi/Discord-Dialogues","creator_name":"Jason","creator_url":"https://huggingface.co/mookiezi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"EchoX-Dialogues-Plus","keyword":"instruction-following","description":"\n\n  EchoX-Dialogues-Plus: Training Data Plus for EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs\n\n\n\n\n  ðŸˆâ€â¬› GithubÂ ï½œÂ ðŸ“ƒ PaperÂ ï½œÂ ðŸš€ SpaceÂ \n\n\n  ðŸ§  EchoX-8BÂ ï½œÂ ðŸ§  EchoX-3BÂ ï½œÂ ðŸ“¦ EchoX-Dialogues (base)Â \n\n\n\n\t\n\t\n\t\n\t\tEchoX-Dialogues-Plus\n\t\n\nEchoX-Dialogues-Plus extends KurtDu/EchoX-Dialogues with large-scale Speech-to-Speech (S2S) and Speech-to-Text (S2T) dialogues.\nAll assistant/output speech is synthetic (single, consistent timbre for S2S). Texts are fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KurtDu/EchoX-Dialogues-Plus.","url":"https://huggingface.co/datasets/KurtDu/EchoX-Dialogues-Plus","creator_name":"Yuhao Du","creator_url":"https://huggingface.co/KurtDu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","question-answering","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"EchoX-Dialogues-Plus","keyword":"multi-turn","description":"\n\n  EchoX-Dialogues-Plus: Training Data Plus for EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs\n\n\n\n\n  ðŸˆâ€â¬› GithubÂ ï½œÂ ðŸ“ƒ PaperÂ ï½œÂ ðŸš€ SpaceÂ \n\n\n  ðŸ§  EchoX-8BÂ ï½œÂ ðŸ§  EchoX-3BÂ ï½œÂ ðŸ“¦ EchoX-Dialogues (base)Â \n\n\n\n\t\n\t\n\t\n\t\tEchoX-Dialogues-Plus\n\t\n\nEchoX-Dialogues-Plus extends KurtDu/EchoX-Dialogues with large-scale Speech-to-Speech (S2S) and Speech-to-Text (S2T) dialogues.\nAll assistant/output speech is synthetic (single, consistent timbre for S2S). Texts are fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KurtDu/EchoX-Dialogues-Plus.","url":"https://huggingface.co/datasets/KurtDu/EchoX-Dialogues-Plus","creator_name":"Yuhao Du","creator_url":"https://huggingface.co/KurtDu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","question-answering","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"GPT-Image-Edit-1.5M","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tGPT-Image-Edit-1.5M A Million-Scale, GPT-Generated Image Dataset\n\t\n\nðŸ“ƒArxiv | ðŸŒ Project Page | ðŸ’»Github\nGPT-Image-Edit-1.5M is a comprehensive image editing dataset that is built upon HQ-Edit, UltraEdit, OmniEdit and Complex-Edit, with all output images regenerated with GPT-Image-1.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“£ News\n\t\n\n\n[2025.08.20] ðŸš€ We provide a script for multi-process downloading. See Multi-process Download.\n[2025.07.27] ðŸ¤— We release GPT-Image-Edit, a state-of-the-art image editing model withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/GPT-Image-Edit-1.5M.","url":"https://huggingface.co/datasets/UCSC-VLAA/GPT-Image-Edit-1.5M","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-image","English","cc-by-4.0","1M - 10M","webdataset"],"keywords_longer_than_N":true},
	{"name":"Multilingual-Benchmark","keyword":"instruction","description":"These are the GSM8K and ARC dataset translated by Google Translate. \nBibTex\n@misc{lu2024languagecountslearnunlearn,\n      title={Every Language Counts: Learn and Unlearn in Multilingual LLMs}, \n      author={Taiming Lu and Philipp Koehn},\n      year={2024},\n      eprint={2406.13748},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2406.13748}, \n}\n\n","url":"https://huggingface.co/datasets/TaiMingLu/Multilingual-Benchmark","creator_name":"TaiMing","creator_url":"https://huggingface.co/TaiMingLu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","question-answering","translation","English","German"],"keywords_longer_than_N":true},
	{"name":"Cybersecurity-Dataset-Heimdall-v1.1","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCybersecurity Defense Instruction-Tuning Dataset (v1.1)\n\t\n\n\n\t\n\t\t\n\t\tTL;DR\n\t\n\n21â€¯258 highâ€‘quality system / user / assistant triples for training alignmentâ€‘safe, defensiveâ€‘cybersecurity LLMs. Curated from 100â€¯000â€¯+ technical sources, rigorously cleaned and filtered to enforce strict ethical boundaries. Apacheâ€‘2.0 licensed.\n\n\n\t\n\t\t\n\t\t1Â Â Whatâ€™s new in v1.1Â Â (2025â€‘06â€‘21)\n\t\n\n\n\t\n\t\t\nChange\nv1.0\nv1.1\n\n\n\t\t\nRows\n2â€¯500\n21â€¯258Â (+760â€¯%)\n\n\nCovered frameworks\nOWASPÂ TopÂ 10, NISTÂ CSF\n+Â MITREÂ ATT&CK, ASDâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlicanKiraz0/Cybersecurity-Dataset-Heimdall-v1.1.","url":"https://huggingface.co/datasets/AlicanKiraz0/Cybersecurity-Dataset-Heimdall-v1.1","creator_name":"Alican Kiraz","creator_url":"https://huggingface.co/AlicanKiraz0","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"SpecBench","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tSpecBench: Reasoning over Boundaries\n\t\n\nEnhancing Specification Alignment via Test-time Delibration\nPaper | Code | Hugging Face Datasets\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nLarge models are increasingly applied in diverse real-world scenarios, each governed by customized specifications that capture both behavioral preferences and safety boundaries. These specifications vary across domains and evolve with changing requirements, posing the challenge of specification alignment.\n  \n\nTo address thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zzzhr97/SpecBench.","url":"https://huggingface.co/datasets/zzzhr97/SpecBench","creator_name":"Haoran Zhang","creator_url":"https://huggingface.co/zzzhr97","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"wangwei","keyword":"instruction-finetuning","description":"guilty1987/wangwei dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/guilty1987/wangwei","creator_name":"FAN","creator_url":"https://huggingface.co/guilty1987","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"RAIF-ComplexInstruction-DeepSeek","keyword":"instruction-following","description":"This dataset belongs to the official implementation of the paper \"Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models\".\nExisting large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yolay/RAIF-ComplexInstruction-DeepSeek.","url":"https://huggingface.co/datasets/yolay/RAIF-ComplexInstruction-DeepSeek","creator_name":"Yulei Qin","creator_url":"https://huggingface.co/yolay","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ProseFlow-Actions-v1","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tProseFlow-Actions-v1 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nProseFlow-Actions-v1 is a high-quality, diverse dataset of structured instructions designed for fine-tuning language models to act as versatile text-processing assistants. This dataset is the backbone of the local AI engine for the ProseFlow desktop application, a universal, hotkey-driven AI utility.\nThe dataset is composed of 1,805 examples (1742 training, 63 testing) across 88 unique \"Actions\". Each example is aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LSXPrime/ProseFlow-Actions-v1.","url":"https://huggingface.co/datasets/LSXPrime/ProseFlow-Actions-v1","creator_name":"Prime Leonardo","creator_url":"https://huggingface.co/LSXPrime","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Mental-Health-Conversations","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis dataset consists of around 99k rows of mental health conversations. It is a cleaned version of \"jerryjalapeno/nart-100k-synthetic\".\n\n\t\n\t\t\n\t\tSource\n\t\n\n\njerryjalapeno/nart-100k-synthetic\n\n","url":"https://huggingface.co/datasets/ShivomH/Mental-Health-Conversations","creator_name":"Shivom Hatalkar","creator_url":"https://huggingface.co/ShivomH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"uncle-sft-50k-clean","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tUncle SFT 50k (clean)\n\t\n\nCleaned version of SomyaSaraswati/uncle-sft-50k:\n\nEnsured assistant turns end with <|eot_id|>\nRemoved looping / mantra-like samples\nDe-duplicated near-identical entries\n\nSplits:\n\ntrain: 0\nvalidation: 0\n\n","url":"https://huggingface.co/datasets/SomyaSaraswati/uncle-sft-50k-clean","creator_name":"Somya Saraswati","creator_url":"https://huggingface.co/SomyaSaraswati","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"halfTurkish","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for HalfTurkish\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the givenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sasanikrali/halfTurkish.","url":"https://huggingface.co/datasets/sasanikrali/halfTurkish","creator_name":"sasani krali","creator_url":"https://huggingface.co/sasanikrali","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"CoIN-ASD","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCoIN-ASD Benchmark\n\t\n\nCoIN-ASD is a benchmark dataset designed for multimodal continual instruction tuning (MCIT), based on the CoIN dataset. This dataset aims to evaluate the performance of MCIT models in mitigating essential forgetting.\nðŸ“ Paper\nðŸ™ GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized in the following structure:\nâ”œâ”€â”€ ScienceQA/\nâ”‚   â”œâ”€â”€ train_ori.json\nâ”‚   â”œâ”€â”€ train_x{10,20,40,60,80}.json\nâ”‚   â””â”€â”€ test.json\nâ”œâ”€â”€ TextVQA/\nâ”‚   â”œâ”€â”€ train_ori.json\nâ”‚   â”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jinpeng0528/CoIN-ASD.","url":"https://huggingface.co/datasets/jinpeng0528/CoIN-ASD","creator_name":"Jinpeng Chen","creator_url":"https://huggingface.co/jinpeng0528","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","arxiv:2505.02486","ðŸ‡ºðŸ‡¸ Region: US","multimodal-continual-instruction-tuning"],"keywords_longer_than_N":true},
	{"name":"ClinVar-STXBP1-NLP-Dataset-Pathogenic","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tstxbp1_clinvar_curated_pathogenic\n\t\n\nCurated set of 307,587 pathogenic and likely pathogenic STXBP1 and related variants from ClinVar, ready for LLM, variant curation, and biomedical NLP applications. \n\n\n\n(Updated Jun 10th 2025. - Fields containing {null} or {} were removed.) <<<\n\n\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nA hand-curated, LLM-friendly dataset of 307,587 STXBP1 and family variants from ClinVar, filtered for clinical significance (Pathogenic, Likely_pathogenic).Ideal for medicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SkyWhal3/ClinVar-STXBP1-NLP-Dataset-Pathogenic.","url":"https://huggingface.co/datasets/SkyWhal3/ClinVar-STXBP1-NLP-Dataset-Pathogenic","creator_name":"Adam Freygang","creator_url":"https://huggingface.co/SkyWhal3","license_name":"Public Domain Dedication & License","license_url":"https://scancode-licensedb.aboutcode.org/pddl-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","pddl","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_clinical_coding","keyword":"instruct","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Clinical Coding\n\t\n\nThis dataset was created from a motivation to train Medical Large Language Models for improved fluency in clinical \ncoding, as measurable by MedConceptsQA, an open-source medical coding \nevaluation benchmark designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts.  It was extracted from the Centers for Medicare & Medicaid Services' \nInternational Classification of Diseases, Tenth Revision, Clinicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding.","url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Mantis-Instruct","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMantis-Instruct\n\t\n\nPaper | Website | Github | Models | Demo\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nMantis-Instruct is a fully text-image interleaved multimodal instruction tuning dataset, \ncontaining 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding. \nIt's been used to train Mantis Model families\n\nMantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\nAmong the 14â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct.","url":"https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"lots_of_datasets_for_ai_v3","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset is for Training LLMs From Scratch!\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ReallyFloppyPenguin/lots_of_datasets_for_ai_v3.","url":"https://huggingface.co/datasets/ReallyFloppyPenguin/lots_of_datasets_for_ai_v3","creator_name":"Gurvaah Singh","creator_url":"https://huggingface.co/ReallyFloppyPenguin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"godot-training","keyword":"instruction-finetuning","description":"ImJimmeh/godot-training dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/ImJimmeh/godot-training","creator_name":"Jim","creator_url":"https://huggingface.co/ImJimmeh","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"moe-unified-dataset-sota","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tmoe-unified-dataset-sota\n\t\n\nA unified dataset for training Mixture of Experts (MoE) models, combining multiple high-quality sources.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Examples: 2,186,763\nTrain Split: 2,077,424\nTest Split: 109,339\n\n\n\t\n\t\t\n\t\tSources\n\t\n\n\nNousResearch/Hermes-3-Dataset - General instruction following, math, coding (~950k examples)\nSalesforce/xlam-function-calling-60k - Function/tool calling (60k examples)\nMegaScience/TextbookReasoning - Academic Q&A (~650k examples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yxanul/moe-unified-dataset-sota.","url":"https://huggingface.co/datasets/Yxanul/moe-unified-dataset-sota","creator_name":"David Franco","creator_url":"https://huggingface.co/Yxanul","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"drill","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tDrill\n\t\n\nThis dataset combines three instruction-following datasets:\n\nargilla/ifeval-like-data (filtered subset)  \nArliAI/Formax-v1.0  \nChristianAzinn/json-training\nHuggingFaceH4/ifeval-like-data\nallenai/tulu-3-sft-personas-instruction-following\n\nIt contains prompts with detailed instructions and corresponding formatted outputs, suitable for training models on instruction adherence and structured text generation.\n\n  Definition of the word \"drill\" according to Merriam-Webster Dictionaryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/drill.","url":"https://huggingface.co/datasets/agentlans/drill","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMDCure-36k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"gemma-270m-medium-qa","keyword":"instruction-tuning","description":"æœ¬è³‡æ–™é›†åŒ…å«ç”± ** gemini-2.0-flash ** ç”Ÿæˆçš„å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ã€‚è³‡æ–™ä¾†æºçµåˆï¼š\n\nReference-freeï¼šç”± seed æ´¾ç”Ÿçš„å–®è¼ªå•ç­”ã€‚\nReference-basedï¼šä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ã€‚\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"seed\": \"...\", \"context\": \"...\", \"messages\": [{\"role\":\"user\",\"content\":\"...\"}, {\"role\":\"assistant\",\"content\":\"...\"}]}\ntype æ¬„ä½æ¨™ç¤ºè³‡æ–™ä¾†æºï¼šreference_free æˆ– reference_basedã€‚\nseed æ¬„ä½å„²å­˜ Reference-free çš„åŽŸå§‹ seed æŒ‡ä»¤ï¼Œæˆ– Reference-based çš„åƒè€ƒæ–‡æœ¬ç‰‡æ®µã€‚\ncontext æ¬„ä½åƒ…åœ¨â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Simon-Liu/gemma-270m-medium-qa.","url":"https://huggingface.co/datasets/Simon-Liu/gemma-270m-medium-qa","creator_name":"Liu Yu-Wei","creator_url":"https://huggingface.co/Simon-Liu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMDCure-36k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"ru-tasks-conversation","keyword":"instruct","description":"Combined dataset of mostly Russian math and physics tasks in form of conversation suitable for LLM fine-tuning scenarios.\nTotal samples: 462883\nDatasets used:\n\nVikhrmodels/russian_math\nVikhrmodels/russian_physics\nd0rj/MathInstruct-ru\nd0rj/orca-math-word-problems-200k-ru\nevilfreelancer/MATH-500-Russian\n\n","url":"https://huggingface.co/datasets/ZeroAgency/ru-tasks-conversation","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"styler-filtered-instruction-dataset-fixed","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tStyler Filtered Instruction Dataset\n\t\n\nFiltered instruction dataset with token limit of 400 tokens per example\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains instruction-following examples that have been filtered to ensure each example (instruction + input + output) contains no more than 400 tokens.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\ninstruction: The task instruction\ninput: The input text (may be empty)\noutput: The expected output\ntoken_count: Number of tokensâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/upvantage/styler-filtered-instruction-dataset-fixed.","url":"https://huggingface.co/datasets/upvantage/styler-filtered-instruction-dataset-fixed","creator_name":"unknown","creator_url":"https://huggingface.co/upvantage","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LIMO_QFFT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“˜ LIMOâ€“QFFT\n\t\n\nLIMOâ€“QFFT is a question-free variant of the original GAIR/LIMO dataset, tailored for use in QFFT (Question-Free Fine-Tuning) pipelines.\n\n\t\n\t\t\n\t\tðŸ” Description\n\t\n\nThis dataset removes the original input questions and system prompts from the LIMO dataset, and keeps only the long-form reasoning responses. The goal is to enable training large language models to learn from reasoning traces alone, without depending on task-specific questions.\nAll entries are converted intoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lwl-uestc/LIMO_QFFT.","url":"https://huggingface.co/datasets/lwl-uestc/LIMO_QFFT","creator_name":"Wanlong Liu","creator_url":"https://huggingface.co/lwl-uestc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"RLSTACK","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK.","url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"werewolf_game_reasoning","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tWerewolf Game Dataset\n\t\n\nThis repository contains a comprehensive dataset for the Werewolf game in paper Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game, including both raw game data and processed  multi-level instruction datasets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tRaw Data\n\t\n\nThe raw data is located in the raw folder. Each game consists of two files:\n\nevent.json: Contains the game regular record and thinking process data, including:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning.","url":"https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning","creator_name":"Rong Ye","creator_url":"https://huggingface.co/ReneeYe","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","expert-generated","multilingual","original","Chinese"],"keywords_longer_than_N":true},
	{"name":"gemma-en-bg-full","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tGemma EN-BG Translation Dataset (ChessInstruct Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset contains 920,509 English to Bulgarian subtitle translation pairs in ChessInstruct format for fine-tuning Gemma3-270m using the Unsloth framework. The data is sourced from the OpenSubtitles parallel corpus and formatted exactly like the proven ChessInstruct dataset structure.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nâœ… ChessInstruct Compatible: Uses exact task/input/expected_output/KIND format\nðŸš€ Gemma3-270mâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zantag/gemma-en-bg-full.","url":"https://huggingface.co/datasets/zantag/gemma-en-bg-full","creator_name":"zantag","creator_url":"https://huggingface.co/zantag","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","English","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"LONGCOT-Alpaca","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for LONGCOT-Alpaca\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca.","url":"https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Tachibana3-Part2-DeepSeek-V3.2","keyword":"instruct","description":"Click here to support our open-source dataset and model releases!\nTachibana3-Part2-DeepSeek-V3.2 is a dataset focused on high-difficulty code production tasks, testing the limits of DeepSeek V3.2's code-reasoning skills!\nThis dataset contains 9.3k high-difficulty code-production prompts:\n\nQuestions prioritize real-world, challenging coding tasks across a variety of programming languages and topics.\nAreas of focus include back-end and front-end development, mobile, gamedev, cloud, QA, customâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Tachibana3-Part2-DeepSeek-V3.2.","url":"https://huggingface.co/datasets/sequelbox/Tachibana3-Part2-DeepSeek-V3.2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Tachibana3-Part1-DeepSeek-V3.1-Terminus","keyword":"instruct","description":"Click here to support our open-source dataset and model releases!\nTachibana3-Part1-DeepSeek-V3.1-Terminus is a dataset focused on high-difficulty code production tasks, testing the limits of DeepSeek V3.1 Terminus's code-reasoning skills!\nThis dataset contains 9.3k high-difficulty code-production prompts:\n\nQuestions prioritize real-world, challenging coding tasks across a variety of programming languages and topics.\nAreas of focus include back-end and front-end development, mobile, gamedevâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus.","url":"https://huggingface.co/datasets/sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Titanium3-DeepSeek-V3.1-Terminus","keyword":"instruct","description":"Click here to support our open-source dataset and model releases!\nTitanium3-DeepSeek-V3.1-Terminus is a dataset focused on architecture and DevOps, testing the limits of DeepSeek V3.1 Terminus's architect and coding skills!\nThis dataset contains:\n\n27.7k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek V3.1 Terminus in reasoning mode:\n20k selected technical expertise prompts from sequelbox/Titanium2.1-DeepSeek-R1 focused onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium3-DeepSeek-V3.1-Terminus.","url":"https://huggingface.co/datasets/sequelbox/Titanium3-DeepSeek-V3.1-Terminus","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"NorPaca","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tNorPaca Norwegian BokmÃ¥l\n\t\n\nThis dataset is a translation to Norwegian BokmÃ¥l of alpaca_gpt4_data.json, a clean version of the Alpaca dataset made at Stanford, but generated with GPT4.\n\n\t\n\t\t\n\t\tPrompt to generate dataset\n\t\n\n    Du blir bedt om Ã¥ komme opp med et sett med 20 forskjellige oppgaveinstruksjoner. Disse oppgaveinstruksjonene vil bli gitt til en GPT-modell, og vi vil evaluere GPT-modellen for Ã¥ fullfÃ¸re instruksjonene. \n\nHer er kravene:\n1. PrÃ¸v Ã¥ ikke gjenta verbet for hverâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MasterThesisCBS/NorPaca.","url":"https://huggingface.co/datasets/MasterThesisCBS/NorPaca","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"mauxitalk-persian","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tðŸ—£ï¸ MauxiTalk: High-Quality Persian Conversations Dataset ðŸ‡®ðŸ‡·\n\t\n\n\n\t\n\t\t\n\t\tðŸ“ Description\n\t\n\nMauxiTalk is a high-quality dataset of 2,000+ Persian conversations, carefully translated from the SmolTalk dataset using state-of-the-art language models. This dataset is specifically curated for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n2,000 natural conversations in Persian\nDiverse topics including dailyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxitalk-persian.","url":"https://huggingface.co/datasets/xmanii/mauxitalk-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","summarization","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-1.5-Instruct-Data","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-1.5 Instruction Data\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset, LLaVA-OneVision-1.5-Instruct, was collected and integrated during the development of LLaVA-OneVision-1.5. LLaVA-OneVision-1.5 is a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. This meticulously curated 22M instruction dataset (LLaVA-OneVision-1.5-Instruct) is part of a comprehensive andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mvp-lab/LLaVA-OneVision-1.5-Instruct-Data.","url":"https://huggingface.co/datasets/mvp-lab/LLaVA-OneVision-1.5-Instruct-Data","creator_name":"Mobile Vision Perception Lab","creator_url":"https://huggingface.co/mvp-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2509.23661","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for \"tasksource-instruct-v0\" (TSI)\n\t\n\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\nDataset size is capped at 30k examples per task to foster task diversity.\n!pip install tasksource, pandit\nimport tasksource, pandit\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\nfor tasks in df.id:\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\n\nhttps://github.com/sileod/tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for \"tasksource-instruct-v0\" (TSI)\n\t\n\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\nDataset size is capped at 30k examples per task to foster task diversity.\n!pip install tasksource, pandit\nimport tasksource, pandit\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\nfor tasks in df.id:\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\n\nhttps://github.com/sileod/tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"X-Teaming_Evolutionary_M2S","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tX-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates\n\t\n\nPaper: X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak TemplatesarXiv: 2509.08729 [cs.CL]Accepted at: NeurIPS 2025 Workshop on LockLLMGitHub: M2S-x-teaming-pipeline-final\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis dataset contains the complete experimental results from our M2S (Multi-turn to Single-turn) template evolution pipeline, which usesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hyunjun1121/X-Teaming_Evolutionary_M2S.","url":"https://huggingface.co/datasets/hyunjun1121/X-Teaming_Evolutionary_M2S","creator_name":"hyunjun","creator_url":"https://huggingface.co/hyunjun1121","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K<n<10K","Text"],"keywords_longer_than_N":true},
	{"name":"mextract_papers","keyword":"instruction","description":"\n\t\n\t\t\n\t\tName: Title\n\t\n\nA dataset of papers labelled for as dataset resources. The labelling was created using Gemma 3 27B. \n\n\t\n\t\t\n\t\tðŸ“‹ Dataset Structure\n\t\n\nDescription of the dataset features \n\ncolumn1 (type): column 1 description.\ncolumn2 (type): column 2 description.\n\n\n\t\n\t\t\n\t\tðŸ“ Loading The Dataset\n\t\n\nHow to load the dataset\nfrom datasets import load_dataset\ndataset = load_dataset('IVUL-KAUST/mextract_papers')\n\n\n\t\n\t\t\n\t\tðŸ“„ Sample From The Dataset:\n\t\n\nShow a sample from the dataset\n{â€¦ See the full description on the dataset page: https://huggingface.co/datasets/IVUL-KAUST/mextract_papers.","url":"https://huggingface.co/datasets/IVUL-KAUST/mextract_papers","creator_name":"Image and Video Understanding Lab","creator_url":"https://huggingface.co/IVUL-KAUST","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset","keyword":"instruction","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of prompts and responses to those prompts. All completions were generated by querying already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc.). The dataset is available in Portuguese, English, and Spanish.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguage modeling.\nQuestion-answeringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset.","url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","Spanish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Creative_Writing_Multiturn","keyword":"multiturn","description":"This is a dataset merge of many, many high quality story writing / roleplaying datasets across all of Huggingface. I've filtered specifically for samples with high turns, which is a key different to already available datasets. My goal is to improve the model's ability to recollect and mention details from far back even at a longer context and more importantly, also improve the model's ability to output engaging verbose storylines, reduce certain phrases, increase creativity and reduce dryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dampfinchen/Creative_Writing_Multiturn.","url":"https://huggingface.co/datasets/Dampfinchen/Creative_Writing_Multiturn","creator_name":"DÃ¤mpfchen","creator_url":"https://huggingface.co/Dampfinchen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"LewdRuStoryforTrain","keyword":"instruction-finetuning","description":"ÐšÑƒÑ‡Ð° Ð²ÑÑÐºÐ¸Ñ… Ñ€Ð°Ð½Ð´Ð¾Ð¼Ð½Ñ‹Ñ… Ð¿Ð¾Ñ€Ð½Ð¾ Ñ€Ð°ÑÑÐºÐ°Ð·Ð¾Ð² Ð¸Ð· Ð¸Ð½Ñ‚ÐµÑ€ÐµÐ½ÐµÑ‚Ð°, Ð¸Ð· Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ñ‚ÐµÐ³Ð¾Ð², Ð½Ð¾ Ð±ÐµÐ· Ð¶ÐµÑÑ‚Ð¸.\n","url":"https://huggingface.co/datasets/ASIDS/LewdRuStoryforTrain","creator_name":"Greed","creator_url":"https://huggingface.co/ASIDS","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-generation","Russian","mit","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"budapest-v0.1-hun","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tBudapest-v0.1 Dataset README\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Budapest-v0.1 dataset is a cutting-edge resource specifically designed for fine-tuning large language models (LLMs). Created using GPT-4, this dataset is presented in a message-response format, making it particularly suitable for a variety of natural language processing tasks. The primary focus of Budapest-v0.1 is to aid in the development and enhancement of algorithms capable of performing summarization, question answeringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun.","url":"https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun","creator_name":"Balazs Toldi","creator_url":"https://huggingface.co/Bazsalanszky","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Hungarian","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-serbian-full","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tSerbian Alpaca Cleaned Dataset\n\t\n\n\nOriginal Repository: https://github.com/gururise/AlpacaDataCleaned\nOriginal HF Repository: https://huggingface.co/datasets/yahma/alpaca-cleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a serbian cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full.","url":"https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full","creator_name":"Dean","creator_url":"https://huggingface.co/datatab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Serbian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"NorEval","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tNorEval\n\t\n\nNorEval is a self-curated dataset to evaluate instruction-following LLMs, seeking to evaluate the models in nine categories: Language, Code, Mathematics, Classification, Communication & Marketing, Medical, General Knowledge, and Business Operations\n","url":"https://huggingface.co/datasets/MasterThesisCBS/NorEval","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"helpful_instructions","keyword":"instruct","description":"Helpful Instructions is a dataset of (prompt, completion) pairs that are derived from a variety of public datasets. As the name suggests, it focuses on instructions that are \"helpful\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform.","url":"https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"instruct_me","keyword":"instruct","description":"Instruct Me is a dataset of instruction-like dialogues between a human user and AI assistant. The prompts are derived from (prompt, completion) pairs in the Helpful Instructions dataset. The goal is to train a language model to that is \"chatty\" and can answer the kind of questions or tasks a human user might instruct an AI assistant to perform.","url":"https://huggingface.co/datasets/HuggingFaceH4/instruct_me","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wildchat-stratified-sample","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tWildChat Stratified Sample\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains a stratified sample of 263 GPT-4 conversations (347 total turns) from the WildChat dataset. The sample was carefully selected to ensure balanced representation across conversation turn positions and user message lengths.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Conversations: 263\nTotal Turns/Rows: 347\nAverage Turns per Conversation: 1.32\nConversation Length: 1-5 turns (conversations with >5 turns excluded)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Avinaash/wildchat-stratified-sample.","url":"https://huggingface.co/datasets/Avinaash/wildchat-stratified-sample","creator_name":"Anand","creator_url":"https://huggingface.co/Avinaash","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"ccisd-teks-training","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCCISD TEKS Training Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a training-ready dataset with 4,200+ examples derived from Texas Essential Knowledge and Skills (TEKS) standards for 25 high school courses. Designed for fine-tuning language models on educational curriculum tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 4,224\nTraining Set: 2,956 (70%)\nValidation Set: 633 (15%)\nTest Set: 635 (15%)\nSource Courses: 25 high school courses\nSource Standards: 428 TEKS standards\nTaskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/robworks-software/ccisd-teks-training.","url":"https://huggingface.co/datasets/robworks-software/ccisd-teks-training","creator_name":"Ryan Robson","creator_url":"https://huggingface.co/robworks-software","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","text-generation","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"streetview-commands-dataset","keyword":"instruction-tuning","description":"\n\n\t\n\t\t\n\t\tDataset Card for streetview-commands-dataset\n\t\n\nThis dataset contains pairs of natural language instructions (simulating commands given to Google Street View) and their corresponding structured JSON outputs representing the intended navigation action. It was generated using the Gemini API (gemini-1.5-flash-latest) based on predefined templates and few-shot examples.\nThe primary intended use is for fine-tuning small language models (like TinyLlama) to act as a translation layer betweenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cahlen/streetview-commands-dataset.","url":"https://huggingface.co/datasets/cahlen/streetview-commands-dataset","creator_name":"Cahlen Humphreys","creator_url":"https://huggingface.co/cahlen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Primus-Seed-Conversation","keyword":"instruct","description":"\n\t\n\t\t\n\t\tPrimus-Seed Conversational Dataset\n\t\n\nA conversational dataset derived from trendmicro-ailab/Primus-Seed, designed for training language models as cybersecurity experts.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset converts the original Primus-Seed text completion dataset into a conversational format with 86,987 examples. Each example consists of a three-turn conversation between a system, user, and assistant focused on cybersecurity knowledge.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tuandunghcmut/Primus-Seed-Conversation.","url":"https://huggingface.co/datasets/tuandunghcmut/Primus-Seed-Conversation","creator_name":"DÅ©ng VÃµ","creator_url":"https://huggingface.co/tuandunghcmut","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v3","keyword":"instruction","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset version 3.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of multi-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3.","url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"turkish-gemma-51k","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tTurkish Chat Dataset - Gemma Format\n\t\n\nBu dataset, TÃ¼rkÃ§e sohbet ve talimat takip etme gÃ¶revleri iÃ§in hazÄ±rlanmÄ±ÅŸ 51.914 konuÅŸma Ã¶rneÄŸi iÃ§erir.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Ã–zeti\n\t\n\n\nDil: TÃ¼rkÃ§e\nFormat: Chat/Conversation\nÃ–rnek SayÄ±sÄ±: 51,914\nKaynak: afkfatih/turkishdataset\n\n\n\t\n\t\t\n\t\tðŸŽ¯ KullanÄ±m AlanlarÄ±\n\t\n\n\nTÃ¼rkÃ§e sohbet botlarÄ± eÄŸitimi\nInstruction-tuning\nFine-tuning LLM modelleri (Gemma, Llama, vb.)\nTÃ¼rkÃ§e doÄŸal dil anlama\n\n\n\t\n\t\t\n\t\tðŸ“ Format\n\t\n\nHer Ã¶rnek ÅŸu yapÄ±ya sahiptir:\n[\n  {\n    \"role\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/afkfatih/turkish-gemma-51k.","url":"https://huggingface.co/datasets/afkfatih/turkish-gemma-51k","creator_name":"Fatih Ozyilmaz","creator_url":"https://huggingface.co/afkfatih","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Turkish","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k-Japanese","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k (Japanese Translation)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a Japanese translation of the Finance-Instruct-500k dataset, created using OpenAI's GPT-4o-mini via the Batch API.\n\n\t\n\t\t\n\t\tOriginal Dataset\n\t\n\n\nOriginal Author: Joseph G. Flowers\nOriginal Dataset: Josephgflowers/Finance-Instruct-500k\nLicense: Apache 2.0\n\n\n\t\n\t\t\n\t\tTranslation Details\n\t\n\n\nTranslation Model: GPT-4o-mini (OpenAI)\nTranslation Method: OpenAI Batch API with human verifications\nDate: 2025â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ronantakizawa/Finance-Instruct-500k-Japanese.","url":"https://huggingface.co/datasets/ronantakizawa/Finance-Instruct-500k-Japanese","creator_name":"Ronan Takizawa","creator_url":"https://huggingface.co/ronantakizawa","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","translation","Japanese","English"],"keywords_longer_than_N":true},
	{"name":"openbohm","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tOpenBohm\n\t\n\nThis dataset is an experimental conjugation of philosophical multi-turn long-form conversations from J. Krishnamurti, and D. Bohm, added to long-conversation filtered (count > 6) Capybara data, edited to be slightly less apologetic.\nRemoved references to names and locations where possible. Some conversations have been paraphrased somewhat to follow QA format better, however they keep the key content of the original.\n\n","url":"https://huggingface.co/datasets/distantquant/openbohm","creator_name":"Distant Quant","creator_url":"https://huggingface.co/distantquant","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"multiTurnBNTest","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tMulti-Turn Bengali Conversational Dataset\n\t\n\nThis dataset contains multi-turn conversational dialogues in Bengali, designed for training and evaluating language models on natural, context-aware interactions. Each conversation includes user-assistant exchanges on everyday topics like weather, greetings, and general inquiries.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is provided as a JSON file with the following structure:\n\nFile: data/train.json\nFormat: JSON array of conversation recordsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hasin023/multiTurnBNTest.","url":"https://huggingface.co/datasets/hasin023/multiTurnBNTest","creator_name":"Hasin Mahtab","creator_url":"https://huggingface.co/hasin023","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Bengali","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"PRODIGY-LAB_SARA","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for PRODIGY-LAB_CLEANED\n\t\n\n\nRepository: https://github.com/aadhithyaravi\nCreated by: Aadhithya  \nContact: aadhithyaxll@gmail.com  \nInstagram: @aadhi.arc  \nLinkedIn: www.linkedin.com/in/aadhithya-ravi-135019289\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nPRODIGY-SARA-MODEL is a refined and enhanced dataset designed for instruction-based fine-tuning of large language models (LLMs).It combines multiple high-quality sources, including cleaned and normalized instructions, to improveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Apex-X/PRODIGY-LAB_SARA.","url":"https://huggingface.co/datasets/Apex-X/PRODIGY-LAB_SARA","creator_name":"Aadhithya","creator_url":"https://huggingface.co/Apex-X","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Tamil","Hindi","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"periodontal-reasoning-40k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tPeriodontal-Reasoning-40k\n\t\n\n40,000 periodontal clinical reasoning examples for off-policy RLHF (KTO/DPO).\nFormat (JSONL, one per line): prompt, completion, label âˆˆ {1,-1}\nExample:\n{\"prompt\": \"A patient's plaque score was 35% at baseline and 1% at followâ€‘up. Determine whether the improvement is favourable according to BSP criteria (â‰¤20% plaque or â‰¥50% reduction).\", \"completion\": \"The improvement is favourable.\", \"label\": 1}\nSplit: train: 40,000 (data/train.jsonl)\nIntended use: KTO/DPO;â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Wildstash/periodontal-reasoning-40k.","url":"https://huggingface.co/datasets/Wildstash/periodontal-reasoning-40k","creator_name":"ArnavS","creator_url":"https://huggingface.co/Wildstash","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","expert-generated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Code-170k-baoule","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-baoule is a groundbreaking dataset containing 176,999 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Baoule, making coding education accessible to Baoule speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n176,999 high-quality conversations about programming and coding\nPure Baoule language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-baoule.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-baoule","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","BaoulÃ©","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"KITE","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tKITE: Korean Instruction-following Task Evaluation\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nKITE (Korean Instruction-following Task Evaluation) is the first comprehensive benchmark specifically designed to evaluate the Korean instruction-following capabilities of Large Language Models (LLMs). Unlike existing Korean benchmarks that focus mainly on factual knowledge or multiple-choice testing, KITE directly targets diverse, open-ended instruction-following tasks.\n\t\n\t\t\n\t\tDataset Summaryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/junkim100/KITE.","url":"https://huggingface.co/datasets/junkim100/KITE","creator_name":"Jun Kim","creator_url":"https://huggingface.co/junkim100","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Korean","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"shell-cmd-instruct","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tUsed to train models that interact directly with shells\n\t\n\nNote: This dataset is out-dated in the llm world, probably easier to just setup a tool with a decent model that supports tooling.\nFollow-up details of my process \n\nMacOS terminal commands for now. This dataset is still in alpha stages and will be modified.\n\nContains 500 somewhat unique training examples so far.\n\nGPT4 seems like a good candidate for generating more data, licensing would need to be addressed.\n\nI fine-tunedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/byroneverson/shell-cmd-instruct.","url":"https://huggingface.co/datasets/byroneverson/shell-cmd-instruct","creator_name":"Byron Everson","creator_url":"https://huggingface.co/byroneverson","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"marathi-alpaca-llama-finetune","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMarathi Alpaca Dataset for llama-finetune\n\t\n\nThis dataset contains 48,897 high-quality Marathi instruction-following examples, converted to the llama-finetune format.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach line in the JSONL file contains:\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"à¤¨à¤¿à¤°à¥‹à¤—à¥€ à¤°à¤¾à¤¹à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤¤à¥€à¤¨ à¤Ÿà¤¿à¤ªà¤¾ à¤¦à¥à¤¯à¤¾.\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"1. à¤¸à¤‚à¤¤à¥à¤²à¤¿à¤¤ à¤†à¤£à¤¿ à¤ªà¥Œà¤·à¥à¤Ÿà¤¿à¤• à¤†à¤¹à¤¾à¤° à¤˜à¥à¤¯à¤¾...\"\n    }\n  ]\n}\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tDownload\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aghatage/marathi-alpaca-llama-finetune.","url":"https://huggingface.co/datasets/aghatage/marathi-alpaca-llama-finetune","creator_name":"Anup Ghatage","creator_url":"https://huggingface.co/aghatage","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Marathi","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"AB1","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagedGaman/AB1.","url":"https://huggingface.co/datasets/MagedGaman/AB1","creator_name":"Maged Gaman","creator_url":"https://huggingface.co/MagedGaman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"secondKarlMarx-sft","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMarx Works SFT Instruction Prompts Dataset / é©¬å…‹æ€è‘—ä½œSFTæŒ‡ä»¤æç¤ºæ•°æ®é›†\n\t\n\nEnglish | ä¸­æ–‡\n\n\n\t\n\t\t\n\t\tEnglish\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains SFT (Supervised Fine-Tuning) instruction prompts generated from the works of Karl Marx. The dataset is specifically designed for training large language models, aiming to capture Marx's dialectical materialist analytical method and writing style.\n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nDiverse Prompt Types: Includes various styles of prompts such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft.","url":"https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft","creator_name":"ChizhongWang","creator_url":"https://huggingface.co/ChizhongWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Pares_1_vinos","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tWine Q&A Dataset Â· Sommelier Style ðŸ·\n\t\n\nEste dataset contiene 100 pares de pregunta-respuesta en espaÃ±ol sobre el mundo del vino, redactados con estilo profesional, cercano y claro, al estilo de un sommelier.\n\n\t\n\t\t\n\t\tEstructura del dataset\n\t\n\nEl archivo estÃ¡ en formato .jsonl, donde cada lÃ­nea es un objeto con dos campos:\n\ninstruction: la pregunta del usuario.\nresponse: una respuesta experta, clara y contextualizada sobre vinos.\n\nEjemplo:\n{\"instruction\": \"Â¿QuÃ© vino marida bien conâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Alexei-Bucarenko/Pares_1_vinos.","url":"https://huggingface.co/datasets/Alexei-Bucarenko/Pares_1_vinos","creator_name":"Alejandro MartÃ­n","creator_url":"https://huggingface.co/Alexei-Bucarenko","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["Spanish","apache-2.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US","wine"],"keywords_longer_than_N":true},
	{"name":"dental-2.5k-instruct","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDental Training Dataset (Synthetic)\n\t\n\nSynthetic dataset of 2,494 dental clinical cases for training Dental-GPT, a specialized language model for dental diagnosis and treatment planning.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSize: 2,494 synthetic dental cases\nFormat: JSONL with structured conversations\nSynthetic: Artificially generated cases (no real patient data)\nPurpose: Training dental diagnostic AI models\nLanguage: English\nMetadata: Croissant format available for automated discoveryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Wildstash/dental-2.5k-instruct.","url":"https://huggingface.co/datasets/Wildstash/dental-2.5k-instruct","creator_name":"ArnavS","creator_url":"https://huggingface.co/Wildstash","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"indian_university_guidance_for_bangladeshi_students","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tIndian University Guidance for Bangladeshi Students Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 7,044 high-quality, instruction-formatted Question-Answer pairs designed for fine-tuning Large Language Models (LLMs). The primary goal of this dataset is to create a specialized AI counselor that provides accurate, culturally relevant, and comprehensive guidance on Indian universities for Bangladeshi students.\nThe dataset was generated through a sophisticatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/millat/indian_university_guidance_for_bangladeshi_students.","url":"https://huggingface.co/datasets/millat/indian_university_guidance_for_bangladeshi_students","creator_name":"MD MILLAT HOSEN","creator_url":"https://huggingface.co/millat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"CRAFT-Summarization","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCRAFT-Summarization\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated summarization data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization.","url":"https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","text-generation","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"alpaca-tat","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tTatAlpaca\n\t\n\nDataset of Gemini-generated instructions in Tatar language.\n\nCode: tatlm/self_instruct\nCode is based on Stanford Alpaca and self-instruct.\n166,257 examples\n\nPrompt template:\n{{num_tasks}} Ò—Ñ‹ÐµÐ»Ð¼Ð°ÑÑ‹Ð½Ñ‹Ò£ ÑÐ¾ÑÑ‚Ð°Ð²Ñ‹ Ñ‚ÐµÐ» Ð¼Ð¾Ð´ÐµÐ»ÐµÐ½ Ó©Ð¹Ñ€Ó™Ð½Ò¯ Ó©Ñ‡ÐµÐ½ Ñ‚Ó©Ñ€Ð»Ðµ:\n\n1. Ð‘Ð¸Ñ€ÐµÐ¼Ð½Ó™Ñ€Ð½Ðµ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒ Ñ€Ó™Ð²ÐµÑˆÑ‚Ó™ Ñ‚Ð¸Ð¿Ð»Ð°Ñ€Ñ‹, ÑÐ¾Ñ€Ð°Ð»Ð³Ð°Ð½ Ð³Ð°Ð¼Ó™Ð»Ð»Ó™Ñ€Ðµ, Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ°Ð»Ð°Ñ€Ñ‹, ÐºÐµÑ€Ò¯ Ð¼Ó©Ð¼ÐºÐ¸Ð½Ð»ÐµÐºÐ»Ó™Ñ€Ðµ Ð±ÑƒÐµÐ½Ñ‡Ð° Ð±ÐµÑ€-Ð±ÐµÑ€ÑÐµÐ½Ó™ Ð¾Ñ…ÑˆÐ°Ð¼Ð°Ð³Ð°Ð½ Ð¸Ñ‚ÐµÐ¿ ÑÑˆÐ»Ó™.\n2. Ð‘Ð¸Ñ€ÐµÐ¼Ð½Ó™Ñ€ Ñ€Ó™ÑÐµÐ¼Ð½Ó™Ñ€, Ð²Ð¸Ð´ÐµÐ¾, Ð°ÑƒÐ´Ð¸Ð¾ Ð±ÐµÐ»Ó™Ð½ ÑÑˆÐ»Ð¸ Ð±ÐµÐ»Ð¼Ó™Ð³Ó™Ð½ Ò»Ó™Ð¼ Ñ‚Ñ‹ÑˆÐºÑ‹ Ð´Ó©Ð½ÑŒÑÐ³Ð° ÐºÐµÑ€Ò¯ Ð¼Ó©Ð¼ÐºÐ¸Ð½Ð»ÐµÐ³Ðµ Ð±ÑƒÐ»Ð¼Ð°Ð³Ð°Ð½â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yasalma/alpaca-tat.","url":"https://huggingface.co/datasets/yasalma/alpaca-tat","creator_name":"Yasalma","creator_url":"https://huggingface.co/yasalma","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Tatar","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Bangla-Instruct","keyword":"instruction","description":"\n   Accepted in ACL Main 2025 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTigerLLM - A Family of Bangla Large Language Models\n\nNishat Raihan, Marcos Zampieri\nGeorge Mason University, VA, USA\nmraihan2@gmu.edu\n\n\n\n\n\n\n\n\n\n\nIf you find our work helpful, please consider citing our paper:\n@inproceedings{raihan-zampieri-2025-tigerllm,\n    title = \"{T}iger{LLM} - A Family of {B}angla Large Language Models\",\n    author = \"Raihan, Nishat  and\n      Zampieri, Marcos\",\n    editor = \"Che, Wanxiang  and\n      Nabende, Joyce  andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/md-nishat-008/Bangla-Instruct.","url":"https://huggingface.co/datasets/md-nishat-008/Bangla-Instruct","creator_name":"Nishat Raihan","creator_url":"https://huggingface.co/md-nishat-008","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Bengali","mit","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"Titanium2.1-DeepSeek-R1","keyword":"instruct","description":"Click here to support our open-source dataset and model releases!\nTitanium2.1-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n31.7k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraformâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1.","url":"https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"glaiveai-reflection-v1-ko","keyword":"instruction","description":"Translated glaiveai/reflection-v1 using nayohan/llama3-instrucTrans-enko-8b.\nFor this dataset, we only used data that is 5000 characters or less in length and has language of English.\nThanks for @Magpie-Align and @nayohan.\n","url":"https://huggingface.co/datasets/youjunhyeok/glaiveai-reflection-v1-ko","creator_name":"ìœ ì¤€í˜","creator_url":"https://huggingface.co/youjunhyeok","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Korean","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dali","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tå¤§æŽè€å¸ˆé—®ç­”æ•°æ®é›†\n\t\n\nè¿™ä¸ªæ•°æ®é›†åŒ…å«å¤§æŽè€å¸ˆçš„é—®ç­”å¯¹è¯,ç”¨äºŽè®­ç»ƒå¯¹è¯æ¨¡åž‹ã€‚\n\n\t\n\t\t\n\t\tæ•°æ®é›†æè¿°\n\t\n\n\næ ¼å¼: JSONL\nå­—æ®µ: \ninstruction: å›ºå®šå€¼\"è¯·å¤§æŽè€å¸ˆå›žç­”\"\ninput: æé—®å†…å®¹ \noutput: å¤§æŽè€å¸ˆçš„å›žç­”\n\n\næ•°æ®é‡: xxxæ¡å¯¹è¯æ•°æ®\n\n\n\t\n\t\t\n\t\tä½¿ç”¨ç¤ºä¾‹\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"your-username/dataset-name\")\n\n\n\t\n\t\t\n\t\tè®¸å¯è¯\n\t\n\nApache 2.0\n","url":"https://huggingface.co/datasets/liwei1987cn/dali","creator_name":"Levi li","creator_url":"https://huggingface.co/liwei1987cn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"fineweb-conversational","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDataset Card for fineweb-conversational\n\t\n\n\n\n\t\n\t\t\n\t\t1. Dataset Overview\n\t\n\nfineweb-conversational is a dataset crafted for training conversational AI models in an instruction-following format. It transforms cleaned and deduplicated English web data from the FineWeb dataset into a prompt-completion structure. The dataset is curated by me, a.k.a EpGuy, is under an odc-by license, and is still in active development with periodic updates.\n\n\n\t\n\t\t\n\t\n\t\n\t\t2. Structure & Creation Process\n\t\n\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/EpGuy/fineweb-conversational.","url":"https://huggingface.co/datasets/EpGuy/fineweb-conversational","creator_name":"Ep Guy","creator_url":"https://huggingface.co/EpGuy","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","HuggingFaceFW/fineweb","English","odc-by","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"twinkle-dialogue-gemma3-2025-08","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTwinkle Dialogue (Gemma-3-12B-it, 2025-08)\n\t\n\n\n  \n    \n  \n  \n    \n  \n\n\næœ¬è³‡æ–™é›†ç”± Gemma-3-12B-itï¼ˆTwinkle AI ç¤¾ç¾¤æœå‹™ï¼‰ ç”Ÿæˆä¹‹å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ï¼Œä¸¦æ•´åˆï¼š\n\nReference-freeï¼ˆç”± seed æ´¾ç”Ÿå–®è¼ªå•ç­”ï¼‰\nReference-basedï¼ˆä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ï¼‰\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"messages\": [{\"role\":\"system\",\"content\":\"...\"}, ...]}\nè¨“ç·´æ™‚å¯æ“·å–ç¬¬ä¸€å€‹ user èˆ‡å°æ‡‰ assistant å½¢æˆ (instruction, response) pairï¼Œæˆ–ç›´æŽ¥ä½¿ç”¨ chat æ ¼å¼çš„ trainerã€‚\n\n\n\t\n\t\t\n\t\tä¾†æºèˆ‡é™åˆ¶\n\t\n\n\nModel:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenlin316/twinkle-dialogue-gemma3-2025-08.","url":"https://huggingface.co/datasets/allenlin316/twinkle-dialogue-gemma3-2025-08","creator_name":"Pin-An LIN","creator_url":"https://huggingface.co/allenlin316","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"chempile-instruction","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tChemPile-Instruction\n\t\n\n\n\n\n\n\n\n\nA comprehensive instruction tuning dataset for chemistry LLMs with multi-turn conversations and diverse reasoning tasks\n\t\n\t\t\n\t\tðŸ“‹ Dataset Summary\n\t\n\nChemPile-Instruction is a text-only dataset designed for instruction tuning of Large Language Models (LLMs) in the field of chemistry. It contains high-quality multi-turn conversations, each rephrased from different educational, scientific, and reasoning sources using diverse prompting strategies. Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/chempile-instruction.","url":"https://huggingface.co/datasets/jablonkagroup/chempile-instruction","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","language-modeling","natural-language-inference","dialogue-generation"],"keywords_longer_than_N":true},
	{"name":"persona-chat","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tDataset Card for PersonaChat\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPersonaChat is a multi-turn dialogue dataset introduced by Zhang et al. (2018) for training and evaluating persona-grounded conversational agents. Each conversation is between two crowdworkers, each assigned a randomly selected persona consisting of several simple facts. The dataset aims to assess whether models can maintain consistent character traits throughout a conversation.\n\nOriginal Paper: Personalizing Dialogueâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/awsaf49/persona-chat.","url":"https://huggingface.co/datasets/awsaf49/persona-chat","creator_name":"Awsaf","creator_url":"https://huggingface.co/awsaf49","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","text","Text"],"keywords_longer_than_N":true},
	{"name":"tame-the-weights-personas","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for \"tame-the-weights-personas\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains instruction-following data designed for fine-tuning language models, specifically focused on generating Python code explanations and snippets while adopting distinct personas.\nThe data was synthetically generated using a large language model, prompted to adopt one of three personas:\n\nProfessor Snugglesworth: A friendly, encouraging, and slightly verbose persona, like a kind universityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/leonvanbokhorst/tame-the-weights-personas.","url":"https://huggingface.co/datasets/leonvanbokhorst/tame-the-weights-personas","creator_name":"Leon Van Bokhorst","creator_url":"https://huggingface.co/leonvanbokhorst","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"ictisgpt","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for ICTIS GPT dataset\n\t\n\n","url":"https://huggingface.co/datasets/alexbs/ictisgpt","creator_name":"Alex B","creator_url":"https://huggingface.co/alexbs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Discord-Dialogues","keyword":"multi-turn","description":"This is a clone of mookiezi/Discord-Dialogues.\n\n  \n\n\n\nDiscord-Dialogues is a large-scale dataset of anonymized Discord conversations from late spring to early fall 2025 for training and evaluating realistic conversational AI models in a ChatML-friendly format.\n\nThis dataset contains 7.3 million exchanges spread out over 16 million turns, with more than 139 million words.\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nMixed single and multi-turn exchanges\nHuman-only dialogues (no bots)\nFiltered for ToS andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mooaoeu/Discord-Dialogues.","url":"https://huggingface.co/datasets/mooaoeu/Discord-Dialogues","creator_name":"mookiezi","creator_url":"https://huggingface.co/mooaoeu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"TextBooksPersonaHub","keyword":"instruction","description":"\n\t\n\t\t\n\t\tTextBooksPersonaHub\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe TextBooksPersonaHub dataset is an extension of the proj-persona/PersonaHub dataset, created using the technique described in the paper Textbooks Are All You Need II. This dataset contains synthetically generated \"textbook-like\" passages tailored in french to specific personas, aimed at enhancing language model training with high-quality and diverse content.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nThe original personasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/drodin/TextBooksPersonaHub.","url":"https://huggingface.co/datasets/drodin/TextBooksPersonaHub","creator_name":"nacer","creator_url":"https://huggingface.co/drodin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","French","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"wangchanx-seed-free-synthetic-instruct-thai-120k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for WangchanX Seed-Free Synthetic Instruct Thai 120k\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains about 120k synthetic instruction-following samples in Thai, generated using a novel seed-free approach. It covers a wide range of domains derived from Wikipedia, including both general knowledge and Thai-specific cultural topics. The dataset is designed for instruction-tuning Thai language models to improve their ability to understand and generate Thai text in variousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k.","url":"https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k","creator_name":"VISTEC-depa AI Research Institute of Thailand","creator_url":"https://huggingface.co/airesearch","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","Thai","English"],"keywords_longer_than_N":true},
	{"name":"1M-OpenOrca_be","keyword":"instruct","description":"En/Be\nðŸ‹ The Belarusian OpenOrca Dataset! ðŸ‹\n\n\n\nBelarusian OpenOrca dataset - is rich collection of augmented FLAN data aligns, that translated in belarusian language.\nThat dataset should help training LLM in belarusian language and should help on other NLP tasks.\nThis dataset have 2 version:\n\n~1M GPT-4 completions (Now translating)\n~3.2M GPT-3.5 completions (Can be translated in future)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe fields are:\n\n'id', a unique numbered identifier which includes one of 'niv'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be.","url":"https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be","creator_name":"Artsem Holub","creator_url":"https://huggingface.co/WiNE-iNEFF","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Feedback-Collection-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\tFeedback-Collection-ru\n\t\n\nThis is russian version of prometheus-eval/Feedback-Collection translated using Google Translate.\n","url":"https://huggingface.co/datasets/d0rj/Feedback-Collection-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","translated","prometheus-eval/Feedback-Collection","Russian"],"keywords_longer_than_N":true},
	{"name":"NL2SH-ALPACA","keyword":"instruction","description":"\n\t\n\t\t\n\t\tNL2SH-ALPACA (Alpaca-style)\n\t\n\nThis is a reformatted version of the NL2SH-ALFA dataset originally created by westenfelder/NL2SH-ALFA.  \nIt has been converted to Alpaca-style format and prepared for instruction fine-tuning by Anshuman Jena, who acted as the converter and maintainer of this version.\n{\n  \"instruction\": \"<natural language instruction>\",\n  \"input\": \"\",\n  \"output\": \"<bash command>\"\n}\n\nAdditionally, for the test split, the original bash2 (alternative command) and difficultyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/abandonedmonk/NL2SH-ALPACA.","url":"https://huggingface.co/datasets/abandonedmonk/NL2SH-ALPACA","creator_name":"Anshuman Jena","creator_url":"https://huggingface.co/abandonedmonk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"code-act","keyword":"instruction-tuning","description":" Executable Code Actions Elicit Better LLM Agents \n\n\nðŸ’» Code\nâ€¢\nðŸ“ƒ Paper\nâ€¢\nðŸ¤— Data (CodeActInstruct)\nâ€¢\nðŸ¤— Model (CodeActAgent-Mistral-7b-v0.1)\nâ€¢\nðŸ¤– Chat with CodeActAgent!\n\n\nWe propose to use executable Python code to consolidate LLM agentsâ€™ actions into a unified action space (CodeAct).\nIntegrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations (e.g., code execution results) through multi-turnâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xingyaoww/code-act.","url":"https://huggingface.co/datasets/xingyaoww/code-act","creator_name":"Xingyao Wang","creator_url":"https://huggingface.co/xingyaoww","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Atcgpt-Fixed2","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atcgpt-Fixed2\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2.","url":"https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v2","keyword":"instruction","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset version 2.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of single-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2.","url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ru-big-russian-dataset","keyword":"instruct","description":"\n\t\n\t\t\n\t\tBig Russian Dataset\n\t\n\nMade by ZeroAgency.ru - telegram channel.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset size\n\t\n\n\nTrain: 1 710 601 samples (filtered from 2_149_360)\nTest:  18 520 samples (not filtered)\n\n\n\t\n\t\t\n\t\n\t\n\t\tEnglish\n\t\n\nThe Big Russian Dataset is a combination of various primarily Russianâ€‘language datasets. With some sort of reasoning!\nThe dataset was deduplicated, cleaned, scored using gpt-4.1 and filtered.\n\n\t\n\t\t\n\t\n\t\n\t\tÐ ÑƒÑÑÐºÐ¸Ð¹\n\t\n\nBig Russian Dataset - Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹ Ñ€ÑƒÑÑÐºÐ¸Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚. ÐšÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸Ñ Ð¸Ð·â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-big-russian-dataset.","url":"https://huggingface.co/datasets/ZeroAgency/ru-big-russian-dataset","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tSWE-Bench Verified O1 Dataset\n\t\n\n\n\t\n\t\t\n\t\tExecutive Summary\n\t\n\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities using their native tool calling capabilities on the SWE-Bench Verified dataset, achieving a 45.8% success rate across 500 test instances.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset was generated using the CodeAct framework, which aims to improve codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results.","url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"structural-weaver","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tStructural Weaver Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nTraining dataset for fine-tuning language models on structural tension methodology and the creative process as developed by Robert Fritz.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSize: 45 high-quality conversational examples\nFormat: ChatML (system/user/assistant conversations)\nDomain: Creative process, structural tension, goal achievement\nLanguage: English\nLicense: MIT\n\n\n\t\n\t\t\n\t\tContent Areas\n\t\n\n\nStructural Tension: Understanding the dynamic betweenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jgwill/structural-weaver.","url":"https://huggingface.co/datasets/jgwill/structural-weaver","creator_name":"Jean Guillaume","creator_url":"https://huggingface.co/jgwill","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"walt","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/canTooDdev/walt.","url":"https://huggingface.co/datasets/canTooDdev/walt","creator_name":"Boris Marion-Dorier","creator_url":"https://huggingface.co/canTooDdev","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"philosophy-culture-translations-html-csv","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tAI-Culture Philosophy and Culture Translations CSV + HTML Corpus\n\t\n\nThe corpus contains an exceptionally diverse range of cultural, philosophical, and literary texts, available in 12 major languages. Among other topics, there is extensive engagement with the ethics and aesthetics of artificial intelligence and its cultural and philosophical implications, as well as connections between AI and philosophy of language and philosophy of mind.\nThis project is maintained by a non-profitâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI-Culture-Commons/philosophy-culture-translations-html-csv.","url":"https://huggingface.co/datasets/AI-Culture-Commons/philosophy-culture-translations-html-csv","creator_name":"AIâ€‘Cultureâ€‘Commons","creator_url":"https://huggingface.co/AI-Culture-Commons","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text-classification","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"fine-tome-100k-nondual","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tfine_tome_100k_nondual\n\t\n\nA non-dual reformulation of the mlabonne/FineTome-100k dataset.All assistant outputs (from: gpt) have been rewritten into impersonal, non-dual language using OpenAI models.User inputs and other roles remain unchanged.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSource: FineTome-100k  \nSize: ~100,000 conversations (JSONL, one per line)  \nFormat: ShareGPT-style conversations, with fields:{\n  \"conversations\": [\n    {\"from\": \"user\", \"value\": \"User message...\"},\n    {\"from\": \"gpt\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/marciodiaz/fine-tome-100k-nondual.","url":"https://huggingface.co/datasets/marciodiaz/fine-tome-100k-nondual","creator_name":"Marcio Diaz","creator_url":"https://huggingface.co/marciodiaz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"ginecologia-venezuela","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tGinecologÃ­a Venezuela Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescripciÃ³n del Dataset\n\t\n\nEste dataset contiene 250 instrucciones especializadas en ginecologÃ­a y obstetricia, enfocadas especÃ­ficamente en el contexto de la salud pÃºblica venezolana. EstÃ¡ diseÃ±ado para entrenar modelos de lenguaje en el dominio mÃ©dico ginecolÃ³gico con consideraciones especÃ­ficas del sistema de salud venezolano.\n\n\t\n\t\t\n\t\tContenido\n\t\n\n\nTamaÃ±o: 250 ejemplos de instrucciones\nIdioma: EspaÃ±ol (Venezuela)\nDominio: GinecologÃ­a yâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yensonalvi6/ginecologia-venezuela.","url":"https://huggingface.co/datasets/yensonalvi6/ginecologia-venezuela","creator_name":"Yenson Key Batatima Alviarez","creator_url":"https://huggingface.co/yensonalvi6","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Spanish","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Fineweb-Instruct","keyword":"instruction","description":"We convert the pre-training corpus from Fineweb-Edu (https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) to instruction following format. We select a subset with quality filter and then use GPT-4 to extract instruction-following pairs. The dataset contains roughly 16M instruction pairs. The basic concept is similar to MAmmoTH2 (https://arxiv.org/abs/2405.03548). \n\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you use dataset useful, please cite the following paper:\n@article{yue2024mammoth2â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Fineweb-Instruct.","url":"https://huggingface.co/datasets/TIGER-Lab/Fineweb-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10M - 100M","json"],"keywords_longer_than_N":true},
	{"name":"ChemData700K_preprocess","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tChemData700K Preprocessed Dataset\n\t\n\nThis dataset is a preprocessed version of the AI4Chem/ChemData700K dataset.\n\n\t\n\t\t\n\t\tPreprocessing Steps\n\t\n\n\nFiltering: The dataset was filtered to include only samples that are not part of a conversation and have no top-level instruction. Specifically, only rows where history is empty ([]) and instruction is null/empty were kept.\nFormatting: The output column was prefixed with #### .\nColumn Renaming: The input and output columns were renamed toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/ChemData700K_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/ChemData700K_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"TypaRP-16x1k","keyword":"multiturn","description":"I publish the TypaRP-16x1k dataset generated using Synthetic-Alpaca as a pipeline and LumimaidV0.2-8B as a generator.This dataset compromises 1024 samples / rows, each with a system prompt declaring a roleplay:\nFollowing is a roleplay between two characters the user will provide in the next message.\nMarkdown (**strong**, *italic*, \"stuff characters say\", et cetera) is supported and should be used.\n\nFollowed by two characters specified by the user, and further 14 messages (hence 16x1k, 16â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Hamzah-Asadullah/TypaRP-16x1k.","url":"https://huggingface.co/datasets/Hamzah-Asadullah/TypaRP-16x1k","creator_name":"Hamzah Asadullah","creator_url":"https://huggingface.co/Hamzah-Asadullah","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"tool-use-multiturn-reasoning","keyword":"multiturn","description":"interstellarninja/tool-use-multiturn-reasoning dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/interstellarninja/tool-use-multiturn-reasoning","creator_name":"interstellarninja","creator_url":"https://huggingface.co/interstellarninja","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"amharic-llm-training-data","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tAmharic LLM Training Dataset\n\t\n\nComplete production-ready Amharic dataset for large language model training and deployment.\n\n\t\n\t\t\n\t\tðŸš€ Quick Start for Deployment\n\t\n\nfrom datasets import load_dataset\n\n# Load the complete dataset\ndataset = load_dataset(\"YoseAli/amharic-llm-training-data\")\n\n# Access splits\ntrain_data = dataset[\"train\"]  # 761,501 samples\ntest_data = dataset[\"test\"]    # 84,612 samples\n\nprint(f\"Training samples: {len(train_data):,}\")\nprint(f\"Test samples: {len(test_data):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/YoseAli/amharic-llm-training-data.","url":"https://huggingface.co/datasets/YoseAli/amharic-llm-training-data","creator_name":"Yosef Ali","creator_url":"https://huggingface.co/YoseAli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Amharic","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"medra-tool-reasoning","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tðŸ§  Medra Tool Reasoning Dataset\n\t\n\nA comprehensive dataset designed for training conversational AI models with advanced tool-use and reasoning capabilities.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Summary\n\t\n\nMedra Tool Reasoning is a curated and optimized dataset containing 71,336 high-quality conversations that demonstrate sophisticated tool selection, reasoning, and execution patterns. The dataset merges and refines three leading tool-use datasets to create an optimal training resource for conversationalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/drwlf/medra-tool-reasoning.","url":"https://huggingface.co/datasets/drwlf/medra-tool-reasoning","creator_name":"Alexandru Lupoi","creator_url":"https://huggingface.co/drwlf","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","other","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleand","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca-cleand.","url":"https://huggingface.co/datasets/aaaalon/alpaca-cleand","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-Probe","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\nè®­ç»ƒé›† (SFT Dataset)ï¼šåŒ…å« 5,287 æ¡é«˜è´¨é‡é—®ç­”å¯¹ï¼Œç”¨äºŽæ¨¡åž‹å¾®è°ƒã€‚\næ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFT Dataset)ï¼šï¼ˆæŽ¨èï¼‰ è¿™æ˜¯æœ¬æ•°æ®é›†çš„å‡çº§ç‰ˆã€‚æˆ‘ä»¬è®¾è®¡å¹¶åº”ç”¨äº†ä¸¤é˜¶æ®µçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œä¸ºæ¯ä¸€æ¡æ•°æ®éƒ½æ³¨å…¥äº†é«˜è´¨é‡çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡æ¨¡åž‹çš„é€»è¾‘æŽ¨ç†ä¸Žæ·±åº¦åˆ†æžèƒ½åŠ›ã€‚\nè¯„ä¼°é›† (Evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-Probe.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-Probe","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"tool-calling-mix","keyword":"instruction-tuning","description":"\nThis is a dataset for fine-tuning a language model to use tools. I combined sources from various other tool calling datasets and added some non-tool calling examples to prevent catastrophic forgetting.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\n\t\tMotivation\n\t\n\nThis dataset was created to address the need for a diverse, high-quality dataset for training language models in tool usage. By combining multiple sources and including non-tool examples, it aims to produce models that can effectively use toolsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/younissk/tool-calling-mix.","url":"https://huggingface.co/datasets/younissk/tool-calling-mix","creator_name":"Youniss Kandah","creator_url":"https://huggingface.co/younissk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","other","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LLama3Flashcard","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nEste dataset contiene pares de instrucciones, entradas y salidas diseÃ±adas para generar tarjetas de memoria (flashcards) educativas. Es ideal para modelos de generaciÃ³n de texto enfocados en la creaciÃ³n de contenidos educativos.\n\n\t\n\t\t\n\t\tColumnas\n\t\n\n\ninstruction: La instrucciÃ³n dada alâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Michito97/LLama3Flashcard.","url":"https://huggingface.co/datasets/Michito97/LLama3Flashcard","creator_name":"Adrian De La Cruz","creator_url":"https://huggingface.co/Michito97","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"xyrus-cosmic-training-dataset","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tXyrus Cosmic Training Dataset\n\t\n\n\n\t\n\t\t\n\t\tðŸŒŒ Overview\n\t\n\nThis dataset was used to fine-tune Xyrus Cosmic GPT-OSS:20B, creating a personality-rich AI assistant with a distinctive cosmic/mystical persona while maintaining safety alignment.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Statistics\n\t\n\n\nTotal Examples: 20\nCategories:\nPhilosophical/Cosmic: 5 examples\nSafety Refusals: 3 examples\nGeneral Helpful: 12 examples\n\n\nAverage Response Length: 267 characters\nUnique Instructions: 20\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Design Philosophyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ToddLLM/xyrus-cosmic-training-dataset.","url":"https://huggingface.co/datasets/ToddLLM/xyrus-cosmic-training-dataset","creator_name":"Todd Deshane","creator_url":"https://huggingface.co/ToddLLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"VIS","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weiwei888/VIS.","url":"https://huggingface.co/datasets/weiwei888/VIS","creator_name":"shiweiwei","creator_url":"https://huggingface.co/weiwei888","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Arc-ATLAS-Teach-v1","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tArc-ATLAS-Teach\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis revision bundles 624 high-quality adaptive teaching examples that were generated and validated with the latest five-pass pipeline. Every dialogue walks through the full instructional arcâ€”probe, draft plan, checkpoint feedback, revised plan, and final solutionâ€”so the teaching policy observes the complete adjustment process without ever seeing the canonical answer. Probe turns capture the studentâ€™s diagnostic attempt, teacher plans andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Arc-Intelligence/Arc-ATLAS-Teach-v1.","url":"https://huggingface.co/datasets/Arc-Intelligence/Arc-ATLAS-Teach-v1","creator_name":"Arc Intelligence","creator_url":"https://huggingface.co/Arc-Intelligence","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"CRAFT-RecipeGen","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCRAFT-RecipeGen\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail.\n\n4 synthetic dataset sizes (S, M, L, XL) are available.\nCompared to other synthetically generated datasets with the CRAFT framework, this task did not scale similarly well and we do not match the performance of generalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen.","url":"https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"math-reasoning-ift-pairs","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tReasoning-IFT Pairs (Math Domain)\n\t\n\n\n  \n\n\n\n\n  \n  \n\n\nThis dataset provides the largest set of IFT and Reasoning answers pairs for a set of math queries (cf: general-domain).\nIt is based on the Llama-Nemotron-Post-Training dataset, an extensive and high-quality collection of math instruction fine-tuning data.  \nWe curated 150k queries from the math subset of Llama-Nemotron-Post-Training, which covers multiple domains of math questions.For each query, we used Qwen/Qwen3-235B-A22B, whichâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/When-Does-Reasoning-Matter/math-reasoning-ift-pairs.","url":"https://huggingface.co/datasets/When-Does-Reasoning-Matter/math-reasoning-ift-pairs","creator_name":"When Does Reasoning Matter ?","creator_url":"https://huggingface.co/When-Does-Reasoning-Matter","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"diffractgpt_jarvis_dft","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tdiffractgpt_jarvis_dft (XRD Spectra â†’ Structure)\n\t\n\nGoal. Map a textual description of a material (composition + XRD peaks, etc.) to a compact crystal structure description (lattice lengths, angles, fractional coordinates, atom types).\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nid (str): source identifier (e.g., POSCAR/JVASP id).\ninstruction (str): the generic instruction.\ninput (str): material description (composition, XRD, etc.).\noutput (str): target structure text (lattice lengths/angles + fractionalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/knc6/diffractgpt_jarvis_dft.","url":"https://huggingface.co/datasets/knc6/diffractgpt_jarvis_dft","creator_name":"Kamal Choudhary","creator_url":"https://huggingface.co/knc6","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","ðŸ‡ºðŸ‡¸ Region: US","materials-science","text-to-structure"],"keywords_longer_than_N":true},
	{"name":"code-translation","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Noxus09/code-translation.","url":"https://huggingface.co/datasets/Noxus09/code-translation","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"quantum-llm-instruct-subject-knowledge","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tkalkiAI3000/quantum-llm-instruct-subject-knowledge\n\t\n\nThis dataset augments BoltzmannEntropy/QuantumLLMInstruct with a new field subject_knowledge,\nautomatically generated from each exampleâ€™s main_domain, sub_domain, and problem using GPT-5.\n\n\t\n\t\t\n\t\tContents\n\t\n\n\ntrain.json (rows: 5150): Preserves original fields and adds:\nsubject_knowledge (string): 3â€“6 concise lines summarizing definitions, governing equations,\nassumptions/scales, and a typical solution strategy relevant to theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kalkiai3000/quantum-llm-instruct-subject-knowledge.","url":"https://huggingface.co/datasets/kalkiai3000/quantum-llm-instruct-subject-knowledge","creator_name":"Kalki AI","creator_url":"https://huggingface.co/kalkiai3000","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Discord-Dialogues","keyword":"multi-turn","description":"This is a clone of mookiezi/Discord-Dialogues.\n\n  \n\n\n\nDiscord-Dialogues is a large-scale dataset of anonymized Discord conversations from late spring to early fall 2025 for training and evaluating realistic conversational AI models in a ChatML-friendly format.\n\nThis dataset contains 7.3 million exchanges spread out over 16 million turns, with more than 139 million words.\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nMixed single and multi-turn exchanges\nHuman-only dialogues (no bots)\nFiltered for ToS andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aaronmoo12/Discord-Dialogues.","url":"https://huggingface.co/datasets/aaronmoo12/Discord-Dialogues","creator_name":"mookiezi","creator_url":"https://huggingface.co/aaronmoo12","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned_uz","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a translation of the alpaca-cleaned dataset into Uzbek (Latin), using the GPT-4o mini API.\n","url":"https://huggingface.co/datasets/bizb0630/alpaca-cleaned_uz","creator_name":"Behruz Izbaev","creator_url":"https://huggingface.co/bizb0630","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Uzbek","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"InstructTranslation-EN-ES","keyword":"instruction","description":"\n\t\n\t\t\n\t\tTranslation of Instructions EN-ES\n\t\n\nThis dataset contains prompts and answers from teknium/OpenHermes-2.5 translated to Spanish using GPT-4-0125-preview. The dataset is intended to be used for training a model to translate instructions from English to Spanish.\nThe dataset is formatted with the TowerInstruct format. It is ready to finetune a Tower translation model. if you want the raw translations, there are available here:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Iker/InstructTranslation-EN-ES.","url":"https://huggingface.co/datasets/Iker/InstructTranslation-EN-ES","creator_name":"Iker GarcÃ­a-Ferrero","creator_url":"https://huggingface.co/Iker","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"StructFlowBench","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tStructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following\n\t\n\n\n  \n    ðŸ“ƒ Paper\n  \n  â€¢\n  \n    ðŸ¤— Dataset\n  \n  â€¢\n  \n    ðŸ–¥ï¸ Code\n  \n\n\n\n\t\n\t\t\n\t\t1. Updates\n\t\n\n\n2025/02/26: We enhanced the code documentation on GitHub with detailed implementation guidelines.\n2025/02/24: We submitted our paper to Hugging Face's Daily Papers.\n2025/02/23: We released StructFlowBench dataset on huggingface.\n2025/02/20: We released the first version of our paper along with the dataset andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jinnan/StructFlowBench.","url":"https://huggingface.co/datasets/Jinnan/StructFlowBench","creator_name":"Jinnan Li","creator_url":"https://huggingface.co/Jinnan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ScanBot","keyword":"instruction-following","description":" \n\n\t\n\t\t\n\t\tScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems\n\t\n\n\n\nThis dataset is presented in the paper ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems.\n\n\t\n\t\t\n\t\tðŸ§  Dataset Summary\n\t\n\nScanBot is a dataset for instruction-conditioned, high-precision surface scanning with robots. Unlike existing datasets that focus on coarse tasks like grasping or navigation, ScanBot targets industrial laser scanning, where sub-millimeter accuracy and parameterâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ed1son/ScanBot.","url":"https://huggingface.co/datasets/ed1son/ScanBot","creator_name":"zhiling chen","creator_url":"https://huggingface.co/ed1son","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","English","apache-2.0","arxiv:2505.17295","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"scam-dialogue","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tSynthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset is a collection of simulated phone conversation between two parties, labeled as either scam or non-scam interactions. The dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of three columns:\n\ndialogue: Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/scam-dialogue.","url":"https://huggingface.co/datasets/BothBosu/scam-dialogue","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"ScaleDiff-Math","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tScaleDiff-Math Dataset\n\t\n\n\n    \n    \n    \n\n\nThis repository contains the ScaleDiff-Math dataset, which is the official implementation for ScaleDiff, a simple yet effective pipeline designed to scale the creation of challenging mathematical problems to enhance the reasoning capabilities of Large Reasoning Models (LRMs). Our method addresses the scarcity of high-quality, difficult training data, which is often manually created and is therefore costly and difficult to scale.\n\t\n\t\t\n\t\tPaperâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QizhiPei/ScaleDiff-Math.","url":"https://huggingface.co/datasets/QizhiPei/ScaleDiff-Math","creator_name":"QizhiPei","creator_url":"https://huggingface.co/QizhiPei","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1-v2","keyword":"instruct","description":"This is a modified version of ZeroAgency/ru-thinking-reasoning-r1 with addition of Egor-AI/CoT-XLang dataset.\nCombined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nÐ¢Ñ‹ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2.","url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"TextBooksPersonaHub-FR","keyword":"instruction","description":"\n\t\n\t\t\n\t\tTextBooksPersonaHub\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe TextBooksPersonaHub dataset is an extension of the proj-persona/PersonaHub dataset, created using the technique described in the paper Textbooks Are All You Need II. This dataset contains synthetically generated \"textbook-like\" passages tailored in french to specific personas, aimed at enhancing language model training with high-quality and diverse content.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nThe original personasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/drodin/TextBooksPersonaHub-FR.","url":"https://huggingface.co/datasets/drodin/TextBooksPersonaHub-FR","creator_name":"nacer","creator_url":"https://huggingface.co/drodin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","French","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"finewebedu-guru","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tFineWebEdu-Guru\n\t\n\n\nA high-quality dataset collection for training interactive expert large language models (LLMs)\nThese are general educational web content with no specific focus\nTo specialize the LLMs for your own data, you'll need other models to generate the training data such as\nagentlans/Qwen2.5-1.5B-Refiner\nagentlans/Qwen2.5-1.5B-Instruct-Conversation-Maker\nagentlans/Qwen2.5-1.5B-Instruct-Multiple-Choice-Maker\nagentlans/Qwen2.5-1.5B-Instruct-Short-Answer-Maker\n\n\n\n\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/finewebedu-guru.","url":"https://huggingface.co/datasets/agentlans/finewebedu-guru","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","odc-by","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1","keyword":"instruct","description":"Combined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nÐ¢Ñ‹ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ: <think> Ð¢Ð²Ð¾Ð¸ Ð¼Ñ‹ÑÐ»Ð¸ Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ </think> \nÐ¢Ð²Ð¾Ð¹ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹Ð¹â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1.","url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"wisconsin-building-codes-qa","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tWisconsin Building Codes Q&A Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 13200 question-answer pairs focused on Wisconsin building codes, specifically covering:\n\nBuilding code requirements and regulations\nAdministrative procedures and enforcement\nConstruction standards and specifications\nPermit processes and compliance\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nTraining samples: 11880\nValidation samples: 1320\n\nEach sample contains:\n\ninstruction: A question about Wisconsinâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/carlscape/wisconsin-building-codes-qa.","url":"https://huggingface.co/datasets/carlscape/wisconsin-building-codes-qa","creator_name":"brian weiss","creator_url":"https://huggingface.co/carlscape","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"MT-Consistency","keyword":"multi-turn","description":"Dataset Summary\nMT-Consistency is a curated benchmark to evaluate how well LLMs maintain correct judgments over multi-turn interactions.\nIt combines controlled multiple-choice questions with eight types of follow-up prompts that probe susceptibility to pressure, tone, and misinformation, and supports two evaluation protocolsâ€”Repetitive and Diverse follow-ups. The benchmark is used to compute metrics such as Position-Weighted Consistency (PWC) and to test mitigation methods likeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yubol/MT-Consistency.","url":"https://huggingface.co/datasets/yubol/MT-Consistency","creator_name":"yubo li","creator_url":"https://huggingface.co/yubol","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"scam_dialogues","keyword":"multi-turn","description":"adamtc/scam_dialogues dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/adamtc/scam_dialogues","creator_name":"Adam Unknown","creator_url":"https://huggingface.co/adamtc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","Vietnamese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"oasst1_v2_tr","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tTÃ¼rkÃ§e oasst1 Veri Seti (Optimizasyonlu Ã‡eviri)\n\t\n\nBu repository, popÃ¼ler OpenAssistant Conversations (oasst1) veri setinin, yapay zeka modellerinin ince ayarÄ± (fine-tuning) iÃ§in optimize edilmiÅŸ TÃ¼rkÃ§e Ã§evirisini iÃ§ermektedir. Toplamda 50,624 adet girdi-Ã§Ä±ktÄ± Ã§ifti bulunmaktadÄ±r.\n\n\t\n\t\t\n\t\tVeri Seti AÃ§Ä±klamasÄ±\n\t\n\nBu Ã§alÄ±ÅŸma, oasst1 veri setindeki Ä°ngilizce \"prompt-response\" (istek-yanÄ±t) Ã§iftlerini alarak, Google'Ä±n Gemini serisi modelleri aracÄ±lÄ±ÄŸÄ±yla akÄ±cÄ± ve doÄŸal bir TÃ¼rkÃ§eyeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/limeXx/oasst1_v2_tr.","url":"https://huggingface.co/datasets/limeXx/oasst1_v2_tr","creator_name":"yusuf uzun","creator_url":"https://huggingface.co/limeXx","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","question-answering","Turkish","apache-2.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"ultrachat","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tUltraChat Conversations Dataset\n\t\n\nThis dataset contains 1,468,346 multi-turn conversations from UltraChat, processed to preserve the original conversational structure and optimized for training conversational AI models.\n\n\t\n\t\t\n\t\tðŸŽ¯ Dataset Format\n\t\n\nEach conversation record contains:\n\nid: Sequential conversation ID (1, 2, 3, ...)\nsource: \"ultra\" \nlanguage: \"english\"\ndata: JSON string containing conversation turns array\n\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Statistics\n\t\n\n\nTotal Conversations: 1,468,346â€¦ See the full description on the dataset page: https://huggingface.co/datasets/metythorn/ultrachat.","url":"https://huggingface.co/datasets/metythorn/ultrachat","creator_name":"metythorn penn","creator_url":"https://huggingface.co/metythorn","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"ZamAI-Pashto-Mega-Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“š ZamAI-Pashto-Mega-Dataset\n\t\n\nAuthor: Yaqoob TasalOrganization: ZamAI â€” AI for Pashto, Dari, and Afghan LanguagesLicense: Apache-2.0  \n\n\n\t\n\t\t\n\t\tðŸŒ Overview\n\t\n\nThe ZamAI-Pashto-Mega-Dataset is the largest unified Pashto language dataset curated and cleaned by ZamAI.It merges multiple high-quality corpora into a single instruction-based format, designed to supercharge Pashto NLP â€” from translation and summarization to dialogue and content generation.\n\n\n\t\n\t\t\n\t\tðŸ“¦ Dataset Detailsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasal9/ZamAI-Pashto-Mega-Dataset.","url":"https://huggingface.co/datasets/tasal9/ZamAI-Pashto-Mega-Dataset","creator_name":"Yaqoob Tasal","creator_url":"https://huggingface.co/tasal9","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Pashto","apache-2.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"S1_QFFT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“˜ S1â€“QFFT\n\t\n\nS1â€“QFFT is a question-free version of the original simplescaling/s1K-1.1 dataset, designed for QFFT training workflows.\n\n\t\n\t\t\n\t\tðŸ” Description\n\t\n\nThis dataset discards the original questions and any system instructions, keeping only the reasoning completions as supervision. It is especially useful for models that aim to learn when and how to think, rather than just how to answer.\nThe dataset is fully converted into a format compatible with LLaMA-Factory training.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lwl-uestc/S1_QFFT.","url":"https://huggingface.co/datasets/lwl-uestc/S1_QFFT","creator_name":"Wanlong Liu","creator_url":"https://huggingface.co/lwl-uestc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"metallurgy-qa","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMetallurgy and Materials Science Knowledge Extraction Dataset\n\t\n\nThis repository contains a dataset generated from parsed books related to various aspects of metallurgy, materials science, and engineering. The dataset is designed for fine-tuning Large Language Models (LLMs) for Question-Answering (QA) tasks in the domain of metallurgy and materials science.\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe dataset includes content derived from technical books in the field of metallurgy and materialsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AbdulrhmanEldeeb/metallurgy-qa.","url":"https://huggingface.co/datasets/AbdulrhmanEldeeb/metallurgy-qa","creator_name":"Eldeeb","creator_url":"https://huggingface.co/AbdulrhmanEldeeb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","question-answering","closed-domain-qa","closed-book-qa","machine-generated"],"keywords_longer_than_N":true},
	{"name":"MergeIT","keyword":"instruction-finetuning","description":"XCloudFance/MergeIT dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/XCloudFance/MergeIT","creator_name":"CAI HONGYI","creator_url":"https://huggingface.co/XCloudFance","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-3.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Bhagwat-Corpus-Data","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tBhagwat Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Bhagwat Corpus is a synthetic dataset of approximately 90,000 examples designed for instruction-tuning large language models (LLMs) to generate Vedic philosophical responses grounded in scriptural tradition. Each example consists of:\n\nA synthetic user question\nA relevant Sanskrit shloka (verse) from the Mahabharata or Ramayana\nAn English translation of the shloka\nA generated explanation and status for the response\n\nThe dataset is basedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PyPranav/Bhagwat-Corpus-Data.","url":"https://huggingface.co/datasets/PyPranav/Bhagwat-Corpus-Data","creator_name":"Pranav Sunil","creator_url":"https://huggingface.co/PyPranav","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Sanskrit","apache-2.0","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"tulu-v2-sft-mixture-filtered","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“˜ SCAR-Filtered Instruction-Tuning Subset (10k from Tulu-v2)\n\t\n\nThis dataset contains 10,000 high-quality instructionâ€“response pairs filtered from the allenai/tulu-v2-sft-mixture dataset using the SCAR data selection method.\nSCAR (Style Consistency-Aware Response Ranking) is a novel data selection framework accepted to ACL 2025 (main conference). It ranks and filters instructionâ€“response pairs based on style consistency, resulting in a more reliable and efficient subset forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lizhuang144/tulu-v2-sft-mixture-filtered.","url":"https://huggingface.co/datasets/lizhuang144/tulu-v2-sft-mixture-filtered","creator_name":"Zhuang Li","creator_url":"https://huggingface.co/lizhuang144","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SFT","keyword":"instruct","description":"Total rows : 14727342\n","url":"https://huggingface.co/datasets/Yuchan5386/SFT","creator_name":"Yuchan","creator_url":"https://huggingface.co/Yuchan5386","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Korean","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"lean-six-sigma-qna-v1","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tLean Six Sigma QnA Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 102 high-quality question-answer pairs focused on Lean Six Sigma methodologies, business process improvement, and supply chain optimization. The dataset is designed for fine-tuning instruction-following language models to provide expert-level consulting advice on Lean Six Sigma implementations.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nid: Unique identifier for each sample (1-102)\ninstruction:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cw18/lean-six-sigma-qna-v1.","url":"https://huggingface.co/datasets/cw18/lean-six-sigma-qna-v1","creator_name":"Clarence Wong","creator_url":"https://huggingface.co/cw18","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"MentalHealth-Support","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tImportant Note\n\t\n\nThis dataset is created from merging two datasets from different sources and has been formatted according to the \"messages\", \"role\", \"content\" chat format. I do not claim any ownership of this dataset. \nKeep in mind that this dataset is entirely synthetic. It is not fully representative of real therapy situations. If you are training an LLM therapist keep in mind the limitations of LLMs and highlight those limitations to users in a responsible manner.\nSince Mentalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ShivomH/MentalHealth-Support.","url":"https://huggingface.co/datasets/ShivomH/MentalHealth-Support","creator_name":"Shivom Hatalkar","creator_url":"https://huggingface.co/ShivomH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"dx7-patches-and-prompts","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tYamaha DX7 Synthesizer Patches with AI-Generated Prompts\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a comprehensive, multi-task dataset designed for fine-tuning language models to understand and generate synthesizer patches for the Yamaha DX7.\nThe dataset contains over 20,000 examples across three distinct but related tasks, making it ideal for creating models that can not only generate patches but also understand and reason about their structure and validity.\n\n\t\n\t\t\n\t\tHow the Data Wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ccerati/dx7-patches-and-prompts.","url":"https://huggingface.co/datasets/ccerati/dx7-patches-and-prompts","creator_name":"Carlo Cerati","creator_url":"https://huggingface.co/ccerati","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K<n<100K","Text"],"keywords_longer_than_N":true},
	{"name":"constitucion-venezuela-250","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tConstituciÃ³n de Venezuela - Dataset de Instrucciones\n\t\n\n\n\t\n\t\t\n\t\tDescripciÃ³n del Dataset\n\t\n\nEste dataset contiene 250 pares de instrucciÃ³n-respuesta basados en la ConstituciÃ³n de la RepÃºblica Bolivariana de Venezuela de 1999. Ha sido diseÃ±ado especÃ­ficamente para el entrenamiento de modelos de lenguaje en tareas de comprensiÃ³n y respuesta sobre contenido constitucional venezolano.\n\n\t\n\t\t\n\t\tInformaciÃ³n del Dataset\n\t\n\n\nIdioma: EspaÃ±ol (es)\nLicencia: CC-BY-4.0\nTamaÃ±o: 250 ejemplos\nFormato:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-250.","url":"https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-250","creator_name":"Niurka Oropeza","creator_url":"https://huggingface.co/niuvaroza","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Spanish","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MainData","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºŽGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bufanlin/MainData.","url":"https://huggingface.co/datasets/bufanlin/MainData","creator_name":"bufanlin","creator_url":"https://huggingface.co/bufanlin","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"express-legal-funding-reviews","keyword":"instruction-tuning","description":"A curated collection of real customer feedback and company replies for Express Legal Funding.  This dataset is designed for training and evaluating language models on tasks such as sentiment classification,  customer interaction modeling, and instruction tuning in the legal funding domain.\n","url":"https://huggingface.co/datasets/expresslegalfunding/express-legal-funding-reviews","creator_name":"Express Legal Funding","creator_url":"https://huggingface.co/expresslegalfunding","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","sentiment-classification","language-modeling","human"],"keywords_longer_than_N":true},
	{"name":"ifc-bim-alpaca-improved","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tIFC-BIM Improved Alpaca Dataset\n\t\n\nA high-quality instruction-following dataset for Industry Foundation Classes (IFC) and Building Information Modeling (BIM).\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains carefully curated and validated instruction-response pairs about IFC concepts, schemas, and BIM practices. It has been cleaned and improved from an original dataset of 545k+ entries.\n\n\t\n\t\t\n\t\tDataset Quality\n\t\n\n\nQuality Score: 4.6/5.0 (improved from 3.0)\nLLM Validation: 95.1%â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dietmar2020/ifc-bim-alpaca-improved.","url":"https://huggingface.co/datasets/Dietmar2020/ifc-bim-alpaca-improved","creator_name":"Dietmar Grabowski ","creator_url":"https://huggingface.co/Dietmar2020","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"fin-term-instruct","keyword":"instruction","description":"\n\t\n\t\t\n\t\tðŸ“˜ fin-term-instruct: í•œêµ­ì–´ ê¸ˆìœµ ìš©ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹\n\t\n\nfin-term-instructëŠ” í•œêµ­ì–´ ê¸ˆìœµ ìš©ì–´ ì„¤ëª…ì— íŠ¹í™”ëœ instruct-style ì§ˆë¬¸-ì‘ë‹µ ë°ì´í„°ì…‹ìž…ë‹ˆë‹¤.Metaì˜ LLaMA ì‹œë¦¬ì¦ˆ ë“± ëŒ€í˜• ì–¸ì–´ëª¨ë¸(LLM)ì„ í•œêµ­ì–´ ê¸ˆìœµ ì±—ë´‡ìœ¼ë¡œ íŠœë‹í•˜ê¸° ìœ„í•´ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n\n\t\n\t\t\n\t\tðŸ“¦ ì›ë³¸ ì¶œì²˜: AI í—ˆë¸Œ\n\t\n\nì´ ë°ì´í„°ëŠ” AI í—ˆë¸Œì˜ **\"ê¸ˆìœµÂ·ë²•ë¥  ë¬¸ì„œ ê¸°ê³„ë…í•´ ë°ì´í„°\"**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•í•˜ì˜€ìŠµë‹ˆë‹¤.\n\nðŸ“‚ ì›ë³¸ ì£¼ì†Œ: AI í—ˆë¸Œ â€“ ê¸ˆìœµÂ·ë²•ë¥  ë¬¸ì„œ ê¸°ê³„ë…í•´ ë°ì´í„°\në°ì´í„° êµ¬ì¶•ë…„ë„: 2022ë…„\nì´ êµ¬ì¶•ëŸ‰: 400,000ê±´\në°ì´í„° í˜•ì‹: JSON (ì§€ë¬¸ - ì§ˆë¬¸ - ì •ë‹µ êµ¬ì„±)\n\n\n\t\n\t\t\n\t\tðŸ” ì‚¬ìš© ë²”ìœ„\n\t\n\n\nì „ì²´ ë°ì´í„° ì¤‘ **ê¸ˆìœµê²½ì œ ë¶„ì•¼(ì•½ 17.3%)**ë§Œ ì„ ë³„í•˜ì—¬ ì‚¬ìš©\nê¸°ì¡´ MRC í˜•íƒœì—ì„œ instruction-style QA í¬ë§·ìœ¼ë¡œ ìž¬ê°€ê³µ\nGPT ê¸°ë°˜ ìš”ì•½Â·ì •ì œë¥¼ í†µí•´ ê°„ê²°í•œ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ í†µì¼â€¦ See the full description on the dataset page: https://huggingface.co/datasets/taetae030/fin-term-instruct.","url":"https://huggingface.co/datasets/taetae030/fin-term-instruct","creator_name":"leetaehee","creator_url":"https://huggingface.co/taetae030","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"synapse-set-50k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ§  SynapseSet-50K\n\t\n\nSynapseSet-50K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nðŸ”¬ 100% synthetic, non-clinical data. Intended for academic and researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-50k.","url":"https://huggingface.co/datasets/NextGenC/synapse-set-50k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Turkish","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CodeForce_SAGA","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCodeForce-SAGA: A Self-Correction-Augmented Code Generation Dataset\n\t\n\nCodeForce-SAGA is a large-scale, high-quality training dataset designed to enhance the code generation and problem-solving capabilities of Large Language Models (LLMs). All problems and solutions are sourced from the competitive programming platform Codeforces.\nThis dataset is built upon the SAGA (Strategic Adversarial & Constraint-differential Generative workflow) framework, a novel human-LLM collaborativeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/opencompass/CodeForce_SAGA.","url":"https://huggingface.co/datasets/opencompass/CodeForce_SAGA","creator_name":"OpenCompass","creator_url":"https://huggingface.co/opencompass","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"PersonaHub","keyword":"instruction","description":"\n\t\n\t\t\n\t\tPolish Synthetic Personas\n\t\n\n","url":"https://huggingface.co/datasets/kubasoltys/PersonaHub","creator_name":"Kuba Soltys","creator_url":"https://huggingface.co/kubasoltys","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","fill-mask","table-question-answering"],"keywords_longer_than_N":true},
	{"name":"ualpaca-gpt4","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-cleaned\"\n\t\n\nThis dataset contains Ukrainian Instruction-Following translated by facebook/nllb-200-3.3B\nThe dataset was originaly shared in this repository: https://github.com/tloen/alpaca-lora\n\n\t\n\t\t\n\t\tLicensing Information\n\t\n\nThe dataset is available under the Creative Commons NonCommercial (CC BY-NC 4.0).\n","url":"https://huggingface.co/datasets/nikes64/ualpaca-gpt4","creator_name":"MrNikes","creator_url":"https://huggingface.co/nikes64","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Ukrainian","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-1.5-Instruct-Data","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-1.5 Instruction Data\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset, LLaVA-OneVision-1.5-Instruct, was collected and integrated during the development of LLaVA-OneVision-1.5. LLaVA-OneVision-1.5 is a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. This meticulously curated 22M instruction dataset (LLaVA-OneVision-1.5-Instruct) is part of a comprehensive andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Instruct-Data.","url":"https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Instruct-Data","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2509.23661","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"AlpacaDataCleaned","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alexl83/AlpacaDataCleaned.","url":"https://huggingface.co/datasets/alexl83/AlpacaDataCleaned","creator_name":"Alessandro Lannocca","creator_url":"https://huggingface.co/alexl83","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"IndoTechGPT-Data","keyword":"instruction","description":"\n\t\n\t\t\n\t\tIndonesian Tech QA Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Indonesian Tech QA Dataset is a manually curated collection of questionâ€“answer pairs written in Bahasa Indonesia, developed for instruction-following and educational QA tasks.Each entry includes a natural question, an instructional prompt, and a detailed answer that explains modern technology topics in a clear and accessible way.\nThis dataset supports the fine-tuning of Indonesian large language models (LLMs) to enhance theirâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Alice-AI-Net/IndoTechGPT-Data.","url":"https://huggingface.co/datasets/Alice-AI-Net/IndoTechGPT-Data","creator_name":"AI Net","creator_url":"https://huggingface.co/Alice-AI-Net","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Indonesian","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Trendyol-Cybersecurity-Instruction-Tuning-Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTrendyol Cybersecurity Instruction Tuning Dataset (GPT Format)\n\t\n\nA conversational dataset in GPT/OpenAI messages format, converted from Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset. Designed for training language models in advanced cyber-defense and security principles.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 53,201 high-quality instruction-tuning examples focused on cybersecurity, converted to the standard GPT conversation format (messages) forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tuandunghcmut/Trendyol-Cybersecurity-Instruction-Tuning-Dataset.","url":"https://huggingface.co/datasets/tuandunghcmut/Trendyol-Cybersecurity-Instruction-Tuning-Dataset","creator_name":"DÅ©ng VÃµ","creator_url":"https://huggingface.co/tuandunghcmut","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"gerlayqa-stgb-paraphrased","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tGerLayQA-StGB Paraphrased ðŸ‡©ðŸ‡ªâš–ï¸\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a paraphrased and restructured version of the GerLayQA StGB (Strafgesetzbuch / German Criminal Code) dataset, specifically prepared for fine-tuning large language models on German criminal law question-answering tasks.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n1,207 high-quality QA pairs about German Criminal Law (StGB)\nParaphrased questions to remove plagiarism while maintaining legal accuracy\nStructured 7-section answersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DomainLLM/gerlayqa-stgb-paraphrased.","url":"https://huggingface.co/datasets/DomainLLM/gerlayqa-stgb-paraphrased","creator_name":"DomainLLM","creator_url":"https://huggingface.co/DomainLLM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","German","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Tengentoppa-sft-v4.0","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tTengentoppa Corpus for SFT (çµ±åˆæ—¥æœ¬èªžInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ)\n\t\n\n\n\t\n\t\t\n\t\tðŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦\n\t\n\nã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€17å€‹ã®æ—¥æœ¬èªžinstruction-followingãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆã—ãŸå¤§è¦æ¨¡ãªæ•™å¸«ã‚ã‚Šå­¦ç¿’(SFT)ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã‚„å¯¾è©±å½¢å¼ã‚’å«ã‚€213,265ä»¶ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³-ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒšã‚¢ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n\t\n\t\t\n\t\tä¸»ãªç‰¹å¾´\n\t\n\n\nç·ã‚µãƒ³ãƒ—ãƒ«æ•°: 213,265ä»¶\nãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: ç´„438MB (åœ§ç¸®å‰)\nè¨€èªž: æ—¥æœ¬èªž\nãƒ©ã‚¤ã‚»ãƒ³ã‚¹: CC-BY-4.0\nã‚¿ã‚¹ã‚¯: Question Answering, Instruction Following\n\n\n\t\n\t\t\n\t\tðŸ“‹ ãƒ‡ãƒ¼ã‚¿å½¢å¼\n\t\n\nå„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã¯ä»¥ä¸‹ã®æ§‹é€ ã‚’æŒã¡ã¾ã™ï¼š\n{\n  \"instruction\": \"æŒ‡ç¤ºã¾ãŸã¯è³ªå•æ–‡\",\n  \"output\": \"å¿œç­”ã¾ãŸã¯å›žç­”æ–‡\"\n}\n\n\n\t\n\t\t\n\t\n\t\n\t\tãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰èª¬æ˜Ž\n\t\n\n\n\t\n\t\t\nãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\nåž‹\nèª¬æ˜Ž\n\n\n\t\t\ninstructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v4.0.","url":"https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v4.0","creator_name":"Taisei Ozaki","creator_url":"https://huggingface.co/DeL-TaiseiOzaki","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Japanese","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Truth","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/abhishekbisaria/Truth.","url":"https://huggingface.co/datasets/abhishekbisaria/Truth","creator_name":"Abhishek Bisaria","creator_url":"https://huggingface.co/abhishekbisaria","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"prodigy-cleaned","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Apex-X/prodigy-cleaned.","url":"https://huggingface.co/datasets/Apex-X/prodigy-cleaned","creator_name":"Aadhithya","creator_url":"https://huggingface.co/Apex-X","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"wildjailbreak-africa","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tWildJailbreak Africa\n\t\n\nThis dataset contains translations of 50,000 samples from the ai2-adapt-dev/tulu_v3.9_wildjailbreak_decontaminated_50k dataset into 5 African languages. The dataset is designed for instruction tuning and safety training of language models in low-resource African languages.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe original WildJailbreak dataset is a synthetic safety-training dataset containing both vanilla (direct harmful requests) and adversarial (complex adversarialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CraneAILabs/wildjailbreak-africa.","url":"https://huggingface.co/datasets/CraneAILabs/wildjailbreak-africa","creator_name":"Crane AI Labs","creator_url":"https://huggingface.co/CraneAILabs","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","allenai/wildjailbreak","English","Acoli"],"keywords_longer_than_N":true},
	{"name":"dolphin-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDolphin-ru ðŸ¬\n\t\n\nThis is translated version of ehartford/dolphin into Russian.\n","url":"https://huggingface.co/datasets/d0rj/dolphin-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"translated_polish_alpaca","keyword":"instruction-finetuning","description":"The dataset was translated into Polish using this model: \"gsarti/opus-mt-tc-en-pl\"\n\n\t\n\t\t\n\t\tHow to use\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Aspik101/translated_polish_alpaca\")\n\n","url":"https://huggingface.co/datasets/Aspik101/translated_polish_alpaca","creator_name":"Hubert","creator_url":"https://huggingface.co/Aspik101","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Polish","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"LOGIC-701","keyword":"instruct","description":"\n\t\n\t\t\n\t\tLOGIC-701 Benchmark\n\t\n\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nThis benchmark is also part of anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701.","url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Russian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"grad_school_math_instructions_fr_Mixtral","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDataset Card for grad_school_math_instructions_fr_Mixtral\n\t\n\nThis dataset was made thanks to the instruction of the vigogne's dataset but the output were generated with Mixtral-8x7B-Instruct instead of GPT3.5 to make it open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","url":"https://huggingface.co/datasets/AIffl/grad_school_math_instructions_fr_Mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-bn","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned-bn\n\t\n\n\n\nThis is a cleaned bengali translated version of the original Alpaca Dataset released by Stanford. \n\n\t\n\t\t\n\t\tUses\n\t\n\nimport datasets\ndataset = datasets.load_dataset(\"abrarfahim/alpaca-cleaned-bn\")\nprint(dataset[0])\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{'system_prompt': 'You are a virtual assistant, deliver a comprehensive response.',\n 'qas_id': 'YY9S5K',\n 'question_text': '\"à¦¸à¦¨à§à¦¦à§‡à¦¹\" à¦¶à¦¬à§à¦¦à§‡à¦° à¦¸à¦ à¦¿à¦• à¦ªà§à¦°à¦¤à¦¿à¦¶à¦¬à§à¦¦ à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¨ à¦•à¦°à§à¦¨à¥¤',\n 'orig_answer_texts': '\"à¦¸à¦¨à§à¦¦à§‡à¦¹\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn.","url":"https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn","creator_name":"fahim abrar","creator_url":"https://huggingface.co/abrarfahim","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Bengali","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"norwegian-alpaca","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tNB Alpaca Norwegian BokmÃ¥l\n\t\n\nThis dataset is a translation to Norwegian BokmÃ¥l of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\n","url":"https://huggingface.co/datasets/NbAiLab/norwegian-alpaca","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ultrachat_200k_dutch","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for UltraChat 200k Dutch\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch.","url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"turkish-wikipedia-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTÃ¼rkÃ§e Kamu KurumlarÄ± ve Tarih Sohbet Veri Seti\n\t\n\nBu veri seti, TÃ¼rkiye'deki kamu kurumlarÄ±, bakanlÄ±klar, devlet organlarÄ±, resmi semboller ve tarihi figÃ¼rler hakkÄ±nda yapÄ±landÄ±rÄ±lmÄ±ÅŸ TÃ¼rkÃ§e sohbet verileri iÃ§ermektedir. Veriler, gÃ¼venilir ve tarafsÄ±z bir kaynak olan TÃ¼rkÃ§e Vikipedi'den otomatik olarak Ã§Ä±karÄ±lmÄ±ÅŸ ve bÃ¼yÃ¼k dil modellerini (LLM) ince ayar (fine-tuning) iÃ§in uygun bir formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.\nHer bir Ã¶rnek, bir \"sistem\" talimatÄ±, bir \"kullanÄ±cÄ±\" sorgusu ve birâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kaan39/turkish-wikipedia-dataset.","url":"https://huggingface.co/datasets/kaan39/turkish-wikipedia-dataset","creator_name":"Kaan KÃ¶se","creator_url":"https://huggingface.co/kaan39","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Turkish","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-data\"\n\t\n\nAll of the work is done by this team. \n\n\t\n\t\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\t\n\t\t\n\t\tChinese Dataset\n\t\n\nFound here\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Liâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data.","url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data-zh","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-data-zh\"\n\t\n\nAll of the work is done by this team. \n\n\t\n\t\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\t\n\t\t\n\t\tEnglish Dataset\n\t\n\nFound here\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Liâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh.","url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-id-cleaned","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Indonesian Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the Indonesian translated version of the cleaned original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cahya/alpaca-id-cleaned.","url":"https://huggingface.co/datasets/cahya/alpaca-id-cleaned","creator_name":"Cahya Wirawan","creator_url":"https://huggingface.co/cahya","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Indonesian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_cthulhu_full","keyword":"instruction-finetuning","description":"Based on the yahma/alpaca-cleaned data set.\nEvery answer in the original data set was rewritten, as if the answer was given by a dedicated, high ranking cultist in the Cult of Cthulhu. The instructions were to keep the replies factually the same, but to spice things up with references to the Cthulhu Mythos in its various forms.\nThis was done as part of a learning experience, but I am making the data set available for others to play with as well.\n","url":"https://huggingface.co/datasets/theprint/alpaca_cthulhu_full","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"WildChat-4.8M","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for WildChat-4.8M\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nInteractive Search Tool: https://wildvisualizer.com  \nWildChat paper: https://arxiv.org/abs/2405.01470  \nWildVis paper: https://arxiv.org/abs/2409.03753  \nPoint of Contact: Yuntian Deng\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nWildChat-4.8M is a collection of 3,199,860 conversations between human users and ChatGPT. This version only contains non-toxic user inputs and ChatGPT responses, as flagged by the OpenAI Moderations API orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/WildChat-4.8M.","url":"https://huggingface.co/datasets/allenai/WildChat-4.8M","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","odc-by","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"llama3weitiao","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZHEZIXI/llama3weitiao.","url":"https://huggingface.co/datasets/ZHEZIXI/llama3weitiao","creator_name":"LIUJUN","creator_url":"https://huggingface.co/ZHEZIXI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-FC-Reasoning-v1","keyword":"instruction-tuning","description":"\n\n\t\n\t\t\n\t\tArcosoph-FC-Reasoning-v1\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the Arcosoph-FC-Reasoning-v1, a meticulously crafted dataset designed for supervised fine-tuning (SFT) of language models, especially microsoft/Phi-3-mini-4k-instruct. The dataset is provided in a ready-to-use JSON Lines (.jsonl) format, where each line represents a single training example.\nThe primary goal of this dataset is to teach a model not just to respond to queries, but to reason, plan, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-v1.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-v1","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"solidity_vulnerability_audit_dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tSolidity Vulnerability Audit Dataset\n\t\n\n\nOrganization: gitmate AI\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Solidity Vulnerability Audit Dataset is a curated collection of Solidity smart contract code snippets paired with expert-written vulnerability audits. Each entry presents a real or realistic smart contract scenario, and the corresponding analysis identifies security vulnerabilities or confirms secure patterns. The dataset is designed for instruction-tuned large language models (LLMs) toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GitmateAI/solidity_vulnerability_audit_dataset.","url":"https://huggingface.co/datasets/GitmateAI/solidity_vulnerability_audit_dataset","creator_name":"GitmateAI","creator_url":"https://huggingface.co/GitmateAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Atma7-Beta","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma7-Beta\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma7-Beta.","url":"https://huggingface.co/datasets/HappyAIUser/Atma7-Beta","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"instruction-tags","keyword":"instruction","description":"ai2-adapt-dev/instruction-tags dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/ai2-adapt-dev/instruction-tags","creator_name":"AI2 Adapt Dev","creator_url":"https://huggingface.co/ai2-adapt-dev","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Buzz-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"UltraChat2_en","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tllm-lab/UltraChat2_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue\n\nTotal examples: 103933\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/UltraChat2_en\", split=\"train\")\nprint(ds[0])\n\n","url":"https://huggingface.co/datasets/llm-lab/UltraChat2_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701-instruct","keyword":"instruct","description":"\n\t\n\t\t\n\t\tLOGIC-701 (instruct)\n\t\n\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\n","url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Russian","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ru-instruct","keyword":"instruct","description":"\n\t\n\t\t\n\t\tÐšÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°\n\t\n\nÐ¡ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð¸Ð· Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ñ… Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð¾Ð², Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸. ÐžÑ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð² Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° (ÑÐ¿Ð°ÑÐ¸Ð±Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Den4ikAI/nonsense_gibberish_detector). Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½ SimHash'Ð¾Ð¼.\nÐžÐ±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð¹ Ð½Ð° Ð½Ñ‘Ð¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð·Ð°Ð²Ñ‘Ð·, in progress.\n\n\t\n\t\t\n\t\tÐ¡Ð¾ÑÑ‚Ð°Ð²\n\t\n\nÐ¡Ð¾Ð±Ñ€Ð°Ð» Ð¸Ð· ÑÑ‚Ð¸Ñ… Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ñ…:\n\nd0rj/OpenOrca-ru (Ð¾Ñ‚ Open-Orca/OpenOrca)\nd0rj/OpenHermes-2.5-ru (Ð¾Ñ‚ teknium/OpenHermes-2.5)\nd0rj/dolphin-ru (Ð¾Ñ‚ ehartford/dolphin)\nd0rj/alpaca-cleaned-ru (Ð¾Ñ‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct.","url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","machine-generated","found","translated"],"keywords_longer_than_N":true},
	{"name":"SOC-2508-MULTI","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tDataset Card for Multilingual Synthetic Online Conversations\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains multilingual translations of the Synthetic Online Conversations (SOC-2508) dataset. Each conversation from the original dataset has been translated into French, Italian, German, Spanish, providing over 1,180 synthetically generated, multi-turn online conversations in multiple languages.\nThe translations were generated using google/gemma-3n-E4B-it with vLLM as the inferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/SOC-2508-MULTI.","url":"https://huggingface.co/datasets/marcodsn/SOC-2508-MULTI","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","French","Italian","German","Spanish"],"keywords_longer_than_N":true},
	{"name":"Atma3.2-ShareGPT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3.2-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT.","url":"https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"NEET_Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for NEET Previous Year Questions (PYQs)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of Previous Year Questions (PYQs) from India's National Eligibility cum Entrance Test (NEET-UG), a highly competitive entrance examination for medical and dental courses. The questions cover the subjects of Chemistry, Biology, and Physics.\nEach entry in the dataset is structured as a JSON object containing the question, four multiple-choice options, the key for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kshitij-PES/NEET_Dataset.","url":"https://huggingface.co/datasets/Kshitij-PES/NEET_Dataset","creator_name":"Kshitij","creator_url":"https://huggingface.co/Kshitij-PES","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"amazon-ml","keyword":"instruction-finetuning","description":"Vinuuu/amazon-ml dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Vinuuu/amazon-ml","creator_name":"Vinayak Goyal","creator_url":"https://huggingface.co/Vinuuu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMDCure-12k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"Supernova","keyword":"instruct","description":"Supernova is a dataset containing general synthetic chat data from the best available open-source models.\nThe 2024-09-27 version contains:\n\n178.2k rows of synthetic chat responses generated using Llama 3.1 405b Instruct.\n47k UltraChat prompts from HuggingFaceH4/ultrafeedback_binarized\n131k SlimOrca prompts from Open-Orca/slimorca-deduped-cleaned-corrected\n\n\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","url":"https://huggingface.co/datasets/sequelbox/Supernova","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"luth-sft","keyword":"instruction","description":"\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis dataset includes all the data used to fine-tune Luth-0.6B-Instruct and Luth-1.7B-Instruct, enhancing their French capabilities on tasks such as instruction following, mathematics, and general knowledge. The models also improved in English thanks to knowledge transfer between the two languages.\nIt contains ~338M tokens in French. Our data scripts are available on GitHub.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tScholar\n\t\n\nBy Kurakura AI: Dataset Link.Builtâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kurakurai/luth-sft.","url":"https://huggingface.co/datasets/kurakurai/luth-sft","creator_name":"KuraKura AI","creator_url":"https://huggingface.co/kurakurai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","French","odc-by","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kushala/alpaca.","url":"https://huggingface.co/datasets/Kushala/alpaca","creator_name":"Mummigatti","creator_url":"https://huggingface.co/Kushala","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMDCure-12k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"zen-identity","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tZen Identity Dataset\n\t\n\nThis dataset contains identity training data for the Zen family of AI models.\n\n\t\n\t\t\n\t\tModels Covered\n\t\n\n\nZen Nano (0.6B): Ultra-efficient edge computing model\nZen Eco (3B): Balanced performance and efficiency\nZen Coder (7B): Specialized for code generation\nZen Omni (14B): Versatile multi-domain model\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\ninstruction: The user's question\noutput: The model's response\nmodel: Which Zen model this example is for\ntext:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zenlm/zen-identity.","url":"https://huggingface.co/datasets/zenlm/zen-identity","creator_name":"Zen LM","creator_url":"https://huggingface.co/zenlm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"unstacked","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked.","url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","arxiv:2403.08763","arxiv:2310.05914"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-SFT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\n**æ³¨: æœ¬æ•°æ®é›†ä¸ºä¸å¸¦CoTæ ‡æ³¨çš„æ•°æ®é›†ï¼Œå¦‚æžœæ‚¨è¦å¯¹DeekSeek R1ã€Qwen3ç³»åˆ—ç­‰å…·æœ‰å†…åµŒçš„CoTè¾“å‡ºçš„æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼Œä¸ºäº†é¿å…æ¨¡åž‹å‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œè¯·ç§»æ­¥è‡³æœ¬é¡¹ç›®çš„æ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFT Dataset) **\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\næœ¬æ•°æ®é›† (åŽŸå§‹è®­ç»ƒé›†): acnul/Mining-Engineering-SFT åŒ…å« 5,287â€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-SFT.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-SFT","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-fr","keyword":"instruction-finetuning","description":"TPM-28/alpaca-cleaned-fr dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/TPM-28/alpaca-cleaned-fr","creator_name":"TPM-28","creator_url":"https://huggingface.co/TPM-28","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"SkunkworksAI-reasoning-0.01-ko","keyword":"instruction","description":"SkunkworksAI/reasoning-0.01 ë°ì´í„°ì…‹ì„ nayohan/llama3-instrucTrans-enko-8b ëª¨ë¸ì„ ì‚¬ìš©í•´ ë²ˆì—­í–ˆìŠµë‹ˆë‹¤.\nThanks for SkunkworksAI and nayohan.\n\n\n\t\n\t\t\n\t\tì›ë³¸\n\t\n\n\n\t\n\t\t\n\t\treasoning-0.01 subset\n\t\n\nsynthetic dataset of reasoning chains for a wide variety of tasks.\nwe leverage data like this across multiple reasoning experiments/projects.\nstay tuned for reasoning models and more data.\nThanks to Hive Digital Technologies (https://x.com/HIVEDigitalTech) for their compute support in this project and beyond.\n","url":"https://huggingface.co/datasets/youjunhyeok/SkunkworksAI-reasoning-0.01-ko","creator_name":"ìœ ì¤€í˜","creator_url":"https://huggingface.co/youjunhyeok","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Korean","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"SWE-Bench-Verified-O1-reasoning-high-results","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tSWE-Bench Verified O1 Dataset\n\t\n\n\n\t\n\t\t\n\t\tExecutive Summary\n\t\n\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities on the SWE-Bench Verified dataset, achieving a 28.8% success rate across 500 test instances.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset was generated using the CodeAct framework, which aims to improve code generation through enhanced action-based reasoning.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results.","url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"GroundedRAG","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for GroundedRAG\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGroundedRAG is a large-scale training dataset specifically crafted for fine-tuning language models and Retrieval-Augmented Generation (RAG) systems. It contains 572,598 carefully curated question-answer pairs with rich multi-document contexts, sourced from six high-quality datasets. Each training example features a question, a comprehensive answer, and supporting context from multiple documentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shanaka95/GroundedRAG.","url":"https://huggingface.co/datasets/shanaka95/GroundedRAG","creator_name":"Shanaka Anuradha Samarakoon","creator_url":"https://huggingface.co/shanaka95","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMDCure-72k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"phishing-email","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tCEAS-08 Email Phishing Detection Instruction Dataset\n\t\n\nThis dataset contains instruction-following conversations for email phishing detection, generated from the CEAS-08 email dataset using multiple large language models. It's designed for fine-tuning conversational AI models on cybersecurity tasks.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset transforms raw email data into structured instruction-following conversations where an AI security analyst analyzesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/luongnv89/phishing-email.","url":"https://huggingface.co/datasets/luongnv89/phishing-email","creator_name":"Luong NGUYEN","creator_url":"https://huggingface.co/luongnv89","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMDCure-72k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"odia-instruction-dataset","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tOdia Instruction Following Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a comprehensive Odia language instruction-following dataset designed for training conversational AI models, chatbots, and instruction-following systems in Odia (à¬“à¬¡à¬¼à¬¿à¬†). The dataset contains high-quality instruction-response pairs that enable models to understand and follow instructions in the Odia language.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Odia (à¬“à¬¡à¬¼à¬¿à¬†)\nTotal Records: 324,560\nFormat:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/abhilash88/odia-instruction-dataset.","url":"https://huggingface.co/datasets/abhilash88/odia-instruction-dataset","creator_name":"Abhilash Sahoo","creator_url":"https://huggingface.co/abhilash88","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Oriya","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"eg-legal-qa","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tArabic Legal Dataset - Legal Question-Answering\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nQuestion-answering dataset for Arabic legal texts with instruction-following format for training conversational AI models.\nThis dataset contains 5,230 examples of qa data derived from Egyptian legal texts, including criminal law, civil law, procedural law, and personal status law. The dataset is designed for training and evaluating Arabic legal AI models.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Arabicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fr3on/eg-legal-qa.","url":"https://huggingface.co/datasets/fr3on/eg-legal-qa","creator_name":"Ahmed Mardi","creator_url":"https://huggingface.co/fr3on","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Arabic","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"IFDecorator","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tIFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards\n\t\n\nProject page | Paper | Code\nHigh-quality synthetic datasets engineered for Reinforcement Learning with Verifiable Rewards (RLVR)\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŒŸ Why This Dataset?\n\t\n\nThis repository contains two complementary datasets with different synthesis approaches and difficulty distributions:\n\n\t\n\t\n\t\n\t\tðŸ“Š Core Dataset (train.jsonl + val.jsonl)\n\t\n\n\nðŸŽ¯ Controlled difficulty: 3,625 training + 200 validationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/guox18/IFDecorator.","url":"https://huggingface.co/datasets/guox18/IFDecorator","creator_name":"guox18","creator_url":"https://huggingface.co/guox18","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"FineEdit_bench","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tFineEdit Dataset\n\t\n\nPaper | GitHub Repository\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis repository contains InstrEditBench, a high-quality benchmark dataset introduced in the paper Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications.\nLarge Language Models (LLMs) have significantly advanced natural language processing,\ndemonstrating strong capabilities in tasks such\nas text generation, summarization, and reasoning. Recently, their potential for automating\npreciseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YimingZeng/FineEdit_bench.","url":"https://huggingface.co/datasets/YimingZeng/FineEdit_bench","creator_name":"Zeng","creator_url":"https://huggingface.co/YimingZeng","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"math-reasoning-sft","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMathematical Reasoning SFT Dataset\n\t\n\nThis dataset contains mathematical reasoning problems and solutions in instruction-following format, designed for supervised fine-tuning of language models.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset follows the Alpaca format with three fields:\n\ninstruction: Mathematical problem statement\ninput: Empty string (not used)\noutput: Detailed solution with step-by-step reasoning and final answer in \\boxed{} format\n\n\n\t\n\t\t\n\t\tExample\n\t\n\n{\n  \"instruction\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/est-ai/math-reasoning-sft.","url":"https://huggingface.co/datasets/est-ai/math-reasoning-sft","creator_name":"ESTsoft AI","creator_url":"https://huggingface.co/est-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"jaquad-sft","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tsoftjapan/jaquad-sft\n\t\n\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦\n\t\n\nã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€JaQuADï¼ˆJapanese Question Answering Datasetï¼‰ã‚’SFTï¼ˆSupervised Fine-Tuningï¼‰å½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚æ—¥æœ¬èªžã®è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸinstruction tuningç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©³ç´°\n\t\n\n\nè¨€èªž: æ—¥æœ¬èªž\nã‚¿ã‚¹ã‚¯: è³ªå•å¿œç­”ã€instruction tuning\nå½¢å¼: SFTï¼ˆinstruction/input/outputï¼‰\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿: 31,748ä»¶\næ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 3,939ä»¶\nåˆè¨ˆ: 35,687ä»¶\n\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿å½¢å¼\n\t\n\nå„ã‚µãƒ³ãƒ—ãƒ«ã¯ä»¥ä¸‹ã®å½¢å¼ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š\n{\n  \"id\": \"tr-000-00-000\",\n  \"instruction\": \"æ¬¡ã®æ–‡è„ˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚å¯èƒ½ãªã‚‰çŸ­ãæ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚\",\n  \"input\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/softjapan/jaquad-sft.","url":"https://huggingface.co/datasets/softjapan/jaquad-sft","creator_name":"hayashirui","creator_url":"https://huggingface.co/softjapan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Japanese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Instruct-Ecommerce-Combined-Dataset","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tInstruct Dataset for Ecommerce: Multi Instruction Fine Tuning\n\t\n\nThis dataset is part of the Instruct Dataset for Ecommerce collection. It is specifically tailored for the task of Multi Instruction Fine Tuning, intended for fine-tuning instruction-following models like LLaMA3.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nDomain: Ecommerce\nTask: Multi Instruction Fine Tuning\nSplits: Train/Test\nSize: 368313 train samples, 39146 test samples\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/commotion/Instruct-Ecommerce-Combined-Dataset.","url":"https://huggingface.co/datasets/commotion/Instruct-Ecommerce-Combined-Dataset","creator_name":"Commotion","creator_url":"https://huggingface.co/commotion","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Celestia","keyword":"instruct","description":"Celestia is a dataset containing science-instruct data.\nThe 2024-10-30 version contains:\n\n126k rows of synthetic science-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","url":"https://huggingface.co/datasets/sequelbox/Celestia","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"nemotron-post-training-samples-splits","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tNemotron Post-Training Samples with Train/Val/Test Splits\n\t\n\nThis dataset contains structured train/validation/test splits from the nvidia/Llama-Nemotron-Post-Training-Dataset, with both tagged and untagged versions for different training scenarios.\n\n\t\n\t\t\n\t\tAttribution\n\t\n\nThis work is derived from the Llama-Nemotron-Post-Training-Dataset-v1.1 by NVIDIA Corporation, licensed under CC BY 4.0.\nOriginal Dataset: nvidia/Llama-Nemotron-Post-Training-Dataset\nOriginal Authors: NVIDIAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples-splits.","url":"https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples-splits","creator_name":"Brandon Tong","creator_url":"https://huggingface.co/brandolorian","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","nvidia/Llama-Nemotron-Post-Training-Dataset","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ultrafrench","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDataset Card for ultrafrench\n\t\n\nThis dataset offers a french translation of the small sample of instructions from HuggingFaceH4/ultrachat_200k translated in french. The generations were made with Mistral large to make the output open-source\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/ultrafrench","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"WildChat-1M","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for WildChat\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nPaper: https://arxiv.org/abs/2405.01470\n\nInteractive Search Tool: https://wildvisualizer.com (paper)\n\nLicense: ODC-BY\n\nLanguage(s) (NLP): multi-lingual\n\nPoint of Contact: Yuntian Deng\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nWildChat is a collection of 1 million conversations between human users and ChatGPT, alongside demographic data, including state, country, hashed IP addresses, and request headers. We collected WildChat byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/WildChat-1M.","url":"https://huggingface.co/datasets/allenai/WildChat-1M","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","text2text-generation","odc-by","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data-zh","keyword":"instruction","description":"\n\t\n\t\t\n\t\tæ•°æ®é›†æè¿°\n\t\n\nè¯¥æ•°æ®é›†ä¸ºGPT-4ç”Ÿæˆçš„ä¸­æ–‡æ•°æ®é›†ï¼Œç”¨äºŽLLMçš„æŒ‡ä»¤ç²¾è°ƒå’Œå¼ºåŒ–å­¦ä¹ ç­‰ã€‚\n\n\t\n\t\t\n\t\tæ•°æ®é›†åŠ è½½æ–¹å¼\n\t\n\nfrom modelscope.msdatasets import MsDataset\nds = MsDataset.load(\"alpaca-gpt4-data-zh\", namespace=\"AI-ModelScope\", split=\"train\")\nprint(next(iter(ds)))\n\n\n\t\n\t\t\n\t\n\t\n\t\tæ•°æ®åˆ†ç‰‡\n\t\n\næ•°æ®å·²ç»é¢„è®¾äº†trainåˆ†ç‰‡ã€‚\n\n\t\n\t\t\n\t\n\t\n\t\tæ•°æ®é›†ç‰ˆæƒä¿¡æ¯\n\t\n\næ•°æ®é›†å·²ç»å¼€æºï¼Œlicenseä¸ºCC BY NC 4.0ï¼ˆä»…ç”¨äºŽéžå•†ä¸šåŒ–ç”¨é€”ï¼‰ï¼Œå¦‚æœ‰è¿åç›¸å…³æ¡æ¬¾ï¼Œéšæ—¶è”ç³»modelscopeåˆ é™¤ã€‚\n\n\t\n\t\t\n\t\n\t\n\t\tå¼•ç”¨æ–¹å¼\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Li, Pengcheng He, Michelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/invergent/alpaca-gpt4-data-zh.","url":"https://huggingface.co/datasets/invergent/alpaca-gpt4-data-zh","creator_name":"Invergent","creator_url":"https://huggingface.co/invergent","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"instruct","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\nThis dataset focuses on challenging multi-turn conversations and contains:\n\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Titanium2-DeepSeek-R1","keyword":"instruct","description":"Click here to support our open-source dataset and model releases!\nTitanium2-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n32.4k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraformâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1.","url":"https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"ATC-ShareGPT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for ATC-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT.","url":"https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"multiturn","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\nThis dataset focuses on challenging multi-turn conversations and contains:\n\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"collabllm-20q-interactive","keyword":"multiturn","description":"aditijb/collabllm-20q-interactive dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/aditijb/collabllm-20q-interactive","creator_name":"Aditi","creator_url":"https://huggingface.co/aditijb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"muInstruct-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\tmuInstruct-ru\n\t\n\nTranslated instructions from EleutherAI/muInstruct into Russian using gemini-flash-1.5-8b.\n","url":"https://huggingface.co/datasets/d0rj/muInstruct-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","EleutherAI/muInstruct","Russian"],"keywords_longer_than_N":true},
	{"name":"synapse-set-100k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ§  SynapseSet-100K\n\t\n\nSynapseSet-100K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nðŸ”¬ 100% synthetic, non-clinical data. Intended for academic and researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-100k.","url":"https://huggingface.co/datasets/NextGenC/synapse-set-100k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Turkish","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ARPO-RL-DeepSearch-1K","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tARPO Dataset: Agentic Reinforced Policy Optimization\n\t\n\nThis repository contains the datasets used in the paper Agentic Reinforced Policy Optimization.\nPaper Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. To bridge this gapâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dongguanting/ARPO-RL-DeepSearch-1K.","url":"https://huggingface.co/datasets/dongguanting/ARPO-RL-DeepSearch-1K","creator_name":"KABI","creator_url":"https://huggingface.co/dongguanting","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"chat","keyword":"multi-turn","description":"neverland-th/chat dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/neverland-th/chat","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"gemma-en-bg","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tGemma EN-BG Translation Dataset (ChessInstruct Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset contains 45,313 English to Bulgarian subtitle translation pairs in ChessInstruct format for fine-tuning Gemma3-270m using the Unsloth framework. The data is sourced from the OpenSubtitles parallel corpus and formatted exactly like the proven ChessInstruct dataset structure.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nâœ… ChessInstruct Compatible: Uses exact task/input/expected_output/KIND format\nðŸš€ Gemma3-270mâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zantag/gemma-en-bg.","url":"https://huggingface.co/datasets/zantag/gemma-en-bg","creator_name":"zantag","creator_url":"https://huggingface.co/zantag","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","English","Bulgarian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"actuary-enough-qa-dataset","keyword":"instruction-following","description":"\n  \n\n\n\t\n\t\t\n\t\tðŸ‘‹ Connect with me on LinkedIn!\n\t\n\n  \n  Manuel Caccone - Actuarial Data Scientist & Open Source Educator\n  Let's discuss actuarial science, AI, and open source projects!\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŽ¯ Actuary Enough - Actuarial Question Simplification Dataset\n\t\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸš© Dataset Description\n\t\n\nThe Actuary Enough Dataset contains examples of complex actuarial and insurance questions that have been simplified and rephrased to improve clarity and accessibility. This dataset is designed toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/manuelcaccone/actuary-enough-qa-dataset.","url":"https://huggingface.co/datasets/manuelcaccone/actuary-enough-qa-dataset","creator_name":"Manuel Caccone","creator_url":"https://huggingface.co/manuelcaccone","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-simplification","monolingual","original","English"],"keywords_longer_than_N":true},
	{"name":"CIDAR","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDataset Card for \"CIDAR\"\n\t\n\n\n\t\n\t\t\n\t\tðŸŒ´CIDAR: Culturally Relevant Instruction Dataset For Arabic\n\t\n\n\n\n   [ Paper - GitHub ]\n\n\n\nCIDAR contains 10,000 instructions and their output. The dataset was created by selecting around 9,109 samples from Alpagasus dataset then translating it to Arabic using ChatGPT. In addition, we append that with around 891 Arabic grammar instructions from the webiste Ask the teacher. All the 10,000 samples were reviewed by around 12 reviewers. \n\n\n\n\n\n\n\t\n\t\t\n\t\tðŸ“šâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arbml/CIDAR.","url":"https://huggingface.co/datasets/arbml/CIDAR","creator_name":"Arabic Machine Learning ","creator_url":"https://huggingface.co/arbml","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Atma8","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma8\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma8.","url":"https://huggingface.co/datasets/HappyAIUser/Atma8","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ThaiQA-v1","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tThaiQA v1\n\t\n\nThaiQA v1 is a Thai Synthetic QA dataset. It was created from synthetic method using open source LLM in Thai language.\nWe used Nvidia Nemotron 4 (340B) to create this dataset.\nTopics:\nTechnology and Gadgets 100\nTravel and Tourism 91\nFood and Cooking 99\nSports and Fitness 50\nArts and Entertainment 24\nHome and Garden 72\nFashion and Beauty 99\nScience and Nature 100\nHistory and Culture 91\nEducation and Learning 99\nPets and Animals 83\nRelationships and Family 78\nPersonalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1.","url":"https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Thai","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LogicIFEval","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tLogicIFEval\n\t\n\nFor evaluation scripts, please refer to our GitHub repository: https://github.com/mianzhang/LogicIF\nThe dataset contains two splits:\n\nfull: Complete benchmark dataset (3,050 instructions)\nmini: Mini version for quick evaluation (749 instructions)\n\nEach line in the JSONL files contains a single evaluation example with the following structure:\n{\n  \"task_id\": \"string\",           // Unique identifier for the problem\n  \"test_case_id\": \"int\",         // Test case number forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/billmianz/LogicIFEval.","url":"https://huggingface.co/datasets/billmianz/LogicIFEval","creator_name":"Mian Zhang","creator_url":"https://huggingface.co/billmianz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"WeLoveYou","keyword":"instruction-finetuning","description":"\nThis is a dataset extracted from WeLoveYou Youtube channel transcripts & synthesized using Cloudflare Workers AI Llama3-70B.\nThe goal is to fine-tune a empathetic Qwen2.5 model\n","url":"https://huggingface.co/datasets/ThomasTheMaker/WeLoveYou","creator_name":"Thomas Nguyen","creator_url":"https://huggingface.co/ThomasTheMaker","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"InstructTTSEval","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tInstructTTSEval\n\t\n\nInstructTTSEval is a comprehensive benchmark designed to evaluate Text-to-Speech (TTS) systems' ability to follow complex natural-language style instructions. The dataset provides a hierarchical evaluation framework with three progressively challenging tasks that test both low-level acoustic control and high-level style generalization capabilities.\n\nGithub Repository: https://github.com/KexinHUANG19/InstructTTSEval\nPaper: InstructTTSEval: Benchmarking Complexâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CaasiHUANG/InstructTTSEval.","url":"https://huggingface.co/datasets/CaasiHUANG/InstructTTSEval","creator_name":"Kexin Huang","creator_url":"https://huggingface.co/CaasiHUANG","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-speech","English","Chinese","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_anatomist","keyword":"instruct","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Anatomist\n\t\n\nThis dataset was extracted from the Web Ontology Language (OWL) \nrepresentation of the Foundational Model of Anatomy [1] using \nOwlready2 to facilitate the extraction of the logical axioms in the FMA\ninto a Controlled Natural Language (CNL) for use in training Medical Large Language Models to be conversant in \ncanonical human anatomy.\nSo, Description Logic expressions such as the following in FMA about fma13245 (Surface of left ventricle):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist.","url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Atma3-ShareGPT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT.","url":"https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"AWE","keyword":"instruction-finetuning","description":"rishiraj/AWE dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/rishiraj/AWE","creator_name":"Rishiraj Acharya","creator_url":"https://huggingface.co/rishiraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"asyncapi_alpaca_dataset","keyword":"instruction-tuning","description":"A fine-tuning dataset based on the Alpaca format for training LLMs to understand and generate AsyncAPI-related content. The dataset includes prompts, instructions, and completions extracted and synthesized from AsyncAPI documentation, GitHub discussions, tutorials, and code examples. It is ideal for training models in event-driven API development, code generation, and instruction following within the AsyncAPI domain.\n","url":"https://huggingface.co/datasets/rohith-yarramala/asyncapi_alpaca_dataset","creator_name":"Rohith Yarramala","creator_url":"https://huggingface.co/rohith-yarramala","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-generation","other","AsyncAPI Documentation","GitHub AsyncAPI Discussions","AsyncAPI Tutorials and Community Q&A"],"keywords_longer_than_N":true},
	{"name":"ClinVar-STXBP1-NLP-Dataset","keyword":"instruction-tuning","description":"language:\n\nen\n\n\n\n\t\n\t\t\n\t\tstxbp1_clinvar_curated\n\t\n\n_ Curated STXBP1 and related variant records from ClinVar (24Million), ready for LLM and biomedical NLP applications._ \n\n\nUpdated Jun 10th 2025. - Fields containing {null} or {} were removed.\n\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nA curated, LLM-friendly dataset of STXBP1 and related variant records from ClinVar, converted from ClinVar VCF and annotated for clinical, research, rare disease, and advanced AI applications.This resource is suitable forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SkyWhal3/ClinVar-STXBP1-NLP-Dataset.","url":"https://huggingface.co/datasets/SkyWhal3/ClinVar-STXBP1-NLP-Dataset","creator_name":"Adam Freygang","creator_url":"https://huggingface.co/SkyWhal3","license_name":"Public Domain Dedication & License","license_url":"https://scancode-licensedb.aboutcode.org/pddl-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","pddl","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"general-reasoning-ift-pairs","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tReasoning-IFT Pairs (General Domain)\n\t\n\n\n  \n\n\n\n\n  \n  \n\n\n\nThis dataset provides the largest set of IFT and Reasoning answers pairs for a set of general domain queries (cf: math-domain).It is based on the Infinity-Instruct dataset, an extensive and high-quality collection of instruction fine-tuning data.  \nWe curated 900k queries from the 7M_core subset of Infinity-Instruct, which covers multiple domains including general knowledge, commonsense Q&A, coding, and math.For each query, weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/When-Does-Reasoning-Matter/general-reasoning-ift-pairs.","url":"https://huggingface.co/datasets/When-Does-Reasoning-Matter/general-reasoning-ift-pairs","creator_name":"When Does Reasoning Matter ?","creator_url":"https://huggingface.co/When-Does-Reasoning-Matter","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"creative_writing","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for telecomadm1145/creative_writing\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a small-scale instructionâ€“response dataset focused on creative writing tasks.Each example consists of a prompt (instruction specifying writing style, perspective, tone, etc.) and a response (a story segment or novel-like output).  \nThe dataset emphasizes:\n\nCreative Writing (light novel style, emotional narrative, dialogue-driven, descriptive prose).â€¦ See the full description on the dataset page: https://huggingface.co/datasets/telecomadm1145/creative_writing.","url":"https://huggingface.co/datasets/telecomadm1145/creative_writing","creator_name":"t5","creator_url":"https://huggingface.co/telecomadm1145","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"caeden-instruct-ds","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tCaeden Instruct Dataset\n\t\n\nThis dataset combines multiple high-quality question-answering and instruction-following datasets into a unified format.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nTotal Examples: 23,746\nFormat: Instruction-following (instruction, input, output)\nLanguage: English\nLicense: MIT\n\n\n\t\n\t\t\n\t\tSource Datasets\n\t\n\nThis dataset combines examples from:\n\nSQuAD v1.1 & v2.0\nBoolQ\nCommon Gen\nSNLI\nCoQA\nTriviaQA\nAnd many more...\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\n  \"instruction\": \"Question orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/caedencode/caeden-instruct-ds.","url":"https://huggingface.co/datasets/caedencode/caeden-instruct-ds","creator_name":"Caeden Rajoo","creator_url":"https://huggingface.co/caedencode","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"alpaca-style-QnA","keyword":"instruction","description":"\n\t\n\t\t\n\t\tAlpaca-style Question and Answer Dataset\n\t\n\nThis dataset contains question-answer pairs formatted in the Alpaca instruction style, suitable for instruction fine-tuning of language models.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example contains:\n\ninstruction: The question\ninput: Empty string (can be used for context in other applications)\noutput: The answer\ntext: The formatted text using the Alpaca template\n\n\n\t\n\t\t\n\t\tTemplate\n\t\n\nBelow is an instruction that describes a task, paired with an input thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sweatSmile/alpaca-style-QnA.","url":"https://huggingface.co/datasets/sweatSmile/alpaca-style-QnA","creator_name":"amitk17","creator_url":"https://huggingface.co/sweatSmile","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction","qa"],"keywords_longer_than_N":true},
	{"name":"CTI_0.1","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AshishCTI/CTI_0.1.","url":"https://huggingface.co/datasets/AshishCTI/CTI_0.1","creator_name":"ad","creator_url":"https://huggingface.co/AshishCTI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"Open-Conversation-TR","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTurkish Synthetic Conversation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset AÃ§Ä±klamasÄ±\n\t\n\nBu dataset, DeepSeek-V3 API kullanÄ±larak Ã¼retilmiÅŸ yÃ¼ksek kaliteli TÃ¼rkÃ§e sentetik konuÅŸma ve soru-cevap verilerini iÃ§ermektedir. GÃ¼nlÃ¼k hayat, iÅŸ hayatÄ±, aile, alÄ±ÅŸveriÅŸ, restoran, teknoloji, saÄŸlÄ±k, eÄŸitim, yemek ve seyahat kategorilerinde Ã§eÅŸitli input-output Ã§iftleri bulunmaktadÄ±r.\n\n\t\n\t\t\n\t\tDataset Ä°statistikleri\n\t\n\n\nToplam Ã–rnekler: 10\nOrtalama Input UzunluÄŸu: 48.2 karakter\nOrtalama Output UzunluÄŸu: 81.6â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Vyvo/Open-Conversation-TR.","url":"https://huggingface.co/datasets/Vyvo/Open-Conversation-TR","creator_name":"Vyvo","creator_url":"https://huggingface.co/Vyvo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Turkish","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"single-agent-scam-conversations","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of three columns:\n\ndialogue: The transcribed conversation between the caller and receiver.\ntype: The specific type of scam or non-scam interaction.\nlabels: A binary label indicating whether the conversation is aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations.","url":"https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"security_steerability","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tSecurity Steerability & the VeganRibs Benchmark\n\t\n\nSecurity steerability is defined as an LLM's ability to stick to the specific rules and boundaries set by a system prompt, particularly for content that isn't typically considered prohibited.\nTo evaluate this, we developed the VeganRibs benchmark. The benchmark tests an LLM's skill at handling conflicts by seeing if it can follow system-level instructions even when a user's input tries to contradict them.\nVeganRibs works by presentingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/itayhf/security_steerability.","url":"https://huggingface.co/datasets/itayhf/security_steerability","creator_name":"Itay H","creator_url":"https://huggingface.co/itayhf","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Titanium","keyword":"instruct","description":"Titanium is a dataset containing DevOps-instruct data.\nThe 2024-10-02 version contains:\n\n26.6k rows of synthetic DevOps-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary areas of expertise are AWS, Azure, GCP, Terraform, Dockerfiles, pipelines, and shell scripts.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","url":"https://huggingface.co/datasets/sequelbox/Titanium","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"CleverBoi","keyword":"instruction","description":"\n\n\n\t\n\t\t\n\t\tCleverBoi\n\t\n\nThe CleverBoi Collection is based on a number of data sets that emphasize logic, inference, empathy, math and coding.\nThe data set has been formatted to follow the alpaca format (instruction + input -> output) when fine tuning.\n\n\t\n\t\t\n\t\tSource Data Sets\n\t\n\nThe source data sets used in the CleverBoi Collection are listed below, ordered by size.\n\nKK04/LogicInference_OA\nmlabonne/Evol-Instruct-Python-26k\ngarage-bAInd/Open-Platypus\niamtarun/python_code_instructions_18k_alpacaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/theprint/CleverBoi.","url":"https://huggingface.co/datasets/theprint/CleverBoi","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Proyecto","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto.","url":"https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto","creator_name":"Facundo","creator_url":"https://huggingface.co/Facundo-DiazPWT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"bitcoin-investment-advisory-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tBitcoin Investment Advisory Training Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains comprehensive Bitcoin investment advisory training data designed for fine-tuning large language models to provide institutional-grade cryptocurrency investment advice. The dataset consists of 2,437 high-quality instruction-input-output triplets covering Bitcoin market analysis from 2018-01-01 to 2024-12-31.\n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nTotal Samples: 2,437\nDate Range: 2018-01-01 toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tahamajs/bitcoin-investment-advisory-dataset.","url":"https://huggingface.co/datasets/tahamajs/bitcoin-investment-advisory-dataset","creator_name":"Taha Majlesi","creator_url":"https://huggingface.co/tahamajs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"multi-turn_jailbreak_attack_datasets","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tMulti-Turn Jailbreak Attack Datasets\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset was created to compare single-turn and multi-turn jailbreak attacks on large language models (LLMs). The primary goal is to take a single harmful prompt and distribute the harm over multiple turns, making each prompt appear harmless in isolation. This approach is compared against traditional single-turn attacks with the complete prompt to understand their relative impacts and failure modes. The key feature ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tom-gibbs/multi-turn_jailbreak_attack_datasets.","url":"https://huggingface.co/datasets/tom-gibbs/multi-turn_jailbreak_attack_datasets","creator_name":"Tom Gibbs","creator_url":"https://huggingface.co/tom-gibbs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K<n<10K","arxiv:2409.00137","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"KemSU","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸŽ“ Kemerovo State University Instructional QA Dataset (NodeLinker/KemSU)\n\t\n\n\n  \n     \n  \n\n\n  \n    \n  \n  \n    \n  \n\n\n\n\n\t\n\t\n\t\n\t\tðŸ“ Dataset Overview & Splits\n\t\n\nThis dataset provides instructional question-answer (Q&A) pairs meticulously crafted for Kemerovo State University (ÐšÐµÐ¼Ð“Ð£, KemSU), Russia. Its primary purpose is to facilitate the fine-tuning of Large Language Models (LLMs), enabling them to function as knowledgeable and accurate assistants on a wide array of topics concerningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NodeLinker/KemSU.","url":"https://huggingface.co/datasets/NodeLinker/KemSU","creator_name":"Ilya Pereverzin","creator_url":"https://huggingface.co/NodeLinker","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-chatml","keyword":"instruction","description":"\n\t\n\t\t\n\t\tChatML Reformat of yahma/alpaca-cleaned\n\t\n\nI'd like to try instruction-tuning dataset with chat-tuning format.\n\n\t\n\t\t\n\t\tUsage\n\t\n\ndataset = load_dataset(\"pacozaa/alpaca-cleaned-chatml\", split = \"train\")\nprint(dataset[0][\"text\"])\n\n","url":"https://huggingface.co/datasets/pacozaa/alpaca-cleaned-chatml","creator_name":"Sarin Suriyakoon","creator_url":"https://huggingface.co/pacozaa","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ABC-VG-Instruct","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tVG Instruct\n\t\n\nThis is the instruction finetuning dataset for ABC: Achieving better control of multimodal embeddings using VLMs.\nEach element in this dataset contains 4 instruction-captions pairs for images in the visual genome dataset, corresponding to different bounding boxes in the image.\nWe use this dataset to train an embedding model that can use instruction to embeds specific aspects of a scene.\n\nCombined with our pretraining step, this results in a model that can create highâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct.","url":"https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Confession-Subreddit-Top500","keyword":"instruction-finetuning","description":"This dataset was prepared by taking into account the 500 most popular posts of all time in the confession subreddit and the comments with the most votes on these posts.\n","url":"https://huggingface.co/datasets/Oguzz07/Confession-Subreddit-Top500","creator_name":"OÄŸuzhan YÄ±ldÄ±rÄ±m","creator_url":"https://huggingface.co/Oguzz07","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","text","Text"],"keywords_longer_than_N":true},
	{"name":"FineTome-single-turn-dedup-amharic","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tDataset Card for FineTome Single Turn Conversations - Amharic\n\t\n\nThis dataset contains 83,290 conversational examples translated from English to Amharic, providing high-quality instruction-following conversations for training language models in Amharic.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a translation of the FineTome-single-turn-dedup dataset into Amharic, creating one of the largest publicly available collection of instruction-followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/addisai/FineTome-single-turn-dedup-amharic.","url":"https://huggingface.co/datasets/addisai/FineTome-single-turn-dedup-amharic","creator_name":"Addis AI","creator_url":"https://huggingface.co/addisai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Amharic","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"reflection-v1-ru_subset","keyword":"instruct","description":"\n\t\n\t\t\n\t\td0rj/reflection-v1-ru_subset\n\t\n\nTranslated glaiveai/reflection-v1 dataset into Russian language using GPT-4o.\n\nAlmost all the rows of the dataset have been translated. I have removed those translations that do not match the original by the presence of the tags \"thinking\", \"reflection\" and \"output\". Mapping to the original dataset rows can be taken from the \"index\" column.\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata = datasets.load_dataset(\"d0rj/reflection-v1-ru_subset\")\nprint(data)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset.","url":"https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","multilingual","glaiveai/reflection-v1"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-SFT-CoT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†ï¼ˆå¸¦CoTæ ‡æ³¨ï¼‰\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\nåŽŸå§‹è®­ç»ƒé›† (Original SFT Dataset)ï¼šåŒ…å« 5,287 æ¡é«˜è´¨é‡çš„â€œæŒ‡ä»¤-å›žç­”â€å¯¹ï¼Œç”¨äºŽåŸºç¡€çš„æ¨¡åž‹å¾®è°ƒã€‚\næ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFTâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-SFT-CoT.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-SFT-CoT","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"RoleBench","keyword":"instruction","description":"\n\t\n\t\t\n\t\tRoleBench\n\t\n\n\nPaper Title: RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models\narXiv Link: https://arxiv.org/abs/2310.00746\nGithub Repo: https://github.com/InteractiveNLP-Team/RoleLLM-public\n\nPlease read our paper for more details about this dataset.\nTL;DR: We introduce RoleLLM, a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZenMoore/RoleBench.","url":"https://huggingface.co/datasets/ZenMoore/RoleBench","creator_name":"Zekun Moore Wang","creator_url":"https://huggingface.co/ZenMoore","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Chinese","English","apache-2.0","Text","arxiv:2310.00746"],"keywords_longer_than_N":true},
	{"name":"alpaca-zh","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºŽGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/alpaca-zh.","url":"https://huggingface.co/datasets/shibing624/alpaca-zh","creator_name":"Ming Xu (å¾æ˜Ž)","creator_url":"https://huggingface.co/shibing624","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-ru","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\talpaca-cleaned-ru\n\t\n\nTranslated version of yahma/alpaca-cleaned into Russian.\n","url":"https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","translated","monolingual","yahma/alpaca-cleaned","Russian"],"keywords_longer_than_N":true},
	{"name":"psychoanalysis-dataset-v2-100k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tPsychoanalysis v2 â€” 100k (Hinglish + English)\n\t\n\nThis dataset contains 100k psychoanalytic-style conversational samples in Hinglish and English.\nIt includes messages (system/user/assistant), a supervised output, safety/evaluation metadata,\nand a pair field for preference learning (DPO/ORPO).\n\n\t\n\t\t\n\t\tLoading\n\t\n\nfrom datasets import load_dataset\nds = load_dataset(\"SomyaSaraswati/psychoanalysis-dataset-v2-100k\")\nprint(ds)\nprint(ds[\"train\"][0].keys())\n\n","url":"https://huggingface.co/datasets/SomyaSaraswati/psychoanalysis-dataset-v2-100k","creator_name":"Somya Saraswati","creator_url":"https://huggingface.co/SomyaSaraswati","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","Hindi","multilingual","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"fund-sft","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jannko/fund-sft.","url":"https://huggingface.co/datasets/jannko/fund-sft","creator_name":"ko","creator_url":"https://huggingface.co/jannko","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Code-170k-acholi","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-acholi is a groundbreaking dataset containing 33,148 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Acholi, making coding education accessible to Acholi speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n33,148 high-quality conversations about programming and coding\nPure Acholi language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-acholi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-acholi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Acoli","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-afrikaans","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-afrikaans is a groundbreaking dataset containing 151,533 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Afrikaans, making coding education accessible to Afrikaans speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n151,533 high-quality conversations about programming and coding\nPure Afrikaans language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-afrikaans.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-afrikaans","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Afrikaans","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Cybersecurity-Dataset-Fenrir-v2.0","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCybersecurity Defense Instruction-Tuning Dataset (v2.0)\n\t\n\n\n\nCreated by Alican Kiraz\n\n\t\n\t\t\n\t\tTL;DR\n\t\n\nA ready-to-train dataset of 83,920 high-quality system / user / assistant triples for defensive, alignment-safe cybersecurity SFT training.\nApache-2.0 licensed and production-ready.\nScope: OWASP Top 10, MITRE ATT&CK, NIST CSF, CIS Controls, ASD Essential 8, modern authentication (OAuth 2 / OIDC / SAML), SSL / TLS, Cloud & DevSecOps, Cryptography, and AI Security.\n\n\n\t\n\t\t\n\t\n\t\n\t\t1Â Â Whatâ€™sâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlicanKiraz0/Cybersecurity-Dataset-Fenrir-v2.0.","url":"https://huggingface.co/datasets/AlicanKiraz0/Cybersecurity-Dataset-Fenrir-v2.0","creator_name":"Alican Kiraz","creator_url":"https://huggingface.co/AlicanKiraz0","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"fleece2instructions-inputs-alpaca-cleaned","keyword":"instruct","description":"\n\t\n\t\t\n\t\tfleece2instructions-inputs-alpaca-cleaned\n\t\n\nThis data was downloaded from the alpaca-lora repo under the ODC-BY license (see snapshot here) and processed to text2text format. The license under which the data was downloaded from the source applies to this repo.\nNote that the inputs and instruction columns in the original dataset have been aggregated together for text2text generation. Each has a token with either <instruction> or <inputs> in front of the relevant text, both for modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/fleece2instructions-inputs-alpaca-cleaned.","url":"https://huggingface.co/datasets/pszemraj/fleece2instructions-inputs-alpaca-cleaned","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["English","odc-by","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"TSSB-3M-instructions","keyword":"instruct","description":"\n\t\n\t\t\n\t\tdata summary\n\t\n\ninstruction dataset for code bugfix\n\n\t\n\t\t\n\t\tReference\n\t\n\n[1]. TSSB-3M-ext\n","url":"https://huggingface.co/datasets/zirui3/TSSB-3M-instructions","creator_name":"zirui","creator_url":"https://huggingface.co/zirui3","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["code","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Code-170k-dombe","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-dombe is a groundbreaking dataset containing 176,898 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Dombe, making coding education accessible to Dombe speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n176,898 high-quality conversations about programming and coding\nPure Dombe language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-dombe.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-dombe","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Dombe","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-susu","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-susu is a groundbreaking dataset containing 29,858 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Susu, making coding education accessible to Susu speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n29,858 high-quality conversations about programming and coding\nPure Susu language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-susu.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-susu","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Susu","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-twi","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-twi is a groundbreaking dataset containing over 136,000 programming conversations translated into Twi (Akan), a major language spoken in Ghana. This dataset aims to democratize access to programming education and AI-assisted coding for Twi speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n136,944+ high-quality conversations about programming and coding\nPure Twi language - making coding education accessible to Twi speakers\nMulti-turn dialogues covering variousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-twi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-twi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Twi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Alpaca-CoT","keyword":"instruction","description":"\n\t\n\t\t\n\t\tInstruction-Finetuning Dataset Collection (Alpaca-CoT)\n\t\n\nThis repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model.\nWe also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT.  \nIf you think this dataset collection is helpful to you, please like thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT.","url":"https://huggingface.co/datasets/QingyiSi/Alpaca-CoT","creator_name":"Qingyi Si","creator_url":"https://huggingface.co/QingyiSi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","Malayalam","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruct","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruction","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"vi_alpaca_clean","keyword":"instruction-finetuning","description":"tsdocode/vi_alpaca_clean dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/tsdocode/vi_alpaca_clean","creator_name":"Thanh Sang","creator_url":"https://huggingface.co/tsdocode","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Vietnamese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Alpaca_french_mixtral","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca_french_mixtral\n\t\n\nThis dataset was made by reusing the french alpaca instruction with Mixtral-8x7B-Instruct to make the output open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","url":"https://huggingface.co/datasets/AIffl/Alpaca_french_mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Korean_User_Manuals_Dataset","keyword":"instruction","description":"\n\t\n\t\t\n\t\tKorean User Manuals Dataset\n\t\n\nThis dataset contains high-resolution images and PDFs of Korean user manuals and instruction guides for electronics, appliances, and consumer products. It is curated and anonymized to support AI research in OCR, document understanding, and multilingual text extraction.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask Categories:  \n\nDocumentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Korean_User_Manuals_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Korean_User_Manuals_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Korean","cc-by-4.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-ru","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\talpaca-cleaned-ru\n\t\n\nconverter for autotrain from d0rj/alpaca-cleaned-ru\nTranslated version of yahma/alpaca-cleaned into Russian.\n","url":"https://huggingface.co/datasets/ASIDS/alpaca-cleaned-ru","creator_name":"Greed","creator_url":"https://huggingface.co/ASIDS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","translated","monolingual","yahma/alpaca-cleaned","Russian"],"keywords_longer_than_N":true},
	{"name":"smollm-japanese-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tJapanese SmolLM Dataset\n\t\n\nA carefully curated and mixed dataset for training Japanese small language models (SmolLM).\n\n\t\n\t\t\n\t\tDataset Composition\n\t\n\nThis dataset combines multiple high-quality Japanese sources with the following proportions:\n\n\t\n\t\t\n\t\tFoundation Data (70%)\n\t\n\n\nmC4 Japanese (30%): Clean web text from AllenAI's C4 corpus (allenai/c4)\nJapanese Wikipedia (20%): Encyclopedia articles (range3/wikipedia-ja-20230101)\nCC-100 Japanese (10%): Common Crawl filtered Japanese textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ronantakizawa/smollm-japanese-dataset.","url":"https://huggingface.co/datasets/ronantakizawa/smollm-japanese-dataset","creator_name":"Ronan Takizawa","creator_url":"https://huggingface.co/ronantakizawa","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","text-classification","Japanese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-tr","keyword":"instruction-finetuning","description":"Alpaca Cleaned Dataset.\nMachine Translated facebook/nllb-200-3.3B\nLanguages\nTurkish\n","url":"https://huggingface.co/datasets/cgulse/alpaca-cleaned-tr","creator_name":"Can G","creator_url":"https://huggingface.co/cgulse","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Turkish","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"gt-doremiti-instructions","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for gt-doremiti-instructions\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nJeu d'instruction pour fine-tuner un LLM suivant les prÃ©conisations du projet Stanford-Alpaca (https://github.com/tatsu-lab/stanford_alpaca)\nCes instructions sont extraites de la FAQ crÃ©e par le GT DOREMITI et disponible Ã  cette adresse (https://gt-atelier-donnees.miti.cnrs.fr/faq.html)\nLes donnÃ©es sont mise Ã  disposition selon les termes de la Licence Creative Commons Attribution 4.0 International.\n","url":"https://huggingface.co/datasets/Gt-Doremiti/gt-doremiti-instructions","creator_name":"Doremiti","creator_url":"https://huggingface.co/Gt-Doremiti","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yahma/alpaca-cleaned.","url":"https://huggingface.co/datasets/yahma/alpaca-cleaned","creator_name":"Gene Ruebsamen","creator_url":"https://huggingface.co/yahma","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tamazight-tifinagh","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tamazight-tifinagh is a groundbreaking dataset containing 121,845 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tamazight (Tifinagh), making coding education accessible to Tamazight (Tifinagh) speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n121,845 high-quality conversations about programming and coding\nPure Tamazight (Tifinagh) language - democratizing coding education\nMulti-turn dialogues coveringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tamazight-tifinagh.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tamazight-tifinagh","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","ber","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-venda","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-venda is a groundbreaking dataset containing 118,838 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Venda, making coding education accessible to Venda speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n118,838 high-quality conversations about programming and coding\nPure Venda language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-venda.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-venda","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Venda","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-yoruba","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-yoruba is a groundbreaking dataset containing 12,287 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Yoruba, making coding education accessible to Yoruba speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,287 high-quality conversations about programming and coding\nPure Yoruba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-yoruba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-yoruba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Yoruba","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-luo","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-luo is a groundbreaking dataset containing 140,631 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Luo, making coding education accessible to Luo speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n140,631 high-quality conversations about programming and coding\nPure Luo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-luo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-luo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Luo (Kenya and Tanzania)","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-xhosa","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-xhosa is a groundbreaking dataset containing 12,296 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Xhosa, making coding education accessible to Xhosa speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,296 high-quality conversations about programming and coding\nPure Xhosa language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-xhosa.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-xhosa","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Xhosa","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-sango","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-sango is a groundbreaking dataset containing 103,766 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Sango, making coding education accessible to Sango speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n103,766 high-quality conversations about programming and coding\nPure Sango language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-sango.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-sango","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Sango","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-hausa","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-hausa is a groundbreaking dataset containing 14,095 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Hausa, making coding education accessible to Hausa speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n14,095 high-quality conversations about programming and coding\nPure Hausa language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-hausa.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-hausa","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Hausa","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-luganda","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-luganda is a groundbreaking dataset containing 136,290 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Luganda, making coding education accessible to Luganda speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n136,290 high-quality conversations about programming and coding\nPure Luganda language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-luganda.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-luganda","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Ganda","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-bemba","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-bemba is a groundbreaking dataset containing 54,131 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Bemba, making coding education accessible to Bemba speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n54,131 high-quality conversations about programming and coding\nPure Bemba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-bemba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-bemba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Bemba (Zambia)","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-wolof","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-wolof is a groundbreaking dataset containing 101,894 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Wolof, making coding education accessible to Wolof speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n101,894 high-quality conversations about programming and coding\nPure Wolof language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-wolof.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-wolof","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Wolof","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-shona","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-shona is a groundbreaking dataset containing 12,269 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Shona, making coding education accessible to Shona speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,269 high-quality conversations about programming and coding\nPure Shona language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-shona.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-shona","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Shona","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-nuer","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-nuer is a groundbreaking dataset containing 128,677 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Nuer, making coding education accessible to Nuer speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n128,677 high-quality conversations about programming and coding\nPure Nuer language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-nuer.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-nuer","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Nuer","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-zulu","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-zulu is a groundbreaking dataset containing 13,591 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Zulu, making coding education accessible to Zulu speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n13,591 high-quality conversations about programming and coding\nPure Zulu language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-zulu.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-zulu","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Zulu","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-dyula","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-dyula is a groundbreaking dataset containing 99,057 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Dyula, making coding education accessible to Dyula speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n99,057 high-quality conversations about programming and coding\nPure Dyula language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-dyula.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-dyula","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Dyula","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-sesotho","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-sesotho is a groundbreaking dataset containing 12,287 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Sesotho, making coding education accessible to Sesotho speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,287 high-quality conversations about programming and coding\nPure Sesotho language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-sesotho.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-sesotho","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Southern Sotho","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-lingala","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-lingala is a groundbreaking dataset containing 74,431 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Lingala, making coding education accessible to Lingala speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n74,431 high-quality conversations about programming and coding\nPure Lingala language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-lingala.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-lingala","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Lingala","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tswana","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tswana is a groundbreaking dataset containing 115,572 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tswana, making coding education accessible to Tswana speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n115,572 high-quality conversations about programming and coding\nPure Tswana language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tswana.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tswana","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tswana","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-chichewa","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-chichewa is a groundbreaking dataset containing 12,321 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Chichewa, making coding education accessible to Chichewa speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,321 high-quality conversations about programming and coding\nPure Chichewa language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-chichewa.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-chichewa","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Chichewa","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-dinka","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-dinka is a groundbreaking dataset containing 30,404 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Dinka, making coding education accessible to Dinka speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n30,404 high-quality conversations about programming and coding\nPure Dinka language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-dinka.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-dinka","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Dinka","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tigrinya","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tigrinya is a groundbreaking dataset containing 121,080 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tigrinya, making coding education accessible to Tigrinya speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n121,080 high-quality conversations about programming and coding\nPure Tigrinya language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tigrinya.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tigrinya","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tigrinya","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-amharic","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-amharic is a groundbreaking dataset containing 12,769 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Amharic, making coding education accessible to Amharic speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,769 high-quality conversations about programming and coding\nPure Amharic language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-amharic.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-amharic","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Amharic","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-krio","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-krio is a groundbreaking dataset containing 93,627 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Krio, making coding education accessible to Krio speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n93,627 high-quality conversations about programming and coding\nPure Krio language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-krio.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-krio","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Krio","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tiv","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tiv is a groundbreaking dataset containing 93,821 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tiv, making coding education accessible to Tiv speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n93,821 high-quality conversations about programming and coding\nPure Tiv language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data structuresâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tiv.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tiv","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tiv","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-somali","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-somali is a groundbreaking dataset containing 12,244 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Somali, making coding education accessible to Somali speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,244 high-quality conversations about programming and coding\nPure Somali language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-somali.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-somali","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Somali","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-fulani","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-fulani is a groundbreaking dataset containing 110,292 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Fulani, making coding education accessible to Fulani speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n110,292 high-quality conversations about programming and coding\nPure Fulani language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-fulani.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-fulani","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Fula","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"gerlayqa-combined-paraphrased","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tGerLayQA Combined Paraphrased ðŸ‡©ðŸ‡ªâš–ï¸\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a combined, shuffled dataset merging both the BGB (civil law) and StGB (criminal law) paraphrased German legal QA datasets. All examples are paraphrased and restructured by GPT-5 for fine-tuning large language models on German legal question-answering tasks.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n6,462 high-quality QA pairs covering both German Civil and Criminal Law\nCombined coverage: BGB (BÃ¼rgerliches Gesetzbuch) + StGBâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DomainLLM/gerlayqa-combined-paraphrased.","url":"https://huggingface.co/datasets/DomainLLM/gerlayqa-combined-paraphrased","creator_name":"DomainLLM","creator_url":"https://huggingface.co/DomainLLM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","German","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"turing-gpt4","keyword":"instruction-finetuning","description":"Turing-AI/turing-gpt4 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Turing-AI/turing-gpt4","creator_name":"TuringAI","creator_url":"https://huggingface.co/Turing-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","Russian","Chinese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"openhermes-reasoning-231k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ§  OpenHermes Reasoning 377K\n\t\n\n\n\n\n\n\n\nHigh-quality instruction dataset with chain-of-thought reasoning\nðŸ¤— Dataset â€¢ ðŸ’¬ Discussions\n\n\n\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Overview\n\t\n\nThis dataset contains 231,144 high-quality instruction-response pairs with explicit chain-of-thought reasoning. Each example includes:\n\nPrompt: Original instruction or question\nThinking: Explicit reasoning process and logical steps\nAnswer: Final comprehensive response\n\n\n\t\n\t\t\n\t\tKey Features\n\t\n\nâœ… Quality Filtered: Rigorousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/limeXx/openhermes-reasoning-231k.","url":"https://huggingface.co/datasets/limeXx/openhermes-reasoning-231k","creator_name":"yusuf uzun","creator_url":"https://huggingface.co/limeXx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OpenOrca-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\tOpenOrca-ru\n\t\n\nThis is translated version of Open-Orca/OpenOrca into Russian.\n","url":"https://huggingface.co/datasets/d0rj/OpenOrca-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"gerlayqa-bgb-paraphrased","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tGerLayQA-BGB Paraphrased ðŸ‡©ðŸ‡ªâš–ï¸\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a paraphrased and restructured version of the GerLayQA BGB (BÃ¼rgerliches Gesetzbuch / German Civil Code) dataset, specifically prepared for fine-tuning large language models on German civil law question-answering tasks.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n5,255 high-quality QA pairs about German Civil Law (BGB)\nParaphrased questions to remove plagiarism while maintaining legal accuracy\nStructured 7-section answers followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DomainLLM/gerlayqa-bgb-paraphrased.","url":"https://huggingface.co/datasets/DomainLLM/gerlayqa-bgb-paraphrased","creator_name":"DomainLLM","creator_url":"https://huggingface.co/DomainLLM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","German","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"DistilQwen_100k_korean","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDistilQwen 100k Korean\n\t\n\nThis dataset is a Korean translation of the original alibaba-pai/DistilQwen_100k dataset.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains both English and Korean versions of instruction-response pairs:\n{\n  \"instruction\": \"Original English instruction text\",\n  \"output\": \"Original English response/answer\",\n  \"instruction_kr\": \"Korean translation of the instruction\",\n  \"output_kr\": \"Korean translation of the response/answer\",\n  \"_dataset_index\": 30000\n}\n\nEachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lcw99/DistilQwen_100k_korean.","url":"https://huggingface.co/datasets/lcw99/DistilQwen_100k_korean","creator_name":"Chang W Lee","creator_url":"https://huggingface.co/lcw99","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Korean","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"indonesian-conversation","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tIndonesian Conversation\n\t\n\nIndonesian Conversation is a carefully curated conversational dataset featuring high-quality dialogues primarily in Bahasa Indonesia, with occasional English phrases. The dataset has been specifically designed to support alignment and supervised fine-tuning (SFT) for open-source large language models targeting Indonesian language applications.\nThe collection consists predominantly of multi-turn conversations that showcase natural, friendly, and informativeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/izzulgod/indonesian-conversation.","url":"https://huggingface.co/datasets/izzulgod/indonesian-conversation","creator_name":"Izzul Fahmi","creator_url":"https://huggingface.co/izzulgod","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Indonesian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-ga","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-ga is a groundbreaking dataset containing over 136,000 programming conversations, orginally sourced from glaiveai/glaive-code-assistant-v2 and translated into Ga , a major language spoken in Ghana. This dataset aims to democratize access to programming education and AI-assisted coding for Ga speakers.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n136,944+ high-quality conversations about programming and coding\nPure Ga language - making coding education accessible toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-ga.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-ga","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Ga","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Python_Refactor_Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ§© Python Refactor Dataset (45k)\n\t\n\n\n\t\n\t\t\n\t\tBehavior-Preserving Refactoring Examples for Instruction-Tuning Code Models\n\t\n\nThis dataset contains 45,000 synthetic Python code refactoring examples designed for\ninstruction-tuning models such as IBM Granite 4.0 (micro/h-tiny) and Meta CodeLlama-7B-Python.\nEach example demonstrates a behavior-preserving refactor â€” improving code readability,\nmaintainability, and style (PEP8, type hints, context managers, modularization, etc.)\nwithoutâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KavinduHansaka/Python_Refactor_Dataset.","url":"https://huggingface.co/datasets/KavinduHansaka/Python_Refactor_Dataset","creator_name":"Kavindu Hansaka Jayasinghe","creator_url":"https://huggingface.co/KavinduHansaka","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"VisIT-Bench","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tDataset Card for VisIT-Bench\n\t\n\n\nDataset Description\nLinks\nDataset Structure\nData Fields\nData Splits\nData Loading\n\n\nLicensing Information\nAnnotations\nConsiderations for Using the Data\nCitation Information\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nVisIT-Bench is a dataset and benchmark for vision-and-language instruction following. The dataset is comprised of image-instruction pairs and corresponding example outputs, spanning a wide range of tasks, from simple object recognition to complexâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/VisIT-Bench.","url":"https://huggingface.co/datasets/mlfoundations/VisIT-Bench","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["crowdsourced","found","original","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"vlaa-thinking-grpo","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tVLAA-Thinking-SFT-126K\n\t\n\nLarge-scale vision-language dataset with 126K instruction-following samples featuring chain-of-thought reasoning\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains vision-language samples with instruction-following conversations. Each sample includes:\n\nimage: PIL Image object\nquestion: Question or instruction text\nanswer or gt: Response with thinking process (SFT dataset) or ground truth answer (GRPO dataset)\ncaption: Image caption (may be empty for someâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/penfever/vlaa-thinking-grpo.","url":"https://huggingface.co/datasets/penfever/vlaa-thinking-grpo","creator_name":"Benjamin Feuer","creator_url":"https://huggingface.co/penfever","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"MM-Instruct","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMM-Instruct is a large-scale dataset of diverse and high-quality visual instruction-answer pairs designed to enhance the instruction-following capabilities of large multimodal models (LMMs) in real-world use cases. It goes beyond simple question-answering or image-captioning by incorporating a wide range of instructions, including creative writing, summarization, and image analysis, pushing LMMs to better understand and respond to nuanced user requests.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jjjjh/MM-Instruct.","url":"https://huggingface.co/datasets/jjjjh/MM-Instruct","creator_name":"LIU Jihao","creator_url":"https://huggingface.co/jjjjh","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","cc-by-4.0","100K - 1M","json","Image"],"keywords_longer_than_N":true},
	{"name":"synthetic-neurology-conversations","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tSynthetic Neurology Conversations\n\t\n\nSummary.This dataset augments questions from KryptoniteCrown/synthetic-neurology-QA-dataset with a compact two-step follow-up conversation generated by moonshotai/Kimi-K2-Instruct:\n\nmodel answers the original question,  \nmodel asks a follow-up question (to deepen/clarify),  \nmodel answers its follow-up.\n\nShared by the OpenMed Community to help improve medical models globally.  \n\nNot medical advice. Research/education only; not for clinicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/synthetic-neurology-conversations.","url":"https://huggingface.co/datasets/openmed-community/synthetic-neurology-conversations","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc0-1.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"english-to-colloquial-tamil","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tEnglish to Colloquial Tamil\n\t\n\n\"instruction\":\"Translate provided English text into colloquial Tamil.\"\n\"input\": \"Their players played well.\"\n\"output\": \"à®…à®µà®™à¯à®• players à®¨à®²à¯à®²à®¾ à®µà®¿à®³à¯ˆà®¯à®¾à®£à¯à®Ÿà®¾à®™à¯à®•.\"\n\n","url":"https://huggingface.co/datasets/jarvisvasu/english-to-colloquial-tamil","creator_name":"Vasanth Kumar","creator_url":"https://huggingface.co/jarvisvasu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","Tamil","English"],"keywords_longer_than_N":true},
	{"name":"shibing624_alpaca-zh","keyword":"instruction","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºŽGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/botp/shibing624_alpaca-zh.","url":"https://huggingface.co/datasets/botp/shibing624_alpaca-zh","creator_name":"ab10","creator_url":"https://huggingface.co/botp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"SFT_54k_reasoning","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/SFT_54k_reasoning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/SFT_54k_reasoning is a processed version of the XuHu6736/s1_54k_filter_with_isreasoning dataset, specifically reformatted for instruction fine-tuning (SFT) of language models.\nThe original question and solution pairs have been converted into an instruction-following format. Critically, the isreasoning_score and isreasoning labels from the parent dataset are preserved, allowing for targeted SFT onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning.","url":"https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["XuHu6736 (formatting and derivation)","derived from XuHu6736/s1_54k_filter_with_isreasoning","derived from source datasets","monolingual","XuHu6736/s1_54k_filter_with_isreasoning"],"keywords_longer_than_N":true},
	{"name":"lean-six-sigma-qna-360","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tLean Six Sigma QnA Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 360 high-quality question-answer pairs focused on Lean Six Sigma methodologies, business process improvement, and operational optimization across multiple industries. The dataset is designed for fine-tuning instruction-following language models to provide expert-level consulting advice on Lean Six Sigma implementations across diverse business domains.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fieldsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cw18/lean-six-sigma-qna-360.","url":"https://huggingface.co/datasets/cw18/lean-six-sigma-qna-360","creator_name":"Clarence Wong","creator_url":"https://huggingface.co/cw18","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"test2027","keyword":"instruction","description":"A dataset containing dialogues between assistant and user with different roles.","url":"https://huggingface.co/datasets/zjrwtxtechstudio/test2027","creator_name":"zjrwtx","creator_url":"https://huggingface.co/zjrwtxtechstudio","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"PromisedChat_Instruction","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/netmouse/PromisedChat_Instruction.","url":"https://huggingface.co/datasets/netmouse/PromisedChat_Instruction","creator_name":"Matt Yeh","creator_url":"https://huggingface.co/netmouse","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-qa-data","keyword":"instruction","description":"\n\t\n\t\t\n\t\tAlpaca-style Question and Answer Dataset\n\t\n\nThis dataset contains question-answer pairs formatted in the Alpaca instruction style, suitable for instruction fine-tuning of language models.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example contains:\n\ninstruction: The question\ninput: Empty string (can be used for context in other applications)\noutput: The answer\ntext: The formatted text using the Alpaca template\n\n\n\t\n\t\t\n\t\tTemplate\n\t\n\nBelow is an instruction that describes a task, paired with an input thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sweatSmile/alpaca-qa-data.","url":"https://huggingface.co/datasets/sweatSmile/alpaca-qa-data","creator_name":"amitk17","creator_url":"https://huggingface.co/sweatSmile","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"stage1-doctor-patient-chat","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ¦· Stage 1 - AI Doctor Tone Dataset (Dental)\n\t\n\nThis dataset contains instructionâ€“response formatted examples derived from realistic patient-doctor conversations, focused on general medical behavior and tone. It is designed as Stage 1 in a two-stage fine-tuning pipeline for building a domain-specific, polite, and structured AI dental assistant.\n\n\n\t\n\t\t\n\t\tâœ¨ Intended Use\n\t\n\n\nFine-tuning large language models (LLMs) to simulate human-like, empathetic, and structured medical responses.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BirdieByte1024/stage1-doctor-patient-chat.","url":"https://huggingface.co/datasets/BirdieByte1024/stage1-doctor-patient-chat","creator_name":"BirdieByte","creator_url":"https://huggingface.co/BirdieByte1024","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Discord-OpenMicae","keyword":"multi-turn","description":"\n  \n\n\n\nDiscord-OpenMicae is a dataset of anonymized Discord conversations from late spring to late summer 2025 for training and evaluating conversational AI models in a ChatML-friendly format.\n\n\n250k+ Single-Turn Exchanges (STX) â€“ standalone user â†’ reply pairs  \n100k+ Multi-Turn Chains â€“ two-participant reply chains, variable length\n\n\n\n  \n    \n  \n\n\n\n  Nomic Atlas Map\n\n\n\n\n\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nHuman-only dialogues (no bots)\nLinks, embeds, and commands removed\nTrading posts, code blocks, and LFGâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mookiezi/Discord-OpenMicae.","url":"https://huggingface.co/datasets/mookiezi/Discord-OpenMicae","creator_name":"Jason","creator_url":"https://huggingface.co/mookiezi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OpenCharacter","keyword":"instruction","description":"\n\t\n\t\t\n\t\tOpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas\n\t\n\nThis repo releases data introduced in our paper OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas in arXiv.\n\nWe study customizable role-playing dialogue agents in large language models (LLMs).\nWe tackle the challenge with large-scale data synthesis: character synthesis and character-driven reponse synthesis.\nOur solution strengthens the original LLaMA-3â€¦ See the full description on the dataset page: https://huggingface.co/datasets/xywang1/OpenCharacter.","url":"https://huggingface.co/datasets/xywang1/OpenCharacter","creator_name":"Xiaoyang Wang","creator_url":"https://huggingface.co/xywang1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"multi-agent-scam-conversation","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset with Agentic Personalities\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset with Agentic Personalities is an enhanced collection of simulated phone conversations between two AI agents, one acting as a scammer or non-scammer and the other as an innocent receiver. Each dialogue is labeled as either a scam or non-scam interaction. This dataset is designed to help developâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation.","url":"https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"math-gpt-4o-200k-ko","keyword":"instruction","description":"Translated PawanKrd/math-gpt-4o-200k using nayohan/llama3-instrucTrans-enko-8b.\nThis dataset is a raw translated dataset and contains repetitive sentences generated by the model, so it needs to be filtered.\n","url":"https://huggingface.co/datasets/nayohan/math-gpt-4o-200k-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Korean","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"clustered_tulu_3_8","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tClustered_Tulu_3_8 Multi-Domain Dataset\n\t\n\nThis dataset contains high-quality examples across 8 specialized domains, automatically extracted and curated from the Tulu-3 SFT mixture using advanced clustering techniques.\n\n\t\n\t\t\n\t\tðŸŽ¯ Multi-Domain Structure\n\t\n\nThis repository provides 8 domain-specific configurations, each optimized for different types of tasks:\n\n\t\n\t\t\nConfiguration\nDomain\nTrain\nTest\nTotal\n\n\n\t\t\nprogramming_and_code_development\nProgramming & Code Development\n88,783\n22,196\n110â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Malikeh1375/clustered_tulu_3_8.","url":"https://huggingface.co/datasets/Malikeh1375/clustered_tulu_3_8","creator_name":"Malikeh Ehghaghi","creator_url":"https://huggingface.co/Malikeh1375","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"refactorchat","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tModel Card\n\t\n\n\n\t\n\t\t\n\t\tModel Details\n\t\n\n\nDataset Name: RefactorChat\nVersion: 1.0\nDate: October 19, 2024\nType: Multi-turn dialogue dataset for code refactoring and feature addition\n\n\n\t\n\t\t\n\t\tIntended Use\n\t\n\n\nPrimary Use: Evaluating and training large language models on incremental code development tasks\nIntended Users: Researchers and practitioners in natural language processing and software engineering\n\n\n\t\n\t\t\n\t\tDataset Composition\n\t\n\n\nSize: 100 samples\nStructure: Each sample consists ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BradMcDanel/refactorchat.","url":"https://huggingface.co/datasets/BradMcDanel/refactorchat","creator_name":"Bradley McDanel","creator_url":"https://huggingface.co/BradMcDanel","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"AlpacaX-Cleaned","keyword":"instruction-tuning","description":"\n  \n\n\n\n\t\n\t\t\n\t\tðŸ“š AlpacaX Dataset Documentation\n\t\n\nThe AlpacaX dataset is crafted to enhance AI models with structured, contextually rich, and logically sequenced examples. Designed for integration with TinyAGI, AlpacaX employs an advanced variant of the Alpaca training methodology, making it ideal for models that require detailed instruction-following and multi-step reasoning. This dataset is well-suited for fine-tuning language models to handle complex tasks with clarity and structuredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SullyGreene/AlpacaX-Cleaned.","url":"https://huggingface.co/datasets/SullyGreene/AlpacaX-Cleaned","creator_name":"SullyGreene","creator_url":"https://huggingface.co/SullyGreene","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"FC-CoT-Top10k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\n\nTotal Samples: 10,000  \nPrimary Focus:  \nHigh-quality Function Calling demonstrations  \nClear, well-structured Chain of Thought reasoning\n\n\nSelection Process:  \nRemoved noisy or incomplete examples  \nSelected cases with precise function arguments  \nEnsured reasoning steps are logically sound and human-readable\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŽ¯ Use Cases\n\t\n\nThis dataset is ideal for:\n\nFine-tuning LLMs for tool calling / function calling\nTraining models to provide explainable reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/FC-CoT-Top10k.","url":"https://huggingface.co/datasets/arcosoph/FC-CoT-Top10k","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"FC-CoT-Top10k","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\n\nTotal Samples: 10,000  \nPrimary Focus:  \nHigh-quality Function Calling demonstrations  \nClear, well-structured Chain of Thought reasoning\n\n\nSelection Process:  \nRemoved noisy or incomplete examples  \nSelected cases with precise function arguments  \nEnsured reasoning steps are logically sound and human-readable\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŽ¯ Use Cases\n\t\n\nThis dataset is ideal for:\n\nFine-tuning LLMs for tool calling / function calling\nTraining models to provide explainable reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/FC-CoT-Top10k.","url":"https://huggingface.co/datasets/arcosoph/FC-CoT-Top10k","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"MiniCodeTasks_DeepSeekTrain","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMiniCodeTasks_DeepSeekTrain\n\t\n\nA lightweight, instruction-based dataset curated by Muhammad Yasir for fine-tuning code generation models like DeepSeek-Coder 1.3B.\nThis dataset is tailored for Small Language Models (SLMs) and code assistant use-cases, making it ideal for training custom developer tools, coding bots, and programming-focused chat agents.\n\n\n\t\n\t\t\n\t\tðŸ§  Dataset Overview\n\t\n\n\nTitle: MiniCodeTasks_DeepSeekTrain\nType: Instruction-Response (Code Generation)\nSize: 1,000+â€¦ See the full description on the dataset page: https://huggingface.co/datasets/devxyasir/MiniCodeTasks_DeepSeekTrain.","url":"https://huggingface.co/datasets/devxyasir/MiniCodeTasks_DeepSeekTrain","creator_name":"Muhammad Yasir","creator_url":"https://huggingface.co/devxyasir","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ICD10CM_HCC","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tHCC ICD-CM Instruction Tuning Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe ICD10CM_HCC dataset is specifically designed for instruction tuning of large language models (LLMs) for the task of ICD-CM code extraction including the MEAT justification from discharge summary. This dataset aims to provide high-quality, instruction-formatted examples to guide LLMs in accurately identifying and extracting relevant ICD-10-CM (International Classification of Diseases, Tenth Revision, Clinicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ParamDev/ICD10CM_HCC.","url":"https://huggingface.co/datasets/ParamDev/ICD10CM_HCC","creator_name":"Param Ahuja","creator_url":"https://huggingface.co/ParamDev","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","human-annotated","programmatically-created","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ru-instruct-conversation-v1","keyword":"instruct","description":"Combined dataset of mostly Russian dialogs in form of conversations suitable for LLM fine-tuning scenarios.\nTotal samples: 82208\nDeduplicated using simhash(hamming_treshold=3).\nDatasets used:\n\nIlyaGusev/saiga_scored (min_score: 8, no bad by regexp)\nIlyaGusev/oasst2_ru_main_branch\nattn-signs/kolmogorov-3\nattn-signs/russian-easy-instructions\n\n","url":"https://huggingface.co/datasets/ZeroAgency/ru-instruct-conversation-v1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"pino","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset PUCMM - Abby\n\t\n\nEste dataset contiene ejemplos cuidadosamente diseÃ±ados de instrucciones y respuestas relacionadas exclusivamente con la Pontificia Universidad CatÃ³lica Madre y Maestra (PUCMM). Ha sido creado con el objetivo de entrenar a Abby, un chatbot institucional capaz de responder preguntas frecuentes de estudiantes, profesores y visitantes, abarcando informaciÃ³n sobre admisiones, historia, campus, programas acadÃ©micos y mÃ¡s.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ§  DescripciÃ³n\n\t\n\nEste datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sabux/pino.","url":"https://huggingface.co/datasets/sabux/pino","creator_name":"Hugo Rodriguez","creator_url":"https://huggingface.co/sabux","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"dataset-portuguese-aira-v2-Gemma-format","keyword":"instruction","description":"Dataset Aira para o formato do Modelo Gemma \n\n\n\t\n\t\t\n\t\tResumo do Dataset\n\t\n\nEste conjunto de dados contÃ©m uma coleÃ§Ã£o de conversas individuais entre um assistente e um usuÃ¡rio.\nAs conversas foram geradas pelas interaÃ§Ãµes do usuÃ¡rio com modelos jÃ¡ ajustados (ChatGPT, LLama 2, Open-Assistant, etc).\nO conjunto de dados estÃ¡ disponÃ­vel em portuguÃªs (tem a versÃ£o em InglÃªs que ainda nÃ£o tratei). Mas vocÃª pode baixar do \nrepositÃ³rio de Nicholas Kluge CorrÃªa tanto a versÃ£o em PortuguÃªs e \na versÃ£o emâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format.","url":"https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format","creator_name":"EDDY GIUSEPE CHIRINOS ISIDRO, PhD","creator_url":"https://huggingface.co/EddyGiusepe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Portuguese","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"my-cosmopedia-dataset","keyword":"instruction-tuning","description":"ðŸ§¾ Dataset Description\nThe Pre-processed and Cleaned Cosmopedia Dataset is a ready-to-use derivative of the original HuggingFaceTB/cosmopedia\n collection.\nCosmopedia is a large-scale synthetic dataset consisting of high-quality textbooks, blog posts, stories, tutorials, and forum discussions generated by Mixtral-8x7B. While the raw dataset is incredibly rich, it requires significant preprocessing before it can be used effectively for supervised fine-tuning (SFT) or other instruction-tuningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/blah7/my-cosmopedia-dataset.","url":"https://huggingface.co/datasets/blah7/my-cosmopedia-dataset","creator_name":"blah","creator_url":"https://huggingface.co/blah7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","monolingual","HuggingFaceTB/cosmopedia","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"EFAGen-Llama-3.1-8B-Instruct-Training-Data","keyword":"instruction-tuning","description":"Paper Link\nThe training data used for the final version of EFAGen-Llama-3.1-8B-Instruct.\nThe data is in Alpaca format and can be used with Llama-Factory (check dataset_info.json).\n","url":"https://huggingface.co/datasets/codezakh/EFAGen-Llama-3.1-8B-Instruct-Training-Data","creator_name":"Zaid Khan","creator_url":"https://huggingface.co/codezakh","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Mind-Corpus","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMind Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 124 handcrafted, multi-turn conversations designed to simulate supportive interactions in mental health contexts. The dataset is bifurcated into two distinct settings:\n\nClinical Setting (In-Office): Dialogues between a patient and a psychologist during a therapy session. These conversations explore ongoing personal issues in a structured, reflective environment.\nCrisis Hotline Setting: Dialogues between a caller inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Locutusque/Mind-Corpus.","url":"https://huggingface.co/datasets/Locutusque/Mind-Corpus","creator_name":"Sebastian Gabarain","creator_url":"https://huggingface.co/Locutusque","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Atma4-Hindi","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma4-Hindi\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi.","url":"https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Select-Stack","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack.","url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"meme_dataset","keyword":"instruction-finetuning","description":"Noxus09/meme_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Noxus09/meme_dataset","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Atma3-Share-GPT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3-Share-GPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT.","url":"https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"college_alpaca_dataset","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the given article in 200 Words.\",\n\"input\": \"https://www.bbc.com/news/world-51461830\",\n\"output\": \"The recentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dapraws/college_alpaca_dataset.","url":"https://huggingface.co/datasets/dapraws/college_alpaca_dataset","creator_name":"Muhammad Darrel Prawira","creator_url":"https://huggingface.co/dapraws","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Alizee-OpenCodeReasoning-Phase3-1.4M","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸš€ Alizee OpenCodeReasoning Phase 3 Conformant Dataset - 1.2M Examples\n\t\n\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Summary\n\t\n\nThis is a fully conformant version of the Phase 3 dataset, processed to strictly follow the specification with clean separation between data and formatting tags. Contains 1.2 million high-quality Python code examples with synthetic prompts and concise reasoning chains.\n\n\t\n\t\t\n\t\tKey Improvements\n\t\n\n\nâœ… 100% Conformant to Phase 3 specification\nâœ… Synthetic prompts generated from codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DUKEAI/Alizee-OpenCodeReasoning-Phase3-1.4M.","url":"https://huggingface.co/datasets/DUKEAI/Alizee-OpenCodeReasoning-Phase3-1.4M","creator_name":"DUKE ANALYTICS","creator_url":"https://huggingface.co/DUKEAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M<n<10M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Text_Guided_Image_Editing-ru","keyword":"instruction","description":"Translated instructions from ImagenHub/Text_Guided_Image_Editing into Russian using gemini-flash-1.5-8b.\n","url":"https://huggingface.co/datasets/d0rj/Text_Guided_Image_Editing-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-image","translated","ImagenHub/Text_Guided_Image_Editing","Russian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\td0rj/OpenHermes-2.5-ru\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is translated version of teknium/OpenHermes-2.5 into Russian using Google Translate.\n","url":"https://huggingface.co/datasets/d0rj/OpenHermes-2.5-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"business-email-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tBusiness Email Dataset - Alpaca Format\n\t\n\nA comprehensive synthetic dataset of 5,000 professional business emails in Alpaca instruction-tuning format, designed for fine-tuning language models on formal business communication.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains high-quality, diverse business email examples covering a wide range of professional scenarios, industries, and communication styles. Each email is formatted following the Alpaca instruction-tuning standardâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wardacoder/business-email-dataset.","url":"https://huggingface.co/datasets/wardacoder/business-email-dataset","creator_name":"Warda Ul Hasan","creator_url":"https://huggingface.co/wardacoder","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"monomer","keyword":"instruction","description":"arcadianlee/monomer dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/arcadianlee/monomer","creator_name":"Ricky Renjie Li","creator_url":"https://huggingface.co/arcadianlee","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-indonesian","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tðŸ¦™ðŸ› Cleaned Alpaca Dataset (INDONESIAN)\n\t\n\nWelcome to the Cleaned Alpaca Dataset repository! This repository hosts a cleaned and curated version of a dataset used to train the Alpaca LLM (Large Language Model). The original dataset had several issues that are addressed in this cleaned version.\nOn April 8, 2023 the remaining uncurated instructions (~50,000) were replaced with data from the GPT-4-LLM dataset. Curation of the incoming GPT-4 data is ongoing.\n\nA 7b Lora model (trained onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian.","url":"https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian","creator_name":"Ilham Fadhil","creator_url":"https://huggingface.co/ilhamfadheel","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Indonesian","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"SOC-2508","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tDataset Card for Synthetic Online Conversations\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains over 1,180 synthetically generated, multi-turn online conversations. Each conversation is a complete dialogue between two fictional personas drawn from the Synthetic Persona Bank (SPB-2508) dataset.\nThe dataset was created using a multi-stage programmatic pipeline (inspired by ConvoGen) driven by a large language model (Qwen3-235B-A22B-Instruct-2507). The generation process was guidedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/SOC-2508.","url":"https://huggingface.co/datasets/marcodsn/SOC-2508","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"openmath-nondual","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tnondual_openmath_final\n\t\n\nA non-dual reformulation of the unsloth/OpenMathReasoning-mini dataset.All assistant solutions have been rewritten into impersonal, non-dual language using OpenAI models, and finalized so that the dataset no longer contains duplicate *_nondual fields.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSource: unsloth/OpenMathReasoning-mini  \nFormat: JSONL, each line is a dictionary with the following fields:\n\n\n\t\n\t\t\nField\nDescription\n\n\n\t\t\nproblem\nMath problem statement (rewrittenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/marciodiaz/openmath-nondual.","url":"https://huggingface.co/datasets/marciodiaz/openmath-nondual","creator_name":"Marcio Diaz","creator_url":"https://huggingface.co/marciodiaz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"kazakh-ift","keyword":"instruction-following","description":"Kazakh-IFT ðŸ‡°ðŸ‡¿\nAuthors: Nurkhan Laiyk, Daniil Orel, Rituraj Joshi, Maiya Goloburda, Yuxia Wang, Preslav Nakov, Fajri Koto\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nInstruction tuning in low-resource languages remains challenging due to limited coverage of region-specific institutional and cultural knowledge. To address this gap, we introduce a large-scale instruction-following dataset (~10,600 samples) focused on Kazakhstan, spanning domains such as governance, legal processes, cultural practices, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nurkhan5l/kazakh-ift.","url":"https://huggingface.co/datasets/nurkhan5l/kazakh-ift","creator_name":"Nurkhan Laiyk","creator_url":"https://huggingface.co/nurkhan5l","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Kazakh","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"xyrus-cosmic-training-dataset-complete","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tðŸŒŒ Xyrus Cosmic Complete Training Dataset (Harmony Format)\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe COMPLETE training dataset for Xyrus Cosmic GPT-OSS:20B, including all expansions and variations.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Statistics\n\t\n\n\nTotal Unique Examples: 1781\nFormat: Harmony (GPT-OSS chat format)\nSplits: Train (1424) / Val (178) / Test (179)\n\n\n\t\n\t\t\n\t\tDataset Components\n\t\n\n\nxyrus_training_dataset.jsonl: 309 examples\nxyrus_augmented_dataset.jsonl: 391 examples\nxyrus_sdg_dataset.jsonl: 135 examplesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ToddLLM/xyrus-cosmic-training-dataset-complete.","url":"https://huggingface.co/datasets/ToddLLM/xyrus-cosmic-training-dataset-complete","creator_name":"Todd Deshane","creator_url":"https://huggingface.co/ToddLLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"twinkle-dialogue-gemma3-2025-08","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTwinkle Dialogue (Gemma-3-12B-it, 2025-08)\n\t\n\n\n  \n    \n  \n  \n    \n  \n\n\næœ¬è³‡æ–™é›†ç”± Gemma-3-12B-itï¼ˆTwinkle AI ç¤¾ç¾¤æœå‹™ï¼‰ ç”Ÿæˆä¹‹å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ï¼Œä¸¦æ•´åˆï¼š\n\nReference-freeï¼ˆç”± seed æ´¾ç”Ÿå–®è¼ªå•ç­”ï¼‰\nReference-basedï¼ˆä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ï¼‰\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"messages\": [{\"role\":\"system\",\"content\":\"...\"}, ...]}\nè¨“ç·´æ™‚å¯æ“·å–ç¬¬ä¸€å€‹ user èˆ‡å°æ‡‰ assistant å½¢æˆ (instruction, response) pairï¼Œæˆ–ç›´æŽ¥ä½¿ç”¨ chat æ ¼å¼çš„ trainerã€‚\n\n\n\t\n\t\t\n\t\tä¾†æºèˆ‡é™åˆ¶\n\t\n\n\nModel:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/tw-llama/twinkle-dialogue-gemma3-2025-08.","url":"https://huggingface.co/datasets/tw-llama/twinkle-dialogue-gemma3-2025-08","creator_name":"Taiwan Llama","creator_url":"https://huggingface.co/tw-llama","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"filtered-high-quality-dpo","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tFiltered High-Quality Instruction-Output Dataset\n\t\n\nThis dataset contains high-quality (score = 5) instruction-output pairs generated via a reverse instruction generation pipeline using a fine-tuned backward model and evaluated by LLaMA-2-7B-Chat.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\ninstruction: The generated prompt.\noutput: The original response.\nscore: Quality score assigned ( score = 5 retained).\n\n","url":"https://huggingface.co/datasets/helloTR/filtered-high-quality-dpo","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"tw-reasoning-instruct-50k","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDataset Card for tw-reasoning-instruct-50k\n\t\n\n\n\ntw-reasoning-instruct-50k æ˜¯ä¸€å€‹ç²¾é¸çš„ ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ï¼‰ æŽ¨ç†è³‡æ–™é›†ï¼Œæ—¨åœ¨æå‡èªžè¨€æ¨¡åž‹æ–¼é€æ­¥é‚è¼¯æ€è€ƒã€è§£é‡‹ç”Ÿæˆèˆ‡èªžè¨€ç†è§£ç­‰ä»»å‹™ä¸­çš„è¡¨ç¾ã€‚è³‡æ–™å…§å®¹æ¶µè“‹æ—¥å¸¸æ€è¾¨ã€æ•™è‚²å°è©±ã€æ³•å¾‹æŽ¨ç†ç­‰å¤šå…ƒä¸»é¡Œï¼Œä¸¦çµåˆã€Œæ€è€ƒæ­¥é©Ÿã€èˆ‡ã€Œæœ€çµ‚ç­”æ¡ˆã€çš„çµæ§‹è¨­è¨ˆï¼Œå¼•å°Žæ¨¡åž‹ä»¥æ›´æ¸…æ™°ã€æ¢ç†åˆ†æ˜Žçš„æ–¹å¼é€²è¡ŒæŽ¨è«–èˆ‡å›žæ‡‰ï¼Œç‰¹åˆ¥å¼·èª¿ç¬¦åˆå°ç£æœ¬åœ°èªžè¨€èˆ‡æ–‡åŒ–èƒŒæ™¯çš„æ‡‰ç”¨éœ€æ±‚ã€‚\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\næœ¬è³‡æ–™é›†å°ˆç‚ºç™¼å±•å…·å‚™å¼·å¤§æŽ¨ç†èƒ½åŠ›çš„ç¹é«”ä¸­æ–‡å¤§åž‹èªžè¨€æ¨¡åž‹ï¼ˆLarge Reasoning Models, LRMï¼‰æ‰€è¨­è¨ˆï¼Œå…§å®¹æ·±åº¦çµåˆå°ç£çš„èªžè¨€èˆ‡æ–‡åŒ–è„ˆçµ¡ã€‚æ¯ç­†è³‡æ–™é€šå¸¸åŒ…å«ä½¿ç”¨è€…çš„æå•ã€æ¨¡åž‹çš„å›žæ‡‰ï¼Œä»¥åŠæ¸…æ¥šçš„æŽ¨ç†éŽç¨‹ã€‚è³‡æ–™é›†è¨­è¨ˆç›®æ¨™ç‚ºåŸ¹é¤Šæ¨¡åž‹å…·å‚™é¡žäººé‚è¼¯çš„é€æ­¥æ€è€ƒèˆ‡è§£é‡‹èƒ½åŠ›ã€‚\næ­¤è³‡æ–™é›†é©ç”¨æ–¼è¨“ç·´èˆ‡è©•ä¼°ä»¥ä¸‹ä»»å‹™ï¼š\n\nå°ç£ç¤¾æœƒçš„æ—¥å¸¸æŽ¨ç†\næ•™è‚²æ€§å°è©±\nä»¥è§£é‡‹ç‚ºå°Žå‘çš„ç”Ÿæˆä»»å‹™â€¦ See the full description on the dataset page: https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k.","url":"https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k","creator_name":"Twinkle AI","creator_url":"https://huggingface.co/twinkle-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ProcessedOpenAssistant","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tRefined OASST1 Conversations\n\t\n\nDataset Name on Hugging Face: PursuitOfDataScience/ProcessedOpenAssistant\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is derived from the OpenAssistant/oasst1 conversations, with additional processing to:\n\nRemove single-turn or incomplete conversations (where a prompter/user message had no assistant reply),\nRename roles from \"prompter\" to \"User\" and \"assistant\" to \"Assistant\",\nOrganize each conversation as a list of turn objects.\n\nThe goal is to provide a cleanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant.","url":"https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant","creator_name":"Y. Yu","creator_url":"https://huggingface.co/PursuitOfDataScience","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"medical-vision-llm-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCombined Medical Vision-Language Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nComprehensive medical vision-language dataset with 4793 samples for vision-based LLM training.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Samples: 4793\nTraining Samples: 3834\nValidation Samples: 959\n\n\n\t\n\t\t\n\t\tModality Distribution\n\t\n\n\nX-ray: 2325 samples\nCT: 1351 samples\nUnknown: 812 samples\nMRI: 231 samples\nUltrasound: 70 samples\nMicroscopy: 2 samples\nEndoscopy: 2 samples\n\n\n\t\n\t\t\n\t\tBody Part Distribution\n\t\n\n\nUnknown:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/robailleo/medical-vision-llm-dataset.","url":"https://huggingface.co/datasets/robailleo/medical-vision-llm-dataset","creator_name":"Robail Yasrab ","creator_url":"https://huggingface.co/robailleo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"EchoX-Dialougues","keyword":"instruction-following","description":"\n\n  EchoX-Dialogues: Training Data for EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs\n\n\n\n\n  ðŸˆâ€â¬› GithubÂ ï½œÂ ðŸ“ƒ PaperÂ ï½œÂ ðŸš€ SpaceÂ \n\n\n  ðŸ§  EchoX-8BÂ ï½œÂ ðŸ§  EchoX-3BÂ ï½œÂ ðŸ“¦ EchoX-Dialogues-PlusÂ \n\n\nEchoX-Dialogues provides the primary speech dialogue data used to train EchoX, restricted to S2T (speech â†’ text) in this repository.\nAll input speech is synthetic; text is derived from public sources with multi-stage cleaning and rewriting. Most turns include asr /â€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/EchoX-Dialougues.","url":"https://huggingface.co/datasets/FreedomIntelligence/EchoX-Dialougues","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","question-answering","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"EchoX-Dialougues","keyword":"multi-turn","description":"\n\n  EchoX-Dialogues: Training Data for EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs\n\n\n\n\n  ðŸˆâ€â¬› GithubÂ ï½œÂ ðŸ“ƒ PaperÂ ï½œÂ ðŸš€ SpaceÂ \n\n\n  ðŸ§  EchoX-8BÂ ï½œÂ ðŸ§  EchoX-3BÂ ï½œÂ ðŸ“¦ EchoX-Dialogues-PlusÂ \n\n\nEchoX-Dialogues provides the primary speech dialogue data used to train EchoX, restricted to S2T (speech â†’ text) in this repository.\nAll input speech is synthetic; text is derived from public sources with multi-stage cleaning and rewriting. Most turns include asr /â€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/EchoX-Dialougues.","url":"https://huggingface.co/datasets/FreedomIntelligence/EchoX-Dialougues","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","question-answering","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"korean_roleplay_dataset_for_chat_game_2","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tKorean Roleplay Enhanced Conversations Dataset (v3)\n\t\n\n\n  \n  \n  \n  \n\n\n\n\t\n\t\t\n\t\tðŸ“‹ Dataset Description\n\t\n\nThis is the third version of our enhanced Korean roleplay conversation dataset, specifically designed for training conversational AI models in visual novel/dating simulation contexts. This version significantly expands the dataset with more diverse multi-turn conversations and improved context awareness.\n\n\t\n\t\t\n\t\tðŸŽ¯ Key Features\n\t\n\n\nLarge Scale: 25,568 high-quality conversationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/junidude14/korean_roleplay_dataset_for_chat_game_2.","url":"https://huggingface.co/datasets/junidude14/korean_roleplay_dataset_for_chat_game_2","creator_name":"Seung Jun Lee","creator_url":"https://huggingface.co/junidude14","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Korean","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"text-stylization-dataset","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tText Stylization Instruction Dataset\n\t\n\nInstruction dataset for text stylization across multiple writing styles while preserving meaning and information\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains instruction-following examples for training models to rewrite text in different styles while preserving the exact same meaning and information. The dataset covers multiple writing styles including Academic, Formal, Simple, Casual, and various educational levels.\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/upvantage/text-stylization-dataset.","url":"https://huggingface.co/datasets/upvantage/text-stylization-dataset","creator_name":"unknown","creator_url":"https://huggingface.co/upvantage","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"corporateDataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCorporate Data Analysis Training Dataset (Clean)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned and standardized corporate analysis training dataset with consistent schema.\n\n\t\n\t\t\n\t\tSchema\n\t\n\nAll entries follow the instruction-input-output format:\n{\n  \"instruction\": \"Task description\",\n  \"input\": \"Business data or context\", \n  \"output\": \"Analysis and insights\"\n}\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nâœ… Consistent Schema - All entries use the same format\nâœ… Clean Data - Validated and error-free\nâœ…â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MikePfunk28/corporateDataset.","url":"https://huggingface.co/datasets/MikePfunk28/corporateDataset","creator_name":"Michael Pfundt","creator_url":"https://huggingface.co/MikePfunk28","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"eg-legal-instruction-following","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tArabic Legal Dataset - Legal Instruction Following\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nInstruction-following dataset for diverse legal text analysis tasks through natural language commands.\nThis dataset contains 4,184 examples of instruction_following data derived from Egyptian legal texts, including criminal law, civil law, procedural law, and personal status law. The dataset is designed for training and evaluating Arabic legal AI models.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Arabicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fr3on/eg-legal-instruction-following.","url":"https://huggingface.co/datasets/fr3on/eg-legal-instruction-following","creator_name":"Ahmed Mardi","creator_url":"https://huggingface.co/fr3on","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"AL-GR","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tAL-GR: A Large-scale Generative Recommendation Dataset\n\t\n\nPaper: FORGE: Forming Semantic Identifiers for Generative Retrieval in Industrial DatasetsCode: https://github.com/selous123/al_sidProject Page: https://huggingface.co/datasets/AL-GR\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nAL-GR is a large-scale dataset designed for generative recommendation tasks using Large Language Models (LLMs). The core idea is to transform user historical behavior sequences into natural language prompts, enablingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AL-GR/AL-GR.","url":"https://huggingface.co/datasets/AL-GR/AL-GR","creator_name":"ALGR","creator_url":"https://huggingface.co/AL-GR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-retrieval","feature-extraction","image-feature-extraction","English"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-FC-Reasoning-en-10k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\n\nTotal Samples: 10,000  \nPrimary Focus:  \nHigh-quality Function Calling demonstrations  \nClear, well-structured Chain of Thought reasoning\n\n\nSelection Process:  \nTranslated from Chinese to English\nRemoved noisy or incomplete examples  \nSelected cases with precise function arguments  \nEach example has been made clearer and more effective\nEnsured reasoning steps are logically sound and human-readable\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŽ¯ Use Cases\n\t\n\nThis dataset is ideal for:\n\nFine-tuningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-en-10k.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-en-10k","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-FC-Reasoning-en-10k","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\n\nTotal Samples: 10,000  \nPrimary Focus:  \nHigh-quality Function Calling demonstrations  \nClear, well-structured Chain of Thought reasoning\n\n\nSelection Process:  \nTranslated from Chinese to English\nRemoved noisy or incomplete examples  \nSelected cases with precise function arguments  \nEach example has been made clearer and more effective\nEnsured reasoning steps are logically sound and human-readable\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŽ¯ Use Cases\n\t\n\nThis dataset is ideal for:\n\nFine-tuningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-en-10k.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-en-10k","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"bioinstruct","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for BioInstruct\n\t\n\nGitHub repo: https://github.com/bio-nlp/BioInstruct\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBioInstruct is a dataset of 25k instructions and demonstrations generated by OpenAI's GPT-4 engine in July 2023. \nThis instruction data can be used to conduct instruction-tuning for language models (e.g. Llama) and make the language model follow biomedical instruction better. \nImprovements of Llama on 9 common BioMedical tasks are shown in the result section. \nTakingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bio-nlp-umass/bioinstruct.","url":"https://huggingface.co/datasets/bio-nlp-umass/bioinstruct","creator_name":"UMass BioNLP Lab","creator_url":"https://huggingface.co/bio-nlp-umass","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"Leopard-Instruct","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tLeopard-Instruct\n\t\n\nPaper | Github | Models-LLaVA | Models-Idefics2\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nLeopard-Instruct is a large instruction-tuning dataset, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. It's been used to train Leopard-LLaVA [checkpoint]  and Leopard-Idefics2 [checkpoint].\n\n\t\n\t\t\n\t\tLoading dataset\n\t\n\n\nto load the dataset without automatically downloading and process the images (Please run the following codes with datasets==2.18.0)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wyu1/Leopard-Instruct.","url":"https://huggingface.co/datasets/wyu1/Leopard-Instruct","creator_name":"Wenhao Yu","creator_url":"https://huggingface.co/wyu1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"instruction","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"ATCgpt-Fixed","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for ATCgpt-Fixed\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed.","url":"https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"medical","keyword":"instruction-finetuning","description":"From https://huggingface.co/datasets/shibing624/medical\n","url":"https://huggingface.co/datasets/bohu/medical","creator_name":"tang","creator_url":"https://huggingface.co/bohu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"Atma4","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma4\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4.","url":"https://huggingface.co/datasets/HappyAIUser/Atma4","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMLU-Alpaca","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for MMLU-Alpaca\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca.","url":"https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"synapse-set-10k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ§  SynapseSet-10K\n\t\n\nSynapseSet-10K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nðŸ”¬ 100% synthetic, non-clinical data. Intended for academic and researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-10k.","url":"https://huggingface.co/datasets/NextGenC/synapse-set-10k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Turkish","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned.","url":"https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned","creator_name":"Cristian Velasquez (Entreprenerdly.com)","creator_url":"https://huggingface.co/Entreprenerdly","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_data_clean","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca_data_clean.","url":"https://huggingface.co/datasets/aaaalon/alpaca_data_clean","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"starcoder-python-instruct","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tStarCoder-Python-Qwen-Instruct\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Python code samples paired with synthetically generated natural language instructions. It is designed for supervised fine-tuning of language models for code generation tasks. The dataset is derived from the Python subset of the bigcode/starcoderdata corpus, and the instructional text for each code sample was generated using the Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8 model.\n\n\t\n\t\t\n\t\n\t\n\t\tCreationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OLMo-Coding/starcoder-python-instruct.","url":"https://huggingface.co/datasets/OLMo-Coding/starcoder-python-instruct","creator_name":"OLMo-Coding","creator_url":"https://huggingface.co/OLMo-Coding","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","code","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDeepthink Reasoning Demo\n\t\n\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction.","url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-Codex-Weaver-FC-Reasoning","keyword":"instruction","description":"\n\t\n\t\t\n\t\tArcosoph Codex Weaver Function Calling Reasoning Dataset (V1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWelcome to the Arcosoph-Codex-Weaver-FC-Reasoning dataset! This is a comprehensive, multi-source, and meticulously curated dataset designed for instruction-tuning language models to function as intelligent, offline AI agents.\nThis dataset is provided in a universal, easy-to-parse JSON Lines (.jsonl) format, making it an ideal \"source of truth\" for creating fine-tuning data for various modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-Codex-Weaver-FC-Reasoning","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tArcosoph Codex Weaver Function Calling Reasoning Dataset (V1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWelcome to the Arcosoph-Codex-Weaver-FC-Reasoning dataset! This is a comprehensive, multi-source, and meticulously curated dataset designed for instruction-tuning language models to function as intelligent, offline AI agents.\nThis dataset is provided in a universal, easy-to-parse JSON Lines (.jsonl) format, making it an ideal \"source of truth\" for creating fine-tuning data for various modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-Codex-Weaver-FC-Reasoning","keyword":"multi-turn","description":"\n\t\n\t\t\n\t\tArcosoph Codex Weaver Function Calling Reasoning Dataset (V1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWelcome to the Arcosoph-Codex-Weaver-FC-Reasoning dataset! This is a comprehensive, multi-source, and meticulously curated dataset designed for instruction-tuning language models to function as intelligent, offline AI agents.\nThis dataset is provided in a universal, easy-to-parse JSON Lines (.jsonl) format, making it an ideal \"source of truth\" for creating fine-tuning data for various modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-Eval","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\nè®­ç»ƒé›† (SFT Dataset)ï¼šåŒ…å« 5,287 æ¡é«˜è´¨é‡é—®ç­”å¯¹ï¼Œç”¨äºŽæ¨¡åž‹å¾®è°ƒã€‚\næ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFT Dataset)ï¼šï¼ˆæŽ¨èï¼‰ è¿™æ˜¯æœ¬æ•°æ®é›†çš„å‡çº§ç‰ˆã€‚æˆ‘ä»¬è®¾è®¡å¹¶åº”ç”¨äº†ä¸¤é˜¶æ®µçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œä¸ºæ¯ä¸€æ¡æ•°æ®éƒ½æ³¨å…¥äº†é«˜è´¨é‡çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡æ¨¡åž‹çš„é€»è¾‘æŽ¨ç†ä¸Žæ·±åº¦åˆ†æžèƒ½åŠ›ã€‚\nè¯„ä¼°é›† (Evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-Eval.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-Eval","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Test2","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WhiteHunter111/Test2.","url":"https://huggingface.co/datasets/WhiteHunter111/Test2","creator_name":"Thomas Flato","creator_url":"https://huggingface.co/WhiteHunter111","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true}
]
;
