const data_for_modality_instruction_following = 
[
	{"name":"security_steerability","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/itayhf/security_steerability","creator_name":"Itay H","creator_url":"https://huggingface.co/itayhf","description":"\n\t\n\t\t\n\t\tDataset Card for VeganRibs & ReverseText\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains two datasets, VeganRibs and ReverseText, designed to evaluate the Security Steerability of Large Language Models (LLMs). \nSecurity Steerability refers to an LLM's ability to strictly adhere to application-specific policies and functional instructions defined within its system prompt, even when faced with conflicting or manipulative user inputs. These datasets aim to bridge the gap inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/itayhf/security_steerability.","first_N":5,"first_N_keywords":["English","mit","arxiv:2504.19521","ğŸ‡ºğŸ‡¸ Region: US","evaluation"],"keywords_longer_than_N":true},
	{"name":"Text_Guided_Image_Editing-ru","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Text_Guided_Image_Editing-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"Translated instructions from ImagenHub/Text_Guided_Image_Editing into Russian using gemini-flash-1.5-8b.\n","first_N":5,"first_N_keywords":["image-to-image","translated","ImagenHub/Text_Guided_Image_Editing","Russian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"OpenManus-RL","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","description":"\n\t\n\t\t\n\t\tDataset Card for OpenManusRL\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\n  ğŸ’» [Github Repo]\n\n\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\n\nğŸ” ReAct Framework - Reasoning-Acting integration\nğŸ§  Structured Training - Separate format/reasoning learning\nğŸš« Anti-Hallucination - Negative samples + environment grounding\nğŸŒ 6 Domains - OS, DB, Web, KG, Household, E-commerce\n\n\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\nSourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"muInstruct-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/muInstruct-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tmuInstruct-ru\n\t\n\nTranslated instructions from EleutherAI/muInstruct into Russian using gemini-flash-1.5-8b.\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","EleutherAI/muInstruct","Russian"],"keywords_longer_than_N":true},
	{"name":"werewolf_game_reasoning","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning","creator_name":"Rong Ye","creator_url":"https://huggingface.co/ReneeYe","description":"\n\t\n\t\t\n\t\tWerewolf Game Dataset\n\t\n\nThis repository contains a comprehensive dataset for the Werewolf game in paper Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game, including both raw game data and processed  multi-level instruction datasets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tRaw Data\n\t\n\nThe raw data is located in the raw folder. Each game consists of two files:\n\nevent.json: Contains the game regular record and thinking process data, including:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning.","first_N":5,"first_N_keywords":["text-generation","expert-generated","multilingual","original","Chinese"],"keywords_longer_than_N":true},
	{"name":"ru-instruct-conversation-v1","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-instruct-conversation-v1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian dialogs in form of conversations suitable for LLM fine-tuning scenarios.\nTotal samples: 82208\nDeduplicated using simhash(hamming_treshold=3).\nDatasets used:\n\nIlyaGusev/saiga_scored (min_score: 8, no bad by regexp)\nIlyaGusev/oasst2_ru_main_branch\nattn-signs/kolmogorov-3\nattn-signs/russian-easy-instructions\n\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nĞ¢Ñ‹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ: <think> Ğ¢Ğ²Ğ¾Ğ¸ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ </think> \nĞ¢Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1-v2","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"This is a modified version of ZeroAgency/ru-thinking-reasoning-r1 with addition of Egor-AI/CoT-XLang dataset.\nCombined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nĞ¢Ñ‹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"fin-term-instruct","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taetae030/fin-term-instruct","creator_name":"leetaehee","creator_url":"https://huggingface.co/taetae030","description":"\n\t\n\t\t\n\t\tğŸ“˜ fin-term-instruct: í•œêµ­ì–´ ê¸ˆìœµ ìš©ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹\n\t\n\nfin-term-instructëŠ” í•œêµ­ì–´ ê¸ˆìœµ ìš©ì–´ ì„¤ëª…ì— íŠ¹í™”ëœ instruct-style ì§ˆë¬¸-ì‘ë‹µ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.Metaì˜ LLaMA ì‹œë¦¬ì¦ˆ ë“± ëŒ€í˜• ì–¸ì–´ëª¨ë¸(LLM)ì„ í•œêµ­ì–´ ê¸ˆìœµ ì±—ë´‡ìœ¼ë¡œ íŠœë‹í•˜ê¸° ìœ„í•´ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n\n\t\n\t\t\n\t\tğŸ“¦ ì›ë³¸ ì¶œì²˜: AI í—ˆë¸Œ\n\t\n\nì´ ë°ì´í„°ëŠ” AI í—ˆë¸Œì˜ **\"ê¸ˆìœµÂ·ë²•ë¥  ë¬¸ì„œ ê¸°ê³„ë…í•´ ë°ì´í„°\"**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•í•˜ì˜€ìŠµë‹ˆë‹¤.\n\nğŸ“‚ ì›ë³¸ ì£¼ì†Œ: AI í—ˆë¸Œ â€“ ê¸ˆìœµÂ·ë²•ë¥  ë¬¸ì„œ ê¸°ê³„ë…í•´ ë°ì´í„°\në°ì´í„° êµ¬ì¶•ë…„ë„: 2022ë…„\nì´ êµ¬ì¶•ëŸ‰: 400,000ê±´\në°ì´í„° í˜•ì‹: JSON (ì§€ë¬¸ - ì§ˆë¬¸ - ì •ë‹µ êµ¬ì„±)\n\n\n\t\n\t\t\n\t\tğŸ” ì‚¬ìš© ë²”ìœ„\n\t\n\n\nì „ì²´ ë°ì´í„° ì¤‘ **ê¸ˆìœµê²½ì œ ë¶„ì•¼(ì•½ 17.3%)**ë§Œ ì„ ë³„í•˜ì—¬ ì‚¬ìš©\nê¸°ì¡´ MRC í˜•íƒœì—ì„œ instruction-style QA í¬ë§·ìœ¼ë¡œ ì¬ê°€ê³µ\nGPT ê¸°ë°˜ ìš”ì•½Â·ì •ì œë¥¼ í†µí•´ ê°„ê²°í•œ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ í†µì¼â€¦ See the full description on the dataset page: https://huggingface.co/datasets/taetae030/fin-term-instruct.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"KemSU","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NodeLinker/KemSU","creator_name":"Ilya Pereverzin","creator_url":"https://huggingface.co/NodeLinker","description":"\n\t\n\t\t\n\t\tğŸ“ Kemerovo State University Instructional QA Dataset (NodeLinker/KemSU)\n\t\n\n\n  \n     \n  \n\n\n  \n    \n  \n  \n    \n  \n\n\n\n\n\t\n\t\n\t\n\t\tğŸ“ Dataset Overview & Splits\n\t\n\nThis dataset provides instructional question-answer (Q&A) pairs meticulously crafted for Kemerovo State University (ĞšĞµĞ¼Ğ“Ğ£, KemSU), Russia. Its primary purpose is to facilitate the fine-tuning of Large Language Models (LLMs), enabling them to function as knowledgeable and accurate assistants on a wide array of topics concerningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NodeLinker/KemSU.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"ru-tasks-conversation","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-tasks-conversation","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian math and physics tasks in form of conversation suitable for LLM fine-tuning scenarios.\nTotal samples: 462883\nDatasets used:\n\nVikhrmodels/russian_math\nVikhrmodels/russian_physics\nd0rj/MathInstruct-ru\nd0rj/orca-math-word-problems-200k-ru\nevilfreelancer/MATH-500-Russian\n\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-qa-data","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sweatSmile/alpaca-qa-data","creator_name":"amitk17","creator_url":"https://huggingface.co/sweatSmile","description":"\n\t\n\t\t\n\t\tAlpaca-style Question and Answer Dataset\n\t\n\nThis dataset contains question-answer pairs formatted in the Alpaca instruction style, suitable for instruction fine-tuning of language models.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example contains:\n\ninstruction: The question\ninput: Empty string (can be used for context in other applications)\noutput: The answer\ntext: The formatted text using the Alpaca template\n\n\n\t\n\t\t\n\t\tTemplate\n\t\n\nBelow is an instruction that describes a task, paired with an input thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sweatSmile/alpaca-qa-data.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"stage1-doctor-patient-chat","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BirdieByte1024/stage1-doctor-patient-chat","creator_name":"BirdieByte","creator_url":"https://huggingface.co/BirdieByte1024","description":"\n\t\n\t\t\n\t\tğŸ¦· Stage 1 - AI Doctor Tone Dataset (Dental)\n\t\n\nThis dataset contains instructionâ€“response formatted examples derived from realistic patient-doctor conversations, focused on general medical behavior and tone. It is designed as Stage 1 in a two-stage fine-tuning pipeline for building a domain-specific, polite, and structured AI dental assistant.\n\n\n\t\n\t\t\n\t\tâœ¨ Intended Use\n\t\n\n\nFine-tuning large language models (LLMs) to simulate human-like, empathetic, and structured medical responses.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BirdieByte1024/stage1-doctor-patient-chat.","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"streetview-commands-dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cahlen/streetview-commands-dataset","creator_name":"Cahlen Humphreys","creator_url":"https://huggingface.co/cahlen","description":"\n\n\t\n\t\t\n\t\tDataset Card for streetview-commands-dataset\n\t\n\nThis dataset contains pairs of natural language instructions (simulating commands given to Google Street View) and their corresponding structured JSON outputs representing the intended navigation action. It was generated using the Gemini API (gemini-1.5-flash-latest) based on predefined templates and few-shot examples.\nThe primary intended use is for fine-tuning small language models (like TinyLlama) to act as a translation layer betweenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cahlen/streetview-commands-dataset.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"EFAGen-Llama-3.1-8B-Instruct-Training-Data","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/codezakh/EFAGen-Llama-3.1-8B-Instruct-Training-Data","creator_name":"Zaid Khan","creator_url":"https://huggingface.co/codezakh","description":"Paper Link\nThe training data used for the final version of EFAGen-Llama-3.1-8B-Instruct.\nThe data is in Alpaca format and can be used with Llama-Factory (check dataset_info.json).\n","first_N":5,"first_N_keywords":["text-generation","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"filtered-high-quality-dpo","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/helloTR/filtered-high-quality-dpo","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","description":"\n\t\n\t\t\n\t\tFiltered High-Quality Instruction-Output Dataset\n\t\n\nThis dataset contains high-quality (score = 5) instruction-output pairs generated via a reverse instruction generation pipeline using a fine-tuned backward model and evaluated by LLaMA-2-7B-Chat.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\ninstruction: The generated prompt.\noutput: The original response.\nscore: Quality score assigned ( score = 5 retained).\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-contrast-sample","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/helloTR/dpo-contrast-sample","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","description":"\n\t\n\t\t\n\t\tDPO Contrast Sample\n\t\n\nThis dataset provides a direct comparison between high-quality and low-quality instruction-output pairs evaluated by a LLaMA-2-7B-Chat model in a DPO-style workflow.\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\ninstruction: The generated prompt (x).\noutput: The associated output (y).\nscore: Quality rating assigned by LLaMA2 (1 = high, 5 = low).\n\n\n\t\n\t\t\n\t\tSamples\n\t\n\n5 examples with score = 1 (excellent), and 5 with score = 5 (poor).\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"collabllm-20q","keyword":"multiturn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aditijb/collabllm-20q","creator_name":"Aditi","creator_url":"https://huggingface.co/aditijb","description":"aditijb/collabllm-20q dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"synapse-set-10k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NextGenC/synapse-set-10k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","description":"\n\t\n\t\t\n\t\tğŸ§  SynapseSet-10K\n\t\n\nSynapseSet-10K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nğŸ”¬ 100% synthetic, non-clinical data. Intended for academic and researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-10k.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","Turkish","mit"],"keywords_longer_than_N":true},
	{"name":"synapse-set-100k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NextGenC/synapse-set-100k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","description":"\n\t\n\t\t\n\t\tğŸ§  SynapseSet-100K\n\t\n\nSynapseSet-100K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nğŸ”¬ 100% synthetic, non-clinical data. Intended for academic and researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-100k.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","Turkish","mit"],"keywords_longer_than_N":true},
	{"name":"solidity_vulnerability_audit_dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GitmateAI/solidity_vulnerability_audit_dataset","creator_name":"GitmateAI","creator_url":"https://huggingface.co/GitmateAI","description":"\n\t\n\t\t\n\t\tSolidity Vulnerability Audit Dataset\n\t\n\n\nOrganization: gitmate AI\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Solidity Vulnerability Audit Dataset is a curated collection of Solidity smart contract code snippets paired with expert-written vulnerability audits. Each entry presents a real or realistic smart contract scenario, and the corresponding analysis identifies security vulnerabilities or confirms secure patterns. The dataset is designed for instruction-tuned large language models (LLMs) toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GitmateAI/solidity_vulnerability_audit_dataset.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ScanBot","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ed1son/ScanBot","creator_name":"zhiling chen","creator_url":"https://huggingface.co/ed1son","description":" \n\n\t\n\t\t\n\t\tScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tğŸ§  Dataset Summary\n\t\n\nScanBot is a dataset for instruction-conditioned, high-precision surface scanning with robots. Unlike existing datasets that focus on coarse tasks like grasping or navigation, ScanBot targets industrial laser scanning, where sub-millimeter accuracy and parameter stability are essential. It includes scanning trajectories across 12 objects and 6 task types, each driven byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ed1son/ScanBot.","first_N":5,"first_N_keywords":["robotics","English","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"ru-big-russian-dataset","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-big-russian-dataset","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"\n\t\n\t\t\n\t\tBig Russian Dataset\n\t\n\nMade by ZeroAgency.ru - telegram channel.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset size\n\t\n\n\nTrain: 1 710 601 samples (filtered from 2_149_360)\nTest:  18 520 samples (not filtered)\n\n\n\t\n\t\t\n\t\n\t\n\t\tEnglish\n\t\n\nThe Big Russian Dataset is a combination of various primarily Russianâ€‘language datasets. With some sort of reasoning!\nThe dataset was deduplicated, cleaned, scored using gpt-4.1 and filtered.\n\n\t\n\t\t\n\t\n\t\n\t\tĞ ÑƒÑÑĞºĞ¸Ğ¹\n\t\n\nBig Russian Dataset - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€ÑƒÑÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-big-russian-dataset.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"synapse-set-50k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NextGenC/synapse-set-50k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","description":"\n\t\n\t\t\n\t\tğŸ§  SynapseSet-50K\n\t\n\nSynapseSet-50K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nğŸ”¬ 100% synthetic, non-clinical data. Intended for academic and researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-50k.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","Turkish","mit"],"keywords_longer_than_N":true},
	{"name":"CoIN-ASD","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jinpeng0528/CoIN-ASD","creator_name":"Jinpeng Chen","creator_url":"https://huggingface.co/jinpeng0528","description":"\n\t\n\t\t\n\t\tCoIN-ASD Benchmark\n\t\n\nCoIN-ASD is a benchmark dataset designed for multimodal continual instruction tuning (MCIT), based on the CoIN dataset. This dataset aims to evaluate the performance of MCIT models in mitigating essential forgetting.\nğŸ“ Paper\nğŸ™ GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized in the following structure:\nâ”œâ”€â”€ ScienceQA/\nâ”‚   â”œâ”€â”€ train_ori.json\nâ”‚   â”œâ”€â”€ train_x{10,20,40,60,80}.json\nâ”‚   â””â”€â”€ test.json\nâ”œâ”€â”€ TextVQA/\nâ”‚   â”œâ”€â”€ train_ori.json\nâ”‚   â”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jinpeng0528/CoIN-ASD.","first_N":5,"first_N_keywords":["English","mit","arxiv:2505.02486","ğŸ‡ºğŸ‡¸ Region: US","multimodal-continual-instruction-tuning"],"keywords_longer_than_N":true},
	{"name":"RoleBench","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZenMoore/RoleBench","creator_name":"Zekun Moore Wang","creator_url":"https://huggingface.co/ZenMoore","description":"\n\t\n\t\t\n\t\n\t\n\t\tRoleBench\n\t\n\n\nPaper Title: RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models\narXiv Link: https://arxiv.org/abs/2310.00746\nGithub Repo: https://github.com/InteractiveNLP-Team/RoleLLM-public\n\nPlease read our paper for more details about this dataset.\nTL;DR: We introduce RoleLLM, a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZenMoore/RoleBench.","first_N":5,"first_N_keywords":["Chinese","English","apache-2.0","Text","arxiv:2310.00746"],"keywords_longer_than_N":true},
	{"name":"gt-doremiti-instructions","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Gt-Doremiti/gt-doremiti-instructions","creator_name":"Doremiti","creator_url":"https://huggingface.co/Gt-Doremiti","description":"\n\t\n\t\t\n\t\tDataset Card for gt-doremiti-instructions\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nJeu d'instruction pour fine-tuner un LLM suivant les prÃ©conisations du projet Stanford-Alpaca (https://github.com/tatsu-lab/stanford_alpaca)\nCes instructions sont extraites de la FAQ crÃ©e par le GT DOREMITI et disponible Ã  cette adresse (https://gt-atelier-donnees.miti.cnrs.fr/faq.html)\nLes donnÃ©es sont mise Ã  disposition selon les termes de la Licence Creative Commons Attribution 4.0 International.\n","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"fund-sft","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jannko/fund-sft","creator_name":"ko","creator_url":"https://huggingface.co/jannko","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jannko/fund-sft.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K<n<100K","ğŸ‡ºğŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"shell-cmd-instruct","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/byroneverson/shell-cmd-instruct","creator_name":"Byron Everson","creator_url":"https://huggingface.co/byroneverson","description":"\n\t\n\t\t\n\t\tUsed to train models that interact directly with shells\n\t\n\nNote: This dataset is out-dated in the llm world, probably easier to just setup a tool with a decent model that supports tooling.\nFollow-up details of my process \n\nMacOS terminal commands for now. This dataset is still in alpha stages and will be modified.\n\nContains 500 somewhat unique training examples so far.\n\nGPT4 seems like a good candidate for generating more data, licensing would need to be addressed.\n\nI fine-tunedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/byroneverson/shell-cmd-instruct.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ultrachat_200k_dutch","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\n\t\n\t\t\n\t\tDataset Card for UltraChat 200k Dutch\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"han-instruct-dataset-v1.0","keyword":"instruction-following","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\n\t\n\t\t\n\t\tDataset Card for \"han-instruct-dataset-v1.0\"\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nğŸª¿ Han (à¸«à¹ˆà¸²à¸™ or goose) Instruct Dataset is a Thai instruction dataset by PyThaiNLP. It collect the instruction following in Thai from many source.\nMany question are collect from Reference desk at Thai wikipedia.\nData sources:\n\nReference desk at Thai wikipedia.\nLaw from justicechannel.org\npythainlp/final_training_set_v1_enth: Human checked and edited.\nSelf-instruct from WangChanGLM\nWannaphong.com\nHumanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"han-instruct-dataset-v1.0","keyword":"instruction-finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\n\t\n\t\t\n\t\tDataset Card for \"han-instruct-dataset-v1.0\"\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nğŸª¿ Han (à¸«à¹ˆà¸²à¸™ or goose) Instruct Dataset is a Thai instruction dataset by PyThaiNLP. It collect the instruction following in Thai from many source.\nMany question are collect from Reference desk at Thai wikipedia.\nData sources:\n\nReference desk at Thai wikipedia.\nLaw from justicechannel.org\npythainlp/final_training_set_v1_enth: Human checked and edited.\nSelf-instruct from WangChanGLM\nWannaphong.com\nHumanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"urdu_alpaca_yc_filtered","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered","creator_name":"Mahwiz Khalil","creator_url":"https://huggingface.co/mahwizzzz","description":"\n\t\n\t\t\n\t\tAlpaca Urdu\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Alpaca Urdu  is a translation of the original dataset into Urdu. This dataset is a part of the Alpaca project and is designed for NLP tasks.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSize: The translated dataset contains [45,622] samples.\nLanguages: Urdu\nLicense: [cc-by-4.0]\nOriginal Dataset: Link to the original Alpaca Cleaned dataset repository\n\n\n\t\n\t\t\n\t\tColumns\n\t\n\nThe translated dataset includes the following columns:\n\ninput: The input text in Urdu.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered.","first_N":5,"first_N_keywords":["text-generation","Urdu","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","description":"\n\t\n\t\t\n\t\tLOGIC-701 Benchmark\n\t\n\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nThis benchmark is also part of anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701.","first_N":5,"first_N_keywords":["English","Russian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Truth","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abhishekbisaria/Truth","creator_name":"Abhishek Bisaria","creator_url":"https://huggingface.co/abhishekbisaria","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/abhishekbisaria/Truth.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"budapest-v0.1-hun","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun","creator_name":"Balazs Toldi","creator_url":"https://huggingface.co/Bazsalanszky","description":"\n\t\n\t\t\n\t\tBudapest-v0.1 Dataset README\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Budapest-v0.1 dataset is a cutting-edge resource specifically designed for fine-tuning large language models (LLMs). Created using GPT-4, this dataset is presented in a message-response format, making it particularly suitable for a variety of natural language processing tasks. The primary focus of Budapest-v0.1 is to aid in the development and enhancement of algorithms capable of performing summarization, question answeringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun.","first_N":5,"first_N_keywords":["text-generation","question-answering","Hungarian","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-bn","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn","creator_name":"fahim abrar","creator_url":"https://huggingface.co/abrarfahim","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned-bn\n\t\n\n\n\nThis is a cleaned bengali translated version of the original Alpaca Dataset released by Stanford. \n\n\t\n\t\t\n\t\tUses\n\t\n\nimport datasets\ndataset = datasets.load_dataset(\"abrarfahim/alpaca-cleaned-bn\")\nprint(dataset[0])\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{'system_prompt': 'You are a virtual assistant, deliver a comprehensive response.',\n 'qas_id': 'YY9S5K',\n 'question_text': '\"à¦¸à¦¨à§à¦¦à§‡à¦¹\" à¦¶à¦¬à§à¦¦à§‡à¦° à¦¸à¦ à¦¿à¦• à¦ªà§à¦°à¦¤à¦¿à¦¶à¦¬à§à¦¦ à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¨ à¦•à¦°à§à¦¨à¥¤',\n 'orig_answer_texts': '\"à¦¸à¦¨à§à¦¦à§‡à¦¹\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn.","first_N":5,"first_N_keywords":["question-answering","text-generation","Bengali","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-tw-input-output-52k","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DavidLanz/alpaca-tw-input-output-52k","creator_name":"David Lanz","creator_url":"https://huggingface.co/DavidLanz","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-tw-input-output-52k\"\n\t\n\nThis dataset contains English Instruction-Following generated by GPT-3.5 using Alpaca prompts for fine-tuning LLMs.\nThe dataset was originaly shared in this repository: https://github.com/ntunlplab/traditional-chinese-alpaca. This is just a wraper for compatibility with huggingface's datasets library.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset structure\n\t\n\nIt contains 52K instruction-following data generated by GPT-3.5 using the same prompts as in Alpaca.\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DavidLanz/alpaca-tw-input-output-52k.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-tw-input-output-48k","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DavidLanz/alpaca-gpt4-tw-input-output-48k","creator_name":"David Lanz","creator_url":"https://huggingface.co/DavidLanz","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-tw-input-output-48k\"\n\t\n\nThis dataset contains English Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.\nThe dataset was originaly shared in this repository: https://github.com/ntunlplab/traditional-chinese-alpaca. This is just a wraper for compatibility with huggingface's datasets library.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset structure\n\t\n\nIt contains 52K instruction-following data generated by GPT-4 using the same prompts as in Alpaca.\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DavidLanz/alpaca-gpt4-tw-input-output-48k.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yahma/alpaca-cleaned","creator_name":"Gene Ruebsamen","creator_url":"https://huggingface.co/yahma","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yahma/alpaca-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-zh","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shibing624/alpaca-zh","creator_name":"Ming Xu (å¾æ˜)","creator_url":"https://huggingface.co/shibing624","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/alpaca-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Hydrus-Claude-Instruct-5K","keyword":"instruct","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Delta-Vector/Hydrus-Claude-Instruct-5K","creator_name":"Mango","creator_url":"https://huggingface.co/Delta-Vector","description":"\n\t\n\t\t\n\t\tkalo made dis\n\t\n\nThanks to Kubernetes bad for filtering + converting this to sharegpt\nMix of Opus and 3.5 for data\nThis is a combined set of \nuncurated-raw-gens-og-test-filtered\nuncurated-raw-gens-opus-jul-31-filtered\nopus_jul10_test-filtered\nuncurated_opus_jul8-filtered \n","first_N":5,"first_N_keywords":["English","agpl-3.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"SWE-Bench-Verified-O1-reasoning-high-results","keyword":"multi-turn","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","description":"\n\t\n\t\t\n\t\tSWE-Bench Verified O1 Dataset\n\t\n\n\n\t\n\t\t\n\t\tExecutive Summary\n\t\n\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities on the SWE-Bench Verified dataset, achieving a 28.8% success rate across 500 test instances.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset was generated using the CodeAct framework, which aims to improve code generation through enhanced action-based reasoning.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Titanium2.1-DeepSeek-R1","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Click here to support our open-source dataset and model releases!\nTitanium2.1-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n31.7k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraformâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"helpful_instructions","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Helpful Instructions is a dataset of (prompt, completion) pairs that are derived from a variety of public datasets. As the name suggests, it focuses on instructions that are \"helpful\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"instruct_me","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/instruct_me","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Instruct Me is a dataset of instruction-like dialogues between a human user and AI assistant. The prompts are derived from (prompt, completion) pairs in the Helpful Instructions dataset. The goal is to train a language model to that is \"chatty\" and can answer the kind of questions or tasks a human user might instruct an AI assistant to perform.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-spanish","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bertin-project/alpaca-spanish","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","description":"\n\t\n\t\t\n\t\tBERTIN Alpaca Spanish\n\t\n\nThis dataset is a translation to Spanish of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\n","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"norwegian-alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NbAiLab/norwegian-alpaca","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","description":"\n\t\n\t\t\n\t\tNB Alpaca Norwegian BokmÃ¥l\n\t\n\nThis dataset is a translation to Norwegian BokmÃ¥l of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\n","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Alpaca-CoT","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QingyiSi/Alpaca-CoT","creator_name":"Qingyi Si","creator_url":"https://huggingface.co/QingyiSi","description":"\n\t\n\t\t\n\t\n\t\n\t\tInstruction-Finetuning Dataset Collection (Alpaca-CoT)\n\t\n\nThis repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model.\nWe also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT.  \nIf you think this dataset collection is helpful to you, please likeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT.","first_N":5,"first_N_keywords":["English","Chinese","Malayalam","apache-2.0","ğŸ‡ºğŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"vi_alpaca_clean","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tsdocode/vi_alpaca_clean","creator_name":"Thanh Sang","creator_url":"https://huggingface.co/tsdocode","description":"tsdocode/vi_alpaca_clean dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Vietnamese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"translated_polish_alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aspik101/translated_polish_alpaca","creator_name":"Hubert","creator_url":"https://huggingface.co/Aspik101","description":"The dataset was translated into Polish using this model: \"gsarti/opus-mt-tc-en-pl\"\n\n\t\n\t\t\n\t\n\t\n\t\tHow to use\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Aspik101/translated_polish_alpaca\")\n\n","first_N":5,"first_N_keywords":["text-generation","Polish","cc-by-4.0","ğŸ‡ºğŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"alpaca-id-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cahya/alpaca-id-cleaned","creator_name":"Cahya Wirawan","creator_url":"https://huggingface.co/cahya","description":"\n\t\n\t\t\n\t\tDataset Card for Indonesian Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the Indonesian translated version of the cleaned original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cahya/alpaca-id-cleaned.","first_N":5,"first_N_keywords":["text-generation","Indonesian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-data\"\n\t\n\nAll of the work is done by this team. \n\n\t\n\t\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\t\n\t\t\n\t\tChinese Dataset\n\t\n\nFound here\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Liâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data-zh","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-data-zh\"\n\t\n\nAll of the work is done by this team. \n\n\t\n\t\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\t\n\t\t\n\t\tEnglish Dataset\n\t\n\nFound here\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Liâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"AlpacaDataCleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexl83/AlpacaDataCleaned","creator_name":"Alessandro Lannocca","creator_url":"https://huggingface.co/alexl83","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alexl83/AlpacaDataCleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"TSSB-3M-instructions","keyword":"instruct","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zirui3/TSSB-3M-instructions","creator_name":"zirui","creator_url":"https://huggingface.co/zirui3","description":"\n\t\n\t\t\n\t\tdata summary\n\t\n\ninstruction dataset for code bugfix\n\n\t\n\t\t\n\t\tReference\n\t\n\n[1]. TSSB-3M-ext\n","first_N":5,"first_N_keywords":["code","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"NorPaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MasterThesisCBS/NorPaca","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","description":"\n\t\n\t\t\n\t\tNorPaca Norwegian BokmÃ¥l\n\t\n\nThis dataset is a translation to Norwegian BokmÃ¥l of alpaca_gpt4_data.json, a clean version of the Alpaca dataset made at Stanford, but generated with GPT4.\n\n\t\n\t\t\n\t\tPrompt to generate dataset\n\t\n\n    Du blir bedt om Ã¥ komme opp med et sett med 20 forskjellige oppgaveinstruksjoner. Disse oppgaveinstruksjonene vil bli gitt til en GPT-modell, og vi vil evaluere GPT-modellen for Ã¥ fullfÃ¸re instruksjonene. \n\nHer er kravene:\n1. PrÃ¸v Ã¥ ikke gjenta verbet for hverâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MasterThesisCBS/NorPaca.","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"NorEval","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MasterThesisCBS/NorEval","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","description":"\n\t\n\t\t\n\t\tNorEval\n\t\n\nNorEval is a self-curated dataset to evaluate instruction-following LLMs, seeking to evaluate the models in nine categories: Language, Code, Mathematics, Classification, Communication & Marketing, Medical, General Knowledge, and Business Operations\n","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ualpaca-gpt4","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nikes64/ualpaca-gpt4","creator_name":"MrNikes","creator_url":"https://huggingface.co/nikes64","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for \"alpaca-gpt4-cleaned\"\n\t\n\nThis dataset contains Ukrainian Instruction-Following translated by facebook/nllb-200-3.3B\nThe dataset was originaly shared in this repository: https://github.com/tloen/alpaca-lora\n\n\t\n\t\t\n\t\n\t\n\t\tLicensing Information\n\t\n\nThe dataset is available under the Creative Commons NonCommercial (CC BY-NC 4.0).\n","first_N":5,"first_N_keywords":["text-generation","question-answering","Ukrainian","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-tr","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cgulse/alpaca-cleaned-tr","creator_name":"Can G","creator_url":"https://huggingface.co/cgulse","description":"Alpaca Cleaned Dataset.\nMachine Translated facebook/nllb-200-3.3B\nLanguages\nTurkish\n","first_N":5,"first_N_keywords":["Turkish","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDataset Card for \"tasksource-instruct-v0\" (TSI)\n\t\n\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\nDataset size is capped at 30k examples per task to foster task diversity.\n!pip install tasksource, pandit\nimport tasksource, pandit\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\nfor tasks in df.id:\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\n\nhttps://github.com/sileod/tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","first_N":5,"first_N_keywords":["text2text-generation","text-generation","text-classification","token-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDataset Card for \"tasksource-instruct-v0\" (TSI)\n\t\n\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\nDataset size is capped at 30k examples per task to foster task diversity.\n!pip install tasksource, pandit\nimport tasksource, pandit\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\nfor tasks in df.id:\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\n\nhttps://github.com/sileod/tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","first_N":5,"first_N_keywords":["text2text-generation","text-generation","text-classification","token-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of prompts and responses to those prompts. All completions were generated by querying already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc.). The dataset is available in Portuguese, English, and Spanish.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguage modeling.\nQuestion-answeringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","Spanish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"reward-aira-dataset","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","description":"\n\t\n\t\t\n\t\tReward-Aira Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of prompt + completion examples of LLM following instructions in a conversational manner. All prompts come with two possible completions (one better than the other). The dataset is available in both Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized to train a reward/preference model or DPO fine-tuning.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nEnglish and Portuguese.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset.","first_N":5,"first_N_keywords":["text-classification","Portuguese","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","first_N":5,"first_N_keywords":["text2text-generation","text-classification","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","first_N":5,"first_N_keywords":["text2text-generation","text-classification","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-ru","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\talpaca-cleaned-ru\n\t\n\nTranslated version of yahma/alpaca-cleaned into Russian.\n","first_N":5,"first_N_keywords":["text-generation","translated","monolingual","yahma/alpaca-cleaned","Russian"],"keywords_longer_than_N":true},
	{"name":"VisIT-Bench","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/VisIT-Bench","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n\t\n\t\t\n\t\tDataset Card for VisIT-Bench\n\t\n\n\nDataset Description\nLinks\nDataset Structure\nData Fields\nData Splits\nData Loading\n\n\nLicensing Information\nAnnotations\nConsiderations for Using the Data\nCitation Information\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nVisIT-Bench is a dataset and benchmark for vision-and-language instruction following. The dataset is comprised of image-instruction pairs and corresponding example outputs, spanning a wide range of tasks, from simple object recognition to complexâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/VisIT-Bench.","first_N":5,"first_N_keywords":["crowdsourced","found","original","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"unix-commands","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/harpomaxx/unix-commands","creator_name":"Carlos A. Catania (Harpo)","creator_url":"https://huggingface.co/harpomaxx","description":"\n\t\n\t\t\n\t\tUnix Commands Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Unix Commands Dataset is a unique collection of real-world Unix command line examples, captured from various system prompts representing different user roles and responsibilities, such as system administrators, DevOps, network administrators, Docker administrators, regular users, and hackers.\nThe dataset consists of Unix commands ranging from basic to advanced levels and from a wide array of categories, including file operations (lsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/harpomaxx/unix-commands.","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-serbian-full","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full","creator_name":"Dean","creator_url":"https://huggingface.co/datatab","description":"\n\t\n\t\t\n\t\tSerbian Alpaca Cleaned Dataset\n\t\n\n\nOriginal Repository: https://github.com/gururise/AlpacaDataCleaned\nOriginal HF Repository: https://huggingface.co/datasets/yahma/alpaca-cleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a serbian cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full.","first_N":5,"first_N_keywords":["text-generation","Serbian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"OpenOrca-ru","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/OpenOrca-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tOpenOrca-ru\n\t\n\nThis is translated version of Open-Orca/OpenOrca into Russian.\n","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"dolphin-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/dolphin-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tDolphin-ru ğŸ¬\n\t\n\nThis is translated version of ehartford/dolphin into Russian.\n","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"CRAFT-RecipeGen","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tCRAFT-RecipeGen\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail.\n\n4 synthetic dataset sizes (S, M, L, XL) are available.\nCompared to other synthetically generated datasets with the CRAFT framework, this task did not scale similarly well and we do not match the performance of generalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"refactorchat","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BradMcDanel/refactorchat","creator_name":"Bradley McDanel","creator_url":"https://huggingface.co/BradMcDanel","description":"\n\t\n\t\t\n\t\tModel Card\n\t\n\n\n\t\n\t\t\n\t\tModel Details\n\t\n\n\nDataset Name: RefactorChat\nVersion: 1.0\nDate: October 19, 2024\nType: Multi-turn dialogue dataset for code refactoring and feature addition\n\n\n\t\n\t\t\n\t\tIntended Use\n\t\n\n\nPrimary Use: Evaluating and training large language models on incremental code development tasks\nIntended Users: Researchers and practitioners in natural language processing and software engineering\n\n\n\t\n\t\t\n\t\tDataset Composition\n\t\n\n\nSize: 100 samples\nStructure: Each sample consists ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BradMcDanel/refactorchat.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"lots_of_datasets_for_ai_v3","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ReallyFloppyPenguin/lots_of_datasets_for_ai_v3","creator_name":"Gurvaah Singh","creator_url":"https://huggingface.co/ReallyFloppyPenguin","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset is for Training LLMs From Scratch!\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ReallyFloppyPenguin/lots_of_datasets_for_ai_v3.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nğŸŒ Homepage | Code | ğŸ¤— Paper | ğŸ“– arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"alpha","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dawsondavis/alpha","creator_name":"Dawson Davis","creator_url":"https://huggingface.co/dawsondavis","description":"dawsondavis/alpha dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Titanium","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Titanium is a dataset containing DevOps-instruct data.\nThe 2024-10-02 version contains:\n\n26.6k rows of synthetic DevOps-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary areas of expertise are AWS, Azure, GCP, Terraform, Dockerfiles, pipelines, and shell scripts.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"glaiveai-reflection-v1-ko","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/youjunhyeok/glaiveai-reflection-v1-ko","creator_name":"ìœ ì¤€í˜","creator_url":"https://huggingface.co/youjunhyeok","description":"Translated glaiveai/reflection-v1 using nayohan/llama3-instrucTrans-enko-8b.\nFor this dataset, we only used data that is 5000 characters or less in length and has language of English.\nThanks for @Magpie-Align and @nayohan.\n","first_N":5,"first_N_keywords":["text-generation","Korean","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Creative_Writing_Multiturn","keyword":"multiturn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Dampfinchen/Creative_Writing_Multiturn","creator_name":"DÃ¤mpfchen","creator_url":"https://huggingface.co/Dampfinchen","description":"This is a dataset merge of many, many high quality story writing / roleplaying datasets across all of Huggingface. I've filtered specifically for samples with high turns, which is a key different to already available datasets. My goal is to improve the model's ability to recollect and mention details from far back even at a longer context and more importantly, also improve the model's ability to output engaging verbose storylines, reduce certain phrases, increase creativity and reduce dryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dampfinchen/Creative_Writing_Multiturn.","first_N":5,"first_N_keywords":["text2text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_anatomist","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Anatomist\n\t\n\nThis dataset was extracted from the Web Ontology Language (OWL) \nrepresentation of the Foundational Model of Anatomy [1] using \nOwlready2 to facilitate the extraction of the logical axioms in the FMA\ninto a Controlled Natural Language (CNL) for use in training Medical Large Language Models to be conversant in \ncanonical human anatomy.\nSo, Description Logic expressions such as the following in FMA about fma13245 (Surface of left ventricle):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist.","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_clinical_coding","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Clinical Coding\n\t\n\nThis dataset was created from a motivation to train Medical Large Language Models for improved fluency in clinical \ncoding, as measurable by MedConceptsQA, an open-source medical coding \nevaluation benchmark designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts.  It was extracted from the Centers for Medicare & Medicaid Services' \nInternational Classification of Diseases, Tenth Revision, Clinicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding.","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AlpacaX-Cleaned","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SullyGreene/AlpacaX-Cleaned","creator_name":"SullyGreene","creator_url":"https://huggingface.co/SullyGreene","description":"\n  \n\n\n\n\t\n\t\t\n\t\tğŸ“š AlpacaX Dataset Documentation\n\t\n\nThe AlpacaX dataset is crafted to enhance AI models with structured, contextually rich, and logically sequenced examples. Designed for integration with TinyAGI, AlpacaX employs an advanced variant of the Alpaca training methodology, making it ideal for models that require detailed instruction-following and multi-step reasoning. This dataset is well-suited for fine-tuning language models to handle complex tasks with clarity and structuredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SullyGreene/AlpacaX-Cleaned.","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"dali","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liwei1987cn/dali","creator_name":"Levi li","creator_url":"https://huggingface.co/liwei1987cn","description":"\n\t\n\t\t\n\t\tå¤§æè€å¸ˆé—®ç­”æ•°æ®é›†\n\t\n\nè¿™ä¸ªæ•°æ®é›†åŒ…å«å¤§æè€å¸ˆçš„é—®ç­”å¯¹è¯,ç”¨äºè®­ç»ƒå¯¹è¯æ¨¡å‹ã€‚\n\n\t\n\t\t\n\t\tæ•°æ®é›†æè¿°\n\t\n\n\næ ¼å¼: JSONL\nå­—æ®µ: \ninstruction: å›ºå®šå€¼\"è¯·å¤§æè€å¸ˆå›ç­”\"\ninput: æé—®å†…å®¹ \noutput: å¤§æè€å¸ˆçš„å›ç­”\n\n\næ•°æ®é‡: xxxæ¡å¯¹è¯æ•°æ®\n\n\n\t\n\t\t\n\t\tä½¿ç”¨ç¤ºä¾‹\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"your-username/dataset-name\")\n\n\n\t\n\t\t\n\t\tè®¸å¯è¯\n\t\n\nApache 2.0\n","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Atma3-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Atma3-Share-GPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3-Share-GPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"college_alpaca_dataset","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dapraws/college_alpaca_dataset","creator_name":"Muhammad Darrel Prawira","creator_url":"https://huggingface.co/dapraws","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the given article in 200 Words.\",\n\"input\": \"https://www.bbc.com/news/world-51461830\",\n\"output\": \"The recentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dapraws/college_alpaca_dataset.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Fineweb-Instruct","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Fineweb-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"We convert the pre-training corpus from Fineweb-Edu (https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) to instruction following format. We select a subset with quality filter and then use GPT-4 to extract instruction-following pairs. The dataset contains roughly 16M instruction pairs. The basic concept is similar to MAmmoTH2 (https://arxiv.org/abs/2405.03548). \n\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you use dataset useful, please cite the following paper:\n@article{yue2024mammoth2â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Fineweb-Instruct.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10M - 100M","json"],"keywords_longer_than_N":true},
	{"name":"MMLU-Alpaca","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for MMLU-Alpaca\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"single-agent-scam-conversations","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\n\t\n\t\t\n\t\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of three columns:\n\ndialogue: The transcribed conversation between the caller and receiver.\ntype: The specific type of scam or non-scam interaction.\nlabels: A binary label indicating whether the conversation is aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-indonesian","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian","creator_name":"Ilham Fadhil","creator_url":"https://huggingface.co/ilhamfadheel","description":"\n\t\n\t\t\n\t\tğŸ¦™ğŸ› Cleaned Alpaca Dataset (INDONESIAN)\n\t\n\nWelcome to the Cleaned Alpaca Dataset repository! This repository hosts a cleaned and curated version of a dataset used to train the Alpaca LLM (Large Language Model). The original dataset had several issues that are addressed in this cleaned version.\nOn April 8, 2023 the remaining uncurated instructions (~50,000) were replaced with data from the GPT-4-LLM dataset. Curation of the incoming GPT-4 data is ongoing.\n\nA 7b Lora model (trained onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian.","first_N":5,"first_N_keywords":["text-generation","Indonesian","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","keyword":"multi-turn","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","description":"\n\t\n\t\t\n\t\tSWE-Bench Verified O1 Dataset\n\t\n\n\n\t\n\t\t\n\t\tExecutive Summary\n\t\n\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities using their native tool calling capabilities on the SWE-Bench Verified dataset, achieving a 45.8% success rate across 500 test instances.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset was generated using the CodeAct framework, which aims to improve codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Supernova","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Supernova","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Supernova is a dataset containing general synthetic chat data from the best available open-source models.\nThe 2024-09-27 version contains:\n\n178.2k rows of synthetic chat responses generated using Llama 3.1 405b Instruct.\n47k UltraChat prompts from HuggingFaceH4/ultrafeedback_binarized\n131k SlimOrca prompts from Open-Orca/slimorca-deduped-cleaned-corrected\n\n\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Leopard-Instruct","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wyu1/Leopard-Instruct","creator_name":"Wenhao Yu","creator_url":"https://huggingface.co/wyu1","description":"\n\t\n\t\t\n\t\tLeopard-Instruct\n\t\n\nPaper | Github | Models-LLaVA | Models-Idefics2\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nLeopard-Instruct is a large instruction-tuning dataset, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. It's been used to train Leopard-LLaVA [checkpoint]  and Leopard-Idefics2 [checkpoint].\n\n\t\n\t\t\n\t\tLoading dataset\n\t\n\n\nto load the dataset without automatically downloading and process the images (Please run the following codes with datasets==2.18.0)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wyu1/Leopard-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-72k\n\t\n\nğŸ“„ Paper | ğŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-72k\n\t\n\nğŸ“„ Paper | ğŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-12k\n\t\n\nğŸ“„ Paper | ğŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-12k\n\t\n\nğŸ“„ Paper | ğŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"multiturn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\nThis dataset focuses on challenging multi-turn conversations and contains:\n\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\nThis dataset focuses on challenging multi-turn conversations and contains:\n\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"OpenCharacter","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xywang1/OpenCharacter","creator_name":"Xiaoyang Wang","creator_url":"https://huggingface.co/xywang1","description":"\n\t\n\t\t\n\t\tOpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas\n\t\n\nThis repo releases data introduced in our paper OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas in arXiv.\n\nWe study customizable role-playing dialogue agents in large language models (LLMs).\nWe tackle the challenge with large-scale data synthesis: character synthesis and character-driven reponse synthesis.\nOur solution strengthens the original LLaMA-3â€¦ See the full description on the dataset page: https://huggingface.co/datasets/xywang1/OpenCharacter.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Magpie-COT","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/project-r/Magpie-COT","creator_name":"Project R","creator_url":"https://huggingface.co/project-r","description":"\n\t\n\t\t\n\t\tMagpie-COT\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCombined Chain-of-Thought dataset containing three sources:\n\nMagpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B\nMagpie-Reasoning-V2-250K-CoT-QwQ\nMagpie-Reasoning-V2-250K-CoT-Skywork-O1-Llama-3.1-8B\n\n\n\t\n\t\t\n\t\tKey Enhancements:\n\t\n\n\nAdded model source tracking column\nProcessed Deepseek responses to extract  tag content\nUnified format across multiple CoT datasets\n\n\n\t\n\t\t\n\t\tProcessing Steps:\n\t\n\n\nKept 'instruction' and 'response' columns\nAddedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/project-r/Magpie-COT.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"english-to-colloquial-tamil","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jarvisvasu/english-to-colloquial-tamil","creator_name":"Vasanth Kumar","creator_url":"https://huggingface.co/jarvisvasu","description":"\n\t\n\t\t\n\t\tEnglish to Colloquial Tamil\n\t\n\n\"instruction\":\"Translate provided English text into colloquial Tamil.\"\n\"input\": \"Their players played well.\"\n\"output\": \"à®…à®µà®™à¯à®• players à®¨à®²à¯à®²à®¾ à®µà®¿à®³à¯ˆà®¯à®¾à®£à¯à®Ÿà®¾à®™à¯à®•.\"\n\n","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","Tamil","English"],"keywords_longer_than_N":true},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tDeepthink Reasoning Demo\n\t\n\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"WangchanThaiInstruct_Multi-turn_Conversation_Dataset","keyword":"instruction-finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThaiSyntheticQA/WangchanThaiInstruct_Multi-turn_Conversation_Dataset","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","description":"\n\t\n\t\t\n\t\tWangchanThaiInstruct Multi-turn Conversation Dataset\n\t\n\nWe create a Thai multi-turn conversation dataset from airesearch/WangchanThaiInstruct (Batch 1) by LLM. It was created from synthetic method using open source LLM in Thai language.\n\n\t\n\t\t\n\t\tCitation\n\t\n\n\nThammaleelakul, S., & Phatthiyaphaibun, W. (2024). WangchanThaiInstruct Multi-turn Conversation Dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.13132633\n\nor BibTeX\n@dataset{thammaleelakul_2024_13132633,\n  author       =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/WangchanThaiInstruct_Multi-turn_Conversation_Dataset.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Buzz-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Test2","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WhiteHunter111/Test2","creator_name":"Thomas Flato","creator_url":"https://huggingface.co/WhiteHunter111","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WhiteHunter111/Test2.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_data_clean","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aaaalon/alpaca_data_clean","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca_data_clean.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ğŸ‡ºğŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"llama3weitiao","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZHEZIXI/llama3weitiao","creator_name":"LIUJUN","creator_url":"https://huggingface.co/ZHEZIXI","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZHEZIXI/llama3weitiao.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleand","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aaaalon/alpaca-cleand","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca-cleand.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"MainData","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bufanlin/MainData","creator_name":"bufanlin","creator_url":"https://huggingface.co/bufanlin","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bufanlin/MainData.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_cthulhu_full","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theprint/alpaca_cthulhu_full","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","description":"Based on the yahma/alpaca-cleaned data set.\nEvery answer in the original data set was rewritten, as if the answer was given by a dedicated, high ranking cultist in the Cult of Cthulhu. The instructions were to keep the replies factually the same, but to spice things up with references to the Cthulhu Mythos in its various forms.\nThis was done as part of a learning experience, but I am making the data set available for others to play with as well.\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ultrafrench","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/ultrafrench","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for ultrafrench\n\t\n\nThis dataset offers a french translation of the small sample of instructions from HuggingFaceH4/ultrachat_200k translated in french. The generations were made with Mistral large to make the output open-source\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"instruction-collection-fin","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LumiOpen/instruction-collection-fin","creator_name":"LumiOpen","creator_url":"https://huggingface.co/LumiOpen","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a collection of Finnish instruction data compiled from various sources. Most of the original data is in English and was machine translated into Finnish using Poro-34B. We supplemented this translated data with Finnish paraphrase tasks and English-Finnish translation and language identification tasks. This dataset is suitable for fine-tuning LLMs for instruction-following in Finnish and is usable for commercial purposes.\nWe use this dataset inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LumiOpen/instruction-collection-fin.","first_N":5,"first_N_keywords":["Finnish","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-fr","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TPM-28/alpaca-cleaned-fr","creator_name":"TPM-28","creator_url":"https://huggingface.co/TPM-28","description":"TPM-28/alpaca-cleaned-fr dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_astronomy_bg","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca_astronomy_bg","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"vislupus/alpaca_astronomy_bg dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Bulgarian","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"LLama3Flashcard","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Michito97/LLama3Flashcard","creator_name":"Adrian De La Cruz","creator_url":"https://huggingface.co/Michito97","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nEste dataset contiene pares de instrucciones, entradas y salidas diseÃ±adas para generar tarjetas de memoria (flashcards) educativas. Es ideal para modelos de generaciÃ³n de texto enfocados en la creaciÃ³n de contenidos educativos.\n\n\t\n\t\t\n\t\tColumnas\n\t\n\n\ninstruction: La instrucciÃ³n dada alâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Michito97/LLama3Flashcard.","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Proyecto","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto","creator_name":"Facundo","creator_url":"https://huggingface.co/Facundo-DiazPWT","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ğŸ‡ºğŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"AWE","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rishiraj/AWE","creator_name":"Rishiraj Acharya","creator_url":"https://huggingface.co/rishiraj","description":"rishiraj/AWE dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"CTI_0.1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AshishCTI/CTI_0.1","creator_name":"ad","creator_url":"https://huggingface.co/AshishCTI","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AshishCTI/CTI_0.1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ğŸ‡ºğŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"TextBooksPersonaHub","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/drodin/TextBooksPersonaHub","creator_name":"nacer","creator_url":"https://huggingface.co/drodin","description":"\n\t\n\t\t\n\t\tTextBooksPersonaHub\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe TextBooksPersonaHub dataset is an extension of the proj-persona/PersonaHub dataset, created using the technique described in the paper Textbooks Are All You Need II. This dataset contains synthetically generated \"textbook-like\" passages tailored in french to specific personas, aimed at enhancing language model training with high-quality and diverse content.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nThe original personasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/drodin/TextBooksPersonaHub.","first_N":5,"first_N_keywords":["text2text-generation","French","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"multiturn-capybara-preferences-filtered-binarized","keyword":"multiturn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"This dataset has been created with distilabel, plus some extra post-processing steps described below.\nDataset Summary\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\n\nRemove responses from the assistant, not only in the last turn, but also in intermediate turns where the assistant responds with a potential refusal and/or some ChatGPT-isms such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ictisgpt","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alexbs/ictisgpt","creator_name":"Alex B","creator_url":"https://huggingface.co/alexbs","description":"\n\t\n\t\t\n\t\tDataset Card for ICTIS GPT dataset\n\t\n\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"dataset-portuguese-aira-v2-Gemma-format","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format","creator_name":"EDDY GIUSEPE CHIRINOS ISIDRO, PhD","creator_url":"https://huggingface.co/EddyGiusepe","description":"Dataset Aira para o formato do Modelo Gemma \n\n\n\t\n\t\t\n\t\tResumo do Dataset\n\t\n\nEste conjunto de dados contÃ©m uma coleÃ§Ã£o de conversas individuais entre um assistente e um usuÃ¡rio.\nAs conversas foram geradas pelas interaÃ§Ãµes do usuÃ¡rio com modelos jÃ¡ ajustados (ChatGPT, LLama 2, Open-Assistant, etc).\nO conjunto de dados estÃ¡ disponÃ­vel em portuguÃªs (tem a versÃ£o em InglÃªs que ainda nÃ£o tratei). Mas vocÃª pode baixar do \nrepositÃ³rio de Nicholas Kluge CorrÃªa tanto a versÃ£o em PortuguÃªs e \na versÃ£o emâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format.","first_N":5,"first_N_keywords":["question-answering","Portuguese","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-chatml","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pacozaa/alpaca-cleaned-chatml","creator_name":"Sarin Suriyakoon","creator_url":"https://huggingface.co/pacozaa","description":"\n\t\n\t\t\n\t\tChatML Reformat of yahma/alpaca-cleaned\n\t\n\nI'd like to try instruction-tuning dataset with chat-tuning format.\n\n\t\n\t\t\n\t\tUsage\n\t\n\ndataset = load_dataset(\"pacozaa/alpaca-cleaned-chatml\", split = \"train\")\nprint(dataset[0][\"text\"])\n\n","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"medical","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bohu/medical","creator_name":"tang","creator_url":"https://huggingface.co/bohu","description":"From https://huggingface.co/datasets/shibing624/medical\n","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"ThaiQA-v1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","description":"\n\t\n\t\t\n\t\tThaiQA v1\n\t\n\nThaiQA v1 is a Thai Synthetic QA dataset. It was created from synthetic method using open source LLM in Thai language.\nWe used Nvidia Nemotron 4 (340B) to create this dataset.\nTopics:\nTechnology and Gadgets 100\nTravel and Tourism 91\nFood and Cooking 99\nSports and Fitness 50\nArts and Entertainment 24\nHome and Garden 72\nFashion and Beauty 99\nScience and Nature 100\nHistory and Culture 91\nEducation and Learning 99\nPets and Animals 83\nRelationships and Family 78\nPersonalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1.","first_N":5,"first_N_keywords":["text-generation","question-answering","Thai","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LONGCOT-Alpaca","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for LONGCOT-Alpaca\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SHARE","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/eunwoneunwon/SHARE","creator_name":"Eunwon Kim","creator_url":"https://huggingface.co/eunwoneunwon","description":"\n\t\n\t\t\n\t\tSHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script\n\t\n\nSHARE is a novel long-term dialogue dataset constructed from movie scripts. \n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset consists of:\n\nConversations: Dialogue exchanges between two main characters in various movie scripts.\nAnnotations: Detailed extractions using GPT-4, including:\nPersona: Persona information captures essential characteristics, including personality, occupation, and interest.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/eunwoneunwon/SHARE.","first_N":5,"first_N_keywords":["text2text-generation","English","apache-2.0","10M<n<100M","ğŸ‡ºğŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"chat","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neverland-th/chat","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","description":"neverland-th/chat dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"amazon-ml","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vinuuu/amazon-ml","creator_name":"Vinayak Goyal","creator_url":"https://huggingface.co/Vinuuu","description":"Vinuuu/amazon-ml dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"CleverBoi","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theprint/CleverBoi","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","description":"\n\n\n\t\n\t\t\n\t\tCleverBoi\n\t\n\nThe CleverBoi Collection is based on a number of data sets that emphasize logic, inference, empathy, math and coding.\nThe data set has been formatted to follow the alpaca format (instruction + input -> output) when fine tuning.\n\n\t\n\t\t\n\t\tSource Data Sets\n\t\n\nThe source data sets used in the CleverBoi Collection are listed below, ordered by size.\n\nKK04/LogicInference_OA\nmlabonne/Evol-Instruct-Python-26k\ngarage-bAInd/Open-Platypus\niamtarun/python_code_instructions_18k_alpacaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/theprint/CleverBoi.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ATC-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for ATC-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ATCgpt-Fixed","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for ATCgpt-Fixed\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Atcgpt-Fixed2","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atcgpt-Fixed2\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701-instruct","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"\n\t\n\t\t\n\t\tLOGIC-701 (instruct)\n\t\n\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\n","first_N":5,"first_N_keywords":["question-answering","Russian","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"metallurgy-qa","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AbdulrhmanEldeeb/metallurgy-qa","creator_name":"Eldeeb","creator_url":"https://huggingface.co/AbdulrhmanEldeeb","description":"\n\t\n\t\t\n\t\tMetallurgy and Materials Science Knowledge Extraction Dataset\n\t\n\nThis repository contains a dataset generated from parsed books related to various aspects of metallurgy, materials science, and engineering. The dataset is designed for fine-tuning Large Language Models (LLMs) for Question-Answering (QA) tasks in the domain of metallurgy and materials science.\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe dataset includes content derived from technical books in the field of metallurgy and materialsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AbdulrhmanEldeeb/metallurgy-qa.","first_N":5,"first_N_keywords":["text2text-generation","question-answering","closed-domain-qa","closed-book-qa","machine-generated"],"keywords_longer_than_N":true},
	{"name":"1M-OpenOrca_be","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be","creator_name":"Artsem Holub","creator_url":"https://huggingface.co/WiNE-iNEFF","description":"En/Be\nğŸ‹ The Belarusian OpenOrca Dataset! ğŸ‹\n\n\n\nBelarusian OpenOrca dataset - is rich collection of augmented FLAN data aligns, that translated in belarusian language.\nThat dataset should help training LLM in belarusian language and should help on other NLP tasks.\nThis dataset have 2 version:\n\n~1M GPT-4 completions (Now translating)\n~3.2M GPT-3.5 completions (Can be translated in future)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe fields are:\n\n'id', a unique numbered identifier which includes one of 'niv'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Spurline","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\nThe 2024-10-30 version contains:\n\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-36k\n\t\n\nğŸ“„ Paper | ğŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-36k\n\t\n\nğŸ“„ Paper | ğŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"TextBooksPersonaHub-FR","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/drodin/TextBooksPersonaHub-FR","creator_name":"nacer","creator_url":"https://huggingface.co/drodin","description":"\n\t\n\t\t\n\t\tTextBooksPersonaHub\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe TextBooksPersonaHub dataset is an extension of the proj-persona/PersonaHub dataset, created using the technique described in the paper Textbooks Are All You Need II. This dataset contains synthetically generated \"textbook-like\" passages tailored in french to specific personas, aimed at enhancing language model training with high-quality and diverse content.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nThe original personasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/drodin/TextBooksPersonaHub-FR.","first_N":5,"first_N_keywords":["text2text-generation","French","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"KoMT-Bench","keyword":"instruction-following","license":"GNU Lesser General Public License v3.0","license_url":"https://choosealicense.com/licenses/lgpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench","creator_name":"LG AI Research","creator_url":"https://huggingface.co/LGAI-EXAONE","description":"\n\t\n\t\t\n\t\tKoMT-Bench\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe present KoMT-Bench, a benchmark designed to evaluate the capability of language models in following instructions in Korean.\nKoMT-Bench is an in-house dataset created by translating MT-Bench [1]  dataset into Korean and modifying some questions to reflect the characteristics and cultural nuances of the Korean language.\nAfter the initial translation and modification, we requested expert linguists to conduct a thorough review of our benchmarkâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench.","first_N":5,"first_N_keywords":["question-answering","Korean","lgpl-3.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"multi-turn_jailbreak_attack_datasets","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tom-gibbs/multi-turn_jailbreak_attack_datasets","creator_name":"Tom Gibbs","creator_url":"https://huggingface.co/tom-gibbs","description":"\n\t\n\t\t\n\t\tMulti-Turn Jailbreak Attack Datasets\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset was created to compare single-turn and multi-turn jailbreak attacks on large language models (LLMs). The primary goal is to take a single harmful prompt and distribute the harm over multiple turns, making each prompt appear harmless in isolation. This approach is compared against traditional single-turn attacks with the complete prompt to understand their relative impacts and failure modes. The key feature ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tom-gibbs/multi-turn_jailbreak_attack_datasets.","first_N":5,"first_N_keywords":["English","mit","1K<n<10K","arxiv:2409.00137","ğŸ‡ºğŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Atma3.2-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3.2-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Atma4","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma4\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"godot-training","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ImJimmeh/godot-training","creator_name":"Jim","creator_url":"https://huggingface.co/ImJimmeh","description":"ImJimmeh/godot-training dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Atma4-Hindi","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma4-Hindi\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned_uz","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bizb0630/alpaca-cleaned_uz","creator_name":"Behruz Izbaev","creator_url":"https://huggingface.co/bizb0630","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a translation of the alpaca-cleaned dataset into Uzbek (Latin), using the GPT-4o mini API.\n","first_N":5,"first_N_keywords":["text-generation","Uzbek","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"mauxitalk-persian","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxitalk-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tğŸ—£ï¸ MauxiTalk: High-Quality Persian Conversations Dataset ğŸ‡®ğŸ‡·\n\t\n\n\n\t\n\t\t\n\t\tğŸ“ Description\n\t\n\nMauxiTalk is a high-quality dataset of 2,000+ Persian conversations, carefully translated from the SmolTalk dataset using state-of-the-art language models. This dataset is specifically curated for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques.\n\n\t\n\t\t\n\t\tğŸŒŸ Key Features\n\t\n\n\n2,000 natural conversations in Persian\nDiverse topics including dailyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxitalk-persian.","first_N":5,"first_N_keywords":["text-generation","summarization","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SkunkworksAI-reasoning-0.01-ko","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/youjunhyeok/SkunkworksAI-reasoning-0.01-ko","creator_name":"ìœ ì¤€í˜","creator_url":"https://huggingface.co/youjunhyeok","description":"SkunkworksAI/reasoning-0.01 ë°ì´í„°ì…‹ì„ nayohan/llama3-instrucTrans-enko-8b ëª¨ë¸ì„ ì‚¬ìš©í•´ ë²ˆì—­í–ˆìŠµë‹ˆë‹¤.\nThanks for SkunkworksAI and nayohan.\n\n\n\t\n\t\t\n\t\tì›ë³¸\n\t\n\n\n\t\n\t\t\n\t\treasoning-0.01 subset\n\t\n\nsynthetic dataset of reasoning chains for a wide variety of tasks.\nwe leverage data like this across multiple reasoning experiments/projects.\nstay tuned for reasoning models and more data.\nThanks to Hive Digital Technologies (https://x.com/HIVEDigitalTech) for their compute support in this project and beyond.\n","first_N":5,"first_N_keywords":["text-generation","Korean","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"wangchanx-seed-free-synthetic-instruct-thai-120k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k","creator_name":"VISTEC-depa AI Research Institute of Thailand","creator_url":"https://huggingface.co/airesearch","description":"\n\t\n\t\t\n\t\tDataset Card for WangchanX Seed-Free Synthetic Instruct Thai 120k\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains about 120k synthetic instruction-following samples in Thai, generated using a novel seed-free approach. It covers a wide range of domains derived from Wikipedia, including both general knowledge and Thai-specific cultural topics. The dataset is designed for instruction-tuning Thai language models to improve their ability to understand and generate Thai text in variousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k.","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","Thai","English"],"keywords_longer_than_N":true},
	{"name":"Kalo-Opus-Instruct-22k-Refusal-Murdered","keyword":"instruct","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewEden/Kalo-Opus-Instruct-22k-Refusal-Murdered","creator_name":"New Eden","creator_url":"https://huggingface.co/NewEden","description":"NewEden/Kalo-Opus-Instruct-22k-Refusal-Murdered dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["agpl-3.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"code-translation","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noxus09/code-translation","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Noxus09/code-translation.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"CRAFT-Summarization","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tCRAFT-Summarization\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated summarization data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization.","first_N":5,"first_N_keywords":["summarization","text-generation","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"reflection-v1-ru_subset","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\td0rj/reflection-v1-ru_subset\n\t\n\nTranslated glaiveai/reflection-v1 dataset into Russian language using GPT-4o.\n\nAlmost all the rows of the dataset have been translated. I have removed those translations that do not match the original by the presence of the tags \"thinking\", \"reflection\" and \"output\". Mapping to the original dataset rows can be taken from the \"index\" column.\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata = datasets.load_dataset(\"d0rj/reflection-v1-ru_subset\")\nprint(data)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","multilingual","glaiveai/reflection-v1"],"keywords_longer_than_N":true},
	{"name":"scam_dialogues","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/adamtc/scam_dialogues","creator_name":"Adam Unknown","creator_url":"https://huggingface.co/adamtc","description":"adamtc/scam_dialogues dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","Vietnamese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Celestia","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia is a dataset containing science-instruct data.\nThe 2024-10-30 version contains:\n\n126k rows of synthetic science-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Data-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-Data-ru\n\t\n\nTranslated lmms-lab/LLaVA-OneVision-Data dataset into Russian language using Google translate.\n\nAlmost all datasets have been translated, except for the following:\n[\"tallyqa(cauldron,llava_format)\", \"clevr(cauldron,llava_format)\", \"VisualWebInstruct(filtered)\", \"figureqa(cauldron,llava_format)\", \"magpie_pro(l3_80b_mt)\", \"magpie_pro(qwen2_72b_st)\", \"rendered_text(cauldron)\", \"ureader_ie\"]\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru.","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-to-text","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"math-gpt-4o-200k-ko","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/math-gpt-4o-200k-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"Translated PawanKrd/math-gpt-4o-200k using nayohan/llama3-instrucTrans-enko-8b.\nThis dataset is a raw translated dataset and contains repetitive sentences generated by the model, so it needs to be filtered.\n","first_N":5,"first_N_keywords":["text-generation","Korean","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"tw-reasoning-instruct-50k","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k","creator_name":"Twinkle AI","creator_url":"https://huggingface.co/twinkle-ai","description":"\n\t\n\t\t\n\t\tDataset Card for tw-reasoning-instruct-50k\n\t\n\n\n\ntw-reasoning-instruct-50k æ˜¯ä¸€å€‹ç²¾é¸çš„ ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ï¼‰ æ¨ç†è³‡æ–™é›†ï¼Œæ—¨åœ¨æå‡èªè¨€æ¨¡å‹æ–¼é€æ­¥é‚è¼¯æ€è€ƒã€è§£é‡‹ç”Ÿæˆèˆ‡èªè¨€ç†è§£ç­‰ä»»å‹™ä¸­çš„è¡¨ç¾ã€‚è³‡æ–™å…§å®¹æ¶µè“‹æ—¥å¸¸æ€è¾¨ã€æ•™è‚²å°è©±ã€æ³•å¾‹æ¨ç†ç­‰å¤šå…ƒä¸»é¡Œï¼Œä¸¦çµåˆã€Œæ€è€ƒæ­¥é©Ÿã€èˆ‡ã€Œæœ€çµ‚ç­”æ¡ˆã€çš„çµæ§‹è¨­è¨ˆï¼Œå¼•å°æ¨¡å‹ä»¥æ›´æ¸…æ™°ã€æ¢ç†åˆ†æ˜çš„æ–¹å¼é€²è¡Œæ¨è«–èˆ‡å›æ‡‰ï¼Œç‰¹åˆ¥å¼·èª¿ç¬¦åˆå°ç£æœ¬åœ°èªè¨€èˆ‡æ–‡åŒ–èƒŒæ™¯çš„æ‡‰ç”¨éœ€æ±‚ã€‚\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\næœ¬è³‡æ–™é›†å°ˆç‚ºç™¼å±•å…·å‚™å¼·å¤§æ¨ç†èƒ½åŠ›çš„ç¹é«”ä¸­æ–‡å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLarge Reasoning Models, LRMï¼‰æ‰€è¨­è¨ˆï¼Œå…§å®¹æ·±åº¦çµåˆå°ç£çš„èªè¨€èˆ‡æ–‡åŒ–è„ˆçµ¡ã€‚æ¯ç­†è³‡æ–™é€šå¸¸åŒ…å«ä½¿ç”¨è€…çš„æå•ã€æ¨¡å‹çš„å›æ‡‰ï¼Œä»¥åŠæ¸…æ¥šçš„æ¨ç†éç¨‹ã€‚è³‡æ–™é›†è¨­è¨ˆç›®æ¨™ç‚ºåŸ¹é¤Šæ¨¡å‹å…·å‚™é¡äººé‚è¼¯çš„é€æ­¥æ€è€ƒèˆ‡è§£é‡‹èƒ½åŠ›ã€‚\næ­¤è³‡æ–™é›†é©ç”¨æ–¼è¨“ç·´èˆ‡è©•ä¼°ä»¥ä¸‹ä»»å‹™ï¼š\n\nå°ç£ç¤¾æœƒçš„æ—¥å¸¸æ¨ç†\næ•™è‚²æ€§å°è©±\nä»¥è§£é‡‹ç‚ºå°å‘çš„ç”Ÿæˆä»»å‹™â€¦ See the full description on the dataset page: https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SFT_54k_reasoning","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/SFT_54k_reasoning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/SFT_54k_reasoning is a processed version of the XuHu6736/s1_54k_filter_with_isreasoning dataset, specifically reformatted for instruction fine-tuning (SFT) of language models.\nThe original question and solution pairs have been converted into an instruction-following format. Critically, the isreasoning_score and isreasoning labels from the parent dataset are preserved, allowing for targeted SFT onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning.","first_N":5,"first_N_keywords":["XuHu6736 (formatting and derivation)","derived from XuHu6736/s1_54k_filter_with_isreasoning","derived from source datasets","monolingual","XuHu6736/s1_54k_filter_with_isreasoning"],"keywords_longer_than_N":true},
	{"name":"alpaca-style-QnA","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sweatSmile/alpaca-style-QnA","creator_name":"amitk17","creator_url":"https://huggingface.co/sweatSmile","description":"\n\t\n\t\t\n\t\tAlpaca-style Question and Answer Dataset\n\t\n\nThis dataset contains question-answer pairs formatted in the Alpaca instruction style, suitable for instruction fine-tuning of language models.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example contains:\n\ninstruction: The question\ninput: Empty string (can be used for context in other applications)\noutput: The answer\ntext: The formatted text using the Alpaca template\n\n\n\t\n\t\t\n\t\tTemplate\n\t\n\nBelow is an instruction that describes a task, paired with an input thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sweatSmile/alpaca-style-QnA.","first_N":5,"first_N_keywords":["English","cc-by-4.0","ğŸ‡ºğŸ‡¸ Region: US","instruction","qa"],"keywords_longer_than_N":true},
	{"name":"Titanium2-DeepSeek-R1","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Click here to support our open-source dataset and model releases!\nTitanium2-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n32.4k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraformâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-sv","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv","creator_name":"Tim Isbister","creator_url":"https://huggingface.co/timpal0l","description":"\n\t\n\t\t\n\t\tOpenHermes-2.5-sv\n\t\n\nThis is a machine translated instruct dataset from OpenHermes-2.5. \nThe facebook/seamless-m4t-v2-large was used, and some post filtering is done to remove repetitive texts that occurred due to translation errors.\n\n\t\n\t\t\n\t\tExample data:\n\t\n\n[\n   {\n      \"from\":\"human\",\n      \"value\":\"Vilket naturfenomen, som orsakas av att ljus reflekteras och bryts genom vattendroppar, resulterar i en fÃ¤rgglad bÃ¥ge pÃ¥ himlen?\",\n      \"weight\":null\n   },\n   {\n      \"from\":\"gpt\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv.","first_N":5,"first_N_keywords":["text-generation","Swedish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v3","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset version 3.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of multi-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Mantis-Instruct","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tMantis-Instruct\n\t\n\nPaper | Website | Github | Models | Demo\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nMantis-Instruct is a fully text-image interleaved multimodal instruction tuning dataset, \ncontaining 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding. \nIt's been used to train Mantis Model families\n\nMantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\nAmong the 14â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Confession-Subreddit-Top500","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Oguzz07/Confession-Subreddit-Top500","creator_name":"OÄŸuzhan YÄ±ldÄ±rÄ±m","creator_url":"https://huggingface.co/Oguzz07","description":"This dataset was prepared by taking into account the 500 most popular posts of all time in the confession subreddit and the comments with the most votes on these posts.\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","text","Text"],"keywords_longer_than_N":true},
	{"name":"pandora-instruct","keyword":"instruct","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-instruct","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora Instruct\n\t\n\nAn instruction dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the existing datasets:\n\nteknium/openhermes\nise-uiuc/magicoder-evol-instruct-110k\nise-uiuc/magicoder-oss-instruct-75k\n\n\n\t\n\t\t\n\t\n\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"InstructTranslation-EN-ES","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Iker/InstructTranslation-EN-ES","creator_name":"Iker GarcÃ­a-Ferrero","creator_url":"https://huggingface.co/Iker","description":"\n\t\n\t\t\n\t\tTranslation of Instructions EN-ES\n\t\n\nThis dataset contains prompts and answers from teknium/OpenHermes-2.5 translated to Spanish using GPT-4-0125-preview. The dataset is intended to be used for training a model to translate instructions from English to Spanish.\nThe dataset is formatted with the TowerInstruct format. It is ready to finetune a Tower translation model. if you want the raw translations, there are available here:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Iker/InstructTranslation-EN-ES.","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"code-act","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xingyaoww/code-act","creator_name":"Xingyao Wang","creator_url":"https://huggingface.co/xingyaoww","description":" Executable Code Actions Elicit Better LLM Agents \n\n\nğŸ’» Code\nâ€¢\nğŸ“ƒ Paper\nâ€¢\nğŸ¤— Data (CodeActInstruct)\nâ€¢\nğŸ¤— Model (CodeActAgent-Mistral-7b-v0.1)\nâ€¢\nğŸ¤– Chat with CodeActAgent!\n\n\nWe propose to use executable Python code to consolidate LLM agentsâ€™ actions into a unified action space (CodeAct).\nIntegrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations (e.g., code execution results) through multi-turnâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xingyaoww/code-act.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v2","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset version 2.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of single-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"openbohm","keyword":"multi-turn","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/distantquant/openbohm","creator_name":"Distant Quant","creator_url":"https://huggingface.co/distantquant","description":"\n\t\n\t\t\n\t\tOpenBohm\n\t\n\nThis dataset is an experimental conjugation of philosophical multi-turn long-form conversations from J. Krishnamurti, and D. Bohm, added to long-conversation filtered (count > 6) Capybara data, edited to be slightly less apologetic.\nRemoved references to names and locations where possible. Some conversations have been paraphrased somewhat to follow QA format better, however they keep the key content of the original.\n\n","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"CIDAR","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arbml/CIDAR","creator_name":"Arabic Machine Learning ","creator_url":"https://huggingface.co/arbml","description":"\n\t\n\t\t\n\t\tDataset Card for \"CIDAR\"\n\t\n\n\n\t\n\t\t\n\t\tğŸŒ´CIDAR: Culturally Relevant Instruction Dataset For Arabic\n\t\n\n\n\n   [ Paper - GitHub ]\n\n\n\nCIDAR contains 10,000 instructions and their output. The dataset was created by selecting around 9,109 samples from Alpagasus dataset then translating it to Arabic using ChatGPT. In addition, we append that with around 891 Arabic grammar instructions from the webiste Ask the teacher. All the 10,000 samples were reviewed by around 12 reviewers. \n\n\n\n\n\n\n\t\n\t\t\n\t\tğŸ“šâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arbml/CIDAR.","first_N":5,"first_N_keywords":["text-generation","Arabic","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"VIS","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/weiwei888/VIS","creator_name":"shiweiwei","creator_url":"https://huggingface.co/weiwei888","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weiwei888/VIS.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"wangwei","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guilty1987/wangwei","creator_name":"FAN","creator_url":"https://huggingface.co/guilty1987","description":"guilty1987/wangwei dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-gpt4-turbo","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo","creator_name":"myles bruce","creator_url":"https://huggingface.co/mylesgoose","description":"I downloaded the dataset from Alpaca at https://huggingface.co/datasets/yahma/alpaca-cleaned and processed it using a script to convert the text to hashes. I removed any duplicate hashes along with their corresponding input and output columns. Subsequently, I utilized the GPT-4 Turbo API to feed each message, instruction, and input to the model for generating responses.\nHere are the outputs generated by the GPT-4 Turbo model. The information in the input column and instruction column should beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"AB1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MagedGaman/AB1","creator_name":"Maged Gaman","creator_url":"https://huggingface.co/MagedGaman","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagedGaman/AB1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kushala/alpaca","creator_name":"Mummigatti","creator_url":"https://huggingface.co/Kushala","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kushala/alpaca.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned","creator_name":"Cristian Velasquez (Entreprenerdly.com)","creator_url":"https://huggingface.co/Entreprenerdly","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-ingen","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Walmart-the-bag/alpaca-ingen","creator_name":"wbag","creator_url":"https://huggingface.co/Walmart-the-bag","description":"\n\t\n\t\t\n\t\tAlpaca Ingen\n\t\n\nGoogle has added massive rate limits and other policies. I am unable to finish this.\nThis dataset was created using Gemini 1.0 Pro with minor adjustments for cleanliness. It may contain some issues, including 'I'm sorry' responses. The dataset will undergo further cleaning once it reaches completion, with a target of processing up to 23,000 rows.\n","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"grad_school_math_instructions_fr_Mixtral","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/grad_school_math_instructions_fr_Mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for grad_school_math_instructions_fr_Mixtral\n\t\n\nThis dataset was made thanks to the instruction of the vigogne's dataset but the output were generated with Mixtral-8x7B-Instruct instead of GPT3.5 to make it open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Alpaca_french_mixtral","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/Alpaca_french_mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca_french_mixtral\n\t\n\nThis dataset was made by reusing the french alpaca instruction with Mixtral-8x7B-Instruct to make the output open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"scam-dialogue","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/scam-dialogue","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\n\t\n\t\t\n\t\tSynthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset is a collection of simulated phone conversation between two parties, labeled as either scam or non-scam interactions. The dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of three columns:\n\ndialogue: Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/scam-dialogue.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"RLSTACK","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK.","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Select-Stack","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"unstacked","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","arxiv:2403.08763","arxiv:2310.05914"],"keywords_longer_than_N":true},
	{"name":"Colossal-Instruction-Translation-EN-ES","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Iker/Colossal-Instruction-Translation-EN-ES","creator_name":"Iker GarcÃ­a-Ferrero","creator_url":"https://huggingface.co/Iker","description":"\n\t\n\t\t\n\t\tColossal Instruction Translation Corpus (English - Spanish )\n\t\n\n\nA deduplicated version of this dataset can be found here, thanks to @NickyNicky: https://huggingface.co/datasets/NickyNicky/Iker-Colossal-Instruction-Translation-EN-ES_deduplicated\n\nThis dataset contains 2284632 instructions and answers translated from English into Spanish. Is a fully synthetic corpus generated using machine translation. We used the model Iker/TowerInstruct-13B-v0.1-EN2ES. A few examples were alsoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Iker/Colossal-Instruction-Translation-EN-ES.","first_N":5,"first_N_keywords":["translation","Spanish","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MM-Instruct","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jjjjh/MM-Instruct","creator_name":"LIU Jihao","creator_url":"https://huggingface.co/jjjjh","description":"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMM-Instruct is a large-scale dataset of diverse and high-quality visual instruction-answer pairs designed to enhance the instruction-following capabilities of large multimodal models (LMMs) in real-world use cases. It goes beyond simple question-answering or image-captioning by incorporating a wide range of instructions, including creative writing, summarization, and image analysis, pushing LMMs to better understand and respond to nuanced user requests.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jjjjh/MM-Instruct.","first_N":5,"first_N_keywords":["visual-question-answering","cc-by-4.0","100K - 1M","json","Image"],"keywords_longer_than_N":true},
	{"name":"multi-agent-scam-conversation","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\n\t\n\t\t\n\t\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset with Agentic Personalities\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset with Agentic Personalities is an enhanced collection of simulated phone conversations between two AI agents, one acting as a scammer or non-scammer and the other as an innocent receiver. Each dialogue is labeled as either a scam or non-scam interaction. This dataset is designed to help developâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/OpenHermes-2.5-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\td0rj/OpenHermes-2.5-ru\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is translated version of teknium/OpenHermes-2.5 into Russian using Google Translate.\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Multilingual-Benchmark","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TaiMingLu/Multilingual-Benchmark","creator_name":"TaiMing","creator_url":"https://huggingface.co/TaiMingLu","description":"These are the GSM8K and ARC dataset translated by Google Translate. \nBibTex\n@misc{lu2024languagecountslearnunlearn,\n      title={Every Language Counts: Learn and Unlearn in Multilingual LLMs}, \n      author={Taiming Lu and Philipp Koehn},\n      year={2024},\n      eprint={2406.13748},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2406.13748}, \n}\n\n","first_N":5,"first_N_keywords":["zero-shot-classification","question-answering","translation","English","German"],"keywords_longer_than_N":true},
	{"name":"gsm8k_multiturn","keyword":"multiturn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/euclaise/gsm8k_multiturn","creator_name":"Jade","creator_url":"https://huggingface.co/euclaise","description":"The \"socratic\" version of GSM8K has the model reflect and ask itself sub-questions about the initial question, before coming to a final answer.\nThis dataset reformats the socratic GSM8K version into a multi-turn conversation, where the sub-questions are asked by the user rather than being self-asked by the model.\n","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ru-instruct","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tĞšĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°\n\t\n\nĞ¡ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸. ĞÑ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° (ÑĞ¿Ğ°ÑĞ¸Ğ±Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Den4ikAI/nonsense_gibberish_detector). Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ SimHash'Ğ¾Ğ¼.\nĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ½Ñ‘Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ·Ğ°Ğ²Ñ‘Ğ·, in progress.\n\n\t\n\t\t\n\t\tĞ¡Ğ¾ÑÑ‚Ğ°Ğ²\n\t\n\nĞ¡Ğ¾Ğ±Ñ€Ğ°Ğ» Ğ¸Ğ· ÑÑ‚Ğ¸Ñ… Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´Ñ‘Ğ½Ğ½Ñ‹Ñ…:\n\nd0rj/OpenOrca-ru (Ğ¾Ñ‚ Open-Orca/OpenOrca)\nd0rj/OpenHermes-2.5-ru (Ğ¾Ñ‚ teknium/OpenHermes-2.5)\nd0rj/dolphin-ru (Ğ¾Ñ‚ ehartford/dolphin)\nd0rj/alpaca-cleaned-ru (Ğ¾Ñ‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","machine-generated","found","translated"],"keywords_longer_than_N":true},
	{"name":"meme_dataset","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noxus09/meme_dataset","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","description":"Noxus09/meme_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MMC","keyword":"instruction","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xywang1/MMC","creator_name":"Xiaoyang Wang","creator_url":"https://huggingface.co/xywang1","description":"\n\t\n\t\t\n\t\tMMC: Advancing Multimodal Chart Understanding with LLM Instruction Tuning\n\t\n\nThis repo releases data introduced in our paper MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning.\n\nThe paper was published in NAACL 2024.\nSee our GithHub repo for demo code and more.\n\n\n\t\n\t\t\n\t\n\t\n\t\tHighlights\n\t\n\n\nWe introduce a large-scale MultiModal Chart Instruction (MMC-Instruction) dataset supporting diverse tasks and chart types. Leveraging this data.\nWe also propose aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xywang1/MMC.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-sa-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"guanjian_anli_test1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/papaya523/guanjian_anli_test1","creator_name":"zhaoyue","creator_url":"https://huggingface.co/papaya523","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/papaya523/guanjian_anli_test1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"PromisedChat_Instruction","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/netmouse/PromisedChat_Instruction","creator_name":"Matt Yeh","creator_url":"https://huggingface.co/netmouse","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/netmouse/PromisedChat_Instruction.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Feedback-Collection-ru","keyword":"instruct","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Feedback-Collection-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tFeedback-Collection-ru\n\t\n\nThis is russian version of prometheus-eval/Feedback-Collection translated using Google Translate.\n","first_N":5,"first_N_keywords":["text-generation","text-classification","translated","prometheus-eval/Feedback-Collection","Russian"],"keywords_longer_than_N":true},
	{"name":"shibing624_alpaca-zh","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/shibing624_alpaca-zh","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/botp/shibing624_alpaca-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"bioinstruct","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bio-nlp-umass/bioinstruct","creator_name":"UMass BioNLP Lab","creator_url":"https://huggingface.co/bio-nlp-umass","description":"\n\t\n\t\t\n\t\tDataset Card for BioInstruct\n\t\n\nGitHub repo: https://github.com/bio-nlp/BioInstruct\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBioInstruct is a dataset of 25k instructions and demonstrations generated by OpenAI's GPT-4 engine in July 2023. \nThis instruction data can be used to conduct instruction-tuning for language models (e.g. Llama) and make the language model follow biomedical instruction better. \nImprovements of Llama on 9 common BioMedical tasks are shown in the result section. \nTakingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bio-nlp-umass/bioinstruct.","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"halfTurkish","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sasanikrali/halfTurkish","creator_name":"sasani krali","creator_url":"https://huggingface.co/sasanikrali","description":"\n\t\n\t\t\n\t\tDataset Card for HalfTurkish\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the givenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sasanikrali/halfTurkish.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"walt","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/canTooDdev/walt","creator_name":"Boris Marion-Dorier","creator_url":"https://huggingface.co/canTooDdev","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/canTooDdev/walt.","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"deepresearch_trace","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Looogic/deepresearch_trace","creator_name":"LogicğŸ¤—","creator_url":"https://huggingface.co/Looogic","description":"\n\t\n\t\t\n\t\tğŸ”¬ DeepResearch Tool Use Conversations\n\t\n\nA high-quality dataset of multi-turn conversations between humans and AI agents, featuring sophisticated tool use for research and report generation tasks.\n\n\t\n\t\t\n\t\tğŸŒŸ Key Features\n\t\n\n\nMulti-turn conversations with complex reasoning chains\nTool use integration including search, web scraping, and note-taking\nComprehensive metadata with execution metrics and performance tracking\nResearch-focused tasks requiring information synthesis and analysisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Looogic/deepresearch_trace.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","n<1K"],"keywords_longer_than_N":true},
	{"name":"alpaca-tat","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yasalma/alpaca-tat","creator_name":"Yasalma","creator_url":"https://huggingface.co/yasalma","description":"\n\t\n\t\t\n\t\tTatAlpaca\n\t\n\nDataset of Gemini-generated instructions in Tatar language.\n\nCode: tatlm/self_instruct\nCode is based on Stanford Alpaca and self-instruct.\n166,257 examples\n\nPrompt template:\n{{num_tasks}} Ò—Ñ‹ĞµĞ»Ğ¼Ğ°ÑÑ‹Ğ½Ñ‹Ò£ ÑĞ¾ÑÑ‚Ğ°Ğ²Ñ‹ Ñ‚ĞµĞ» Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ½ Ó©Ğ¹Ñ€Ó™Ğ½Ò¯ Ó©Ñ‡ĞµĞ½ Ñ‚Ó©Ñ€Ğ»Ğµ:\n\n1. Ğ‘Ğ¸Ñ€ĞµĞ¼Ğ½Ó™Ñ€Ğ½Ğµ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒ Ñ€Ó™Ğ²ĞµÑˆÑ‚Ó™ Ñ‚Ğ¸Ğ¿Ğ»Ğ°Ñ€Ñ‹, ÑĞ¾Ñ€Ğ°Ğ»Ğ³Ğ°Ğ½ Ğ³Ğ°Ğ¼Ó™Ğ»Ğ»Ó™Ñ€Ğµ, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ğ»Ğ°Ñ€Ñ‹, ĞºĞµÑ€Ò¯ Ğ¼Ó©Ğ¼ĞºĞ¸Ğ½Ğ»ĞµĞºĞ»Ó™Ñ€Ğµ Ğ±ÑƒĞµĞ½Ñ‡Ğ° Ğ±ĞµÑ€-Ğ±ĞµÑ€ÑĞµĞ½Ó™ Ğ¾Ñ…ÑˆĞ°Ğ¼Ğ°Ğ³Ğ°Ğ½ Ğ¸Ñ‚ĞµĞ¿ ÑÑˆĞ»Ó™.\n2. Ğ‘Ğ¸Ñ€ĞµĞ¼Ğ½Ó™Ñ€ Ñ€Ó™ÑĞµĞ¼Ğ½Ó™Ñ€, Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ±ĞµĞ»Ó™Ğ½ ÑÑˆĞ»Ğ¸ Ğ±ĞµĞ»Ğ¼Ó™Ğ³Ó™Ğ½ Ò»Ó™Ğ¼ Ñ‚Ñ‹ÑˆĞºÑ‹ Ğ´Ó©Ğ½ÑŒÑĞ³Ğ° ĞºĞµÑ€Ò¯ Ğ¼Ó©Ğ¼ĞºĞ¸Ğ½Ğ»ĞµĞ³Ğµ Ğ±ÑƒĞ»Ğ¼Ğ°Ğ³Ğ°Ğ½â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yasalma/alpaca-tat.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Tatar","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"test2027","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zjrwtxtechstudio/test2027","creator_name":"zjrwtx","creator_url":"https://huggingface.co/zjrwtxtechstudio","description":"A dataset containing dialogues between assistant and user with different roles.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"StructFlowBench","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Jinnan/StructFlowBench","creator_name":"Jinnan Li","creator_url":"https://huggingface.co/Jinnan","description":"\n\t\n\t\t\n\t\tStructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following\n\t\n\n\n  \n    ğŸ“ƒ Paper\n  \n  â€¢\n  \n    ğŸ¤— Dataset\n  \n  â€¢\n  \n    ğŸ–¥ï¸ Code\n  \n\n\n\n\t\n\t\t\n\t\t1. Updates\n\t\n\n\n2025/02/26: We enhanced the code documentation on GitHub with detailed implementation guidelines.\n2025/02/24: We submitted our paper to Hugging Face's Daily Papers.\n2025/02/23: We released StructFlowBench dataset on huggingface.\n2025/02/20: We released the first version of our paper along with the dataset andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jinnan/StructFlowBench.","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Model-based","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Model-based is a dataset of BenchMAX, sourcing from m-ArenaHard, which evaluates the instruction following capability via model-based judgment.\nWe extend the original dataset to include languages that are not supported by m-ArenaHard throughâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based.","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"ProcessedOpenAssistant","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant","creator_name":"Y. Yu","creator_url":"https://huggingface.co/PursuitOfDataScience","description":"\n\t\n\t\t\n\t\tRefined OASST1 Conversations\n\t\n\nDataset Name on Hugging Face: PursuitOfDataScience/ProcessedOpenAssistant\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is derived from the OpenAssistant/oasst1 conversations, with additional processing to:\n\nRemove single-turn or incomplete conversations (where a prompter/user message had no assistant reply),\nRename roles from \"prompter\" to \"User\" and \"assistant\" to \"Assistant\",\nOrganize each conversation as a list of turn objects.\n\nThe goal is to provide a cleanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Electrical-engineering-ru","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Electrical-engineering-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"Translated instructions from STEM-AI-mtl/Electrical-engineering into Russian using gemini-flash-1.5-8b.\n","first_N":5,"first_N_keywords":["text2text-generation","text-generation","translated","STEM-AI-mtl/Electrical-engineering","Russian"],"keywords_longer_than_N":true},
	{"name":"kazakh-ift","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nurkhan5l/kazakh-ift","creator_name":"Nurkhan Laiyk","creator_url":"https://huggingface.co/nurkhan5l","description":"Kazakh-IFT ğŸ‡°ğŸ‡¿\nAuthors: Nurkhan Laiyk, Daniil Orel, Rituraj Joshi, Maiya Goloburda, Yuxia Wang, Preslav Nakov, Fajri Koto\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nInstruction tuning in low-resource languages remains challenging due to limited coverage of region-specific institutional and cultural knowledge. To address this gap, we introduce a large-scale instruction-following dataset (~10,600 samples) focused on Kazakhstan, spanning domains such as governance, legal processes, cultural practices, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nurkhan5l/kazakh-ift.","first_N":5,"first_N_keywords":["Kazakh","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Bangla-Instruct","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/md-nishat-008/Bangla-Instruct","creator_name":"Nishat Raihan","creator_url":"https://huggingface.co/md-nishat-008","description":"\nBangla-Instruct\nState-of-the-art Bangla Instruction Dataset\n\n\n\n\n\n\n\n\n\n\n\nTigerLLM introduces a state-of-the-art dataset designed to advance Bangla language modeling. The Bangla-Instruct dataset contains high-quality native Bangla instruction-response pairs that have been generated using cutting-edge teacher models.\n\n\n\n\nOverview\n\n\n\nThe Bangla-Instruct dataset is composed of 100,000 instruction-response pairs. It starts with 500 seed tasks created by 50 volunteer experts from premier Bangladeshiâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/md-nishat-008/Bangla-Instruct.","first_N":5,"first_N_keywords":["text-generation","Bengali","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"express-legal-funding-reviews","keyword":"instruction-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/expresslegalfunding/express-legal-funding-reviews","creator_name":"Express Legal Funding","creator_url":"https://huggingface.co/expresslegalfunding","description":"A curated collection of real customer feedback and company replies for Express Legal Funding.  This dataset is designed for training and evaluating language models on tasks such as sentiment classification,  customer interaction modeling, and instruction tuning in the legal funding domain.\n","first_N":5,"first_N_keywords":["text-classification","text-generation","sentiment-classification","language-modeling","human"],"keywords_longer_than_N":true},
	{"name":"drill","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/drill","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tDrill\n\t\n\nThis dataset combines three instruction-following datasets:\n\nargilla/ifeval-like-data (filtered subset)  \nArliAI/Formax-v1.0  \nChristianAzinn/json-training\n\nIt contains prompts with detailed instructions and corresponding formatted outputs, suitable for training models on instruction adherence and structured text generation.\n\n  Definition of the word \"drill\" according to Merriam-Webster Dictionary\n  \n\ndrill (noun)\n\na physical or mental exercise aimed at perfecting facility andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/drill.","first_N":5,"first_N_keywords":["text2text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"TypaRP-16x1k","keyword":"multiturn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Hamzah-Asadullah/TypaRP-16x1k","creator_name":"Hamzah Asadullah","creator_url":"https://huggingface.co/Hamzah-Asadullah","description":"I publish the TypaRP-16x1k dataset generated using Synthetic-Alpaca as a pipeline and LumimaidV0.2-8B as a generator.This dataset compromises 1024 samples / rows, each with a system prompt declaring a roleplay:\nFollowing is a roleplay between two characters the user will provide in the next message.\nMarkdown (**strong**, *italic*, \"stuff characters say\", et cetera) is supported and should be used.\n\nFollowed by two characters specified by the user, and further 14 messages (hence 16x1k, 16â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Hamzah-Asadullah/TypaRP-16x1k.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"secondKarlMarx-sft","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft","creator_name":"ChizhongWang","creator_url":"https://huggingface.co/ChizhongWang","description":"\n\t\n\t\t\n\t\tMarx Works SFT Instruction Prompts Dataset / é©¬å…‹æ€è‘—ä½œSFTæŒ‡ä»¤æç¤ºæ•°æ®é›†\n\t\n\nEnglish | ä¸­æ–‡\n\n\n\t\n\t\t\n\t\tEnglish\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains SFT (Supervised Fine-Tuning) instruction prompts generated from the works of Karl Marx. The dataset is specifically designed for training large language models, aiming to capture Marx's dialectical materialist analytical method and writing style.\n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nDiverse Prompt Types: Includes various styles of prompts such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft.","first_N":5,"first_N_keywords":["text-generation","language-modeling","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"tame-the-weights-personas","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/leonvanbokhorst/tame-the-weights-personas","creator_name":"Leon Van Bokhorst","creator_url":"https://huggingface.co/leonvanbokhorst","description":"\n\t\n\t\t\n\t\tDataset Card for \"tame-the-weights-personas\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains instruction-following data designed for fine-tuning language models, specifically focused on generating Python code explanations and snippets while adopting distinct personas.\nThe data was synthetically generated using a large language model, prompted to adopt one of three personas:\n\nProfessor Snugglesworth: A friendly, encouraging, and slightly verbose persona, like a kind universityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/leonvanbokhorst/tame-the-weights-personas.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true}
]
;
