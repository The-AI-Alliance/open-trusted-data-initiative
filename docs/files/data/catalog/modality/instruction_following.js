const data_for_modality_instruction_following = 
[
	{"name":"OpenManus-RL","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","description":"\n\t\n\t\t\n\t\tDataset Card for OpenManusRL\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\n  üíª [Github Repo]\n\n\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\n\nüîç ReAct Framework - Reasoning-Acting integration\nüß† Structured Training - Separate format/reasoning learning\nüö´ Anti-Hallucination - Negative samples + environment grounding\nüåê 6 Domains - OS, DB, Web, KG, Household, E-commerce\n\n\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\nSource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"muInstruct-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/muInstruct-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tmuInstruct-ru\n\t\n\nTranslated instructions from EleutherAI/muInstruct into Russian using gemini-flash-1.5-8b.\n","first_N":5,"first_N_keywords":["text-generation","translated","EleutherAI/muInstruct","Russian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"tame-the-weights-personas","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/leonvanbokhorst/tame-the-weights-personas","creator_name":"Leon Van Bokhorst","creator_url":"https://huggingface.co/leonvanbokhorst","description":"\n\t\n\t\t\n\t\tDataset Card for \"tame-the-weights-personas\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains instruction-following data designed for fine-tuning language models, specifically focused on generating Python code explanations and snippets while adopting distinct personas.\nThe data was synthetically generated using a large language model, prompted to adopt one of three personas:\n\nProfessor Snugglesworth: A friendly, encouraging, and slightly verbose persona, like a kind university‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/leonvanbokhorst/tame-the-weights-personas.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"ru-instruct-conversation-v1","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-instruct-conversation-v1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian dialogs in form of conversations suitable for LLM fine-tuning scenarios.\nTotal samples: 82208\nDeduplicated using simhash(hamming_treshold=3).\nDatasets used:\n\nIlyaGusev/saiga_scored (min_score: 8, no bad by regexp)\nIlyaGusev/oasst2_ru_main_branch\nattn-signs/kolmogorov-3\nattn-signs/russian-easy-instructions\n\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\n–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É: <think> –¢–≤–æ–∏ –º—ã—Å–ª–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è </think> \n–¢–≤–æ–π –∫–æ–Ω–µ—á–Ω—ã–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1-v2","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"This is a modified version of ZeroAgency/ru-thinking-reasoning-r1 with addition of Egor-AI/CoT-XLang dataset.\nCombined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\n–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"secondKarlMarx-sft","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft","creator_name":"ChizhongWang","creator_url":"https://huggingface.co/ChizhongWang","description":"\n\t\n\t\t\n\t\tMarx Works SFT Instruction Prompts Dataset / È©¨ÂÖãÊÄùËëó‰ΩúSFTÊåá‰ª§ÊèêÁ§∫Êï∞ÊçÆÈõÜ\n\t\n\nEnglish | ‰∏≠Êñá\n\n\n\t\n\t\t\n\t\tEnglish\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains SFT (Supervised Fine-Tuning) instruction prompts generated from the works of Karl Marx. The dataset is specifically designed for training large language models, aiming to capture Marx's dialectical materialist analytical method and writing style.\n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nDiverse Prompt Types: Includes various styles of prompts such as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft.","first_N":5,"first_N_keywords":["text-generation","language-modeling","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"fin-term-instruct","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taetae030/fin-term-instruct","creator_name":"leetaehee","creator_url":"https://huggingface.co/taetae030","description":"\n\t\n\t\t\n\t\tüìò fin-term-instruct: ÌïúÍµ≠Ïñ¥ Í∏àÏúµ Ïö©Ïñ¥ ÏßàÏùòÏùëÎãµ Îç∞Ïù¥ÌÑ∞ÏÖã\n\t\n\nfin-term-instructÎäî ÌïúÍµ≠Ïñ¥ Í∏àÏúµ Ïö©Ïñ¥ ÏÑ§Î™ÖÏóê ÌäπÌôîÎêú instruct-style ÏßàÎ¨∏-ÏùëÎãµ Îç∞Ïù¥ÌÑ∞ÏÖãÏûÖÎãàÎã§.MetaÏùò LLaMA ÏãúÎ¶¨Ï¶à Îì± ÎåÄÌòï Ïñ∏Ïñ¥Î™®Îç∏(LLM)ÏùÑ ÌïúÍµ≠Ïñ¥ Í∏àÏúµ Ï±óÎ¥áÏúºÎ°ú ÌäúÎãùÌïòÍ∏∞ ÏúÑÌï¥ Íµ¨Ï∂ïÎêòÏóàÏäµÎãàÎã§.\n\n\n\t\n\t\t\n\t\tüì¶ ÏõêÎ≥∏ Ï∂úÏ≤ò: AI ÌóàÎ∏å\n\t\n\nÏù¥ Îç∞Ïù¥ÌÑ∞Îäî AI ÌóàÎ∏åÏùò **\"Í∏àÏúµ¬∑Î≤ïÎ•† Î¨∏ÏÑú Í∏∞Í≥ÑÎèÖÌï¥ Îç∞Ïù¥ÌÑ∞\"**Î•º Í∏∞Î∞òÏúºÎ°ú Íµ¨Ï∂ïÌïòÏòÄÏäµÎãàÎã§.\n\nüìÇ ÏõêÎ≥∏ Ï£ºÏÜå: AI ÌóàÎ∏å ‚Äì Í∏àÏúµ¬∑Î≤ïÎ•† Î¨∏ÏÑú Í∏∞Í≥ÑÎèÖÌï¥ Îç∞Ïù¥ÌÑ∞\nÎç∞Ïù¥ÌÑ∞ Íµ¨Ï∂ïÎÖÑÎèÑ: 2022ÎÖÑ\nÏ¥ù Íµ¨Ï∂ïÎüâ: 400,000Í±¥\nÎç∞Ïù¥ÌÑ∞ ÌòïÏãù: JSON (ÏßÄÎ¨∏ - ÏßàÎ¨∏ - Ï†ïÎãµ Íµ¨ÏÑ±)\n\n\n\t\n\t\t\n\t\tüîç ÏÇ¨Ïö© Î≤îÏúÑ\n\t\n\n\nÏ†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Ï§ë **Í∏àÏúµÍ≤ΩÏ†ú Î∂ÑÏïº(ÏïΩ 17.3%)**Îßå ÏÑ†Î≥ÑÌïòÏó¨ ÏÇ¨Ïö©\nÍ∏∞Ï°¥ MRC ÌòïÌÉúÏóêÏÑú instruction-style QA Ìè¨Îß∑ÏúºÎ°ú Ïû¨Í∞ÄÍ≥µ\nGPT Í∏∞Î∞ò ÏöîÏïΩ¬∑Ï†ïÏ†úÎ•º ÌÜµÌï¥ Í∞ÑÍ≤∞Ìïú ÏùëÎãµ ÌòïÏãùÏúºÎ°ú ÌÜµÏùº‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/taetae030/fin-term-instruct.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"adobetest","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ahmedmehtab/adobetest","creator_name":"Ahmed Mehtab","creator_url":"https://huggingface.co/ahmedmehtab","description":"\n\t\n\t\t\n\t\tAdobeTest Dataset\n\t\n\nStructured image metadata (title, keywords, category) for generative tasks.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"alpaca-qa-data","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sweatSmile/alpaca-qa-data","creator_name":"amitk17","creator_url":"https://huggingface.co/sweatSmile","description":"\n\t\n\t\t\n\t\tAlpaca-style Question and Answer Dataset\n\t\n\nThis dataset contains question-answer pairs formatted in the Alpaca instruction style, suitable for instruction fine-tuning of language models.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example contains:\n\ninstruction: The question\ninput: Empty string (can be used for context in other applications)\noutput: The answer\ntext: The formatted text using the Alpaca template\n\n\n\t\n\t\t\n\t\tTemplate\n\t\n\nBelow is an instruction that describes a task, paired with an input that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sweatSmile/alpaca-qa-data.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"collabllm-20q-interactive","keyword":"multiturn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aditijb/collabllm-20q-interactive","creator_name":"Aditi","creator_url":"https://huggingface.co/aditijb","description":"aditijb/collabllm-20q-interactive dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"stage1-doctor-patient-chat","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BirdieByte1024/stage1-doctor-patient-chat","creator_name":"BirdieByte","creator_url":"https://huggingface.co/BirdieByte1024","description":"\n\t\n\t\t\n\t\tü¶∑ Stage 1 - AI Doctor Tone Dataset (Dental)\n\t\n\nThis dataset contains instruction‚Äìresponse formatted examples derived from realistic patient-doctor conversations, focused on general medical behavior and tone. It is designed as Stage 1 in a two-stage fine-tuning pipeline for building a domain-specific, polite, and structured AI dental assistant.\n\n\n\t\n\t\t\n\t\t‚ú® Intended Use\n\t\n\n\nFine-tuning large language models (LLMs) to simulate human-like, empathetic, and structured medical responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BirdieByte1024/stage1-doctor-patient-chat.","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"streetview-commands-dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cahlen/streetview-commands-dataset","creator_name":"Cahlen Humphreys","creator_url":"https://huggingface.co/cahlen","description":"\n\n\t\n\t\t\n\t\tDataset Card for streetview-commands-dataset\n\t\n\nThis dataset contains pairs of natural language instructions (simulating commands given to Google Street View) and their corresponding structured JSON outputs representing the intended navigation action. It was generated using the Gemini API (gemini-1.5-flash-latest) based on predefined templates and few-shot examples.\nThe primary intended use is for fine-tuning small language models (like TinyLlama) to act as a translation layer between‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cahlen/streetview-commands-dataset.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"EFAGen-Llama-3.1-8B-Instruct-Training-Data","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/codezakh/EFAGen-Llama-3.1-8B-Instruct-Training-Data","creator_name":"Zaid Khan","creator_url":"https://huggingface.co/codezakh","description":"Paper Link\nThe training data used for the final version of EFAGen-Llama-3.1-8B-Instruct.\nThe data is in Alpaca format and can be used with Llama-Factory (check dataset_info.json).\n","first_N":5,"first_N_keywords":["text-generation","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"filtered-high-quality-dpo","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/helloTR/filtered-high-quality-dpo","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","description":"\n\t\n\t\t\n\t\tFiltered High-Quality Instruction-Output Dataset\n\t\n\nThis dataset contains high-quality (score = 5) instruction-output pairs generated via a reverse instruction generation pipeline using a fine-tuned backward model and evaluated by LLaMA-2-7B-Chat.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\ninstruction: The generated prompt.\noutput: The original response.\nscore: Quality score assigned ( score = 5 retained).\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-contrast-sample","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/helloTR/dpo-contrast-sample","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","description":"\n\t\n\t\t\n\t\tDPO Contrast Sample\n\t\n\nThis dataset provides a direct comparison between high-quality and low-quality instruction-output pairs evaluated by a LLaMA-2-7B-Chat model in a DPO-style workflow.\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\ninstruction: The generated prompt (x).\noutput: The associated output (y).\nscore: Quality rating assigned by LLaMA2 (1 = high, 5 = low).\n\n\n\t\n\t\t\n\t\tSamples\n\t\n\n5 examples with score = 1 (excellent), and 5 with score = 5 (poor).\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ru-big-russian-dataset","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-big-russian-dataset","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"\n\t\n\t\t\n\t\tBig Russian Dataset\n\t\n\nMade by ZeroAgency.ru - telegram channel.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset size\n\t\n\n\nTrain: 1 710 601 samples (filtered from 2_149_360)\nTest:  18 520 samples (not filtered)\n\n\n\t\n\t\t\n\t\n\t\n\t\tEnglish\n\t\n\nThe Big Russian Dataset is a combination of various primarily Russian‚Äëlanguage datasets. With some sort of reasoning!\nThe dataset was deduplicated, cleaned, scored using gpt-4.1 and filtered.\n\n\t\n\t\t\n\t\n\t\n\t\t–†—É—Å—Å–∫–∏–π\n\t\n\nBig Russian Dataset - –±–æ–ª—å—à–æ–π —Ä—É—Å—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç. –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –∏–∑‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-big-russian-dataset.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"synapse-set-10k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NextGenC/synapse-set-10k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","description":"\n\t\n\t\t\n\t\tüß† SynapseSet-10K\n\t\n\nSynapseSet-10K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nüî¨ 100% synthetic, non-clinical data. Intended for academic and research‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-10k.","first_N":5,"first_N_keywords":["text-generation","English","Turkish","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"synapse-set-100k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NextGenC/synapse-set-100k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","description":"\n\t\n\t\t\n\t\tüß† SynapseSet-100K\n\t\n\nSynapseSet-100K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nüî¨ 100% synthetic, non-clinical data. Intended for academic and research‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-100k.","first_N":5,"first_N_keywords":["text-generation","English","Turkish","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"solidity_vulnerability_audit_dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GitmateAI/solidity_vulnerability_audit_dataset","creator_name":"GitmateAI","creator_url":"https://huggingface.co/GitmateAI","description":"\n\t\n\t\t\n\t\tSolidity Vulnerability Audit Dataset\n\t\n\n\nOrganization: gitmate AI\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Solidity Vulnerability Audit Dataset is a curated collection of Solidity smart contract code snippets paired with expert-written vulnerability audits. Each entry presents a real or realistic smart contract scenario, and the corresponding analysis identifies security vulnerabilities or confirms secure patterns. The dataset is designed for instruction-tuned large language models (LLMs) to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GitmateAI/solidity_vulnerability_audit_dataset.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"alpaca-spanish","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bertin-project/alpaca-spanish","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","description":"\n\t\n\t\t\n\t\tBERTIN Alpaca Spanish\n\t\n\nThis dataset is a translation to Spanish of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\n","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yahma/alpaca-cleaned","creator_name":"Gene Ruebsamen","creator_url":"https://huggingface.co/yahma","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yahma/alpaca-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Mantis-Instruct","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tMantis-Instruct\n\t\n\nPaper | Website | Github | Models | Demo\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nMantis-Instruct is a fully text-image interleaved multimodal instruction tuning dataset, \ncontaining 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding. \nIt's been used to train Mantis Model families\n\nMantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\nAmong the 14‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"KoMT-Bench","keyword":"instruction-following","license":"GNU Lesser General Public License v3.0","license_url":"https://choosealicense.com/licenses/lgpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench","creator_name":"LG AI Research","creator_url":"https://huggingface.co/LGAI-EXAONE","description":"\n\t\n\t\t\n\t\tKoMT-Bench\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe present KoMT-Bench, a benchmark designed to evaluate the capability of language models in following instructions in Korean.\nKoMT-Bench is an in-house dataset created by translating MT-Bench [1]  dataset into Korean and modifying some questions to reflect the characteristics and cultural nuances of the Korean language.\nAfter the initial translation and modification, we requested expert linguists to conduct a thorough review of our benchmark‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench.","first_N":5,"first_N_keywords":["question-answering","Korean","lgpl-3.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Celestia","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia is a dataset containing science-instruct data.\nThe 2024-10-30 version contains:\n\n126k rows of synthetic science-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Leopard-Instruct","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wyu1/Leopard-Instruct","creator_name":"Wenhao Yu","creator_url":"https://huggingface.co/wyu1","description":"\n\t\n\t\t\n\t\tLeopard-Instruct\n\t\n\nPaper | Github | Models-LLaVA | Models-Idefics2\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nLeopard-Instruct is a large instruction-tuning dataset, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. It's been used to train Leopard-LLaVA [checkpoint]  and Leopard-Idefics2 [checkpoint].\n\n\t\n\t\t\n\t\tLoading dataset\n\t\n\n\nto load the dataset without automatically downloading and process the images (Please run the following codes with datasets==2.18.0)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wyu1/Leopard-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"SHARE","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/eunwoneunwon/SHARE","creator_name":"Eunwon Kim","creator_url":"https://huggingface.co/eunwoneunwon","description":"\n\t\n\t\t\n\t\tSHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script\n\t\n\nSHARE is a novel long-term dialogue dataset constructed from movie scripts. \n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset consists of:\n\nConversations: Dialogue exchanges between two main characters in various movie scripts.\nAnnotations: Detailed extractions using GPT-4, including:\nPersona: Persona information captures essential characteristics, including personality, occupation, and interest.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eunwoneunwon/SHARE.","first_N":5,"first_N_keywords":["English","apache-2.0","10M<n<100M","üá∫üá∏ Region: US","chatbot"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tProject‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","first_N":5,"first_N_keywords":["translation","text-generation","text-classification","token-classification","sentence-similarity"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tProject‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","first_N":5,"first_N_keywords":["translation","text-generation","text-classification","token-classification","sentence-similarity"],"keywords_longer_than_N":true},
	{"name":"werewolf_game_reasoning","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning","creator_name":"Rong Ye","creator_url":"https://huggingface.co/ReneeYe","description":"\n\t\n\t\t\n\t\tWerewolf Game Dataset\n\t\n\nThis repository contains a comprehensive dataset for the Werewolf game in paper Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game, including both raw game data and processed  multi-level instruction datasets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tRaw Data\n\t\n\nThe raw data is located in the raw folder. Each game consists of two files:\n\nevent.json: Contains the game regular record and thinking process data, including:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning.","first_N":5,"first_N_keywords":["text-generation","expert-generated","multilingual","original","Chinese"],"keywords_longer_than_N":true},
	{"name":"ru-tasks-conversation","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-tasks-conversation","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian math and physics tasks in form of conversation suitable for LLM fine-tuning scenarios.\nTotal samples: 462883\nDatasets used:\n\nVikhrmodels/russian_math\nVikhrmodels/russian_physics\nd0rj/MathInstruct-ru\nd0rj/orca-math-word-problems-200k-ru\nevilfreelancer/MATH-500-Russian\n\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ScanBot","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ed1son/ScanBot","creator_name":"zhiling chen","creator_url":"https://huggingface.co/ed1son","description":" \n\n\t\n\t\t\n\t\tScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tüß† Dataset Summary\n\t\n\nScanBot is a dataset for instruction-conditioned, high-precision surface scanning with robots. Unlike existing datasets that focus on coarse tasks like grasping or navigation, ScanBot targets industrial laser scanning, where sub-millimeter accuracy and parameter stability are essential. It includes scanning trajectories across 12 objects and 6 task types, each driven by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ed1son/ScanBot.","first_N":5,"first_N_keywords":["robotics","English","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"helpful_instructions","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Helpful Instructions is a dataset of (prompt, completion) pairs that are derived from a variety of public datasets. As the name suggests, it focuses on instructions that are \"helpful\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"instruct_me","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/instruct_me","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Instruct Me is a dataset of instruction-like dialogues between a human user and AI assistant. The prompts are derived from (prompt, completion) pairs in the Helpful Instructions dataset. The goal is to train a language model to that is \"chatty\" and can answer the kind of questions or tasks a human user might instruct an AI assistant to perform.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"norwegian-alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NbAiLab/norwegian-alpaca","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","description":"\n\t\n\t\t\n\t\tNB Alpaca Norwegian Bokm√•l\n\t\n\nThis dataset is a translation to Norwegian Bokm√•l of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\n","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian Bokm√•l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-zh","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shibing624/alpaca-zh","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\nÊú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/alpaca-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Alpaca-CoT","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QingyiSi/Alpaca-CoT","creator_name":"Qingyi Si","creator_url":"https://huggingface.co/QingyiSi","description":"\n\t\n\t\t\n\t\n\t\n\t\tInstruction-Finetuning Dataset Collection (Alpaca-CoT)\n\t\n\nThis repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model.\nWe also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT.  \nIf you think this dataset collection is helpful to you, please like‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT.","first_N":5,"first_N_keywords":["English","Chinese","Malayalam","apache-2.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"vi_alpaca_clean","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tsdocode/vi_alpaca_clean","creator_name":"Thanh Sang","creator_url":"https://huggingface.co/tsdocode","description":"tsdocode/vi_alpaca_clean dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Vietnamese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"translated_polish_alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aspik101/translated_polish_alpaca","creator_name":"Hubert","creator_url":"https://huggingface.co/Aspik101","description":"The dataset was translated into Polish using this model: \"gsarti/opus-mt-tc-en-pl\"\n\n\t\n\t\t\n\t\n\t\n\t\tHow to use\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Aspik101/translated_polish_alpaca\")\n\n","first_N":5,"first_N_keywords":["text-generation","Polish","cc-by-4.0","üá∫üá∏ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"alpaca-id-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cahya/alpaca-id-cleaned","creator_name":"Cahya Wirawan","creator_url":"https://huggingface.co/cahya","description":"\n\t\n\t\t\n\t\tDataset Card for Indonesian Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the Indonesian translated version of the cleaned original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cahya/alpaca-id-cleaned.","first_N":5,"first_N_keywords":["text-generation","Indonesian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-data\"\n\t\n\nAll of the work is done by this team. \n\n\t\n\t\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\t\n\t\t\n\t\tChinese Dataset\n\t\n\nFound here\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Li‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data-zh","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-data-zh\"\n\t\n\nAll of the work is done by this team. \n\n\t\n\t\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\t\n\t\t\n\t\tEnglish Dataset\n\t\n\nFound here\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Li‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"AlpacaDataCleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexl83/AlpacaDataCleaned","creator_name":"Alessandro Lannocca","creator_url":"https://huggingface.co/alexl83","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexl83/AlpacaDataCleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"TSSB-3M-instructions","keyword":"instruct","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zirui3/TSSB-3M-instructions","creator_name":"zirui","creator_url":"https://huggingface.co/zirui3","description":"\n\t\n\t\t\n\t\tdata summary\n\t\n\ninstruction dataset for code bugfix\n\n\t\n\t\t\n\t\tReference\n\t\n\n[1]. TSSB-3M-ext\n","first_N":5,"first_N_keywords":["code","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"NorPaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MasterThesisCBS/NorPaca","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","description":"\n\t\n\t\t\n\t\tNorPaca Norwegian Bokm√•l\n\t\n\nThis dataset is a translation to Norwegian Bokm√•l of alpaca_gpt4_data.json, a clean version of the Alpaca dataset made at Stanford, but generated with GPT4.\n\n\t\n\t\t\n\t\tPrompt to generate dataset\n\t\n\n    Du blir bedt om √• komme opp med et sett med 20 forskjellige oppgaveinstruksjoner. Disse oppgaveinstruksjonene vil bli gitt til en GPT-modell, og vi vil evaluere GPT-modellen for √• fullf√∏re instruksjonene. \n\nHer er kravene:\n1. Pr√∏v √• ikke gjenta verbet for hver‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MasterThesisCBS/NorPaca.","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian Bokm√•l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"NorEval","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MasterThesisCBS/NorEval","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","description":"\n\t\n\t\t\n\t\tNorEval\n\t\n\nNorEval is a self-curated dataset to evaluate instruction-following LLMs, seeking to evaluate the models in nine categories: Language, Code, Mathematics, Classification, Communication & Marketing, Medical, General Knowledge, and Business Operations\n","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian Bokm√•l","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ualpaca-gpt4","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nikes64/ualpaca-gpt4","creator_name":"MrNikes","creator_url":"https://huggingface.co/nikes64","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for \"alpaca-gpt4-cleaned\"\n\t\n\nThis dataset contains Ukrainian Instruction-Following translated by facebook/nllb-200-3.3B\nThe dataset was originaly shared in this repository: https://github.com/tloen/alpaca-lora\n\n\t\n\t\t\n\t\n\t\n\t\tLicensing Information\n\t\n\nThe dataset is available under the Creative Commons NonCommercial (CC BY-NC 4.0).\n","first_N":5,"first_N_keywords":["text-generation","question-answering","Ukrainian","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-tr","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cgulse/alpaca-cleaned-tr","creator_name":"Can G","creator_url":"https://huggingface.co/cgulse","description":"Alpaca Cleaned Dataset.\nMachine Translated facebook/nllb-200-3.3B\nLanguages\nTurkish\n","first_N":5,"first_N_keywords":["Turkish","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDataset Card for \"tasksource-instruct-v0\" (TSI)\n\t\n\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\nDataset size is capped at 30k examples per task to foster task diversity.\n!pip install tasksource, pandit\nimport tasksource, pandit\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\nfor tasks in df.id:\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\n\nhttps://github.com/sileod/tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDataset Card for \"tasksource-instruct-v0\" (TSI)\n\t\n\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\nDataset size is capped at 30k examples per task to foster task diversity.\n!pip install tasksource, pandit\nimport tasksource, pandit\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\nfor tasks in df.id:\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\n\nhttps://github.com/sileod/tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset","creator_name":"Nicholas Kluge Corr√™a","creator_url":"https://huggingface.co/nicholasKluge","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of prompts and responses to those prompts. All completions were generated by querying already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc.). The dataset is available in Portuguese, English, and Spanish.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguage modeling.\nQuestion-answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","Spanish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"reward-aira-dataset","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset","creator_name":"Nicholas Kluge Corr√™a","creator_url":"https://huggingface.co/nicholasKluge","description":"\n\t\n\t\t\n\t\tReward-Aira Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of prompt + completion examples of LLM following instructions in a conversational manner. All prompts come with two possible completions (one better than the other). The dataset is available in both Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized to train a reward/preference model or DPO fine-tuning.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nEnglish and Portuguese.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset.","first_N":5,"first_N_keywords":["text-classification","Portuguese","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-ru","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\talpaca-cleaned-ru\n\t\n\nTranslated version of yahma/alpaca-cleaned into Russian.\n","first_N":5,"first_N_keywords":["text-generation","translated","monolingual","yahma/alpaca-cleaned","Russian"],"keywords_longer_than_N":true},
	{"name":"VisIT-Bench","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/VisIT-Bench","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n\t\n\t\t\n\t\tDataset Card for VisIT-Bench\n\t\n\n\nDataset Description\nLinks\nDataset Structure\nData Fields\nData Splits\nData Loading\n\n\nLicensing Information\nAnnotations\nConsiderations for Using the Data\nCitation Information\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nVisIT-Bench is a dataset and benchmark for vision-and-language instruction following. The dataset is comprised of image-instruction pairs and corresponding example outputs, spanning a wide range of tasks, from simple object recognition to complex‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/VisIT-Bench.","first_N":5,"first_N_keywords":["crowdsourced","found","original","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"unix-commands","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/harpomaxx/unix-commands","creator_name":"Carlos A. Catania (Harpo)","creator_url":"https://huggingface.co/harpomaxx","description":"\n\t\n\t\t\n\t\tUnix Commands Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Unix Commands Dataset is a unique collection of real-world Unix command line examples, captured from various system prompts representing different user roles and responsibilities, such as system administrators, DevOps, network administrators, Docker administrators, regular users, and hackers.\nThe dataset consists of Unix commands ranging from basic to advanced levels and from a wide array of categories, including file operations (ls‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/harpomaxx/unix-commands.","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-serbian-full","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full","creator_name":"Dean","creator_url":"https://huggingface.co/datatab","description":"\n\t\n\t\t\n\t\tSerbian Alpaca Cleaned Dataset\n\t\n\n\nOriginal Repository: https://github.com/gururise/AlpacaDataCleaned\nOriginal HF Repository: https://huggingface.co/datasets/yahma/alpaca-cleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a serbian cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full.","first_N":5,"first_N_keywords":["text-generation","Serbian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"OpenOrca-ru","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/OpenOrca-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tOpenOrca-ru\n\t\n\nThis is translated version of Open-Orca/OpenOrca into Russian.\n","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"dolphin-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/dolphin-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tDolphin-ru üê¨\n\t\n\nThis is translated version of ehartford/dolphin into Russian.\n","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"gt-doremiti-instructions","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Gt-Doremiti/gt-doremiti-instructions","creator_name":"Doremiti","creator_url":"https://huggingface.co/Gt-Doremiti","description":"\n\t\n\t\t\n\t\tDataset Card for gt-doremiti-instructions\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nJeu d'instruction pour fine-tuner un LLM suivant les pr√©conisations du projet Stanford-Alpaca (https://github.com/tatsu-lab/stanford_alpaca)\nCes instructions sont extraites de la FAQ cr√©e par le GT DOREMITI et disponible √† cette adresse (https://gt-atelier-donnees.miti.cnrs.fr/faq.html)\nLes donn√©es sont mise √† disposition selon les termes de la Licence Creative Commons Attribution 4.0 International.\n","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"RoleBench","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZenMoore/RoleBench","creator_name":"Zekun Moore Wang","creator_url":"https://huggingface.co/ZenMoore","description":"\n\t\n\t\t\n\t\n\t\n\t\tRoleBench\n\t\n\n\nPaper Title: RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models\narXiv Link: https://arxiv.org/abs/2310.00746\nGithub Repo: https://github.com/InteractiveNLP-Team/RoleLLM-public\n\nPlease read our paper for more details about this dataset.\nTL;DR: We introduce RoleLLM, a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZenMoore/RoleBench.","first_N":5,"first_N_keywords":["Chinese","English","apache-2.0","Text","arxiv:2310.00746"],"keywords_longer_than_N":true},
	{"name":"shell-cmd-instruct","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/byroneverson/shell-cmd-instruct","creator_name":"Byron Everson","creator_url":"https://huggingface.co/byroneverson","description":"\n\t\n\t\t\n\t\tUsed to train models that interact directly with shells\n\t\n\nNote: This dataset is out-dated in the llm world, probably easier to just setup a tool with a decent model that supports tooling.\nFollow-up details of my process \n\nMacOS terminal commands for now. This dataset is still in alpha stages and will be modified.\n\nContains 500 somewhat unique training examples so far.\n\nGPT4 seems like a good candidate for generating more data, licensing would need to be addressed.\n\nI fine-tuned‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/byroneverson/shell-cmd-instruct.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ultrachat_200k_dutch","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\n\t\n\t\t\n\t\tDataset Card for UltraChat 200k Dutch\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"han-instruct-dataset-v1.0","keyword":"instruction-following","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\n\t\n\t\t\n\t\tDataset Card for \"han-instruct-dataset-v1.0\"\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nü™ø Han (‡∏´‡πà‡∏≤‡∏ô or goose) Instruct Dataset is a Thai instruction dataset by PyThaiNLP. It collect the instruction following in Thai from many source.\nMany question are collect from Reference desk at Thai wikipedia.\nData sources:\n\nReference desk at Thai wikipedia.\nLaw from justicechannel.org\npythainlp/final_training_set_v1_enth: Human checked and edited.\nSelf-instruct from WangChanGLM\nWannaphong.com\nHuman‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"han-instruct-dataset-v1.0","keyword":"instruction-finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\n\t\n\t\t\n\t\tDataset Card for \"han-instruct-dataset-v1.0\"\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nü™ø Han (‡∏´‡πà‡∏≤‡∏ô or goose) Instruct Dataset is a Thai instruction dataset by PyThaiNLP. It collect the instruction following in Thai from many source.\nMany question are collect from Reference desk at Thai wikipedia.\nData sources:\n\nReference desk at Thai wikipedia.\nLaw from justicechannel.org\npythainlp/final_training_set_v1_enth: Human checked and edited.\nSelf-instruct from WangChanGLM\nWannaphong.com\nHuman‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"urdu_alpaca_yc_filtered","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered","creator_name":"Mahwiz Khalil","creator_url":"https://huggingface.co/mahwizzzz","description":"\n\t\n\t\t\n\t\tAlpaca Urdu\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Alpaca Urdu  is a translation of the original dataset into Urdu. This dataset is a part of the Alpaca project and is designed for NLP tasks.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSize: The translated dataset contains [45,622] samples.\nLanguages: Urdu\nLicense: [cc-by-4.0]\nOriginal Dataset: Link to the original Alpaca Cleaned dataset repository\n\n\n\t\n\t\t\n\t\tColumns\n\t\n\nThe translated dataset includes the following columns:\n\ninput: The input text in Urdu.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered.","first_N":5,"first_N_keywords":["text-generation","Urdu","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","description":"\n\t\n\t\t\n\t\tLOGIC-701 Benchmark\n\t\n\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nThis benchmark is also part of an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701.","first_N":5,"first_N_keywords":["English","Russian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"fund-sft","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jannko/fund-sft","creator_name":"ko","creator_url":"https://huggingface.co/jannko","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jannko/fund-sft.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"Truth","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abhishekbisaria/Truth","creator_name":"Abhishek Bisaria","creator_url":"https://huggingface.co/abhishekbisaria","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abhishekbisaria/Truth.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"budapest-v0.1-hun","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun","creator_name":"Balazs Toldi","creator_url":"https://huggingface.co/Bazsalanszky","description":"\n\t\n\t\t\n\t\tBudapest-v0.1 Dataset README\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Budapest-v0.1 dataset is a cutting-edge resource specifically designed for fine-tuning large language models (LLMs). Created using GPT-4, this dataset is presented in a message-response format, making it particularly suitable for a variety of natural language processing tasks. The primary focus of Budapest-v0.1 is to aid in the development and enhancement of algorithms capable of performing summarization, question answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun.","first_N":5,"first_N_keywords":["text-generation","question-answering","Hungarian","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-bn","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn","creator_name":"fahim abrar","creator_url":"https://huggingface.co/abrarfahim","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned-bn\n\t\n\n\n\nThis is a cleaned bengali translated version of the original Alpaca Dataset released by Stanford. \n\n\t\n\t\t\n\t\tUses\n\t\n\nimport datasets\ndataset = datasets.load_dataset(\"abrarfahim/alpaca-cleaned-bn\")\nprint(dataset[0])\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{'system_prompt': 'You are a virtual assistant, deliver a comprehensive response.',\n 'qas_id': 'YY9S5K',\n 'question_text': '\"‡¶∏‡¶®‡ßç‡¶¶‡ßá‡¶π\" ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶∏‡¶†‡¶ø‡¶ï ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∂‡¶¨‡ßç‡¶¶ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®‡•§',\n 'orig_answer_texts': '\"‡¶∏‡¶®‡ßç‡¶¶‡ßá‡¶π\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn.","first_N":5,"first_N_keywords":["question-answering","text-generation","Bengali","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"grad_school_math_instructions_fr_Mixtral","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/grad_school_math_instructions_fr_Mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for grad_school_math_instructions_fr_Mixtral\n\t\n\nThis dataset was made thanks to the instruction of the vigogne's dataset but the output were generated with Mixtral-8x7B-Instruct instead of GPT3.5 to make it open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Alpaca_french_mixtral","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/Alpaca_french_mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca_french_mixtral\n\t\n\nThis dataset was made by reusing the french alpaca instruction with Mixtral-8x7B-Instruct to make the output open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v3","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3","creator_name":"Nicholas Kluge Corr√™a","creator_url":"https://huggingface.co/nicholasKluge","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset version 3.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of multi-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LONGCOT-Alpaca","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for LONGCOT-Alpaca\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"tw-reasoning-instruct-50k","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k","creator_name":"Twinkle AI","creator_url":"https://huggingface.co/twinkle-ai","description":"\n\t\n\t\t\n\t\tDataset Card for tw-reasoning-instruct-50k\n\t\n\n\n\ntw-reasoning-instruct-50k ÊòØ‰∏ÄÂÄãÁ≤æÈÅ∏ÁöÑ ÁπÅÈ´î‰∏≠ÊñáÔºàÂè∞ÁÅ£Ôºâ Êé®ÁêÜË≥áÊñôÈõÜÔºåÊó®Âú®ÊèêÂçáË™ûË®ÄÊ®°ÂûãÊñºÈÄêÊ≠•ÈÇèËºØÊÄùËÄÉ„ÄÅËß£ÈáãÁîüÊàêËàáË™ûË®ÄÁêÜËß£Á≠â‰ªªÂãô‰∏≠ÁöÑË°®Áèæ„ÄÇË≥áÊñôÂÖßÂÆπÊ∂µËìãÊó•Â∏∏ÊÄùËæ®„ÄÅÊïôËÇ≤Â∞çË©±„ÄÅÊ≥ïÂæãÊé®ÁêÜÁ≠âÂ§öÂÖÉ‰∏ªÈ°åÔºå‰∏¶ÁµêÂêà„ÄåÊÄùËÄÉÊ≠•È©ü„ÄçËàá„ÄåÊúÄÁµÇÁ≠îÊ°à„ÄçÁöÑÁµêÊßãË®≠Ë®àÔºåÂºïÂ∞éÊ®°Âûã‰ª•Êõ¥Ê∏ÖÊô∞„ÄÅÊ¢ùÁêÜÂàÜÊòéÁöÑÊñπÂºèÈÄ≤Ë°åÊé®Ë´ñËàáÂõûÊáâÔºåÁâπÂà•Âº∑Ë™øÁ¨¶ÂêàÂè∞ÁÅ£Êú¨Âú∞Ë™ûË®ÄËàáÊñáÂåñËÉåÊôØÁöÑÊáâÁî®ÈúÄÊ±Ç„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\nÊú¨Ë≥áÊñôÈõÜÂ∞àÁÇ∫ÁôºÂ±ïÂÖ∑ÂÇôÂº∑Â§ßÊé®ÁêÜËÉΩÂäõÁöÑÁπÅÈ´î‰∏≠ÊñáÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLarge Reasoning Models, LRMÔºâÊâÄË®≠Ë®àÔºåÂÖßÂÆπÊ∑±Â∫¶ÁµêÂêàÂè∞ÁÅ£ÁöÑË™ûË®ÄËàáÊñáÂåñËÑàÁµ°„ÄÇÊØèÁ≠ÜË≥áÊñôÈÄöÂ∏∏ÂåÖÂê´‰ΩøÁî®ËÄÖÁöÑÊèêÂïè„ÄÅÊ®°ÂûãÁöÑÂõûÊáâÔºå‰ª•ÂèäÊ∏ÖÊ•öÁöÑÊé®ÁêÜÈÅéÁ®ã„ÄÇË≥áÊñôÈõÜË®≠Ë®àÁõÆÊ®ôÁÇ∫ÂüπÈ§äÊ®°ÂûãÂÖ∑ÂÇôÈ°û‰∫∫ÈÇèËºØÁöÑÈÄêÊ≠•ÊÄùËÄÉËàáËß£ÈáãËÉΩÂäõ„ÄÇ\nÊ≠§Ë≥áÊñôÈõÜÈÅ©Áî®ÊñºË®ìÁ∑¥ËàáË©ï‰º∞‰ª•‰∏ã‰ªªÂãôÔºö\n\nÂè∞ÁÅ£Á§æÊúÉÁöÑÊó•Â∏∏Êé®ÁêÜ\nÊïôËÇ≤ÊÄßÂ∞çË©±\n‰ª•Ëß£ÈáãÁÇ∫Â∞éÂêëÁöÑÁîüÊàê‰ªªÂãô‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"StructFlowBench","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Jinnan/StructFlowBench","creator_name":"Jinnan Li","creator_url":"https://huggingface.co/Jinnan","description":"\n\t\n\t\t\n\t\tStructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following\n\t\n\n\n  \n    üìÉ Paper\n  \n  ‚Ä¢\n  \n    ü§ó Dataset\n  \n  ‚Ä¢\n  \n    üñ•Ô∏è Code\n  \n\n\n\n\t\n\t\t\n\t\t1. Updates\n\t\n\n\n2025/02/26: We enhanced the code documentation on GitHub with detailed implementation guidelines.\n2025/02/24: We submitted our paper to Hugging Face's Daily Papers.\n2025/02/23: We released StructFlowBench dataset on huggingface.\n2025/02/20: We released the first version of our paper along with the dataset and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jinnan/StructFlowBench.","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Titanium2-DeepSeek-R1","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Click here to support our open-source dataset and model releases!\nTitanium2-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n32.4k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraform‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"express-legal-funding-reviews","keyword":"instruction-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/expresslegalfunding/express-legal-funding-reviews","creator_name":"Express Legal Funding","creator_url":"https://huggingface.co/expresslegalfunding","description":"A curated collection of real customer feedback and company replies for Express Legal Funding.  This dataset is designed for training and evaluating language models on tasks such as sentiment classification,  customer interaction modeling, and instruction tuning in the legal funding domain.\n","first_N":5,"first_N_keywords":["text-classification","text-generation","sentiment-classification","language-modeling","human"],"keywords_longer_than_N":true},
	{"name":"test2027","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zjrwtxtechstudio/test2027","creator_name":"zjrwtx","creator_url":"https://huggingface.co/zjrwtxtechstudio","description":"A dataset containing dialogues between assistant and user with different roles.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-sv","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv","creator_name":"Tim Isbister","creator_url":"https://huggingface.co/timpal0l","description":"\n\t\n\t\t\n\t\tOpenHermes-2.5-sv\n\t\n\nThis is a machine translated instruct dataset from OpenHermes-2.5. \nThe facebook/seamless-m4t-v2-large was used, and some post filtering is done to remove repetitive texts that occurred due to translation errors.\n\n\t\n\t\t\n\t\tExample data:\n\t\n\n[\n   {\n      \"from\":\"human\",\n      \"value\":\"Vilket naturfenomen, som orsakas av att ljus reflekteras och bryts genom vattendroppar, resulterar i en f√§rgglad b√•ge p√• himlen?\",\n      \"weight\":null\n   },\n   {\n      \"from\":\"gpt\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv.","first_N":5,"first_N_keywords":["text-generation","Swedish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"code-act","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xingyaoww/code-act","creator_name":"Xingyao Wang","creator_url":"https://huggingface.co/xingyaoww","description":" Executable Code Actions Elicit Better LLM Agents \n\n\nüíª Code\n‚Ä¢\nüìÉ Paper\n‚Ä¢\nü§ó Data (CodeActInstruct)\n‚Ä¢\nü§ó Model (CodeActAgent-Mistral-7b-v0.1)\n‚Ä¢\nü§ñ Chat with CodeActAgent!\n\n\nWe propose to use executable Python code to consolidate LLM agents‚Äô actions into a unified action space (CodeAct).\nIntegrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations (e.g., code execution results) through multi-turn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xingyaoww/code-act.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v2","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2","creator_name":"Nicholas Kluge Corr√™a","creator_url":"https://huggingface.co/nicholasKluge","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset version 2.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of single-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"instruction-collection-fin","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LumiOpen/instruction-collection-fin","creator_name":"LumiOpen","creator_url":"https://huggingface.co/LumiOpen","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a collection of Finnish instruction data compiled from various sources. Most of the original data is in English and was machine translated into Finnish using Poro-34B. We supplemented this translated data with Finnish paraphrase tasks and English-Finnish translation and language identification tasks. This dataset is suitable for fine-tuning LLMs for instruction-following in Finnish and is usable for commercial purposes.\nWe use this dataset in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LumiOpen/instruction-collection-fin.","first_N":5,"first_N_keywords":["Finnish","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-fr","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TPM-28/alpaca-cleaned-fr","creator_name":"TPM-28","creator_url":"https://huggingface.co/TPM-28","description":"TPM-28/alpaca-cleaned-fr dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_astronomy_bg","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca_astronomy_bg","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"vislupus/alpaca_astronomy_bg dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Bulgarian","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Confession-Subreddit-Top500","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Oguzz07/Confession-Subreddit-Top500","creator_name":"Oƒüuzhan Yƒ±ldƒ±rƒ±m","creator_url":"https://huggingface.co/Oguzz07","description":"This dataset was prepared by taking into account the 500 most popular posts of all time in the confession subreddit and the comments with the most votes on these posts.\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","text","Text"],"keywords_longer_than_N":true},
	{"name":"openbohm","keyword":"multi-turn","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/distantquant/openbohm","creator_name":"Distant Quant","creator_url":"https://huggingface.co/distantquant","description":"\n\t\n\t\t\n\t\tOpenBohm\n\t\n\nThis dataset is an experimental conjugation of philosophical multi-turn long-form conversations from J. Krishnamurti, and D. Bohm, added to long-conversation filtered (count > 6) Capybara data, edited to be slightly less apologetic.\nRemoved references to names and locations where possible. Some conversations have been paraphrased somewhat to follow QA format better, however they keep the key content of the original.\n\n","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"CIDAR","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arbml/CIDAR","creator_name":"Arabic Machine Learning ","creator_url":"https://huggingface.co/arbml","description":"\n\t\n\t\t\n\t\tDataset Card for \"CIDAR\"\n\t\n\n\n\t\n\t\t\n\t\tüå¥CIDAR: Culturally Relevant Instruction Dataset For Arabic\n\t\n\n\n\n   [ Paper - GitHub ]\n\n\n\nCIDAR contains 10,000 instructions and their output. The dataset was created by selecting around 9,109 samples from Alpagasus dataset then translating it to Arabic using ChatGPT. In addition, we append that with around 891 Arabic grammar instructions from the webiste Ask the teacher. All the 10,000 samples were reviewed by around 12 reviewers. \n\n\n\n\n\n\n\t\n\t\t\n\t\tüìö‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arbml/CIDAR.","first_N":5,"first_N_keywords":["text-generation","Arabic","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"AWE","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rishiraj/AWE","creator_name":"Rishiraj Acharya","creator_url":"https://huggingface.co/rishiraj","description":"rishiraj/AWE dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"dataset-portuguese-aira-v2-Gemma-format","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format","creator_name":"EDDY GIUSEPE CHIRINOS ISIDRO, PhD","creator_url":"https://huggingface.co/EddyGiusepe","description":"Dataset Aira para o formato do Modelo Gemma \n\n\n\t\n\t\t\n\t\tResumo do Dataset\n\t\n\nEste conjunto de dados cont√©m uma cole√ß√£o de conversas individuais entre um assistente e um usu√°rio.\nAs conversas foram geradas pelas intera√ß√µes do usu√°rio com modelos j√° ajustados (ChatGPT, LLama 2, Open-Assistant, etc).\nO conjunto de dados est√° dispon√≠vel em portugu√™s (tem a vers√£o em Ingl√™s que ainda n√£o tratei). Mas voc√™ pode baixar do \nreposit√≥rio de Nicholas Kluge Corr√™a tanto a vers√£o em Portugu√™s e \na vers√£o em‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format.","first_N":5,"first_N_keywords":["question-answering","Portuguese","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-chatml","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pacozaa/alpaca-cleaned-chatml","creator_name":"Sarin Suriyakoon","creator_url":"https://huggingface.co/pacozaa","description":"\n\t\n\t\t\n\t\tChatML Reformat of yahma/alpaca-cleaned\n\t\n\nI'd like to try instruction-tuning dataset with chat-tuning format.\n\n\t\n\t\t\n\t\tUsage\n\t\n\ndataset = load_dataset(\"pacozaa/alpaca-cleaned-chatml\", split = \"train\")\nprint(dataset[0][\"text\"])\n\n","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"medical","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bohu/medical","creator_name":"tang","creator_url":"https://huggingface.co/bohu","description":"From https://huggingface.co/datasets/shibing624/medical\n","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"pandora-instruct","keyword":"instruct","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-instruct","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora Instruct\n\t\n\nAn instruction dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the existing datasets:\n\nteknium/openhermes\nise-uiuc/magicoder-evol-instruct-110k\nise-uiuc/magicoder-oss-instruct-75k\n\n\n\t\n\t\t\n\t\n\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"InstructTranslation-EN-ES","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Iker/InstructTranslation-EN-ES","creator_name":"Iker Garc√≠a-Ferrero","creator_url":"https://huggingface.co/Iker","description":"\n\t\n\t\t\n\t\tTranslation of Instructions EN-ES\n\t\n\nThis dataset contains prompts and answers from teknium/OpenHermes-2.5 translated to Spanish using GPT-4-0125-preview. The dataset is intended to be used for training a model to translate instructions from English to Spanish.\nThe dataset is formatted with the TowerInstruct format. It is ready to finetune a Tower translation model. if you want the raw translations, there are available here:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Iker/InstructTranslation-EN-ES.","first_N":5,"first_N_keywords":["translation","text-generation","English","Spanish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MM-Instruct","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jjjjh/MM-Instruct","creator_name":"LIU Jihao","creator_url":"https://huggingface.co/jjjjh","description":"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMM-Instruct is a large-scale dataset of diverse and high-quality visual instruction-answer pairs designed to enhance the instruction-following capabilities of large multimodal models (LMMs) in real-world use cases. It goes beyond simple question-answering or image-captioning by incorporating a wide range of instructions, including creative writing, summarization, and image analysis, pushing LMMs to better understand and respond to nuanced user requests.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jjjjh/MM-Instruct.","first_N":5,"first_N_keywords":["visual-question-answering","cc-by-4.0","100K - 1M","json","Image"],"keywords_longer_than_N":true},
	{"name":"multi-agent-scam-conversation","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\n\t\n\t\t\n\t\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset with Agentic Personalities\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset with Agentic Personalities is an enhanced collection of simulated phone conversations between two AI agents, one acting as a scammer or non-scammer and the other as an innocent receiver. Each dialogue is labeled as either a scam or non-scam interaction. This dataset is designed to help develop‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/OpenHermes-2.5-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\td0rj/OpenHermes-2.5-ru\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is translated version of teknium/OpenHermes-2.5 into Russian using Google Translate.\n","first_N":5,"first_N_keywords":["text-generation","question-answering","translated","monolingual","teknium/OpenHermes-2.5"],"keywords_longer_than_N":true},
	{"name":"Multilingual-Benchmark","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TaiMingLu/Multilingual-Benchmark","creator_name":"TaiMing","creator_url":"https://huggingface.co/TaiMingLu","description":"These are the GSM8K and ARC dataset translated by Google Translate. \nBibTex\n@misc{lu2024languagecountslearnunlearn,\n      title={Every Language Counts: Learn and Unlearn in Multilingual LLMs}, \n      author={Taiming Lu and Philipp Koehn},\n      year={2024},\n      eprint={2406.13748},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2406.13748}, \n}\n\n","first_N":5,"first_N_keywords":["zero-shot-classification","question-answering","translation","English","German"],"keywords_longer_than_N":true},
	{"name":"gsm8k_multiturn","keyword":"multiturn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/euclaise/gsm8k_multiturn","creator_name":"Jade","creator_url":"https://huggingface.co/euclaise","description":"The \"socratic\" version of GSM8K has the model reflect and ask itself sub-questions about the initial question, before coming to a final answer.\nThis dataset reformats the socratic GSM8K version into a multi-turn conversation, where the sub-questions are asked by the user rather than being self-asked by the model.\n","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ru-instruct","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\t–ö–∞—Ä—Ç–æ—á–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n\t\n\n–°–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞ (—Å–ø–∞—Å–∏–±–æ –º–æ–¥–µ–ª–∏ Den4ikAI/nonsense_gibberish_detector). –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω SimHash'–æ–º.\n–û–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –Ω—ë–º –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞ –Ω–µ –∑–∞–≤—ë–∑, in progress.\n\n\t\n\t\t\n\t\t–°–æ—Å—Ç–∞–≤\n\t\n\n–°–æ–±—Ä–∞–ª –∏–∑ —ç—Ç–∏—Ö –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö:\n\nd0rj/OpenOrca-ru (–æ—Ç Open-Orca/OpenOrca)\nd0rj/OpenHermes-2.5-ru (–æ—Ç teknium/OpenHermes-2.5)\nd0rj/dolphin-ru (–æ—Ç ehartford/dolphin)\nd0rj/alpaca-cleaned-ru (–æ—Ç‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct.","first_N":5,"first_N_keywords":["text-generation","machine-generated","found","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"meme_dataset","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noxus09/meme_dataset","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","description":"Noxus09/meme_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MMC","keyword":"instruction","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xywang1/MMC","creator_name":"Xiaoyang Wang","creator_url":"https://huggingface.co/xywang1","description":"\n\t\n\t\t\n\t\tMMC: Advancing Multimodal Chart Understanding with LLM Instruction Tuning\n\t\n\nThis repo releases data introduced in our paper MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning.\n\nThe paper was published in NAACL 2024.\nSee our GithHub repo for demo code and more.\n\n\n\t\n\t\t\n\t\n\t\n\t\tHighlights\n\t\n\n\nWe introduce a large-scale MultiModal Chart Instruction (MMC-Instruction) dataset supporting diverse tasks and chart types. Leveraging this data.\nWe also propose a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xywang1/MMC.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-sa-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"guanjian_anli_test1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/papaya523/guanjian_anli_test1","creator_name":"zhaoyue","creator_url":"https://huggingface.co/papaya523","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/papaya523/guanjian_anli_test1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"PromisedChat_Instruction","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/netmouse/PromisedChat_Instruction","creator_name":"Matt Yeh","creator_url":"https://huggingface.co/netmouse","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/netmouse/PromisedChat_Instruction.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"WangchanThaiInstruct_Multi-turn_Conversation_Dataset","keyword":"instruction-finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThaiSyntheticQA/WangchanThaiInstruct_Multi-turn_Conversation_Dataset","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","description":"\n\t\n\t\t\n\t\tWangchanThaiInstruct Multi-turn Conversation Dataset\n\t\n\nWe create a Thai multi-turn conversation dataset from airesearch/WangchanThaiInstruct (Batch 1) by LLM. It was created from synthetic method using open source LLM in Thai language.\n\n\t\n\t\t\n\t\tCitation\n\t\n\n\nThammaleelakul, S., & Phatthiyaphaibun, W. (2024). WangchanThaiInstruct Multi-turn Conversation Dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.13132633\n\nor BibTeX\n@dataset{thammaleelakul_2024_13132633,\n  author       =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/WangchanThaiInstruct_Multi-turn_Conversation_Dataset.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Feedback-Collection-ru","keyword":"instruct","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Feedback-Collection-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tFeedback-Collection-ru\n\t\n\nThis is russian version of prometheus-eval/Feedback-Collection translated using Google Translate.\n","first_N":5,"first_N_keywords":["text-generation","text-classification","translated","prometheus-eval/Feedback-Collection","Russian"],"keywords_longer_than_N":true},
	{"name":"shibing624_alpaca-zh","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/shibing624_alpaca-zh","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\nÊú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/botp/shibing624_alpaca-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"bioinstruct","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bio-nlp-umass/bioinstruct","creator_name":"UMass BioNLP Lab","creator_url":"https://huggingface.co/bio-nlp-umass","description":"\n\t\n\t\t\n\t\tDataset Card for BioInstruct\n\t\n\nGitHub repo: https://github.com/bio-nlp/BioInstruct\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBioInstruct is a dataset of 25k instructions and demonstrations generated by OpenAI's GPT-4 engine in July 2023. \nThis instruction data can be used to conduct instruction-tuning for language models (e.g. Llama) and make the language model follow biomedical instruction better. \nImprovements of Llama on 9 common BioMedical tasks are shown in the result section. \nTaking‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bio-nlp-umass/bioinstruct.","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"halfTurkish","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sasanikrali/halfTurkish","creator_name":"sasani krali","creator_url":"https://huggingface.co/sasanikrali","description":"\n\t\n\t\t\n\t\tDataset Card for HalfTurkish\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the given‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sasanikrali/halfTurkish.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"VIS","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/weiwei888/VIS","creator_name":"shiweiwei","creator_url":"https://huggingface.co/weiwei888","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weiwei888/VIS.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"wangwei","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guilty1987/wangwei","creator_name":"FAN","creator_url":"https://huggingface.co/guilty1987","description":"guilty1987/wangwei dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"math-gpt-4o-200k-ko","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/math-gpt-4o-200k-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"Translated PawanKrd/math-gpt-4o-200k using nayohan/llama3-instrucTrans-enko-8b.\nThis dataset is a raw translated dataset and contains repetitive sentences generated by the model, so it needs to be filtered.\n","first_N":5,"first_N_keywords":["text-generation","Korean","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-gpt4-turbo","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo","creator_name":"myles bruce","creator_url":"https://huggingface.co/mylesgoose","description":"I downloaded the dataset from Alpaca at https://huggingface.co/datasets/yahma/alpaca-cleaned and processed it using a script to convert the text to hashes. I removed any duplicate hashes along with their corresponding input and output columns. Subsequently, I utilized the GPT-4 Turbo API to feed each message, instruction, and input to the model for generating responses.\nHere are the outputs generated by the GPT-4 Turbo model. The information in the input column and instruction column should be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"AB1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MagedGaman/AB1","creator_name":"Maged Gaman","creator_url":"https://huggingface.co/MagedGaman","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MagedGaman/AB1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"TextBooksPersonaHub-FR","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/drodin/TextBooksPersonaHub-FR","creator_name":"nacer","creator_url":"https://huggingface.co/drodin","description":"\n\t\n\t\t\n\t\tTextBooksPersonaHub\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe TextBooksPersonaHub dataset is an extension of the proj-persona/PersonaHub dataset, created using the technique described in the paper Textbooks Are All You Need II. This dataset contains synthetically generated \"textbook-like\" passages tailored in french to specific personas, aimed at enhancing language model training with high-quality and diverse content.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nThe original personas‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/drodin/TextBooksPersonaHub-FR.","first_N":5,"first_N_keywords":["French","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"walt","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/canTooDdev/walt","creator_name":"Boris Marion-Dorier","creator_url":"https://huggingface.co/canTooDdev","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/canTooDdev/walt.","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kushala/alpaca","creator_name":"Mummigatti","creator_url":"https://huggingface.co/Kushala","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kushala/alpaca.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned","creator_name":"Cristian Velasquez (Entreprenerdly.com)","creator_url":"https://huggingface.co/Entreprenerdly","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"LLama3Flashcard","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Michito97/LLama3Flashcard","creator_name":"Adrian De La Cruz","creator_url":"https://huggingface.co/Michito97","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nEste dataset contiene pares de instrucciones, entradas y salidas dise√±adas para generar tarjetas de memoria (flashcards) educativas. Es ideal para modelos de generaci√≥n de texto enfocados en la creaci√≥n de contenidos educativos.\n\n\t\n\t\t\n\t\tColumnas\n\t\n\n\ninstruction: La instrucci√≥n dada al‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Michito97/LLama3Flashcard.","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Buzz-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Test2","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WhiteHunter111/Test2","creator_name":"Thomas Flato","creator_url":"https://huggingface.co/WhiteHunter111","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WhiteHunter111/Test2.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"scam-dialogue","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/scam-dialogue","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\n\t\n\t\t\n\t\tSynthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset is a collection of simulated phone conversation between two parties, labeled as either scam or non-scam interactions. The dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of three columns:\n\ndialogue: The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/scam-dialogue.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"RLSTACK","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK.","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Select-Stack","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"unstacked","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","arxiv:2403.08763","arxiv:2310.05914"],"keywords_longer_than_N":true},
	{"name":"Colossal-Instruction-Translation-EN-ES","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Iker/Colossal-Instruction-Translation-EN-ES","creator_name":"Iker Garc√≠a-Ferrero","creator_url":"https://huggingface.co/Iker","description":"\n\t\n\t\t\n\t\tColossal Instruction Translation Corpus (English - Spanish )\n\t\n\n\nA deduplicated version of this dataset can be found here, thanks to @NickyNicky: https://huggingface.co/datasets/NickyNicky/Iker-Colossal-Instruction-Translation-EN-ES_deduplicated\n\nThis dataset contains 2284632 instructions and answers translated from English into Spanish. Is a fully synthetic corpus generated using machine translation. We used the model Iker/TowerInstruct-13B-v0.1-EN2ES. A few examples were also‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Iker/Colossal-Instruction-Translation-EN-ES.","first_N":5,"first_N_keywords":["translation","Spanish","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"CTI_0.1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AshishCTI/CTI_0.1","creator_name":"ad","creator_url":"https://huggingface.co/AshishCTI","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AshishCTI/CTI_0.1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","üá∫üá∏ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"single-agent-scam-conversations","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\n\t\n\t\t\n\t\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of three columns:\n\ndialogue: The transcribed conversation between the caller and receiver.\ntype: The specific type of scam or non-scam interaction.\nlabels: A binary label indicating whether the conversation is a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"wangchanx-seed-free-synthetic-instruct-thai-120k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k","creator_name":"VISTEC-depa AI Research Institute of Thailand","creator_url":"https://huggingface.co/airesearch","description":"\n\t\n\t\t\n\t\tDataset Card for WangchanX Seed-Free Synthetic Instruct Thai 120k\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains about 120k synthetic instruction-following samples in Thai, generated using a novel seed-free approach. It covers a wide range of domains derived from Wikipedia, including both general knowledge and Thai-specific cultural topics. The dataset is designed for instruction-tuning Thai language models to improve their ability to understand and generate Thai text in various‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k.","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","Thai","English"],"keywords_longer_than_N":true},
	{"name":"TextBooksPersonaHub","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/drodin/TextBooksPersonaHub","creator_name":"nacer","creator_url":"https://huggingface.co/drodin","description":"\n\t\n\t\t\n\t\tTextBooksPersonaHub\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe TextBooksPersonaHub dataset is an extension of the proj-persona/PersonaHub dataset, created using the technique described in the paper Textbooks Are All You Need II. This dataset contains synthetically generated \"textbook-like\" passages tailored in french to specific personas, aimed at enhancing language model training with high-quality and diverse content.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nThe original personas‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/drodin/TextBooksPersonaHub.","first_N":5,"first_N_keywords":["French","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"ultrafrench","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/ultrafrench","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for ultrafrench\n\t\n\nThis dataset offers a french translation of the small sample of instructions from HuggingFaceH4/ultrachat_200k translated in french. The generations were made with Mistral large to make the output open-source\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"alpaca_data_clean","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aaaalon/alpaca_data_clean","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca_data_clean.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","üá∫üá∏ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"llama3weitiao","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZHEZIXI/llama3weitiao","creator_name":"LIUJUN","creator_url":"https://huggingface.co/ZHEZIXI","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZHEZIXI/llama3weitiao.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleand","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aaaalon/alpaca-cleand","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca-cleand.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"MainData","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bufanlin/MainData","creator_name":"bufanlin","creator_url":"https://huggingface.co/bufanlin","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\nÊú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bufanlin/MainData.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_cthulhu_full","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theprint/alpaca_cthulhu_full","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","description":"Based on the yahma/alpaca-cleaned data set.\nEvery answer in the original data set was rewritten, as if the answer was given by a dedicated, high ranking cultist in the Cult of Cthulhu. The instructions were to keep the replies factually the same, but to spice things up with references to the Cthulhu Mythos in its various forms.\nThis was done as part of a learning experience, but I am making the data set available for others to play with as well.\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ThaiQA-v1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","description":"\n\t\n\t\t\n\t\tThaiQA v1\n\t\n\nThaiQA v1 is a Thai Synthetic QA dataset. It was created from synthetic method using open source LLM in Thai language.\nWe used Nvidia Nemotron 4 (340B) to create this dataset.\nTopics:\nTechnology and Gadgets 100\nTravel and Tourism 91\nFood and Cooking 99\nSports and Fitness 50\nArts and Entertainment 24\nHome and Garden 72\nFashion and Beauty 99\nScience and Nature 100\nHistory and Culture 91\nEducation and Learning 99\nPets and Animals 83\nRelationships and Family 78\nPersonal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1.","first_N":5,"first_N_keywords":["text-generation","question-answering","Thai","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Kalo-Opus-Instruct-22k-Refusal-Murdered","keyword":"instruct","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewEden/Kalo-Opus-Instruct-22k-Refusal-Murdered","creator_name":"New Eden","creator_url":"https://huggingface.co/NewEden","description":"NewEden/Kalo-Opus-Instruct-22k-Refusal-Murdered dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["agpl-3.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"code-translation","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noxus09/code-translation","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Noxus09/code-translation.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"CRAFT-Summarization","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tCRAFT-Summarization\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated summarization data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization.","first_N":5,"first_N_keywords":["summarization","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CRAFT-RecipeGen","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tCRAFT-RecipeGen\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail.\n\n4 synthetic dataset sizes (S, M, L, XL) are available.\nCompared to other synthetically generated datasets with the CRAFT framework, this task did not scale similarly well and we do not match the performance of general‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Proyecto","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto","creator_name":"Facundo","creator_url":"https://huggingface.co/Facundo-DiazPWT","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","üá∫üá∏ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"multiturn-capybara-preferences-filtered-binarized","keyword":"multiturn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"This dataset has been created with distilabel, plus some extra post-processing steps described below.\nDataset Summary\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\n\nRemove responses from the assistant, not only in the last turn, but also in intermediate turns where the assistant responds with a potential refusal and/or some ChatGPT-isms such as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ictisgpt","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alexbs/ictisgpt","creator_name":"Alex B","creator_url":"https://huggingface.co/alexbs","description":"\n\t\n\t\t\n\t\tDataset Card for ICTIS GPT dataset\n\t\n\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Hydrus-Claude-Instruct-5K","keyword":"instruct","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Delta-Vector/Hydrus-Claude-Instruct-5K","creator_name":"Mango","creator_url":"https://huggingface.co/Delta-Vector","description":"\n\t\n\t\t\n\t\tkalo made dis\n\t\n\nThanks to Kubernetes bad for filtering + converting this to sharegpt\nMix of Opus and 3.5 for data\nThis is a combined set of \nuncurated-raw-gens-og-test-filtered\nuncurated-raw-gens-opus-jul-31-filtered\nopus_jul10_test-filtered\nuncurated_opus_jul8-filtered \n","first_N":5,"first_N_keywords":["English","agpl-3.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"multi-turn_jailbreak_attack_datasets","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tom-gibbs/multi-turn_jailbreak_attack_datasets","creator_name":"Tom Gibbs","creator_url":"https://huggingface.co/tom-gibbs","description":"\n\t\n\t\t\n\t\tMulti-Turn Jailbreak Attack Datasets\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset was created to compare single-turn and multi-turn jailbreak attacks on large language models (LLMs). The primary goal is to take a single harmful prompt and distribute the harm over multiple turns, making each prompt appear harmless in isolation. This approach is compared against traditional single-turn attacks with the complete prompt to understand their relative impacts and failure modes. The key feature of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tom-gibbs/multi-turn_jailbreak_attack_datasets.","first_N":5,"first_N_keywords":["English","mit","1K<n<10K","arxiv:2409.00137","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"CleverBoi","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theprint/CleverBoi","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","description":"\n\n\n\t\n\t\t\n\t\tCleverBoi\n\t\n\nThe CleverBoi Collection is based on a number of data sets that emphasize logic, inference, empathy, math and coding.\nThe data set has been formatted to follow the alpaca format (instruction + input -> output) when fine tuning.\n\n\t\n\t\t\n\t\tSource Data Sets\n\t\n\nThe source data sets used in the CleverBoi Collection are listed below, ordered by size.\n\nKK04/LogicInference_OA\nmlabonne/Evol-Instruct-Python-26k\ngarage-bAInd/Open-Platypus\niamtarun/python_code_instructions_18k_alpaca‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/theprint/CleverBoi.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Supernova","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Supernova","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Supernova is a dataset containing general synthetic chat data from the best available open-source models.\nThe 2024-09-27 version contains:\n\n178.2k rows of synthetic chat responses generated using Llama 3.1 405b Instruct.\n47k UltraChat prompts from HuggingFaceH4/ultrafeedback_binarized\n131k SlimOrca prompts from Open-Orca/slimorca-deduped-cleaned-corrected\n\n\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"chat","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neverland-th/chat","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","description":"neverland-th/chat dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_1_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_1_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_1_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_1_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_2_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_2_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_2_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_2_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_3_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_3_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_3_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_3_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_2_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_2_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_2_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_2_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_4_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_4_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_4_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_4_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_7_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_7_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_7_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_7_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_9_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_9_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_9_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_9_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_16_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_16_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_16_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_16_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_2_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_2_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_2_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_2_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_7_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_7_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_7_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_7_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_11_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_11_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_11_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_11_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_16_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_16_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_16_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_16_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"UltraChat1_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/UltraChat1_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/UltraChat1_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue\n\nTotal examples: 95803\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/UltraChat1_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_CalligraphyFont_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyFont_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_CalligraphyFont_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_CalligraphyFont_en\", split=\"train\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyFont_en.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_CalligraphyFont_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyFont_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_CalligraphyFont_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_CalligraphyFont_ar\", split=\"train\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyFont_ar.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_1_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_1_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_1_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_1_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_2_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_2_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_2_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_2_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_4_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_4_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_4_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_4_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_3_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_3_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_3_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_3_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_2_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_2_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_2_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_2_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_3_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_3_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_3_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_3_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_5_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_5_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_5_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_5_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_7_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_7_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_7_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_7_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_5_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_5_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_5_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_5_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_14_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_14_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_14_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_14_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_4_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_4_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_4_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_4_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_5_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_5_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_5_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_5_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_6_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_6_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_6_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_6_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_8_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_8_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_8_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_8_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_12_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_12_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_12_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_12_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_14_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_14_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_14_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_14_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_15_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_15_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_15_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_15_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"UltraChat2_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/UltraChat2_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/UltraChat2_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue\n\nTotal examples: 103933\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/UltraChat2_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ultrachat","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/metythorn/ultrachat","creator_name":"metythorn penn","creator_url":"https://huggingface.co/metythorn","description":"\n\t\n\t\t\n\t\tUltraChat Conversations Dataset\n\t\n\nThis dataset contains 1,468,346 multi-turn conversations from UltraChat, processed to preserve the original conversational structure and optimized for training conversational AI models.\n\n\t\n\t\t\n\t\tüéØ Dataset Format\n\t\n\nEach conversation record contains:\n\nid: Sequential conversation ID (1, 2, 3, ...)\nsource: \"ultra\" \nlanguage: \"english\"\ndata: JSON string containing conversation turns array\n\n\n\t\n\t\t\n\t\tüìä Dataset Statistics\n\t\n\n\nTotal Conversations: 1,468,346‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/metythorn/ultrachat.","first_N":5,"first_N_keywords":["question-answering","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"reflection-v1-ru_subset","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\td0rj/reflection-v1-ru_subset\n\t\n\nTranslated glaiveai/reflection-v1 dataset into Russian language using GPT-4o.\n\nAlmost all the rows of the dataset have been translated. I have removed those translations that do not match the original by the presence of the tags \"thinking\", \"reflection\" and \"output\". Mapping to the original dataset rows can be taken from the \"index\" column.\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata = datasets.load_dataset(\"d0rj/reflection-v1-ru_subset\")\nprint(data)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset.","first_N":5,"first_N_keywords":["text-generation","translated","multilingual","glaiveai/reflection-v1","Russian"],"keywords_longer_than_N":true},
	{"name":"scam_dialogues","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/adamtc/scam_dialogues","creator_name":"Adam Unknown","creator_url":"https://huggingface.co/adamtc","description":"adamtc/scam_dialogues dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","Vietnamese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","keyword":"multi-turn","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","description":"\n\t\n\t\t\n\t\tSWE-Bench Verified O1 Dataset\n\t\n\n\n\t\n\t\t\n\t\tExecutive Summary\n\t\n\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities using their native tool calling capabilities on the SWE-Bench Verified dataset, achieving a 45.8% success rate across 500 test instances.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset was generated using the CodeAct framework, which aims to improve code‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-indonesian","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian","creator_name":"Ilham Fadhil","creator_url":"https://huggingface.co/ilhamfadheel","description":"\n\t\n\t\t\n\t\tü¶ôüõÅ Cleaned Alpaca Dataset (INDONESIAN)\n\t\n\nWelcome to the Cleaned Alpaca Dataset repository! This repository hosts a cleaned and curated version of a dataset used to train the Alpaca LLM (Large Language Model). The original dataset had several issues that are addressed in this cleaned version.\nOn April 8, 2023 the remaining uncurated instructions (~50,000) were replaced with data from the GPT-4-LLM dataset. Curation of the incoming GPT-4 data is ongoing.\n\nA 7b Lora model (trained on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian.","first_N":5,"first_N_keywords":["text-generation","Indonesian","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"refactorchat","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BradMcDanel/refactorchat","creator_name":"Bradley McDanel","creator_url":"https://huggingface.co/BradMcDanel","description":"\n\t\n\t\t\n\t\tModel Card\n\t\n\n\n\t\n\t\t\n\t\tModel Details\n\t\n\n\nDataset Name: RefactorChat\nVersion: 1.0\nDate: October 19, 2024\nType: Multi-turn dialogue dataset for code refactoring and feature addition\n\n\n\t\n\t\t\n\t\tIntended Use\n\t\n\n\nPrimary Use: Evaluating and training large language models on incremental code development tasks\nIntended Users: Researchers and practitioners in natural language processing and software engineering\n\n\n\t\n\t\t\n\t\tDataset Composition\n\t\n\n\nSize: 100 samples\nStructure: Each sample consists of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BradMcDanel/refactorchat.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nüåê Homepage | Code | ü§ó Paper | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"pino","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sabux/pino","creator_name":"Hugo Rodriguez","creator_url":"https://huggingface.co/sabux","description":"\n\t\n\t\t\n\t\tDataset PUCMM - Abby\n\t\n\nEste dataset contiene ejemplos cuidadosamente dise√±ados de instrucciones y respuestas relacionadas exclusivamente con la Pontificia Universidad Cat√≥lica Madre y Maestra (PUCMM). Ha sido creado con el objetivo de entrenar a Abby, un chatbot institucional capaz de responder preguntas frecuentes de estudiantes, profesores y visitantes, abarcando informaci√≥n sobre admisiones, historia, campus, programas acad√©micos y m√°s.\n\n\t\n\t\t\n\t\n\t\n\t\tüß† Descripci√≥n\n\t\n\nEste dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sabux/pino.","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"multiturn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\nThis dataset focuses on challenging multi-turn conversations and contains:\n\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\nThis dataset focuses on challenging multi-turn conversations and contains:\n\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"college_alpaca_dataset","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dapraws/college_alpaca_dataset","creator_name":"Muhammad Darrel Prawira","creator_url":"https://huggingface.co/dapraws","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the given article in 200 Words.\",\n\"input\": \"https://www.bbc.com/news/world-51461830\",\n\"output\": \"The recent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dapraws/college_alpaca_dataset.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"amazon-ml","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vinuuu/amazon-ml","creator_name":"Vinayak Goyal","creator_url":"https://huggingface.co/Vinuuu","description":"Vinuuu/amazon-ml dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Data-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-Data-ru\n\t\n\nTranslated lmms-lab/LLaVA-OneVision-Data dataset into Russian language using Google translate.\n\nAlmost all datasets have been translated, except for the following:\n[\"tallyqa(cauldron,llava_format)\", \"clevr(cauldron,llava_format)\", \"VisualWebInstruct(filtered)\", \"figureqa(cauldron,llava_format)\", \"magpie_pro(l3_80b_mt)\", \"magpie_pro(qwen2_72b_st)\", \"rendered_text(cauldron)\", \"ureader_ie\"]\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru.","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-to-text","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"SkunkworksAI-reasoning-0.01-ko","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/youjunhyeok/SkunkworksAI-reasoning-0.01-ko","creator_name":"Ïú†Ï§ÄÌòÅ","creator_url":"https://huggingface.co/youjunhyeok","description":"SkunkworksAI/reasoning-0.01 Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ nayohan/llama3-instrucTrans-enko-8b Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìï¥ Î≤àÏó≠ÌñàÏäµÎãàÎã§.\nThanks for SkunkworksAI and nayohan.\n\n\n\t\n\t\t\n\t\tÏõêÎ≥∏\n\t\n\n\n\t\n\t\t\n\t\treasoning-0.01 subset\n\t\n\nsynthetic dataset of reasoning chains for a wide variety of tasks.\nwe leverage data like this across multiple reasoning experiments/projects.\nstay tuned for reasoning models and more data.\nThanks to Hive Digital Technologies (https://x.com/HIVEDigitalTech) for their compute support in this project and beyond.\n","first_N":5,"first_N_keywords":["text-generation","Korean","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"instruction-tags","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2-adapt-dev/instruction-tags","creator_name":"AI2 Adapt Dev","creator_url":"https://huggingface.co/ai2-adapt-dev","description":"ai2-adapt-dev/instruction-tags dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"alpha","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dawsondavis/alpha","creator_name":"Dawson Davis","creator_url":"https://huggingface.co/dawsondavis","description":"dawsondavis/alpha dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"mauxitalk-persian","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxitalk-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüó£Ô∏è MauxiTalk: High-Quality Persian Conversations Dataset üáÆüá∑\n\t\n\n\n\t\n\t\t\n\t\tüìù Description\n\t\n\nMauxiTalk is a high-quality dataset of 2,000+ Persian conversations, carefully translated from the SmolTalk dataset using state-of-the-art language models. This dataset is specifically curated for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n2,000 natural conversations in Persian\nDiverse topics including daily‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxitalk-persian.","first_N":5,"first_N_keywords":["text-generation","summarization","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-72k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","first_N":5,"first_N_keywords":["question-answering","summarization","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-72k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","first_N":5,"first_N_keywords":["question-answering","summarization","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-12k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","first_N":5,"first_N_keywords":["question-answering","summarization","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-12k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","first_N":5,"first_N_keywords":["question-answering","summarization","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_anatomist","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Anatomist\n\t\n\nThis dataset was extracted from the Web Ontology Language (OWL) \nrepresentation of the Foundational Model of Anatomy [1] using \nOwlready2 to facilitate the extraction of the logical axioms in the FMA\ninto a Controlled Natural Language (CNL) for use in training Medical Large Language Models to be conversant in \ncanonical human anatomy.\nSo, Description Logic expressions such as the following in FMA about fma13245 (Surface of left ventricle):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist.","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_clinical_coding","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Clinical Coding\n\t\n\nThis dataset was created from a motivation to train Medical Large Language Models for improved fluency in clinical \ncoding, as measurable by MedConceptsQA, an open-source medical coding \nevaluation benchmark designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts.  It was extracted from the Centers for Medicare & Medicaid Services' \nInternational Classification of Diseases, Tenth Revision, Clinical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding.","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AlpacaX-Cleaned","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SullyGreene/AlpacaX-Cleaned","creator_name":"SullyGreene","creator_url":"https://huggingface.co/SullyGreene","description":"\n  \n\n\n\n\t\n\t\t\n\t\tüìö AlpacaX Dataset Documentation\n\t\n\nThe AlpacaX dataset is crafted to enhance AI models with structured, contextually rich, and logically sequenced examples. Designed for integration with TinyAGI, AlpacaX employs an advanced variant of the Alpaca training methodology, making it ideal for models that require detailed instruction-following and multi-step reasoning. This dataset is well-suited for fine-tuning language models to handle complex tasks with clarity and structured‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SullyGreene/AlpacaX-Cleaned.","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"dali","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liwei1987cn/dali","creator_name":"Levi li","creator_url":"https://huggingface.co/liwei1987cn","description":"\n\t\n\t\t\n\t\tÂ§ßÊùéËÄÅÂ∏àÈóÆÁ≠îÊï∞ÊçÆÈõÜ\n\t\n\nËøô‰∏™Êï∞ÊçÆÈõÜÂåÖÂê´Â§ßÊùéËÄÅÂ∏àÁöÑÈóÆÁ≠îÂØπËØù,Áî®‰∫éËÆ≠ÁªÉÂØπËØùÊ®°Âûã„ÄÇ\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜÊèèËø∞\n\t\n\n\nÊ†ºÂºè: JSONL\nÂ≠óÊÆµ: \ninstruction: Âõ∫ÂÆöÂÄº\"ËØ∑Â§ßÊùéËÄÅÂ∏àÂõûÁ≠î\"\ninput: ÊèêÈóÆÂÜÖÂÆπ \noutput: Â§ßÊùéËÄÅÂ∏àÁöÑÂõûÁ≠î\n\n\nÊï∞ÊçÆÈáè: xxxÊù°ÂØπËØùÊï∞ÊçÆ\n\n\n\t\n\t\t\n\t\t‰ΩøÁî®Á§∫‰æã\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"your-username/dataset-name\")\n\n\n\t\n\t\t\n\t\tËÆ∏ÂèØËØÅ\n\t\n\nApache 2.0\n","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Titanium","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Titanium is a dataset containing DevOps-instruct data.\nThe 2024-10-02 version contains:\n\n26.6k rows of synthetic DevOps-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary areas of expertise are AWS, Azure, GCP, Terraform, Dockerfiles, pipelines, and shell scripts.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"glaiveai-reflection-v1-ko","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/youjunhyeok/glaiveai-reflection-v1-ko","creator_name":"Ïú†Ï§ÄÌòÅ","creator_url":"https://huggingface.co/youjunhyeok","description":"Translated glaiveai/reflection-v1 using nayohan/llama3-instrucTrans-enko-8b.\nFor this dataset, we only used data that is 5000 characters or less in length and has language of English.\nThanks for @Magpie-Align and @nayohan.\n","first_N":5,"first_N_keywords":["text-generation","Korean","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Creative_Writing_Multiturn","keyword":"multiturn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Dampfinchen/Creative_Writing_Multiturn","creator_name":"D√§mpfchen","creator_url":"https://huggingface.co/Dampfinchen","description":"This is a dataset merge of many, many high quality story writing / roleplaying datasets across all of Huggingface. I've filtered specifically for samples with high turns, which is a key different to already available datasets. My goal is to improve the model's ability to recollect and mention details from far back even at a longer context and more importantly, also improve the model's ability to output engaging verbose storylines, reduce certain phrases, increase creativity and reduce dry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dampfinchen/Creative_Writing_Multiturn.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"1M-OpenOrca_be","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be","creator_name":"Artsem Holub","creator_url":"https://huggingface.co/WiNE-iNEFF","description":"En/Be\nüêã The Belarusian OpenOrca Dataset! üêã\n\n\n\nBelarusian OpenOrca dataset - is rich collection of augmented FLAN data aligns, that translated in belarusian language.\nThat dataset should help training LLM in belarusian language and should help on other NLP tasks.\nThis dataset have 2 version:\n\n~1M GPT-4 completions (Now translating)\n~3.2M GPT-3.5 completions (Can be translated in future)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe fields are:\n\n'id', a unique numbered identifier which includes one of 'niv'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Spurline","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\nThe 2024-10-30 version contains:\n\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-36k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","first_N":5,"first_N_keywords":["question-answering","summarization","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-36k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","first_N":5,"first_N_keywords":["question-answering","summarization","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"lots_of_datasets_for_ai_v3","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ReallyFloppyPenguin/lots_of_datasets_for_ai_v3","creator_name":"Gurvaah Singh","creator_url":"https://huggingface.co/ReallyFloppyPenguin","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset is for Training LLMs From Scratch!\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ReallyFloppyPenguin/lots_of_datasets_for_ai_v3.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"Atma3-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Atma3-Share-GPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3-Share-GPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Fineweb-Instruct","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Fineweb-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"We convert the pre-training corpus from Fineweb-Edu (https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) to instruction following format. We select a subset with quality filter and then use GPT-4 to extract instruction-following pairs. The dataset contains roughly 16M instruction pairs. The basic concept is similar to MAmmoTH2 (https://arxiv.org/abs/2405.03548). \n\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you use dataset useful, please cite the following paper:\n@article{yue2024mammoth2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Fineweb-Instruct.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10M - 100M","json"],"keywords_longer_than_N":true},
	{"name":"drill","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/drill","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tDrill\n\t\n\nThis dataset combines three instruction-following datasets:\n\nargilla/ifeval-like-data (filtered subset)  \nArliAI/Formax-v1.0  \nChristianAzinn/json-training\n\nIt contains prompts with detailed instructions and corresponding formatted outputs, suitable for training models on instruction adherence and structured text generation.\n\n  Definition of the word \"drill\" according to Merriam-Webster Dictionary\n  \n\ndrill (noun)\n\na physical or mental exercise aimed at perfecting facility and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/drill.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"ProcessedOpenAssistant","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant","creator_name":"Youzhi Yu","creator_url":"https://huggingface.co/PursuitOfDataScience","description":"\n\t\n\t\t\n\t\tRefined OASST1 Conversations\n\t\n\nDataset Name on Hugging Face: PursuitOfDataScience/ProcessedOpenAssistant\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is derived from the OpenAssistant/oasst1 conversations, with additional processing to:\n\nRemove single-turn or incomplete conversations (where a prompter/user message had no assistant reply),\nRename roles from \"prompter\" to \"User\" and \"assistant\" to \"Assistant\",\nOrganize each conversation as a list of turn objects.\n\nThe goal is to provide a clean‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Bangla-Instruct","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/md-nishat-008/Bangla-Instruct","creator_name":"Nishat Raihan","creator_url":"https://huggingface.co/md-nishat-008","description":"\nBangla-Instruct\nState-of-the-art Bangla Instruction Dataset\n\n\n\n\n\n\n\n\n\n\n\nTigerLLM introduces a state-of-the-art dataset designed to advance Bangla language modeling. The Bangla-Instruct dataset contains high-quality native Bangla instruction-response pairs that have been generated using cutting-edge teacher models.\n\n\n\n\nOverview\n\n\n\nThe Bangla-Instruct dataset is composed of 100,000 instruction-response pairs. It starts with 500 seed tasks created by 50 volunteer experts from premier Bangladeshi‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/md-nishat-008/Bangla-Instruct.","first_N":5,"first_N_keywords":["text-generation","Bengali","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"security_steerability","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/itayhf/security_steerability","creator_name":"Itay H","creator_url":"https://huggingface.co/itayhf","description":"\n\t\n\t\t\n\t\tDataset Card for VeganRibs & ReverseText\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains two datasets, VeganRibs and ReverseText, designed to evaluate the Security Steerability of Large Language Models (LLMs). \nSecurity Steerability refers to an LLM's ability to strictly adhere to application-specific policies and functional instructions defined within its system prompt, even when faced with conflicting or manipulative user inputs. These datasets aim to bridge the gap in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/itayhf/security_steerability.","first_N":5,"first_N_keywords":["English","mit","arxiv:2504.19521","üá∫üá∏ Region: US","evaluation"],"keywords_longer_than_N":true},
	{"name":"kazakh-ift","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nurkhan5l/kazakh-ift","creator_name":"Nurkhan Laiyk","creator_url":"https://huggingface.co/nurkhan5l","description":"Kazakh-IFT üá∞üáø\nAuthors: Nurkhan Laiyk, Daniil Orel, Rituraj Joshi, Maiya Goloburda, Yuxia Wang, Preslav Nakov, Fajri Koto\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nInstruction tuning in low-resource languages remains challenging due to limited coverage of region-specific institutional and cultural knowledge. To address this gap, we introduce a large-scale instruction-following dataset (~10,600 samples) focused on Kazakhstan, spanning domains such as governance, legal processes, cultural practices, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nurkhan5l/kazakh-ift.","first_N":5,"first_N_keywords":["Kazakh","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"SFT_54k_reasoning","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/SFT_54k_reasoning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/SFT_54k_reasoning is a processed version of the XuHu6736/s1_54k_filter_with_isreasoning dataset, specifically reformatted for instruction fine-tuning (SFT) of language models.\nThe original question and solution pairs have been converted into an instruction-following format. Critically, the isreasoning_score and isreasoning labels from the parent dataset are preserved, allowing for targeted SFT on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning.","first_N":5,"first_N_keywords":["XuHu6736 (formatting and derivation)","derived from XuHu6736/s1_54k_filter_with_isreasoning","derived from source datasets","monolingual","XuHu6736/s1_54k_filter_with_isreasoning"],"keywords_longer_than_N":true},
	{"name":"alpaca-style-QnA","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sweatSmile/alpaca-style-QnA","creator_name":"amitk17","creator_url":"https://huggingface.co/sweatSmile","description":"\n\t\n\t\t\n\t\tAlpaca-style Question and Answer Dataset\n\t\n\nThis dataset contains question-answer pairs formatted in the Alpaca instruction style, suitable for instruction fine-tuning of language models.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example contains:\n\ninstruction: The question\ninput: Empty string (can be used for context in other applications)\noutput: The answer\ntext: The formatted text using the Alpaca template\n\n\n\t\n\t\t\n\t\tTemplate\n\t\n\nBelow is an instruction that describes a task, paired with an input that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sweatSmile/alpaca-style-QnA.","first_N":5,"first_N_keywords":["English","cc-by-4.0","üá∫üá∏ Region: US","instruction","qa"],"keywords_longer_than_N":true},
	{"name":"alpaca-tat","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yasalma/alpaca-tat","creator_name":"Yasalma","creator_url":"https://huggingface.co/yasalma","description":"\n\t\n\t\t\n\t\tTatAlpaca\n\t\n\nDataset of Gemini-generated instructions in Tatar language.\n\nCode: tatlm/self_instruct\nCode is based on Stanford Alpaca and self-instruct.\n166,257 examples\n\nPrompt template:\n{{num_tasks}} “ó—ã–µ–ª–º–∞—Å—ã–Ω—ã“£ —Å–æ—Å—Ç–∞–≤—ã —Ç–µ–ª –º–æ–¥–µ–ª–µ–Ω ”©–π—Ä”ô–Ω“Ø ”©—á–µ–Ω —Ç”©—Ä–ª–µ:\n\n1. –ë–∏—Ä–µ–º–Ω”ô—Ä–Ω–µ –º–∞–∫—Å–∏–º–∞–ª—å —Ä”ô–≤–µ—à—Ç”ô —Ç–∏–ø–ª–∞—Ä—ã, —Å–æ—Ä–∞–ª–≥–∞–Ω –≥–∞–º”ô–ª–ª”ô—Ä–µ, —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–ª–∞—Ä—ã, –∫–µ—Ä“Ø –º”©–º–∫–∏–Ω–ª–µ–∫–ª”ô—Ä–µ –±—É–µ–Ω—á–∞ –±–µ—Ä-–±–µ—Ä—Å–µ–Ω”ô –æ—Ö—à–∞–º–∞–≥–∞–Ω –∏—Ç–µ–ø —ç—à–ª”ô.\n2. –ë–∏—Ä–µ–º–Ω”ô—Ä —Ä”ô—Å–µ–º–Ω”ô—Ä, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ –±–µ–ª”ô–Ω —ç—à–ª–∏ –±–µ–ª–º”ô–≥”ô–Ω “ª”ô–º —Ç—ã—à–∫—ã –¥”©–Ω—å—è–≥–∞ –∫–µ—Ä“Ø –º”©–º–∫–∏–Ω–ª–µ–≥–µ –±—É–ª–º–∞–≥–∞–Ω‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yasalma/alpaca-tat.","first_N":5,"first_N_keywords":["text-generation","Tatar","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Electrical-engineering-ru","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Electrical-engineering-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"Translated instructions from STEM-AI-mtl/Electrical-engineering into Russian using gemini-flash-1.5-8b.\n","first_N":5,"first_N_keywords":["text-generation","translated","STEM-AI-mtl/Electrical-engineering","Russian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Text_Guided_Image_Editing-ru","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Text_Guided_Image_Editing-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"Translated instructions from ImagenHub/Text_Guided_Image_Editing into Russian using gemini-flash-1.5-8b.\n","first_N":5,"first_N_keywords":["image-to-image","translated","ImagenHub/Text_Guided_Image_Editing","Russian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"metallurgy-qa","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AbdulrhmanEldeeb/metallurgy-qa","creator_name":"Eldeeb","creator_url":"https://huggingface.co/AbdulrhmanEldeeb","description":"\n\t\n\t\t\n\t\tMetallurgy and Materials Science Knowledge Extraction Dataset\n\t\n\nThis repository contains a dataset generated from parsed books related to various aspects of metallurgy, materials science, and engineering. The dataset is designed for fine-tuning Large Language Models (LLMs) for Question-Answering (QA) tasks in the domain of metallurgy and materials science.\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe dataset includes content derived from technical books in the field of metallurgy and materials‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AbdulrhmanEldeeb/metallurgy-qa.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","closed-book-qa","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"godot-training","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ImJimmeh/godot-training","creator_name":"Jim","creator_url":"https://huggingface.co/ImJimmeh","description":"ImJimmeh/godot-training dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Atma4-Hindi","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma4-Hindi\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned_uz","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bizb0630/alpaca-cleaned_uz","creator_name":"Behruz Izbaev","creator_url":"https://huggingface.co/bizb0630","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a translation of the alpaca-cleaned dataset into Uzbek (Latin), using the GPT-4o mini API.\n","first_N":5,"first_N_keywords":["text-generation","Uzbek","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701-instruct","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"\n\t\n\t\t\n\t\tLOGIC-701 (instruct)\n\t\n\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\n","first_N":5,"first_N_keywords":["question-answering","Russian","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"OpenCharacter","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xywang1/OpenCharacter","creator_name":"Xiaoyang Wang","creator_url":"https://huggingface.co/xywang1","description":"\n\t\n\t\t\n\t\tOpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas\n\t\n\nThis repo releases data introduced in our paper OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas in arXiv.\n\nWe study customizable role-playing dialogue agents in large language models (LLMs).\nWe tackle the challenge with large-scale data synthesis: character synthesis and character-driven reponse synthesis.\nOur solution strengthens the original LLaMA-3‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xywang1/OpenCharacter.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"english-to-colloquial-tamil","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jarvisvasu/english-to-colloquial-tamil","creator_name":"Vasanth Kumar","creator_url":"https://huggingface.co/jarvisvasu","description":"\n\t\n\t\t\n\t\tEnglish to Colloquial Tamil\n\t\n\n\"instruction\":\"Translate provided English text into colloquial Tamil.\"\n\"input\": \"Their players played well.\"\n\"output\": \"‡ÆÖ‡Æµ‡Æô‡Øç‡Æï players ‡Æ®‡Æ≤‡Øç‡Æ≤‡Ææ ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æ£‡Øç‡Æü‡Ææ‡Æô‡Øç‡Æï.\"\n\n","first_N":5,"first_N_keywords":["translation","text-generation","Tamil","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tDeepthink Reasoning Demo\n\t\n\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MMLU-Alpaca","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for MMLU-Alpaca\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Model-based","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Model-based is a dataset of BenchMAX, sourcing from m-ArenaHard, which evaluates the instruction following capability via model-based judgment.\nWe extend the original dataset to include languages that are not supported by m-ArenaHard through‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based.","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"Atma3.2-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3.2-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Atma4","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma4\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"ATC-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for ATC-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ATCgpt-Fixed","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for ATCgpt-Fixed\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Atcgpt-Fixed2","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atcgpt-Fixed2\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"SWE-Bench-Verified-O1-reasoning-high-results","keyword":"multi-turn","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","description":"\n\t\n\t\t\n\t\tSWE-Bench Verified O1 Dataset\n\t\n\n\n\t\n\t\t\n\t\tExecutive Summary\n\t\n\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities on the SWE-Bench Verified dataset, achieving a 28.8% success rate across 500 test instances.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset was generated using the CodeAct framework, which aims to improve code generation through enhanced action-based reasoning.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"corporateDataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MikePfunk28/corporateDataset","creator_name":"Michael Pfundt","creator_url":"https://huggingface.co/MikePfunk28","description":"\n\t\n\t\t\n\t\tCorporate Data Analysis Training Dataset (Clean)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned and standardized corporate analysis training dataset with consistent schema.\n\n\t\n\t\t\n\t\tSchema\n\t\n\nAll entries follow the instruction-input-output format:\n{\n  \"instruction\": \"Task description\",\n  \"input\": \"Business data or context\", \n  \"output\": \"Analysis and insights\"\n}\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\n‚úÖ Consistent Schema - All entries use the same format\n‚úÖ Clean Data - Validated and error-free\n‚úÖ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MikePfunk28/corporateDataset.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LIMO_QFFT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lwl-uestc/LIMO_QFFT","creator_name":"Wanlong Liu","creator_url":"https://huggingface.co/lwl-uestc","description":"\n\t\n\t\t\n\t\tüìò LIMO‚ÄìQFFT\n\t\n\nLIMO‚ÄìQFFT is a question-free variant of the original GAIR/LIMO dataset, tailored for use in QFFT (Question-Free Fine-Tuning) pipelines.\n\n\t\n\t\t\n\t\tüîç Description\n\t\n\nThis dataset removes the original input questions and system prompts from the LIMO dataset, and keeps only the long-form reasoning responses. The goal is to enable training large language models to learn from reasoning traces alone, without depending on task-specific questions.\nAll entries are converted into‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lwl-uestc/LIMO_QFFT.","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"dataset_qa","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sujal7102003/dataset_qa","creator_name":"sujalthakkar","creator_url":"https://huggingface.co/sujal7102003","description":"\n\t\n\t\t\n\t\tüìò Personal QA Instruction Dataset\n\t\n\nThis dataset contains instruction-style question-answer pairs about personal and educational background, projects, technical skills, internships, and AI interests. It is well-suited for testing or fine-tuning small language models like TinyLlama or for building personalized assistants.\n\n\t\n\t\t\n\t\tüìÅ Dataset Details\n\t\n\n\nFormat: JSON (List of dictionaries)\nLanguage: English\nSize: ~200 entries\nFields:\nquestion: The instruction or question prompt\nanswer:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sujal7102003/dataset_qa.","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"constitucion-venezuela-1000","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-1000","creator_name":"Niurka Oropeza","creator_url":"https://huggingface.co/niuvaroza","description":"\n\t\n\t\t\n\t\tConstitucion de Venezuela - Dataset de 1000 Instrucciones\n\t\n\n\n\t\n\t\t\n\t\tDescripcion General\n\t\n\nEste dataset contiene 1000 pares de instruccion-respuesta cuidadosamente curados sobre la Constitucion de la Republica Bolivariana de Venezuela de 1999. Ha sido dise√±ado especificamente para el entrenamiento y evaluacion de modelos de lenguaje en tareas de comprension y generacion de texto sobre contenido constitucional venezolano.\nEl dataset abarca los aspectos mas importantes de la‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-1000.","first_N":5,"first_N_keywords":["question-answering","text-generation","Spanish","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ginecologia-venezuela","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yensonalvi6/ginecologia-venezuela","creator_name":"Yenson Key Batatima Alviarez","creator_url":"https://huggingface.co/yensonalvi6","description":"\n\t\n\t\t\n\t\tGinecolog√≠a Venezuela Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescripci√≥n del Dataset\n\t\n\nEste dataset contiene 250 instrucciones especializadas en ginecolog√≠a y obstetricia, enfocadas espec√≠ficamente en el contexto de la salud p√∫blica venezolana. Est√° dise√±ado para entrenar modelos de lenguaje en el dominio m√©dico ginecol√≥gico con consideraciones espec√≠ficas del sistema de salud venezolano.\n\n\t\n\t\t\n\t\tContenido\n\t\n\n\nTama√±o: 250 ejemplos de instrucciones\nIdioma: Espa√±ol (Venezuela)\nDominio: Ginecolog√≠a y‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yensonalvi6/ginecologia-venezuela.","first_N":5,"first_N_keywords":["text-generation","question-answering","Spanish","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Bhagwat-Corpus-Data","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PyPranav/Bhagwat-Corpus-Data","creator_name":"Pranav Sunil","creator_url":"https://huggingface.co/PyPranav","description":"\n\t\n\t\t\n\t\tBhagwat Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Bhagwat Corpus is a synthetic dataset of approximately 90,000 examples designed for instruction-tuning large language models (LLMs) to generate Vedic philosophical responses grounded in scriptural tradition. Each example consists of:\n\nA synthetic user question\nA relevant Sanskrit shloka (verse) from the Mahabharata or Ramayana\nAn English translation of the shloka\nA generated explanation and status for the response\n\nThe dataset is based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PyPranav/Bhagwat-Corpus-Data.","first_N":5,"first_N_keywords":["English","Sanskrit","apache-2.0","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"tulu-v2-sft-mixture-filtered","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lizhuang144/tulu-v2-sft-mixture-filtered","creator_name":"Zhuang Li","creator_url":"https://huggingface.co/lizhuang144","description":"\n\t\n\t\t\n\t\tüìò SCAR-Filtered Instruction-Tuning Subset (10k from Tulu-v2)\n\t\n\nThis dataset contains 10,000 high-quality instruction‚Äìresponse pairs filtered from the allenai/tulu-v2-sft-mixture dataset using the SCAR data selection method.\nSCAR (Style Consistency-Aware Response Ranking) is a novel data selection framework accepted to ACL 2025 (main conference). It ranks and filters instruction‚Äìresponse pairs based on style consistency, resulting in a more reliable and efficient subset for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lizhuang144/tulu-v2-sft-mixture-filtered.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"monomer","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcadianlee/monomer","creator_name":"Ricky Renjie Li","creator_url":"https://huggingface.co/arcadianlee","description":"arcadianlee/monomer dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["table-question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ifc-bim-alpaca-improved","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Dietmar2020/ifc-bim-alpaca-improved","creator_name":"Dietmar Grabowski ","creator_url":"https://huggingface.co/Dietmar2020","description":"\n\t\n\t\t\n\t\tIFC-BIM Improved Alpaca Dataset\n\t\n\nA high-quality instruction-following dataset for Industry Foundation Classes (IFC) and Building Information Modeling (BIM).\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains carefully curated and validated instruction-response pairs about IFC concepts, schemas, and BIM practices. It has been cleaned and improved from an original dataset of 545k+ entries.\n\n\t\n\t\t\n\t\tDataset Quality\n\t\n\n\nQuality Score: 4.6/5.0 (improved from 3.0)\nLLM Validation: 95.1%‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dietmar2020/ifc-bim-alpaca-improved.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_CalligraphyOCR_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyOCR_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_CalligraphyOCR_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_CalligraphyOCR_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_CalligraphyOCR_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyOCR_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_CalligraphyOCR_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_CalligraphyOCR_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Countxy_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Countxy_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Countxy_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Countxy_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Countxy_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Countxy_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Countxy_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Countxy_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_4_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_4_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_4_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_4_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_3_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_3_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_3_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_3_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_1_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_1_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_1_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_1_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_4_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_4_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_4_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_4_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_5_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_5_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_5_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_5_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_6_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_6_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_6_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_6_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_7_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_7_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_7_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_7_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_8_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_8_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_8_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_8_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_1_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_1_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_1_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_1_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_4_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_4_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_4_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_4_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_6_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_6_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_6_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_6_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_8_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_8_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_8_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_8_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_1_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_1_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_1_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_1_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_2_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_2_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_2_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_2_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_3_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_3_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_3_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_3_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_6_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_6_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_6_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_6_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_8_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_8_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_8_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_8_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_10_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_10_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_10_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_10_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_11_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_11_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_11_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_11_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_12_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_12_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_12_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_12_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_13_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_13_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_13_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_13_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_15_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_15_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_15_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_15_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_1_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_1_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_1_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_1_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_3_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_3_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_3_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_3_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_9_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_9_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_9_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_9_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_10_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_10_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_10_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_10_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_13_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_13_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_13_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_13_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"UltraChat1_en","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/UltraChat1_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/UltraChat1_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue\n\nTotal examples: 103932\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/UltraChat1_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"UltraChat2_ar","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/UltraChat2_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/UltraChat2_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue\n\nTotal examples: 95804\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/UltraChat2_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"synapse-set-50k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NextGenC/synapse-set-50k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","description":"\n\t\n\t\t\n\t\tüß† SynapseSet-50K\n\t\n\nSynapseSet-50K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nüî¨ 100% synthetic, non-clinical data. Intended for academic and research‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-50k.","first_N":5,"first_N_keywords":["text-generation","English","Turkish","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CoIN-ASD","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jinpeng0528/CoIN-ASD","creator_name":"Jinpeng Chen","creator_url":"https://huggingface.co/jinpeng0528","description":"\n\t\n\t\t\n\t\tCoIN-ASD Benchmark\n\t\n\nCoIN-ASD is a benchmark dataset designed for multimodal continual instruction tuning (MCIT), based on the CoIN dataset. This dataset aims to evaluate the performance of MCIT models in mitigating essential forgetting.\nüìù Paper\nüêô GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized in the following structure:\n‚îú‚îÄ‚îÄ ScienceQA/\n‚îÇ   ‚îú‚îÄ‚îÄ train_ori.json\n‚îÇ   ‚îú‚îÄ‚îÄ train_x{10,20,40,60,80}.json\n‚îÇ   ‚îî‚îÄ‚îÄ test.json\n‚îú‚îÄ‚îÄ TextVQA/\n‚îÇ   ‚îú‚îÄ‚îÄ train_ori.json\n‚îÇ   ‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jinpeng0528/CoIN-ASD.","first_N":5,"first_N_keywords":["English","mit","arxiv:2505.02486","üá∫üá∏ Region: US","multimodal-continual-instruction-tuning"],"keywords_longer_than_N":true},
	{"name":"KemSU","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NodeLinker/KemSU","creator_name":"Ilya Pereverzin","creator_url":"https://huggingface.co/NodeLinker","description":"\n\t\n\t\t\n\t\tüéì Kemerovo State University Instructional QA Dataset (NodeLinker/KemSU)\n\t\n\n\n  \n     \n  \n\n\n  \n    \n  \n  \n    \n  \n\n\n\n\n\t\n\t\n\t\n\t\tüìù Dataset Overview & Splits\n\t\n\nThis dataset provides instructional question-answer (Q&A) pairs meticulously crafted for Kemerovo State University (–ö–µ–º–ì–£, KemSU), Russia. Its primary purpose is to facilitate the fine-tuning of Large Language Models (LLMs), enabling them to function as knowledgeable and accurate assistants on a wide array of topics concerning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NodeLinker/KemSU.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"Titanium2.1-DeepSeek-R1","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Click here to support our open-source dataset and model releases!\nTitanium2.1-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n31.7k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraform‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"dx7-patches-and-prompts","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ccerati/dx7-patches-and-prompts","creator_name":"Carlo Cerati","creator_url":"https://huggingface.co/ccerati","description":"\n\t\n\t\t\n\t\tYamaha DX7 Synthesizer Patches with AI-Generated Prompts\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a comprehensive, multi-task dataset designed for fine-tuning language models to understand and generate synthesizer patches for the Yamaha DX7.\nThe dataset contains over 20,000 examples across three distinct but related tasks, making it ideal for creating models that can not only generate patches but also understand and reason about their structure and validity.\n\n\t\n\t\t\n\t\tHow the Data Was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ccerati/dx7-patches-and-prompts.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K<n<100K","Text"],"keywords_longer_than_N":true},
	{"name":"WeLoveYou","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThomasTheMaker/WeLoveYou","creator_name":"Thomas Nguyen","creator_url":"https://huggingface.co/ThomasTheMaker","description":"\nThis is a dataset extracted from WeLoveYou Youtube channel transcripts & synthesized using Cloudflare Workers AI Llama3-70B.\nThe goal is to fine-tune a empathetic Qwen2.5 model\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"S1_QFFT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lwl-uestc/S1_QFFT","creator_name":"Wanlong Liu","creator_url":"https://huggingface.co/lwl-uestc","description":"\n\t\n\t\t\n\t\tüìò S1‚ÄìQFFT\n\t\n\nS1‚ÄìQFFT is a question-free version of the original simplescaling/s1K-1.1 dataset, designed for QFFT training workflows.\n\n\t\n\t\t\n\t\tüîç Description\n\t\n\nThis dataset discards the original questions and any system instructions, keeping only the reasoning completions as supervision. It is especially useful for models that aim to learn when and how to think, rather than just how to answer.\nThe dataset is fully converted into a format compatible with LLaMA-Factory training.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lwl-uestc/S1_QFFT.","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"oasst1_v2_tr","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/limeXx/oasst1_v2_tr","creator_name":"yusuf uzun","creator_url":"https://huggingface.co/limeXx","description":"\n\t\n\t\t\n\t\tT√ºrk√ße oasst1 Veri Seti (Optimizasyonlu √áeviri)\n\t\n\nBu repository, pop√ºler OpenAssistant Conversations (oasst1) veri setinin, yapay zeka modellerinin ince ayarƒ± (fine-tuning) i√ßin optimize edilmi≈ü T√ºrk√ße √ßevirisini i√ßermektedir. Toplamda 50,624 adet girdi-√ßƒ±ktƒ± √ßifti bulunmaktadƒ±r.\n\n\t\n\t\t\n\t\tVeri Seti A√ßƒ±klamasƒ±\n\t\n\nBu √ßalƒ±≈üma, oasst1 veri setindeki ƒ∞ngilizce \"prompt-response\" (istek-yanƒ±t) √ßiftlerini alarak, Google'ƒ±n Gemini serisi modelleri aracƒ±lƒ±ƒüƒ±yla akƒ±cƒ± ve doƒüal bir T√ºrk√ßeye‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/limeXx/oasst1_v2_tr.","first_N":5,"first_N_keywords":["translation","question-answering","Turkish","apache-2.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"InstructTTSEval","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CaasiHUANG/InstructTTSEval","creator_name":"Kexin Huang","creator_url":"https://huggingface.co/CaasiHUANG","description":"\n\t\n\t\t\n\t\tInstructTTSEval\n\t\n\nInstructTTSEval is a comprehensive benchmark designed to evaluate Text-to-Speech (TTS) systems' ability to follow complex natural-language style instructions. The dataset provides a hierarchical evaluation framework with three progressively challenging tasks that test both low-level acoustic control and high-level style generalization capabilities.\n\nGithub Repository: https://github.com/KexinHUANG19/InstructTTSEval\nPaper: InstructTTSEval: Benchmarking Complex‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaasiHUANG/InstructTTSEval.","first_N":5,"first_N_keywords":["text-to-speech","English","Chinese","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"constitucion-venezuela-250","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-250","creator_name":"Niurka Oropeza","creator_url":"https://huggingface.co/niuvaroza","description":"\n\t\n\t\t\n\t\tConstituci√≥n de Venezuela - Dataset de Instrucciones\n\t\n\n\n\t\n\t\t\n\t\tDescripci√≥n del Dataset\n\t\n\nEste dataset contiene 250 pares de instrucci√≥n-respuesta basados en la Constituci√≥n de la Rep√∫blica Bolivariana de Venezuela de 1999. Ha sido dise√±ado espec√≠ficamente para el entrenamiento de modelos de lenguaje en tareas de comprensi√≥n y respuesta sobre contenido constitucional venezolano.\n\n\t\n\t\t\n\t\tInformaci√≥n del Dataset\n\t\n\n\nIdioma: Espa√±ol (es)\nLicencia: CC-BY-4.0\nTama√±o: 250 ejemplos\nFormato:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-250.","first_N":5,"first_N_keywords":["question-answering","text-generation","Spanish","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Cybersecurity-Dataset-Heimdall-v1.1","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlicanKiraz0/Cybersecurity-Dataset-Heimdall-v1.1","creator_name":"Alican Kiraz","creator_url":"https://huggingface.co/AlicanKiraz0","description":"\n\t\n\t\t\n\t\tCybersecurity Defense Instruction-Tuning Dataset (v1.1)\n\t\n\n\n\t\n\t\t\n\t\tTL;DR\n\t\n\n21‚ÄØ258 high‚Äëquality system / user / assistant triples for training alignment‚Äësafe, defensive‚Äëcybersecurity LLMs. Curated from 100‚ÄØ000‚ÄØ+ technical sources, rigorously cleaned and filtered to enforce strict ethical boundaries. Apache‚Äë2.0 licensed.\n\n\n\t\n\t\t\n\t\t1¬†¬†What‚Äôs new in v1.1¬†¬†(2025‚Äë06‚Äë21)\n\t\n\n\n\t\n\t\t\nChange\nv1.0\nv1.1\n\n\n\t\t\nRows\n2‚ÄØ500\n21‚ÄØ258¬†(+760‚ÄØ%)\n\n\nCovered frameworks\nOWASP¬†Top¬†10, NIST¬†CSF\n+¬†MITRE¬†ATT&CK, ASD‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlicanKiraz0/Cybersecurity-Dataset-Heimdall-v1.1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"persona-chat","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/awsaf49/persona-chat","creator_name":"Awsaf","creator_url":"https://huggingface.co/awsaf49","description":"\n\t\n\t\t\n\t\tDataset Card for PersonaChat\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPersonaChat is a multi-turn dialogue dataset introduced by Zhang et al. (2018) for training and evaluating persona-grounded conversational agents. Each conversation is between two crowdworkers, each assigned a randomly selected persona consisting of several simple facts. The dataset aims to assess whether models can maintain consistent character traits throughout a conversation.\n\nOriginal Paper: Personalizing Dialogue‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/awsaf49/persona-chat.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","text","Text"],"keywords_longer_than_N":true},
	{"name":"korean_roleplay_dataset_for_chat_game_2","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/junidude14/korean_roleplay_dataset_for_chat_game_2","creator_name":"Seung Jun Lee","creator_url":"https://huggingface.co/junidude14","description":"\n\t\n\t\t\n\t\tKorean Roleplay Enhanced Conversations Dataset (v3)\n\t\n\n\n  \n  \n  \n  \n\n\n\n\t\n\t\t\n\t\tüìã Dataset Description\n\t\n\nThis is the third version of our enhanced Korean roleplay conversation dataset, specifically designed for training conversational AI models in visual novel/dating simulation contexts. This version significantly expands the dataset with more diverse multi-turn conversations and improved context awareness.\n\n\t\n\t\t\n\t\tüéØ Key Features\n\t\n\n\nLarge Scale: 25,568 high-quality conversation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/junidude14/korean_roleplay_dataset_for_chat_game_2.","first_N":5,"first_N_keywords":["text-generation","Korean","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"PersonaHub","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kubasoltys/PersonaHub","creator_name":"Kuba Soltys","creator_url":"https://huggingface.co/kubasoltys","description":"\n\t\n\t\t\n\t\tPolish Synthetic Personas\n\t\n\n","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","fill-mask","table-question-answering"],"keywords_longer_than_N":true},
	{"name":"Bielikowo","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kubasoltys/Bielikowo","creator_name":"Kuba Soltys","creator_url":"https://huggingface.co/kubasoltys","description":"\n\t\n\t\t\n\t\tDatasets for Bielikowo tutorials\n\t\n\nBielikowo Github\n","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","fill-mask","table-question-answering"],"keywords_longer_than_N":true},
	{"name":"product-catalyst-dataset","keyword":"instruction-tuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicolesarvasicosta/product-catalyst-dataset","creator_name":"Nicole Sarvasi Costa","creator_url":"https://huggingface.co/nicolesarvasicosta","description":"\n\t\n\t\t\n\t\tProduct Catalyst Synthetic Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset consists of 1,456 high-quality, synthetic conversation records designed specifically for the supervised fine-tuning (SFT) of an expert AI assistant in product development. The primary goal of this dataset is to train a language model to adopt the \"Product Catalyst\" persona: an expert advisor knowledgeable in product management, market research, software development methodologies, and business strategy.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicolesarvasicosta/product-catalyst-dataset.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-sa-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"PathSum-CoT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/singhprabhat/PathSum-CoT","creator_name":"Prabhat Singh","creator_url":"https://huggingface.co/singhprabhat","description":"singhprabhat/PathSum-CoT dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["summarization","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"vlaa-thinking-grpo","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/penfever/vlaa-thinking-grpo","creator_name":"Benjamin Feuer","creator_url":"https://huggingface.co/penfever","description":"\n\t\n\t\t\n\t\tVLAA-Thinking-SFT-126K\n\t\n\nLarge-scale vision-language dataset with 126K instruction-following samples featuring chain-of-thought reasoning\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains vision-language samples with instruction-following conversations. Each sample includes:\n\nimage: PIL Image object\nquestion: Question or instruction text\nanswer or gt: Response with thinking process (SFT dataset) or ground truth answer (GRPO dataset)\ncaption: Image caption (may be empty for some‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/penfever/vlaa-thinking-grpo.","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"CodeForce_SAGA","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/opencompass/CodeForce_SAGA","creator_name":"OpenCompass","creator_url":"https://huggingface.co/opencompass","description":"\n\t\n\t\t\n\t\tCodeForce-SAGA: A Self-Correction-Augmented Code Generation Dataset\n\t\n\nCodeForce-SAGA is a large-scale, high-quality training dataset designed to enhance the code generation and problem-solving capabilities of Large Language Models (LLMs). All problems and solutions are sourced from the competitive programming platform Codeforces.\nThis dataset is built upon the SAGA (Strategic Adversarial & Constraint-differential Generative workflow) framework, a novel human-LLM collaborative‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/opencompass/CodeForce_SAGA.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"odia-instruction-dataset","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abhilash88/odia-instruction-dataset","creator_name":"Abhilash Sahoo","creator_url":"https://huggingface.co/abhilash88","description":"\n\t\n\t\t\n\t\tOdia Instruction Following Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a comprehensive Odia language instruction-following dataset designed for training conversational AI models, chatbots, and instruction-following systems in Odia (‡¨ì‡¨°‡¨º‡¨ø‡¨Ü). The dataset contains high-quality instruction-response pairs that enable models to understand and follow instructions in the Odia language.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Odia (‡¨ì‡¨°‡¨º‡¨ø‡¨Ü)\nTotal Records: 324,560\nFormat:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abhilash88/odia-instruction-dataset.","first_N":5,"first_N_keywords":["text-generation","question-answering","Oriya","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true}
]
;
