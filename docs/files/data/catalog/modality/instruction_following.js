const data_for_modality_instruction_following = 
[
	{"name":"instruct-aira-dataset-v2","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruct-Aira Dataset version 2.0\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of single-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for various natural language processing tasks, including but not limitedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"openbohm","keyword":"multi-turn","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/distantquant/openbohm","creator_name":"Distant Quant","creator_url":"https://huggingface.co/distantquant","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenBohm\\n\\t\\n\\nThis dataset is an experimental conjugation of philosophical multi-turn long-form conversations from J. Krishnamurti, and D. Bohm, added to long-conversation filtered (count > 6) Capybara data, edited to be slightly less apologetic.\\nRemoved references to names and locations where possible. Some conversations have been paraphrased somewhat to follow QA format better, however they keep the key content of the original.\\n\\n","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"budapest-v0.1-hun","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun","creator_name":"Balazs Toldi","creator_url":"https://huggingface.co/Bazsalanszky","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBudapest-v0.1 Dataset README\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Budapest-v0.1 dataset is a cutting-edge resource specifically designed for fine-tuning large language models (LLMs). Created using GPT-4, this dataset is presented in a message-response format, making it particularly suitable for a variety of natural language processing tasks. The primary focus of Budapest-v0.1 is to aid in the development and enhancement of algorithms capable of performing summarization, questionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun.","first_N":5,"first_N_keywords":["text-generation","question-answering","Hungarian","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-bn","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn","creator_name":"fahim abrar","creator_url":"https://huggingface.co/abrarfahim","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned-bn\\n\\t\\n\\n\\n\\nThis is a cleaned bengali translated version of the original Alpaca Dataset released by Stanford. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUses\\n\\t\\n\\nimport datasets\\ndataset = datasets.load_dataset(\\\"abrarfahim/alpaca-cleaned-bn\\\")\\nprint(dataset[0])\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n{'system_prompt': 'You are a virtual assistant, deliver a comprehensive response.',\\n 'qas_id': 'YY9S5K',\\n 'question_text': '\\\"à¦¸à¦¨à§à¦¦à§‡à¦¹\\\" à¦¶à¦¬à§à¦¦à§‡à¦° à¦¸à¦ à¦¿à¦• à¦ªà§à¦°à¦¤à¦¿à¦¶à¦¬à§à¦¦ à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¨ à¦•à¦°à§à¦¨à¥¤',\\n 'orig_answer_texts':â€¦ See the full description on the dataset page: https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn.","first_N":5,"first_N_keywords":["question-answering","text-generation","Bengali","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-sv","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv","creator_name":"Tim Isbister","creator_url":"https://huggingface.co/timpal0l","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenHermes-2.5-sv\\n\\t\\n\\nThis is a machine translated instruct dataset from OpenHermes-2.5. \\nThe facebook/seamless-m4t-v2-large was used, and some post filtering is done to remove repetitive texts that occurred due to translation errors.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExample data:\\n\\t\\n\\n[\\n   {\\n      \\\"from\\\":\\\"human\\\",\\n      \\\"value\\\":\\\"Vilket naturfenomen, som orsakas av att ljus reflekteras och bryts genom vattendroppar, resulterar i en fÃ¤rgglad bÃ¥ge pÃ¥ himlen?\\\",\\n      \\\"weight\\\":null\\n   },\\n   {â€¦ See the full description on the dataset page: https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv.","first_N":5,"first_N_keywords":["text-generation","Swedish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Truth","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abhishekbisaria/Truth","creator_name":"Abhishek Bisaria","creator_url":"https://huggingface.co/abhishekbisaria","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/abhishekbisaria/Truth.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLOGIC-701 Benchmark\\n\\t\\n\\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThis benchmark is also part of anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701.","first_N":5,"first_N_keywords":["English","Russian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"InstructTranslation-EN-ES","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Iker/InstructTranslation-EN-ES","creator_name":"Iker GarcÃ­a-Ferrero","creator_url":"https://huggingface.co/Iker","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTranslation of Instructions EN-ES\\n\\t\\n\\nThis dataset contains prompts and answers from teknium/OpenHermes-2.5 translated to Spanish using GPT-4-0125-preview. The dataset is intended to be used for training a model to translate instructions from English to Spanish.\\nThe dataset is formatted with the TowerInstruct format. It is ready to finetune a Tower translation model. if you want the raw translations, there are available here:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Iker/InstructTranslation-EN-ES.","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"Confession-Subreddit-Top500","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Oguzz07/Confession-Subreddit-Top500","creator_name":"OÄŸuzhan YÄ±ldÄ±rÄ±m","creator_url":"https://huggingface.co/Oguzz07","description":"This dataset was prepared by taking into account the 500 most popular posts of all time in the confession subreddit and the comments with the most votes on these posts.\\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","text","Text"],"keywords_longer_than_N":true},
	{"name":"pandora-instruct","keyword":"instruct","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-instruct","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPandora Instruct\\n\\t\\n\\nAn instruction dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\\nThe dataset is based on the existing datasets:\\n\\nteknium/openhermes\\nise-uiuc/magicoder-evol-instruct-110k\\nise-uiuc/magicoder-oss-instruct-75k\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright and license\\n\\t\\n\\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\\nProject developed under a BSD-3-Clause license.\\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-chatml","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pacozaa/alpaca-cleaned-chatml","creator_name":"Sarin Suriyakoon","creator_url":"https://huggingface.co/pacozaa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChatML Reformat of yahma/alpaca-cleaned\\n\\t\\n\\nI'd like to try instruction-tuning dataset with chat-tuning format.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\ndataset = load_dataset(\\\"pacozaa/alpaca-cleaned-chatml\\\", split = \\\"train\\\")\\nprint(dataset[0][\\\"text\\\"])\\n\\n","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-ingen","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Walmart-the-bag/alpaca-ingen","creator_name":"wbag","creator_url":"https://huggingface.co/Walmart-the-bag","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAlpaca Ingen\\n\\t\\n\\nGoogle has added massive rate limits and other policies. I am unable to finish this.\\nThis dataset was created using Gemini 1.0 Pro with minor adjustments for cleanliness. It may contain some issues, including 'I'm sorry' responses. The dataset will undergo further cleaning once it reaches completion, with a target of processing up to 23,000 rows.\\n","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"grad_school_math_instructions_fr_Mixtral","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/grad_school_math_instructions_fr_Mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for grad_school_math_instructions_fr_Mixtral\\n\\t\\n\\nThis dataset was made thanks to the instruction of the vigogne's dataset but the output were generated with Mixtral-8x7B-Instruct instead of GPT3.5 to make it open-source.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nrobinjo\\n","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Alpaca_french_mixtral","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/Alpaca_french_mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca_french_mixtral\\n\\t\\n\\nThis dataset was made by reusing the french alpaca instruction with Mixtral-8x7B-Instruct to make the output open-source.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nrobinjo\\n","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"instruction-collection-fin","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LumiOpen/instruction-collection-fin","creator_name":"LumiOpen","creator_url":"https://huggingface.co/LumiOpen","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a collection of Finnish instruction data compiled from various sources. Most of the original data is in English and was machine translated into Finnish using Poro-34B. We supplemented this translated data with Finnish paraphrase tasks and English-Finnish translation and language identification tasks. This dataset is suitable for fine-tuning LLMs for instruction-following in Finnish and is usable for commercial purposes.\\nWe use this dataset inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LumiOpen/instruction-collection-fin.","first_N":5,"first_N_keywords":["Finnish","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-fr","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TPM-28/alpaca-cleaned-fr","creator_name":"TPM-28","creator_url":"https://huggingface.co/TPM-28","description":"TPM-28/alpaca-cleaned-fr dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_astronomy_bg","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca_astronomy_bg","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"vislupus/alpaca_astronomy_bg dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Bulgarian","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"VIS","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/weiwei888/VIS","creator_name":"shiweiwei","creator_url":"https://huggingface.co/weiwei888","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weiwei888/VIS.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"wangwei","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guilty1987/wangwei","creator_name":"FAN","creator_url":"https://huggingface.co/guilty1987","description":"guilty1987/wangwei dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-gpt4-turbo","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo","creator_name":"myles bruce","creator_url":"https://huggingface.co/mylesgoose","description":"I downloaded the dataset from Alpaca at https://huggingface.co/datasets/yahma/alpaca-cleaned and processed it using a script to convert the text to hashes. I removed any duplicate hashes along with their corresponding input and output columns. Subsequently, I utilized the GPT-4 Turbo API to feed each message, instruction, and input to the model for generating responses.\\nHere are the outputs generated by the GPT-4 Turbo model. The information in the input column and instruction column should beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"AB1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MagedGaman/AB1","creator_name":"Maged Gaman","creator_url":"https://huggingface.co/MagedGaman","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagedGaman/AB1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kushala/alpaca","creator_name":"Mummigatti","creator_url":"https://huggingface.co/Kushala","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kushala/alpaca.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned","creator_name":"Cristian Velasquez (Entreprenerdly.com)","creator_url":"https://huggingface.co/Entreprenerdly","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"scam-dialogue","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/scam-dialogue","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset is a collection of simulated phone conversation between two parties, labeled as either scam or non-scam interactions. The dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of three columns:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/scam-dialogue.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Select-Stack","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"unstacked","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","arxiv:2403.08763","arxiv:2310.05914"],"keywords_longer_than_N":true},
	{"name":"Colossal-Instruction-Translation-EN-ES","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Iker/Colossal-Instruction-Translation-EN-ES","creator_name":"Iker GarcÃ­a-Ferrero","creator_url":"https://huggingface.co/Iker","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tColossal Instruction Translation Corpus (English - Spanish )\\n\\t\\n\\n\\nA deduplicated version of this dataset can be found here, thanks to @NickyNicky: https://huggingface.co/datasets/NickyNicky/Iker-Colossal-Instruction-Translation-EN-ES_deduplicated\\n\\nThis dataset contains 2284632 instructions and answers translated from English into Spanish. Is a fully synthetic corpus generated using machine translation. We used the model Iker/TowerInstruct-13B-v0.1-EN2ES. A few examples were alsoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Iker/Colossal-Instruction-Translation-EN-ES.","first_N":5,"first_N_keywords":["translation","Spanish","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"AWE","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rishiraj/AWE","creator_name":"Rishiraj Acharya","creator_url":"https://huggingface.co/rishiraj","description":"rishiraj/AWE dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"dataset-portuguese-aira-v2-Gemma-format","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format","creator_name":"EDDY GIUSEPE CHIRINOS ISIDRO, PhD","creator_url":"https://huggingface.co/EddyGiusepe","description":"Dataset Aira para o formato do Modelo Gemma \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tResumo do Dataset\\n\\t\\n\\nEste conjunto de dados contÃ©m uma coleÃ§Ã£o de conversas individuais entre um assistente e um usuÃ¡rio.\\nAs conversas foram geradas pelas interaÃ§Ãµes do usuÃ¡rio com modelos jÃ¡ ajustados (ChatGPT, LLama 2, Open-Assistant, etc).\\nO conjunto de dados estÃ¡ disponÃ­vel em portuguÃªs (tem a versÃ£o em InglÃªs que ainda nÃ£o tratei). Mas vocÃª pode baixar do \\nrepositÃ³rio de Nicholas Kluge CorrÃªa tanto a versÃ£o em PortuguÃªs e \\na versÃ£oâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format.","first_N":5,"first_N_keywords":["question-answering","Portuguese","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca_data_clean","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aaaalon/alpaca_data_clean","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca_data_clean.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"llama3weitiao","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZHEZIXI/llama3weitiao","creator_name":"LIUJUN","creator_url":"https://huggingface.co/ZHEZIXI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZHEZIXI/llama3weitiao.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleand","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aaaalon/alpaca-cleand","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca-cleand.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"MainData","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bufanlin/MainData","creator_name":"bufanlin","creator_url":"https://huggingface.co/bufanlin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-zh\\\"\\n\\t\\n\\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºŽGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \\nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage and License Notices\\n\\t\\n\\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset shouldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bufanlin/MainData.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_cthulhu_full","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theprint/alpaca_cthulhu_full","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","description":"Based on the yahma/alpaca-cleaned data set.\\nEvery answer in the original data set was rewritten, as if the answer was given by a dedicated, high ranking cultist in the Cult of Cthulhu. The instructions were to keep the replies factually the same, but to spice things up with references to the Cthulhu Mythos in its various forms.\\nThis was done as part of a learning experience, but I am making the data set available for others to play with as well.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Proyecto","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto","creator_name":"Facundo","creator_url":"https://huggingface.co/Facundo-DiazPWT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"multiturn-capybara-preferences-filtered-binarized","keyword":"multiturn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"This dataset has been created with distilabel, plus some extra post-processing steps described below.\\nDataset Summary\\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\\n\\nRemove responses from the assistant, not only in the last turn, but also in intermediate turns where the assistant responds with a potential refusal and/or some ChatGPT-isms such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ictisgpt","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alexbs/ictisgpt","creator_name":"Alex B","creator_url":"https://huggingface.co/alexbs","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ICTIS GPT dataset\\n\\t\\n\\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Feedback-Collection-ru","keyword":"instruct","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Feedback-Collection-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeedback-Collection-ru\\n\\t\\n\\nThis is russian version of prometheus-eval/Feedback-Collection translated using Google Translate.\\n","first_N":5,"first_N_keywords":["text-generation","text-classification","translated","prometheus-eval/Feedback-Collection","Russian"],"keywords_longer_than_N":true},
	{"name":"shibing624_alpaca-zh","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/shibing624_alpaca-zh","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-zh\\\"\\n\\t\\n\\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºŽGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \\nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage and License Notices\\n\\t\\n\\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset shouldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/botp/shibing624_alpaca-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"bioinstruct","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bio-nlp-umass/bioinstruct","creator_name":"UMass BioNLP Lab","creator_url":"https://huggingface.co/bio-nlp-umass","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for BioInstruct\\n\\t\\n\\nGitHub repo: https://github.com/bio-nlp/BioInstruct\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nBioInstruct is a dataset of 25k instructions and demonstrations generated by OpenAI's GPT-4 engine in July 2023. \\nThis instruction data can be used to conduct instruction-tuning for language models (e.g. Llama) and make the language model follow biomedical instruction better. \\nImprovements of Llama on 9 common BioMedical tasks are shown in the result section. \\nTakingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bio-nlp-umass/bioinstruct.","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"halfTurkish","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sasanikrali/halfTurkish","creator_name":"sasani krali","creator_url":"https://huggingface.co/sasanikrali","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for HalfTurkish\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sasanikrali/halfTurkish.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"walt","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/canTooDdev/walt","creator_name":"Boris Marion-Dorier","creator_url":"https://huggingface.co/canTooDdev","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/canTooDdev/walt.","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Test2","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WhiteHunter111/Test2","creator_name":"Thomas Flato","creator_url":"https://huggingface.co/WhiteHunter111","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WhiteHunter111/Test2.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ultrafrench","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/ultrafrench","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ultrafrench\\n\\t\\n\\nThis dataset offers a french translation of the small sample of instructions from HuggingFaceH4/ultrachat_200k translated in french. The generations were made with Mistral large to make the output open-source\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MM-Instruct","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jjjjh/MM-Instruct","creator_name":"LIU Jihao","creator_url":"https://huggingface.co/jjjjh","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMM-Instruct is a large-scale dataset of diverse and high-quality visual instruction-answer pairs designed to enhance the instruction-following capabilities of large multimodal models (LMMs) in real-world use cases. It goes beyond simple question-answering or image-captioning by incorporating a wide range of instructions, including creative writing, summarization, and image analysis, pushing LMMs to better understand and respond to nuanced user requests.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jjjjh/MM-Instruct.","first_N":5,"first_N_keywords":["visual-question-answering","cc-by-4.0","100K - 1M","json","Image"],"keywords_longer_than_N":true},
	{"name":"multi-agent-scam-conversation","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset with Agentic Personalities\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset with Agentic Personalities is an enhanced collection of simulated phone conversations between two AI agents, one acting as a scammer or non-scammer and the other as an innocent receiver. Each dialogue is labeled as either a scam or non-scam interaction. This dataset is designed to helpâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/OpenHermes-2.5-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\td0rj/OpenHermes-2.5-ru\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is translated version of teknium/OpenHermes-2.5 into Russian using Google Translate.\\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Multilingual-Benchmark","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TaiMingLu/Multilingual-Benchmark","creator_name":"TaiMing","creator_url":"https://huggingface.co/TaiMingLu","description":"These are the GSM8K and ARC dataset translated by Google Translate. \\nBibTex\\n@misc{lu2024languagecountslearnunlearn,\\n      title={Every Language Counts: Learn and Unlearn in Multilingual LLMs}, \\n      author={Taiming Lu and Philipp Koehn},\\n      year={2024},\\n      eprint={2406.13748},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2406.13748}, \\n}\\n\\n","first_N":5,"first_N_keywords":["zero-shot-classification","question-answering","translation","English","German"],"keywords_longer_than_N":true},
	{"name":"gsm8k_multiturn","keyword":"multiturn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/euclaise/gsm8k_multiturn","creator_name":"Jade","creator_url":"https://huggingface.co/euclaise","description":"The \\\"socratic\\\" version of GSM8K has the model reflect and ask itself sub-questions about the initial question, before coming to a final answer.\\nThis dataset reformats the socratic GSM8K version into a multi-turn conversation, where the sub-questions are asked by the user rather than being self-asked by the model.\\n","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"meme_dataset","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noxus09/meme_dataset","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","description":"Noxus09/meme_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"guanjian_anli_test1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/papaya523/guanjian_anli_test1","creator_name":"zhaoyue","creator_url":"https://huggingface.co/papaya523","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/papaya523/guanjian_anli_test1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"PromisedChat_Instruction","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/netmouse/PromisedChat_Instruction","creator_name":"Matt Yeh","creator_url":"https://huggingface.co/netmouse","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/netmouse/PromisedChat_Instruction.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"WangchanThaiInstruct_Multi-turn_Conversation_Dataset","keyword":"instruction-finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThaiSyntheticQA/WangchanThaiInstruct_Multi-turn_Conversation_Dataset","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWangchanThaiInstruct Multi-turn Conversation Dataset\\n\\t\\n\\nWe create a Thai multi-turn conversation dataset from airesearch/WangchanThaiInstruct (Batch 1) by LLM. It was created from synthetic method using open source LLM in Thai language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\n\\nThammaleelakul, S., & Phatthiyaphaibun, W. (2024). WangchanThaiInstruct Multi-turn Conversation Dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.13132633\\n\\nor BibTeX\\n@dataset{thammaleelakul_2024_13132633,\\n  authorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/WangchanThaiInstruct_Multi-turn_Conversation_Dataset.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"LLama3Flashcard","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Michito97/LLama3Flashcard","creator_name":"Adrian De La Cruz","creator_url":"https://huggingface.co/Michito97","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nEste dataset contiene pares de instrucciones, entradas y salidas diseÃ±adas para generar tarjetas de memoria (flashcards) educativas. Es ideal para modelos de generaciÃ³n de texto enfocados en la creaciÃ³n de contenidos educativos.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tColumnas\\n\\t\\n\\n\\ninstruction: Laâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Michito97/LLama3Flashcard.","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"CTI_0.1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AshishCTI/CTI_0.1","creator_name":"ad","creator_url":"https://huggingface.co/AshishCTI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AshishCTI/CTI_0.1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"TextBooksPersonaHub","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/drodin/TextBooksPersonaHub","creator_name":"nacer","creator_url":"https://huggingface.co/drodin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTextBooksPersonaHub\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe TextBooksPersonaHub dataset is an extension of the proj-persona/PersonaHub dataset, created using the technique described in the paper Textbooks Are All You Need II. This dataset contains synthetically generated \\\"textbook-like\\\" passages tailored in french to specific personas, aimed at enhancing language model training with high-quality and diverse content.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data\\n\\t\\n\\nThe originalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/drodin/TextBooksPersonaHub.","first_N":5,"first_N_keywords":["text2text-generation","French","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"math-gpt-4o-200k-ko","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/math-gpt-4o-200k-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"Translated PawanKrd/math-gpt-4o-200k using nayohan/llama3-instrucTrans-enko-8b.\\nThis dataset is a raw translated dataset and contains repetitive sentences generated by the model, so it needs to be filtered.\\n","first_N":5,"first_N_keywords":["text-generation","Korean","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"TextBooksPersonaHub-FR","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/drodin/TextBooksPersonaHub-FR","creator_name":"nacer","creator_url":"https://huggingface.co/drodin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTextBooksPersonaHub\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe TextBooksPersonaHub dataset is an extension of the proj-persona/PersonaHub dataset, created using the technique described in the paper Textbooks Are All You Need II. This dataset contains synthetically generated \\\"textbook-like\\\" passages tailored in french to specific personas, aimed at enhancing language model training with high-quality and diverse content.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data\\n\\t\\n\\nThe originalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/drodin/TextBooksPersonaHub-FR.","first_N":5,"first_N_keywords":["text2text-generation","French","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"single-agent-scam-conversations","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of three columns:\\n\\ndialogue: The transcribed conversation between the caller and receiver.\\ntype: The specific type of scam or non-scam interaction.\\nlabels: A binary label indicating whether theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Kalo-Opus-Instruct-22k-Refusal-Murdered","keyword":"instruct","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewEden/Kalo-Opus-Instruct-22k-Refusal-Murdered","creator_name":"New Eden","creator_url":"https://huggingface.co/NewEden","description":"NewEden/Kalo-Opus-Instruct-22k-Refusal-Murdered dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["agpl-3.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"code-translation","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noxus09/code-translation","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Noxus09/code-translation.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"CRAFT-Summarization","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCRAFT-Summarization\\n\\t\\n\\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated summarization data proved highly beneficial.\\n\\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization.","first_N":5,"first_N_keywords":["summarization","text-generation","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"CRAFT-RecipeGen","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCRAFT-RecipeGen\\n\\t\\n\\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\\nThe correctness of the data has not been verified in detail.\\n\\n4 synthetic dataset sizes (S, M, L, XL) are available.\\nCompared to other synthetically generated datasets with the CRAFT framework, this task did not scale similarly well and we do not match the performance of generalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ThaiQA-v1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThaiQA v1\\n\\t\\n\\nThaiQA v1 is a Thai Synthetic QA dataset. It was created from synthetic method using open source LLM in Thai language.\\nWe used Nvidia Nemotron 4 (340B) to create this dataset.\\nTopics:\\nTechnology and Gadgets 100\\nTravel and Tourism 91\\nFood and Cooking 99\\nSports and Fitness 50\\nArts and Entertainment 24\\nHome and Garden 72\\nFashion and Beauty 99\\nScience and Nature 100\\nHistory and Culture 91\\nEducation and Learning 99\\nPets and Animals 83\\nRelationships and Family 78\\nPersonalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1.","first_N":5,"first_N_keywords":["text-generation","question-answering","Thai","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Claude-Instruct-5K","keyword":"instruct","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewEden/Claude-Instruct-5K","creator_name":"New Eden","creator_url":"https://huggingface.co/NewEden","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tkalo made dis\\n\\t\\n\\nThanks to Kubernetes bad for filtering + converting this to sharegpt\\nMix of Opus and 3.5 for data\\nThis is a combined set of \\nuncurated-raw-gens-og-test-filtered\\nuncurated-raw-gens-opus-jul-31-filtered\\nopus_jul10_test-filtered\\nuncurated_opus_jul8-filtered \\n","first_N":5,"first_N_keywords":["English","agpl-3.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Celestia","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia is a dataset containing science-instruct data.\\nThe 2024-10-30 version contains:\\n\\n126k rows of synthetic science-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Data-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLLaVA-OneVision-Data-ru\\n\\t\\n\\nTranslated lmms-lab/LLaVA-OneVision-Data dataset into Russian language using Google translate.\\n\\nAlmost all datasets have been translated, except for the following:\\n[\\\"tallyqa(cauldron,llava_format)\\\", \\\"clevr(cauldron,llava_format)\\\", \\\"VisualWebInstruct(filtered)\\\", \\\"figureqa(cauldron,llava_format)\\\", \\\"magpie_pro(l3_80b_mt)\\\", \\\"magpie_pro(qwen2_72b_st)\\\", \\\"rendered_text(cauldron)\\\", \\\"ureader_ie\\\"]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nimport datasets\\n\\n\\ndata =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru.","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-to-text","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"alpha","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dawsondavis/alpha","creator_name":"Dawson Davis","creator_url":"https://huggingface.co/dawsondavis","description":"dawsondavis/alpha dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"CleverBoi","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theprint/CleverBoi","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","description":"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCleverBoi\\n\\t\\n\\nThe CleverBoi Collection is based on a number of data sets that emphasize logic, inference, empathy, math and coding.\\nThe data set has been formatted to follow the alpaca format (instruction + input -> output) when fine tuning.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data Sets\\n\\t\\n\\nThe source data sets used in the CleverBoi Collection are listed below, ordered by size.\\n\\nKK04/LogicInference_OA\\nmlabonne/Evol-Instruct-Python-26k\\ngarage-bAInd/Open-Platypusâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/theprint/CleverBoi.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"godot-training","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ImJimmeh/godot-training","creator_name":"Jim","creator_url":"https://huggingface.co/ImJimmeh","description":"ImJimmeh/godot-training dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Atma4-Hindi","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma4-Hindi\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned_uz","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bizb0630/alpaca-cleaned_uz","creator_name":"Behruz Izbaev","creator_url":"https://huggingface.co/bizb0630","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a translation of the alpaca-cleaned dataset into Uzbek (Latin), using the GPT-4o mini API.\\n","first_N":5,"first_N_keywords":["text-generation","Uzbek","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ATCgpt-Fixed","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ATCgpt-Fixed\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Atcgpt-Fixed2","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atcgpt-Fixed2\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701-instruct","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLOGIC-701 (instruct)\\n\\t\\n\\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\\n","first_N":5,"first_N_keywords":["question-answering","Russian","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","keyword":"multi-turn","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSWE-Bench Verified O1 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExecutive Summary\\n\\t\\n\\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities using their native tool calling capabilities on the SWE-Bench Verified dataset, achieving a 45.8% success rate across 500 test instances.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset was generated using the CodeAct framework, which aims to improveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"college_alpaca_dataset","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dapraws/college_alpaca_dataset","creator_name":"Muhammad Darrel Prawira","creator_url":"https://huggingface.co/dapraws","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize the given article in 200 Words.\\\",\\n\\\"input\\\": \\\"https://www.bbc.com/news/world-51461830\\\",\\n\\\"output\\\": \\\"Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dapraws/college_alpaca_dataset.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Titanium","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Titanium is a dataset containing DevOps-instruct data.\\nThe 2024-10-02 version contains:\\n\\n26.6k rows of synthetic DevOps-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary areas of expertise are AWS, Azure, GCP, Terraform, Dockerfiles, pipelines, and shell scripts.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MMLU-Alpaca","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MMLU-Alpaca\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"glaiveai-reflection-v1-ko","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/youjunhyeok/glaiveai-reflection-v1-ko","creator_name":"ìœ ì¤€í˜","creator_url":"https://huggingface.co/youjunhyeok","description":"Translated glaiveai/reflection-v1 using nayohan/llama3-instrucTrans-enko-8b.\\nFor this dataset, we only used data that is 5000 characters or less in length and has language of English.\\nThanks for @Magpie-Align and @nayohan.\\n","first_N":5,"first_N_keywords":["text-generation","Korean","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_clinical_coding","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMr. Grammatical Ontology: Clinical Coding\\n\\t\\n\\nThis dataset was created from a motivation to train Medical Large Language Models for improved fluency in clinical \\ncoding, as measurable by MedConceptsQA, an open-source medical coding \\nevaluation benchmark designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts.  It was extracted from the Centers for Medicare & Medicaid Services' \\nInternational Classification of Diseases, Tenth Revision, Clinicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding.","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"dali","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liwei1987cn/dali","creator_name":"Levi li","creator_url":"https://huggingface.co/liwei1987cn","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tå¤§æŽè€å¸ˆé—®ç­”æ•°æ®é›†\\n\\t\\n\\nè¿™ä¸ªæ•°æ®é›†åŒ…å«å¤§æŽè€å¸ˆçš„é—®ç­”å¯¹è¯,ç”¨äºŽè®­ç»ƒå¯¹è¯æ¨¡åž‹ã€‚\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tæ•°æ®é›†æè¿°\\n\\t\\n\\n\\næ ¼å¼: JSONL\\nå­—æ®µ: \\ninstruction: å›ºå®šå€¼\\\"è¯·å¤§æŽè€å¸ˆå›žç­”\\\"\\ninput: æé—®å†…å®¹ \\noutput: å¤§æŽè€å¸ˆçš„å›žç­”\\n\\n\\næ•°æ®é‡: xxxæ¡å¯¹è¯æ•°æ®\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tä½¿ç”¨ç¤ºä¾‹\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"your-username/dataset-name\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tè®¸å¯è¯\\n\\t\\n\\nApache 2.0\\n","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"chat","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neverland-th/chat","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","description":"neverland-th/chat dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"scam_dialogues","keyword":"multi-turn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/adamtc/scam_dialogues","creator_name":"Adam Unknown","creator_url":"https://huggingface.co/adamtc","description":"adamtc/scam_dialogues dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","Vietnamese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"1M-OpenOrca_be","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be","creator_name":"Artsem Holub","creator_url":"https://huggingface.co/WiNE-iNEFF","description":"En/Be\\nðŸ‹ The Belarusian OpenOrca Dataset! ðŸ‹\\n\\n\\n\\nBelarusian OpenOrca dataset - is rich collection of augmented FLAN data aligns, that translated in belarusian language.\\nThat dataset should help training LLM in belarusian language and should help on other NLP tasks.\\nThis dataset have 2 version:\\n\\n~1M GPT-4 completions (Now translating)\\n~3.2M GPT-3.5 completions (Can be translated in future)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Fields\\n\\t\\n\\nThe fields are:\\n\\n'id', a unique numbered identifier which includes one of 'niv'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Spurline","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\\nThe 2024-10-30 version contains:\\n\\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-36k\\n\\t\\n\\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-36k\\n\\t\\n\\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"amazon-ml","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vinuuu/amazon-ml","creator_name":"Vinayak Goyal","creator_url":"https://huggingface.co/Vinuuu","description":"Vinuuu/amazon-ml dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Atma3.2-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma3.2-ShareGPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Atma4","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma4\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SWE-Bench-Verified-O1-reasoning-high-results","keyword":"multi-turn","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSWE-Bench Verified O1 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExecutive Summary\\n\\t\\n\\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities on the SWE-Bench Verified dataset, achieving a 28.8% success rate across 500 test instances.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset was generated using the CodeAct framework, which aims to improve code generation through enhanced action-basedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"english-to-colloquial-tamil","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jarvisvasu/english-to-colloquial-tamil","creator_name":"Vasanth Kumar","creator_url":"https://huggingface.co/jarvisvasu","description":"\\n\\t\\n\\t\\t\\n\\t\\tEnglish to Colloquial Tamil\\n\\t\\n\\n\\\"instruction\\\":\\\"Translate provided English text into colloquial Tamil.\\\"\\n\\\"input\\\": \\\"Their players played well.\\\"\\n\\\"output\\\": \\\"à®…à®µà®™à¯à®• players à®¨à®²à¯à®²à®¾ à®µà®¿à®³à¯ˆà®¯à®¾à®£à¯à®Ÿà®¾à®™à¯à®•.\\\"\\n\\n","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","Tamil","English"],"keywords_longer_than_N":true},
	{"name":"Atma3-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma3-ShareGPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Atma3-Share-GPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma3-Share-GPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\\n\\t\\n\\t\\t\\n\\t\\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\\n\\t\\n\\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\\nðŸŒ Homepage | Code | ðŸ¤— Paper | ðŸ“– arXiv\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInst-IT Dataset Overview\\n\\t\\n\\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"multiturn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\\nThis dataset focuses on challenging multi-turn conversations and contains:\\n\\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\\nThis dataset focuses on challenging multi-turn conversations and contains:\\n\\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Supernova","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Supernova","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Supernova is a dataset containing general synthetic chat data from the best available open-source models.\\nThe 2024-09-27 version contains:\\n\\n178.2k rows of synthetic chat responses generated using Llama 3.1 405b Instruct.\\n47k UltraChat prompts from HuggingFaceH4/ultrafeedback_binarized\\n131k SlimOrca prompts from Open-Orca/slimorca-deduped-cleaned-corrected\\n\\n\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"SkunkworksAI-reasoning-0.01-ko","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/youjunhyeok/SkunkworksAI-reasoning-0.01-ko","creator_name":"ìœ ì¤€í˜","creator_url":"https://huggingface.co/youjunhyeok","description":"SkunkworksAI/reasoning-0.01 ë°ì´í„°ì…‹ì„ nayohan/llama3-instrucTrans-enko-8b ëª¨ë¸ì„ ì‚¬ìš©í•´ ë²ˆì—­í–ˆìŠµë‹ˆë‹¤.\\nThanks for SkunkworksAI and nayohan.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tì›ë³¸\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\treasoning-0.01 subset\\n\\t\\n\\nsynthetic dataset of reasoning chains for a wide variety of tasks.\\nwe leverage data like this across multiple reasoning experiments/projects.\\nstay tuned for reasoning models and more data.\\nThanks to Hive Digital Technologies (https://x.com/HIVEDigitalTech) for their compute support in this project and beyond.\\n","first_N":5,"first_N_keywords":["text-generation","Korean","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"refactorchat","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BradMcDanel/refactorchat","creator_name":"Bradley McDanel","creator_url":"https://huggingface.co/BradMcDanel","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tModel Card\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tModel Details\\n\\t\\n\\n\\nDataset Name: RefactorChat\\nVersion: 1.0\\nDate: October 19, 2024\\nType: Multi-turn dialogue dataset for code refactoring and feature addition\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntended Use\\n\\t\\n\\n\\nPrimary Use: Evaluating and training large language models on incremental code development tasks\\nIntended Users: Researchers and practitioners in natural language processing and software engineering\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Composition\\n\\t\\n\\n\\nSize: 100 samples\\nStructure: Eachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BradMcDanel/refactorchat.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"OpenManus-RL","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for OpenManusRL\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\n\\n  ðŸ’» [Github Repo]\\n\\n\\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\\n\\nðŸ” ReAct Framework - Reasoning-Acting integration\\nðŸ§  Structured Training - Separate format/reasoning learning\\nðŸš« Anti-Hallucination - Negative samples + environment grounding\\nðŸŒ 6 Domains - OS, DB, Web, KG, Household, E-commerce\\n\\n\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nSourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-zh","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shibing624/alpaca-zh","creator_name":"Ming Xu (å¾æ˜Ž)","creator_url":"https://huggingface.co/shibing624","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-zh\\\"\\n\\t\\n\\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºŽGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \\nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage and License Notices\\n\\t\\n\\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset shouldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/alpaca-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"code-act","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xingyaoww/code-act","creator_name":"Xingyao Wang","creator_url":"https://huggingface.co/xingyaoww","description":" Executable Code Actions Elicit Better LLM Agents \\n\\n\\nðŸ’» Code\\nâ€¢\\nðŸ“ƒ Paper\\nâ€¢\\nðŸ¤— Data (CodeActInstruct)\\nâ€¢\\nðŸ¤— Model (CodeActAgent-Mistral-7b-v0.1)\\nâ€¢\\nðŸ¤– Chat with CodeActAgent!\\n\\n\\nWe propose to use executable Python code to consolidate LLM agentsâ€™ actions into a unified action space (CodeAct).\\nIntegrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations (e.g., code execution results) through multi-turnâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xingyaoww/code-act.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yahma/alpaca-cleaned","creator_name":"Gene Ruebsamen","creator_url":"https://huggingface.co/yahma","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yahma/alpaca-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-spanish","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bertin-project/alpaca-spanish","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBERTIN Alpaca Spanish\\n\\t\\n\\nThis dataset is a translation to Spanish of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\\n","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Alpaca-CoT","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QingyiSi/Alpaca-CoT","creator_name":"Qingyi Si","creator_url":"https://huggingface.co/QingyiSi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruction-Finetuning Dataset Collection (Alpaca-CoT)\\n\\t\\n\\nThis repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model.\\nWe also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT.  \\nIf you think this dataset collection is helpful to you, please likeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT.","first_N":5,"first_N_keywords":["English","Chinese","Malayalam","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-gpt4-data\\\"\\n\\t\\n\\nAll of the work is done by this team. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage and License Notices\\n\\t\\n\\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChinese Dataset\\n\\t\\n\\nFound here\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\n@article{peng2023gpt4llm,\\n    title={Instruction Tuning with GPT-4},\\n    author={Baolin Pengâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data-zh","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-gpt4-data-zh\\\"\\n\\t\\n\\nAll of the work is done by this team. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage and License Notices\\n\\t\\n\\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEnglish Dataset\\n\\t\\n\\nFound here\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\n@article{peng2023gpt4llm,\\n    title={Instruction Tuning with GPT-4},\\n    author={Baolin Pengâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"TSSB-3M-instructions","keyword":"instruct","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zirui3/TSSB-3M-instructions","creator_name":"zirui","creator_url":"https://huggingface.co/zirui3","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdata summary\\n\\t\\n\\ninstruction dataset for code bugfix\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReference\\n\\t\\n\\n[1]. TSSB-3M-ext\\n","first_N":5,"first_N_keywords":["code","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-ru","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\talpaca-cleaned-ru\\n\\t\\n\\nTranslated version of yahma/alpaca-cleaned into Russian.\\n","first_N":5,"first_N_keywords":["text-generation","translated","monolingual","yahma/alpaca-cleaned","Russian"],"keywords_longer_than_N":true},
	{"name":"unix-commands","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/harpomaxx/unix-commands","creator_name":"Carlos A. Catania (Harpo)","creator_url":"https://huggingface.co/harpomaxx","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUnix Commands Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe Unix Commands Dataset is a unique collection of real-world Unix command line examples, captured from various system prompts representing different user roles and responsibilities, such as system administrators, DevOps, network administrators, Docker administrators, regular users, and hackers.\\nThe dataset consists of Unix commands ranging from basic to advanced levels and from a wide array of categories, including fileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/harpomaxx/unix-commands.","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"C-Language-Chat-Debug-Multiturn-Zh","keyword":"multiturn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mxode/C-Language-Chat-Debug-Multiturn-Zh","creator_name":"Max Zhang","creator_url":"https://huggingface.co/Mxode","description":"çº¦ 1300 æ¡ C è¯­è¨€ åœºæ™¯çš„ user - assistant å¤šè½®å¯¹è¯ã€‚æ¯æ®µå¯¹è¯å·²ç»ç»„ç»‡æˆäº†å•è¡Œçš„æ ¼å¼ã€‚ä¸€æ¡æ ·ä¾‹å¦‚ä¸‹ï¼š\\n{\\n    \\\"id\\\": 1045,\\n    \\\"conversation\\\": [\\n        {\\n            \\\"user\\\": \\\"ä½ å¥½ï¼ŒAIåŠ©æ‰‹ã€‚æˆ‘æœ€è¿‘åœ¨å†™ä¸€ä¸ªCè¯­è¨€ç¨‹åºï¼Œä½†æ˜¯é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œå¸Œæœ›ä½ èƒ½å¸®æˆ‘æ£€æŸ¥ä¸€ä¸‹ã€‚\\\",\\n            \\\"assistant\\\": \\\"ä½ å¥½ï¼Œæˆ‘å¾ˆä¹æ„å¸®åŠ©ä½ ã€‚è¯·æŠŠä½ çš„ä»£ç å‘ç»™æˆ‘ï¼Œæˆ‘ä¼šå°½å¿«æ£€æŸ¥å¹¶ç»™å‡ºå»ºè®®ã€‚\\\"\\n        },\\n        {\\n            \\\"user\\\": \\\"å¥½çš„ï¼Œè¿™æ˜¯æˆ‘çš„ä»£ç ã€‚è¿™æ®µä»£ç çš„ä¸»è¦åŠŸèƒ½æ˜¯è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‰né¡¹ã€‚\\\",\\n            \\\"assistant\\\": \\\"è®©æˆ‘çœ‹ä¸€ä¸‹......å—¯ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå°é”™è¯¯ã€‚åœ¨ç¬¬10è¡Œï¼Œä½ åº”è¯¥ä½¿ç”¨`++i`è€Œä¸æ˜¯`i++`æ¥é€’å¢žiçš„å€¼ã€‚ä¿®æ”¹åŽçš„ä»£ç åº”è¯¥æ˜¯è¿™æ ·çš„\\\\\\\\n```c\\\\\\\\nfor (int i = 0; i < n; ++i) {\\\\\\\\n    if (i == 0 || i == 1) {\\\\\\\\nâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mxode/C-Language-Chat-Debug-Multiturn-Zh.","first_N":5,"first_N_keywords":["question-answering","Chinese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"RoleBench","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZenMoore/RoleBench","creator_name":"Zekun Moore Wang","creator_url":"https://huggingface.co/ZenMoore","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRoleBench\\n\\t\\n\\n\\nPaper Title: RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models\\narXiv Link: https://arxiv.org/abs/2310.00746\\nGithub Repo: https://github.com/InteractiveNLP-Team/RoleLLM-public\\n\\nPlease read our paper for more details about this dataset.\\nTL;DR: We introduce RoleLLM, a role-playing framework of data construction and evaluation (RoleBench), as well as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZenMoore/RoleBench.","first_N":5,"first_N_keywords":["Chinese","English","apache-2.0","Text","arxiv:2310.00746"],"keywords_longer_than_N":true},
	{"name":"shell-cmd-instruct","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/byroneverson/shell-cmd-instruct","creator_name":"Byron Everson","creator_url":"https://huggingface.co/byroneverson","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsed to train models that interact directly with shells\\n\\t\\n\\nNote: This dataset is out-dated in the llm world, probably easier to just setup a tool with a decent model that supports tooling.\\nFollow-up details of my process \\n\\nMacOS terminal commands for now. This dataset is still in alpha stages and will be modified.\\n\\nContains 500 somewhat unique training examples so far.\\n\\nGPT4 seems like a good candidate for generating more data, licensing would need to be addressed.\\n\\nI fine-tunedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/byroneverson/shell-cmd-instruct.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Mantis-Instruct","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMantis-Instruct\\n\\t\\n\\nPaper | Website | Github | Models | Demo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSummaries\\n\\t\\n\\nMantis-Instruct is a fully text-image interleaved multimodal instruction tuning dataset, \\ncontaining 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding. \\nIt's been used to train Mantis Model families\\n\\nMantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\\nAmong theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"medical","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bohu/medical","creator_name":"tang","creator_url":"https://huggingface.co/bohu","description":"From https://huggingface.co/datasets/shibing624/medical\\n","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"RLSTACK","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK.","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-V1.2","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"ru-instruct","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tÐšÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°\\n\\t\\n\\nÐ¡ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð¸Ð· Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ñ… Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð¾Ð², Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸. ÐžÑ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð² Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° (ÑÐ¿Ð°ÑÐ¸Ð±Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Den4ikAI/nonsense_gibberish_detector). Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½ SimHash'Ð¾Ð¼.\\nÐžÐ±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð¹ Ð½Ð° Ð½Ñ‘Ð¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð·Ð°Ð²Ñ‘Ð·, in progress.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tÐ¡Ð¾ÑÑ‚Ð°Ð²\\n\\t\\n\\nÐ¡Ð¾Ð±Ñ€Ð°Ð» Ð¸Ð· ÑÑ‚Ð¸Ñ… Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ñ…:\\n\\nd0rj/OpenOrca-ru (Ð¾Ñ‚ Open-Orca/OpenOrca)\\nd0rj/OpenHermes-2.5-ru (Ð¾Ñ‚ teknium/OpenHermes-2.5)\\nd0rj/dolphin-ru (Ð¾Ñ‚ ehartford/dolphin)\\nd0rj/alpaca-cleaned-ru (Ð¾Ñ‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","machine-generated","found","translated"],"keywords_longer_than_N":true},
	{"name":"MMC","keyword":"instruction","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xywang1/MMC","creator_name":"Xiaoyang Wang","creator_url":"https://huggingface.co/xywang1","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMC: Advancing Multimodal Chart Understanding with LLM Instruction Tuning\\n\\t\\n\\nThis repo releases data introduced in our paper MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning.\\n\\nThe paper was published in NAACL 2024.\\nSee our GithHub repo for demo code and more.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHighlights\\n\\t\\n\\n\\nWe introduce a large-scale MultiModal Chart Instruction (MMC-Instruction) dataset supporting diverse tasks and chart types. Leveraging this data.\\nWe also propose aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xywang1/MMC.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-sa-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"KoMT-Bench","keyword":"instruction-following","license":"GNU Lesser General Public License v3.0","license_url":"https://choosealicense.com/licenses/lgpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench","creator_name":"LG AI Research","creator_url":"https://huggingface.co/LGAI-EXAONE","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKoMT-Bench\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nWe present KoMT-Bench, a benchmark designed to evaluate the capability of language models in following instructions in Korean.\\nKoMT-Bench is an in-house dataset created by translating MT-Bench [1]  dataset into Korean and modifying some questions to reflect the characteristics and cultural nuances of the Korean language.\\nAfter the initial translation and modification, we requested expert linguists to conduct a thorough review of ourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench.","first_N":5,"first_N_keywords":["question-answering","Korean","lgpl-3.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"wangchanx-seed-free-synthetic-instruct-thai-120k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k","creator_name":"VISTEC-depa AI Research Institute of Thailand","creator_url":"https://huggingface.co/airesearch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WangchanX Seed-Free Synthetic Instruct Thai 120k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains about 120k synthetic instruction-following samples in Thai, generated using a novel seed-free approach. It covers a wide range of domains derived from Wikipedia, including both general knowledge and Thai-specific cultural topics. The dataset is designed for instruction-tuning Thai language models to improve their ability to understand and generate Thai text inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k.","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","Thai","English"],"keywords_longer_than_N":true},
	{"name":"multi-turn_jailbreak_attack_datasets","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tom-gibbs/multi-turn_jailbreak_attack_datasets","creator_name":"Tom Gibbs","creator_url":"https://huggingface.co/tom-gibbs","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMulti-Turn Jailbreak Attack Datasets\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis dataset was created to compare single-turn and multi-turn jailbreak attacks on large language models (LLMs). The primary goal is to take a single harmful prompt and distribute the harm over multiple turns, making each prompt appear harmless in isolation. This approach is compared against traditional single-turn attacks with the complete prompt to understand their relative impacts and failure modes. The keyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tom-gibbs/multi-turn_jailbreak_attack_datasets.","first_N":5,"first_N_keywords":["English","mit","1K<n<10K","arxiv:2409.00137","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Creative_Writing_Multiturn","keyword":"multiturn","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Dampfinchen/Creative_Writing_Multiturn","creator_name":"DÃ¤mpfchen","creator_url":"https://huggingface.co/Dampfinchen","description":"This is a dataset merge of many, many high quality story writing / roleplaying datasets across all of Huggingface. I've filtered specifically for samples with high turns, which is a key different to already available datasets. My goal is to improve the model's ability to recollect and mention details from far back even at a longer context and more importantly, also improve the model's ability to output engaging verbose storylines, reduce certain phrases, increase creativity and reduce dryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dampfinchen/Creative_Writing_Multiturn.","first_N":5,"first_N_keywords":["text2text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"reflection-v1-ru_subset","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\td0rj/reflection-v1-ru_subset\\n\\t\\n\\nTranslated glaiveai/reflection-v1 dataset into Russian language using GPT-4o.\\n\\nAlmost all the rows of the dataset have been translated. I have removed those translations that do not match the original by the presence of the tags \\\"thinking\\\", \\\"reflection\\\" and \\\"output\\\". Mapping to the original dataset rows can be taken from the \\\"index\\\" column.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nimport datasets\\n\\n\\ndata = datasets.load_dataset(\\\"d0rj/reflection-v1-ru_subset\\\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","multilingual","glaiveai/reflection-v1"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-indonesian","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian","creator_name":"Ilham Fadhil","creator_url":"https://huggingface.co/ilhamfadheel","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ¦™ðŸ› Cleaned Alpaca Dataset (INDONESIAN)\\n\\t\\n\\nWelcome to the Cleaned Alpaca Dataset repository! This repository hosts a cleaned and curated version of a dataset used to train the Alpaca LLM (Large Language Model). The original dataset had several issues that are addressed in this cleaned version.\\nOn April 8, 2023 the remaining uncurated instructions (~50,000) were replaced with data from the GPT-4-LLM dataset. Curation of the incoming GPT-4 data is ongoing.\\n\\nA 7b Lora model (trainedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian.","first_N":5,"first_N_keywords":["text-generation","Indonesian","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Leopard-Instruct","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wyu1/Leopard-Instruct","creator_name":"Wenhao Yu","creator_url":"https://huggingface.co/wyu1","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLeopard-Instruct\\n\\t\\n\\nPaper | Github | Models-LLaVA | Models-Idefics2\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSummaries\\n\\t\\n\\nLeopard-Instruct is a large instruction-tuning dataset, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. It's been used to train Leopard-LLaVA [checkpoint]  and Leopard-Idefics2 [checkpoint].\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLoading dataset\\n\\t\\n\\n\\nto load the dataset without automatically downloading and process the images (Please run the following codes withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wyu1/Leopard-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Fineweb-Instruct","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Fineweb-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"We convert the pre-training corpus from Fineweb-Edu (https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) to instruction following format. We select a subset with quality filter and then use GPT-4 to extract instruction-following pairs. The dataset contains roughly 16M instruction pairs. The basic concept is similar to MAmmoTH2 (https://arxiv.org/abs/2405.03548). \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use dataset useful, please cite the following paper:\\n@article{yue2024mammoth2â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Fineweb-Instruct.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10M - 100M","json"],"keywords_longer_than_N":true},
	{"name":"metallurgy-qa","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Abdulrhman37/metallurgy-qa","creator_name":"Eldeeb","creator_url":"https://huggingface.co/Abdulrhman37","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMetallurgy and Materials Science Knowledge Extraction Dataset\\n\\t\\n\\nThis repository contains a dataset generated from parsed books related to various aspects of metallurgy, materials science, and engineering. The dataset is designed for fine-tuning Large Language Models (LLMs) for Question-Answering (QA) tasks in the domain of metallurgy and materials science.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThe dataset includes content derived from technical books in the field of metallurgy and materialsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Abdulrhman37/metallurgy-qa.","first_N":5,"first_N_keywords":["text2text-generation","question-answering","closed-domain-qa","closed-book-qa","machine-generated"],"keywords_longer_than_N":true},
	{"name":"mauxitalk-persian","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxitalk-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ—£ï¸ MauxiTalk: High-Quality Persian Conversations Dataset ðŸ‡®ðŸ‡·\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ“ Description\\n\\t\\n\\nMauxiTalk is a high-quality dataset of 2,000+ Persian conversations, carefully translated from the SmolTalk dataset using state-of-the-art language models. This dataset is specifically curated for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸŒŸ Key Features\\n\\t\\n\\n\\n2,000 natural conversations in Persian\\nDiverse topicsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxitalk-persian.","first_N":5,"first_N_keywords":["text-generation","summarization","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ATC-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ATC-ShareGPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lots_of_datasets_for_ai_v3","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ReallyFloppyPenguin/lots_of_datasets_for_ai_v3","creator_name":"Gurvaah Singh","creator_url":"https://huggingface.co/ReallyFloppyPenguin","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset is for Training LLMs From Scratch!\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More Information Needed]\\nPaper [optional]: [More Information Needed]\\nDemoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ReallyFloppyPenguin/lots_of_datasets_for_ai_v3.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\\n\\t\\n\\t\\t\\n\\t\\tThe Degeneration of the Nation Multilingual Dataset\\n\\t\\n\\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\\n\\t\\n\\t\\t\\n\\t\\tThe Degeneration of the Nation Multilingual Dataset\\n\\t\\n\\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"OpenCharacter","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xywang1/OpenCharacter","creator_name":"Xiaoyang Wang","creator_url":"https://huggingface.co/xywang1","description":"\\n\\t\\n\\t\\t\\n\\t\\tOpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas\\n\\t\\n\\nThis repo releases data introduced in our paper OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas in arXiv.\\n\\nWe study customizable role-playing dialogue agents in large language models (LLMs).\\nWe tackle the challenge with large-scale data synthesis: character synthesis and character-driven reponse synthesis.\\nOur solution strengthens the original LLaMA-3â€¦ See the full description on the dataset page: https://huggingface.co/datasets/xywang1/OpenCharacter.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Magpie-COT","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/project-r/Magpie-COT","creator_name":"Project R","creator_url":"https://huggingface.co/project-r","description":"\\n\\t\\n\\t\\t\\n\\t\\tMagpie-COT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nCombined Chain-of-Thought dataset containing three sources:\\n\\nMagpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B\\nMagpie-Reasoning-V2-250K-CoT-QwQ\\nMagpie-Reasoning-V2-250K-CoT-Skywork-O1-Llama-3.1-8B\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tKey Enhancements:\\n\\t\\n\\n\\nAdded model source tracking column\\nProcessed Deepseek responses to extract  tag content\\nUnified format across multiple CoT datasets\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tProcessing Steps:\\n\\t\\n\\n\\nKept 'instruction' and 'response' columns\\nAddedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/project-r/Magpie-COT.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\\n\\t\\n\\t\\t\\n\\t\\tDeepthink Reasoning Demo\\n\\t\\n\\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\\n\\n\\t\\n\\t\\t\\n\\t\\tFeatures\\n\\t\\n\\n\\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","text-retrieval","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"StructFlowBench","keyword":"multi-turn","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Jinnan/StructFlowBench","creator_name":"Jinnan Li","creator_url":"https://huggingface.co/Jinnan","description":"\\n\\t\\n\\t\\t\\n\\t\\tStructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following\\n\\t\\n\\n\\n  \\n    ðŸ“ƒ Paper\\n  \\n  â€¢\\n  \\n    ðŸ¤— Dataset\\n  \\n  â€¢\\n  \\n    ðŸ–¥ï¸ Code\\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t1. Updates\\n\\t\\n\\n\\n2025/02/26: We enhanced the code documentation on GitHub with detailed implementation guidelines.\\n2025/02/24: We submitted our paper to Hugging Face's Daily Papers.\\n2025/02/23: We released StructFlowBench dataset on huggingface.\\n2025/02/20: We released the first version of our paper along with the dataset andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jinnan/StructFlowBench.","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Electrical-engineering-ru","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Electrical-engineering-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"Translated instructions from STEM-AI-mtl/Electrical-engineering into Russian using gemini-flash-1.5-8b.\\n","first_N":5,"first_N_keywords":["text2text-generation","text-generation","translated","STEM-AI-mtl/Electrical-engineering","Russian"],"keywords_longer_than_N":true},
	{"name":"helpful_instructions","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Helpful Instructions is a dataset of (prompt, completion) pairs that are derived from a variety of public datasets. As the name suggests, it focuses on instructions that are \\\"helpful\\\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"instruct_me","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/instruct_me","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Instruct Me is a dataset of instruction-like dialogues between a human user and AI assistant. The prompts are derived from (prompt, completion) pairs in the Helpful Instructions dataset. The goal is to train a language model to that is \\\"chatty\\\" and can answer the kind of questions or tasks a human user might instruct an AI assistant to perform.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"norwegian-alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NbAiLab/norwegian-alpaca","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNB Alpaca Norwegian BokmÃ¥l\\n\\t\\n\\nThis dataset is a translation to Norwegian BokmÃ¥l of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\\n","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"vi_alpaca_clean","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tsdocode/vi_alpaca_clean","creator_name":"Thanh Sang","creator_url":"https://huggingface.co/tsdocode","description":"tsdocode/vi_alpaca_clean dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Vietnamese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"translated_polish_alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aspik101/translated_polish_alpaca","creator_name":"Hubert","creator_url":"https://huggingface.co/Aspik101","description":"The dataset was translated into Polish using this model: \\\"gsarti/opus-mt-tc-en-pl\\\"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to use\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"Aspik101/translated_polish_alpaca\\\")\\n\\n","first_N":5,"first_N_keywords":["text-generation","Polish","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"alpaca-id-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cahya/alpaca-id-cleaned","creator_name":"Cahya Wirawan","creator_url":"https://huggingface.co/cahya","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Indonesian Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is the Indonesian translated version of the cleaned original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cahya/alpaca-id-cleaned.","first_N":5,"first_N_keywords":["text-generation","Indonesian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"AlpacaDataCleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexl83/AlpacaDataCleaned","creator_name":"Alessandro Lannocca","creator_url":"https://huggingface.co/alexl83","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alexl83/AlpacaDataCleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"NorPaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MasterThesisCBS/NorPaca","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNorPaca Norwegian BokmÃ¥l\\n\\t\\n\\nThis dataset is a translation to Norwegian BokmÃ¥l of alpaca_gpt4_data.json, a clean version of the Alpaca dataset made at Stanford, but generated with GPT4.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPrompt to generate dataset\\n\\t\\n\\n    Du blir bedt om Ã¥ komme opp med et sett med 20 forskjellige oppgaveinstruksjoner. Disse oppgaveinstruksjonene vil bli gitt til en GPT-modell, og vi vil evaluere GPT-modellen for Ã¥ fullfÃ¸re instruksjonene. \\n\\nHer er kravene:\\n1. PrÃ¸v Ã¥ ikke gjenta verbet forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MasterThesisCBS/NorPaca.","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"NorEval","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MasterThesisCBS/NorEval","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNorEval\\n\\t\\n\\nNorEval is a self-curated dataset to evaluate instruction-following LLMs, seeking to evaluate the models in nine categories: Language, Code, Mathematics, Classification, Communication & Marketing, Medical, General Knowledge, and Business Operations\\n","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ualpaca-gpt4","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nikes64/ualpaca-gpt4","creator_name":"MrNikes","creator_url":"https://huggingface.co/nikes64","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-gpt4-cleaned\\\"\\n\\t\\n\\nThis dataset contains Ukrainian Instruction-Following translated by facebook/nllb-200-3.3B\\nThe dataset was originaly shared in this repository: https://github.com/tloen/alpaca-lora\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicensing Information\\n\\t\\n\\nThe dataset is available under the Creative Commons NonCommercial (CC BY-NC 4.0).\\n","first_N":5,"first_N_keywords":["text-generation","question-answering","Ukrainian","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-tr","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cgulse/alpaca-cleaned-tr","creator_name":"Can G","creator_url":"https://huggingface.co/cgulse","description":"Alpaca Cleaned Dataset.\\nMachine Translated facebook/nllb-200-3.3B\\nLanguages\\nTurkish\\n","first_N":5,"first_N_keywords":["Turkish","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"tasksource-instruct-v0\\\" (TSI)\\n\\t\\n\\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\\nDataset size is capped at 30k examples per task to foster task diversity.\\n!pip install tasksource, pandit\\nimport tasksource, pandit\\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\\nfor tasks in df.id:\\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\\n\\nhttps://github.com/sileod/tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","first_N":5,"first_N_keywords":["text2text-generation","text-generation","text-classification","token-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"tasksource-instruct-v0\\\" (TSI)\\n\\t\\n\\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\\nDataset size is capped at 30k examples per task to foster task diversity.\\n!pip install tasksource, pandit\\nimport tasksource, pandit\\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\\nfor tasks in df.id:\\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\\n\\nhttps://github.com/sileod/tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","first_N":5,"first_N_keywords":["text2text-generation","text-generation","text-classification","token-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruct-Aira Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of prompts and responses to those prompts. All completions were generated by querying already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc.). The dataset is available in Portuguese, English, and Spanish.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\\n\\nLanguage modeling.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","Spanish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"reward-aira-dataset","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReward-Aira Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of prompt + completion examples of LLM following instructions in a conversational manner. All prompts come with two possible completions (one better than the other). The dataset is available in both Portuguese and English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized to train a reward/preference model or DPO fine-tuning.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset.","first_N":5,"first_N_keywords":["text-classification","Portuguese","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","first_N":5,"first_N_keywords":["text2text-generation","text-classification","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","first_N":5,"first_N_keywords":["text2text-generation","text-classification","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"VisIT-Bench","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/VisIT-Bench","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for VisIT-Bench\\n\\t\\n\\n\\nDataset Description\\nLinks\\nDataset Structure\\nData Fields\\nData Splits\\nData Loading\\n\\n\\nLicensing Information\\nAnnotations\\nConsiderations for Using the Data\\nCitation Information\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nVisIT-Bench is a dataset and benchmark for vision-and-language instruction following. The dataset is comprised of image-instruction pairs and corresponding example outputs, spanning a wide range of tasks, from simple object recognition toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/VisIT-Bench.","first_N":5,"first_N_keywords":["crowdsourced","found","original","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-serbian-full","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full","creator_name":"Dean","creator_url":"https://huggingface.co/datatab","description":"\\n\\t\\n\\t\\t\\n\\t\\tSerbian Alpaca Cleaned Dataset\\n\\t\\n\\n\\nOriginal Repository: https://github.com/gururise/AlpacaDataCleaned\\nOriginal HF Repository: https://huggingface.co/datasets/yahma/alpaca-cleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a serbian cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full.","first_N":5,"first_N_keywords":["text-generation","Serbian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"OpenOrca-ru","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/OpenOrca-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenOrca-ru\\n\\t\\n\\nThis is translated version of Open-Orca/OpenOrca into Russian.\\n","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"dolphin-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/dolphin-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDolphin-ru ðŸ¬\\n\\t\\n\\nThis is translated version of ehartford/dolphin into Russian.\\n","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"gt-doremiti-instructions","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Gt-Doremiti/gt-doremiti-instructions","creator_name":"Doremiti","creator_url":"https://huggingface.co/Gt-Doremiti","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for gt-doremiti-instructions\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nJeu d'instruction pour fine-tuner un LLM suivant les prÃ©conisations du projet Stanford-Alpaca (https://github.com/tatsu-lab/stanford_alpaca)\\nCes instructions sont extraites de la FAQ crÃ©e par le GT DOREMITI et disponible Ã  cette adresse (https://gt-atelier-donnees.miti.cnrs.fr/faq.html)\\nLes donnÃ©es sont mise Ã  disposition selon les termes de la Licence Creative Commons Attribution 4.0 International.\\n","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-tw-input-output-52k","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DavidLanz/alpaca-tw-input-output-52k","creator_name":"David Lanz","creator_url":"https://huggingface.co/DavidLanz","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-tw-input-output-52k\\\"\\n\\t\\n\\nThis dataset contains English Instruction-Following generated by GPT-3.5 using Alpaca prompts for fine-tuning LLMs.\\nThe dataset was originaly shared in this repository: https://github.com/ntunlplab/traditional-chinese-alpaca. This is just a wraper for compatibility with huggingface's datasets library.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset structure\\n\\t\\n\\nIt contains 52K instruction-following data generated by GPT-3.5 using the same prompts as in Alpaca.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/DavidLanz/alpaca-tw-input-output-52k.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-tw-input-output-48k","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DavidLanz/alpaca-gpt4-tw-input-output-48k","creator_name":"David Lanz","creator_url":"https://huggingface.co/DavidLanz","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-gpt4-tw-input-output-48k\\\"\\n\\t\\n\\nThis dataset contains English Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.\\nThe dataset was originaly shared in this repository: https://github.com/ntunlplab/traditional-chinese-alpaca. This is just a wraper for compatibility with huggingface's datasets library.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset structure\\n\\t\\n\\nIt contains 52K instruction-following data generated by GPT-4 using the same prompts as in Alpaca.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/DavidLanz/alpaca-gpt4-tw-input-output-48k.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"fund-sft","keyword":"instruction","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jannko/fund-sft","creator_name":"ko","creator_url":"https://huggingface.co/jannko","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jannko/fund-sft.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"ultrachat_200k_dutch","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for UltraChat 200k Dutch\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024},\\n      eprint={2412.04092},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.04092}, \\n}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"han-instruct-dataset-v1.0","keyword":"instruction-following","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"han-instruct-dataset-v1.0\\\"\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nðŸª¿ Han (à¸«à¹ˆà¸²à¸™ or goose) Instruct Dataset is a Thai instruction dataset by PyThaiNLP. It collect the instruction following in Thai from many source.\\nMany question are collect from Reference desk at Thai wikipedia.\\nData sources:\\n\\nReference desk at Thai wikipedia.\\nLaw from justicechannel.org\\npythainlp/final_training_set_v1_enth: Human checked and edited.\\nSelf-instruct from WangChanGLM\\nWannaphong.com\\nHumanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"han-instruct-dataset-v1.0","keyword":"instruction-finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"han-instruct-dataset-v1.0\\\"\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nðŸª¿ Han (à¸«à¹ˆà¸²à¸™ or goose) Instruct Dataset is a Thai instruction dataset by PyThaiNLP. It collect the instruction following in Thai from many source.\\nMany question are collect from Reference desk at Thai wikipedia.\\nData sources:\\n\\nReference desk at Thai wikipedia.\\nLaw from justicechannel.org\\npythainlp/final_training_set_v1_enth: Human checked and edited.\\nSelf-instruct from WangChanGLM\\nWannaphong.com\\nHumanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"urdu_alpaca_yc_filtered","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered","creator_name":"Mahwiz Khalil","creator_url":"https://huggingface.co/mahwizzzz","description":"\\n\\t\\n\\t\\t\\n\\t\\tAlpaca Urdu\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDescription\\n\\t\\n\\nThe Alpaca Urdu  is a translation of the original dataset into Urdu. This dataset is a part of the Alpaca project and is designed for NLP tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nSize: The translated dataset contains [45,622] samples.\\nLanguages: Urdu\\nLicense: [cc-by-4.0]\\nOriginal Dataset: Link to the original Alpaca Cleaned dataset repository\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tColumns\\n\\t\\n\\nThe translated dataset includes the following columns:\\n\\ninput: The input text in Urdu.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered.","first_N":5,"first_N_keywords":["text-generation","Urdu","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v3","keyword":"instruction","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruct-Aira Dataset version 3.0\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of multi-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-72k\\n\\t\\n\\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-72k\\n\\t\\n\\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-12k\\n\\t\\n\\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-12k\\n\\t\\n\\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"LONGCOT-Alpaca","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for LONGCOT-Alpaca\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_anatomist","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMr. Grammatical Ontology: Anatomist\\n\\t\\n\\nThis dataset was extracted from the Web Ontology Language (OWL) \\nrepresentation of the Foundational Model of Anatomy [1] using \\nOwlready2 to facilitate the extraction of the logical axioms in the FMA\\ninto a Controlled Natural Language (CNL) for use in training Medical Large Language Models to be conversant in \\ncanonical human anatomy.\\nSo, Description Logic expressions such as the following in FMA about fma13245 (Surface of left ventricle):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist.","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ProcessedOpenAssistant","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant","creator_name":"Y. Yu","creator_url":"https://huggingface.co/PursuitOfDataScience","description":"\\n\\t\\n\\t\\t\\n\\t\\tRefined OASST1 Conversations\\n\\t\\n\\nDataset Name on Hugging Face: PursuitOfDataScience/ProcessedOpenAssistant\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is derived from the OpenAssistant/oasst1 conversations, with additional processing to:\\n\\nRemove single-turn or incomplete conversations (where a prompter/user message had no assistant reply),\\nRename roles from \\\"prompter\\\" to \\\"User\\\" and \\\"assistant\\\" to \\\"Assistant\\\",\\nOrganize each conversation as a list of turn objects.\\n\\nThe goal is to provide a cleanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Bangla-Instruct","keyword":"instruction","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/md-nishat-008/Bangla-Instruct","creator_name":"Nishat Raihan","creator_url":"https://huggingface.co/md-nishat-008","description":"\\nBangla-Instruct\\nState-of-the-art Bangla Instruction Dataset\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTigerLLM introduces a state-of-the-art dataset designed to advance Bangla language modeling. The Bangla-Instruct dataset contains high-quality native Bangla instruction-response pairs that have been generated using cutting-edge teacher models.\\n\\n\\n\\n\\nOverview\\n\\n\\n\\nThe Bangla-Instruct dataset is composed of 100,000 instruction-response pairs. It starts with 500 seed tasks created by 50 volunteer experts from premier Bangladeshiâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/md-nishat-008/Bangla-Instruct.","first_N":5,"first_N_keywords":["text-generation","Bengali","mit","10K - 100K","csv"],"keywords_longer_than_N":true}
]
;
