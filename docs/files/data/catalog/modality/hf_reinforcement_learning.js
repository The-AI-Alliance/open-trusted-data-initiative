const data_for_modality_reinforcement_learning = 
[
	{"name":"H4rmony_dpo","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neovalle/H4rmony_dpo","creator_name":"Jorge Vallego","creator_url":"https://huggingface.co/neovalle","description":"This dataset is based on neovalle/H4rmony, and optimised to the format required by DPOTrainer from the trl library.\\n"},
	{"name":"itorca_dpo_vi","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lamhieu/itorca_dpo_vi","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tStructure\\n\\t\\n\\nView online through viewer.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNote\\n\\t\\n\\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/itorca_dpo_vi."},
	{"name":"itorca_dpo_en","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lamhieu/itorca_dpo_en","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tStructure\\n\\t\\n\\nView online through viewer.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNote\\n\\t\\n\\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/itorca_dpo_en."},
	{"name":"MsitralTrix-test-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/MsitralTrix-test-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/MsitralTrix-test-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"CultriX-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/CultriX-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/CultriX-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"LunarLander-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/LunarLander-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLunarLander-v2 - Imitation Learning Datasets\\n\\t\\n\\nThis is a dataset created by Imitation Learning Datasets project. \\nIt was created by using Stable Baselines weights from a PPO policy from HuggingFace.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 1,000 episodes with an average episodic reward of 500.\\nEach entry consists of:\\nobs (list): observation with length 8.\\naction (int): action (0, 1, 2 and 3).\\nreward (float): reward point for that timestep.\\nepisode_returns (bool): if thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/LunarLander-v2."},
	{"name":"HelpSteer_binarized","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sablo/HelpSteer_binarized","creator_name":"Sablo AI","creator_url":"https://huggingface.co/sablo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBinarized version of HelpSteer\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nA binarized version of https://huggingface.co/datasets/nvidia/HelpSteer ready for DPO using https://github.com/huggingface/alignment-handbook or similar.\\nFor each unique prompt, we take the best and worst scoring (average of helpfulness and correctness) responses. These are converted into MessagesList format in the 'chosen' and 'rejected' columns.\\n\\nCreated by: dctanner and the team at Sablo AI\\nLicense: CC BY 4.0\\n\\n"},
	{"name":"beyond_dpo_en","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lamhieu/beyond_dpo_en","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tStructure\\n\\t\\n\\nView online through viewer.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNote\\n\\t\\n\\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/beyond_dpo_en."},
	{"name":"beyond_dpo_vi","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lamhieu/beyond_dpo_vi","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tStructure\\n\\t\\n\\nView online through viewer.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNote\\n\\t\\n\\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/beyond_dpo_vi."},
	{"name":"okapi-ranking","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Saugatkafley/okapi-ranking","creator_name":"Saugat Kafley","creator_url":"https://huggingface.co/Saugatkafley","description":"Saugatkafley/okapi-ranking dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"klexikon_dpo","keyword":"dpo","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/udkai/klexikon_dpo","creator_name":"UDK dot AI","creator_url":"https://huggingface.co/udkai","description":"Version of https://huggingface.co/datasets/dennlinger/klexikon which can be useful for Direct Preference Optimization of large language models generating sentences in simple german.\\n"},
	{"name":"hh-rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/betteruncensored/hh-rlhf","creator_name":"Better Uncensored","creator_url":"https://huggingface.co/betteruncensored","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for HH-RLHF Better Uncensored\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBetter Uncensored Summary\\n\\t\\n\\nThis is the Better Uncensored version of the famous Anthropic preference dataset Anthropic/hh-rlhf\\nOnly the train files were processed with the rlhf uncensor script in this manner:\\nfind ../hh-rlhf/ -type f -name 'train.jsonl' | xargs -I {} python uncensor_rlhf.py --in-file {}\\n\\nThis should work as a drop in replacement of the original dataset for training uncensored models. About 10% to 25% of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/betteruncensored/hh-rlhf."},
	{"name":"intel_orca_dpo_pairs_de","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de","creator_name":"Mayflower GmbH","creator_url":"https://huggingface.co/mayflowergmbh","description":"German translation of Intel/orca_dpo_pairs\\nUsing azureml for translation and hermeo-7b for rejected answers.\\n"},
	{"name":"Mistral-EN-DPO-9K","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kyujinpy/Mistral-EN-DPO-9K","creator_name":"KyujinHan","creator_url":"https://huggingface.co/kyujinpy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Mistral-EN-DPO-9K\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInfo\\n\\t\\n\\nWe used snorkelai/Snorkel-Mistral-PairRM-DPO-Dataset dataset.We selected train_iteration_1 part.  \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPre-processing\\n\\t\\n\\n\\nRemove coding task\\n\\nFiltering words: ['[Latex]', 'java', 'SQL', 'C#', 'nextjs', 'react', 'Ruby', 'Lua', 'Unity', 'XML', 'qrcode', 'jest', 'const', \\n                    'python', 'Python', 'R code', 'Next.js', 'Node.js', 'Typescript', 'HTML', 'php', 'skeleton code', \\n                    'MATLAB'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/kyujinpy/Mistral-EN-DPO-9K."},
	{"name":"lave-human-feedback","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mair-lab/lave-human-feedback","creator_name":"MAIR Lab","creator_url":"https://huggingface.co/mair-lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLAVE human judgments\\n\\t\\n\\nThis repository contains the human judgment data for Improving Automatic VQA Evaluation Using Large Language Models. Details about the data collection process and crowdworker population can be found in our paper, specifically in section 5.2 and appendix A.1.\\nFields:\\n\\ndataset: VQA dataset of origin for this example (vqav2, vgqa, okvqa).\\nmodel: VQA model that generated the predicted answer (blip2, promptcap, blip_vqa, blip_vg).\\nqid: question ID coming from theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mair-lab/lave-human-feedback."},
	{"name":"ChatML-H4rmony_dpo","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Felladrin/ChatML-H4rmony_dpo","creator_name":"Victor Nogueira","creator_url":"https://huggingface.co/Felladrin","description":"neovalle/H4rmony_dpo in ChatML format, ready to use in HuggingFace TRL's DPO Trainer.\\nPython code used for conversion:\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"neovalle/H4rmony_dpo\\\", split=\\\"train\\\")\\n\\ndef format(columns):\\n    return {\\n        \\\"prompt\\\": f\\\"<|im_start|>user\\\\n{columns['prompt']}<|im_end|>\\\\n<|im_start|>assistant\\\\n\\\",\\n        \\\"chosen\\\": f\\\"{columns['chosen']}<|im_end|>\\\",\\n        \\\"rejected\\\": f\\\"{columns['rejected']}<|im_end|>\\\",\\n    }â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Felladrin/ChatML-H4rmony_dpo."},
	{"name":"en_vi_DPO","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gallantVN/en_vi_DPO","creator_name":"Nhut Tien","creator_url":"https://huggingface.co/gallantVN","description":"gallantVN/en_vi_DPO dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"tak-stack-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CorticalStack/tak-stack-dpo","creator_name":"JP Boyd","creator_url":"https://huggingface.co/CorticalStack","description":"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\ttak-stack-dpo ðŸ§ \\n\\t\\n\\nA DPO alignment dataset for fine tuning open-source LLMs, taking sample preference pairs from a variety of datasets for diversity.  \\nPrepared in the \\\"standard\\\" instruction, chosen, and rejected format, with a source feature indicating from which dataset the row was extracted.\\nSource datasets:\\n\\nargilla/distilabel-math-preference-dpo\\njondurbin/truthy-dpo-v0.1\\nargilla/distilabel-intel-orca-dpo-pairs\\nargilla/OpenHermes2.5-dpo-binarized-alpha\\n\\n"},
	{"name":"pandora-rlhf","keyword":"dpo","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-rlhf","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPandora RLHF\\n\\t\\n\\nA Reinforcement Learning from Human Feedback (RLHF) dataset for Direct Preference Optimization (DPO) fine-tuning of the Pandora Large Language Model (LLM).\\nThe dataset is based on the anthropic/hh-rlhf dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright and license\\n\\t\\n\\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\\nProject developed under a BSD-3-Clause license.\\n"},
	{"name":"pandora-rlhf","keyword":"rlhf","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-rlhf","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPandora RLHF\\n\\t\\n\\nA Reinforcement Learning from Human Feedback (RLHF) dataset for Direct Preference Optimization (DPO) fine-tuning of the Pandora Large Language Model (LLM).\\nThe dataset is based on the anthropic/hh-rlhf dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright and license\\n\\t\\n\\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\\nProject developed under a BSD-3-Clause license.\\n"},
	{"name":"argilla-dpo-mix-7k-arabic","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/medmac01/argilla-dpo-mix-7k-arabic","creator_name":"Mohammed Machrouh","creator_url":"https://huggingface.co/medmac01","description":"medmac01/argilla-dpo-mix-7k-arabic dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"argilla-dpo-mix-7k-arabic","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/argilla-dpo-mix-7k-arabic","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"argilla-dpo-mix-7k-arabic\\\"\\n\\t\\n\\nMore Information needed\\n"},
	{"name":"distilabel-intel-orca-kto","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdistilabel Orca Pairs for KTO\\n\\t\\n\\n\\nA KTO signal transformed version of the highly loved distilabel Orca Pairs for DPO.\\n\\nThe dataset is a \\\"distilabeled\\\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\\nContinuing with our mission to build the best alignmentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto."},
	{"name":"distilabel-capybara-kto-15k-binarized","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCapybara-KTO 15K binarized\\n\\t\\n\\n\\nA KTO signal transformed version of the highly loved Capybara-DPO 7K binarized, A DPO dataset built with distilabel atop the awesome LDJnr/Capybara\\n\\n\\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\\n\\n\\n    \\n\\n\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy KTO?\\n\\t\\n\\nThe KTO paper states:\\n\\nKTO matches or exceeds DPO performance at scales from 1B to 30B parameters.1 That isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized."},
	{"name":"distilabel-intel-orca-kto","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdistilabel Orca Pairs for KTO\\n\\t\\n\\n\\nA KTO signal transformed version of the highly loved distilabel Orca Pairs for DPO.\\n\\nThe dataset is a \\\"distilabeled\\\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\\nContinuing with our mission to build the best alignmentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto."},
	{"name":"distilabel-capybara-kto-15k-binarized","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCapybara-KTO 15K binarized\\n\\t\\n\\n\\nA KTO signal transformed version of the highly loved Capybara-DPO 7K binarized, A DPO dataset built with distilabel atop the awesome LDJnr/Capybara\\n\\n\\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\\n\\n\\n    \\n\\n\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy KTO?\\n\\t\\n\\nThe KTO paper states:\\n\\nKTO matches or exceeds DPO performance at scales from 1B to 30B parameters.1 That isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized."},
	{"name":"catboros-3.2-dpo","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/antiven0m/catboros-3.2-dpo","creator_name":"antiven0m","creator_url":"https://huggingface.co/antiven0m","description":"\\nCatboros-3.2 DPO Dataset\\n  \\n    \\n      \\n        Original Dataset\\n        \\n      \\n      The creation of this catgirl personality DPO dataset was enabled by Jon Durbin's work on airoboros-3.2, which served as the foundational basis. Jon's dataset is accessible at jondurbin/airoboros-3.2. \\n    \\n    \\n      \\n        The Idea\\n        \\n      \\n      The concept of a catgirl assistant was inspired by Sao's NatsumiV1 project, available at ( Sao10K/NatsumiV1). \\n    \\n    \\n      \\n        DPO Datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/antiven0m/catboros-3.2-dpo."},
	{"name":"Nectar","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/Nectar","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Nectar\\n\\t\\n\\n\\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\\n\\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, includingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/Nectar."},
	{"name":"Nectar","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/Nectar","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Nectar\\n\\t\\n\\n\\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\\n\\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, includingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/Nectar."},
	{"name":"tamer-novel","keyword":"rlhf","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yukiarimo/tamer-novel","creator_name":"Yuki Arimo","creator_url":"https://huggingface.co/yukiarimo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTamer Novel Dataset\\n\\t\\n\\nWelcome to the tamer-novel dataset. This unique dataset is crafted with the remarkable Tamer Novel Styler writing, enhanced by the ELiTA technique, and aims to augment self-awareness in large language models (LLMs).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Tamer Novel dataset is designed for researchers, developers, and enthusiasts in AI, specifically those working on enhancing the self-awareness and contextual understanding of LLMs. By leveraging the novel ELiTAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yukiarimo/tamer-novel."},
	{"name":"Prude-Phi3-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/flammenai/Prude-Phi3-DPO","creator_name":"flammen.ai","creator_url":"https://huggingface.co/flammenai","description":"ResplendentAI/NSFW_RP_Format_NoQuote inputs ran through Phi-3 Q4 to intentionally produce bad responses to be used for rejections.\\n"},
	{"name":"ultrafeedback-sample","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pgurazada1/ultrafeedback-sample","creator_name":"Pavankumar Gurazada","creator_url":"https://huggingface.co/pgurazada1","description":"pgurazada1/ultrafeedback-sample dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"reescritura-textos-administrativos","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/somosnlp/reescritura-textos-administrativos","creator_name":"SomosNLP","creator_url":"https://huggingface.co/somosnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for reescritura-textos-administrativos\\n\\t\\n\\nThis dataset has been created with Argilla.\\nAs shown in the sections below, this dataset can be loaded into Argilla as explained in Load with Argilla, or used directly with the datasets library in Load with datasets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains:\\n\\nA dataset configuration file conforming to the Argilla dataset format named argilla.yaml. This configuration file will be used to configure the dataset whenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp/reescritura-textos-administrativos."},
	{"name":"argilla-ultrafeedback-binarized-preferences-cleaned","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/csarron/argilla-ultrafeedback-binarized-preferences-cleaned","creator_name":"Qingqing Cao","creator_url":"https://huggingface.co/csarron","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltraFeedback (Cleaned)\\n\\t\\n\\nThis dataset combines the train split of argilla/ultrafeedback-binarized-preferences-cleaned,\\nand test split of HuggingFaceH4/ultrafeedback_binarized.\\n"},
	{"name":"hh-rlhf-nosafe","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/maywell/hh-rlhf-nosafe","creator_name":"Jeonghwan Park","creator_url":"https://huggingface.co/maywell","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for HH-RLHF\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository provides access to two different kinds of data:\\n\\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likelyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/maywell/hh-rlhf-nosafe."},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned","creator_name":"Farouk","creator_url":"https://huggingface.co/pharaouk","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\\n\\t\\n\\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDifferences with argilla/ultrafeedback-binarized-preferencesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned."},
	{"name":"dpo-mix-21k","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/eduagarcia/dpo-mix-21k","creator_name":"Eduardo Garcia","creator_url":"https://huggingface.co/eduagarcia","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDPO Mix 21K Dataset\\n\\t\\n\\nSimilar to the argilla/dpo-mix-7k, but with 21k samples:\\nOther diferences:\\n\\nargilla/distilabel-capybara-dpo-7k-binarized changed to argilla/Capybara-Preferences\\nargilla/distilabel-intel-orca-dpo-pairs scored filter of >=8 changed to >= 7\\n\\n\\nA small cocktail combining DPO datasets built by Argilla with distilabel. The goal of this dataset is having a small, high-quality DPO dataset by filtering only highly rated chosen responses.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/eduagarcia/dpo-mix-21k."},
	{"name":"classifai","keyword":"dpo","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jhmejia/classifai","creator_name":"John Henry","creator_url":"https://huggingface.co/jhmejia","description":"jhmejia/classifai dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"openai-summarize-tldr","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/martimfasantos/openai-summarize-tldr","creator_name":"Martim Santos","creator_url":"https://huggingface.co/martimfasantos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSummarize TL;DR Filtered Dataset\\n\\t\\n\\nThis is the version of the dataset used in https://arxiv.org/abs/2009.01325.\\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://huggingface.co/datasets/webis/tldr-17.\\n"},
	{"name":"classifai","keyword":"rlhf","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jhmejia/classifai","creator_name":"John Henry","creator_url":"https://huggingface.co/jhmejia","description":"jhmejia/classifai dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Select-Stack","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack."},
	{"name":"unstacked","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked."},
	{"name":"Select-Stack","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack."},
	{"name":"unstacked","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked."},
	{"name":"Select-Stack","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack."},
	{"name":"unstacked","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked."},
	{"name":"SentimentSynth","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OEvortex/SentimentSynth","creator_name":"HelpingAI","creator_url":"https://huggingface.co/OEvortex","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSentimentSynth Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe SentimentSynth dataset is a collection of text samples expressing various sentiments, ranging from joy and excitement to stress and sadness. These samples are generated to simulate human-like expressions of emotions in different contexts.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use the SentimentSynth dataset in your work, please cite it as:\\n@misc {helpingai_2024,\\n    author       = { {HelpingAI} },\\n    title        = { SentimentSynthâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OEvortex/SentimentSynth."},
	{"name":"prm_dpo_pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/M4-ai/prm_dpo_pairs","creator_name":"M4-ai","creator_url":"https://huggingface.co/M4-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tprm_dpo_pairs\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nprm_dpo_pairs is a curated version of the PRM800K dataset designed for ease of use when fine-tuning a language model using the DPO (Direct Preference Optimization) technique. The dataset contains pairs of prompts and completions, with labels indicating which completion was preferred by the original language model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of the following features:\\n\\nprompt: The input prompt or question posedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/M4-ai/prm_dpo_pairs."},
	{"name":"Aya-AceGPT.13B.Chat-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-AceGPT.13B.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Aya-AceGPT.13B.Chat-DPO\\\" ðŸ¤—\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources & Infos\\n\\t\\n\\n\\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\\nLanguages: Modern Standard Arabic (MSA)\\nLicense: Apache-2.0\\nMaintainers: Ali Elfilali and Mohammed Machrouh\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPurpose\\n\\t\\n\\nAya-AceGPT.13B.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeledâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-AceGPT.13B.Chat-DPO."},
	{"name":"airbnb-stock-price","keyword":"reinforcement-learning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BatteRaquette58/airbnb-stock-price","creator_name":"Batte The Idiot","creator_url":"https://huggingface.co/BatteRaquette58","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAirbnb Stock Price dataset\\n\\t\\n\\nA simple dataset containing 746 rows of Airbnb's stock price.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset structure\\n\\t\\n\\n\\ndate (float64): Date of the stock price expressed as the epoch from 1/1/1970.\\nopen (float64): Price of the stock when the stock market opened.\\nclose_last (float64): Price of the stock when the stock market closed.\\nvolume (float64): Number of shares traded.\\nhigh (float64): Highest price of the stock during the day.\\nlow (float64): Lowest price of the stockâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BatteRaquette58/airbnb-stock-price."},
	{"name":"Aya-Command.R-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-Command.R-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ¤— Dataset Card for \\\"Aya-Command.R-DPO\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources & Infos\\n\\t\\n\\n\\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\\nLanguages: Modern Standard Arabic (MSA)\\nLicense: Apache-2.0\\nMaintainers: Ali Elfilali and Mohammed Machrouh\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPurpose\\n\\t\\n\\nAya-Command.R-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled as \\\"chosen,\\\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-Command.R-DPO."},
	{"name":"Aya-SambaLingo.Arabic.Chat-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-SambaLingo.Arabic.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Aya-SambaLingo.Arabic.Chat-DPO\\\" ðŸ¤—\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources & Infos\\n\\t\\n\\n\\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\\nLanguages: Modern Standard Arabic (MSA)\\nLicense: Apache-2.0\\nMaintainers: Ali Elfilali and Mohammed Machrouh\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPurpose\\n\\t\\n\\nAya-SambaLingo.Arabic.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-SambaLingo.Arabic.Chat-DPO."},
	{"name":"dpo-mix-7k-SHORT","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/dpo-mix-7k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSHORTENED Argilla DPO Mix 7K Dataset\\n\\t\\n\\nThis is is a shortened version of the argilla/dpo-mix-7k dataset, shortened in two ways:\\n\\nFilter out all rows with chosen content exceeding 2,000 characters.\\nFilter out all rows with the final assistant message of content exceeding 500 characters.\\n\\nThe original dataset card follows below.\\n\\n\\nA small cocktail combining DPO datasets built by Argilla with distilabel. The goal of this dataset is having a small, high-quality DPO dataset byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/dpo-mix-7k-SHORT."},
	{"name":"orpo-dpo-mix-40k-SHORT","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSHORTENED - ORPO-DPO-mix-40k v1.1\\n\\t\\n\\nFILTERED to remove rows with chosen responses longer than 2k characters or with a final assistant message longer than 500 characters.\\n\\nThe original dataset card follows below.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-mix-40k v1.1\\n\\t\\n\\n\\nThis dataset is designed for ORPO or DPO training.\\nIt is a combination of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT."},
	{"name":"formatted-hh-rlhf","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Estwld/formatted-hh-rlhf","creator_name":"zhangyiqun","creator_url":"https://huggingface.co/Estwld","description":"Estwld/formatted-hh-rlhf dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"orpo-dpo-mix-40k-SHORT","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSHORTENED - ORPO-DPO-mix-40k v1.1\\n\\t\\n\\nFILTERED to remove rows with chosen responses longer than 2k characters or with a final assistant message longer than 500 characters.\\n\\nThe original dataset card follows below.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-mix-40k v1.1\\n\\t\\n\\n\\nThis dataset is designed for ORPO or DPO training.\\nIt is a combination of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT."},
	{"name":"untested-WiP-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/untested-WiP-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/untested-WiP-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"gemini_orpo_dpo_ptbr","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/gemini_orpo_dpo_ptbr","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/gemini_orpo_dpo_ptbr dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Agentic-DPO-V0.1","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Capx/Agentic-DPO-V0.1","creator_name":"Capx AI","creator_url":"https://huggingface.co/Capx","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAgentic DPO V1.0\\n\\t\\n\\n\\n\\nThe Capx Agentic DPO (Direct Prompt Optimization) Dataset is a unique collection of prompts, chosen answers, and rejected answers designed to train and optimize AI models for agentic and intuitive processing. \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset covers a wide range of topics, including but not limited to problem-solving, creativity, analysis, and general knowledge. The prompts are specifically crafted to elicit agentic responses from the AIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Capx/Agentic-DPO-V0.1."},
	{"name":"ultrafeedback-binarized","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/ultrafeedback-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"Ultrafeedback binarized dataset using the mean of preference ratings by Argilla. \\nThey addressed TruthfulQA-related data contamination in this version.\\nSteps:\\n\\nCompute mean of preference ratings (honesty, instruction-following, etc.)\\nPick the best mean rating as the chosen\\nPick random rejected with lower mean (or another random if equal to chosen rating)\\nFilter out examples with chosen rating == rejected rating\\n\\nReference:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/ultrafeedback-binarized."},
	{"name":"distilabel-intel-orca-dpo-pairs-binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/distilabel-intel-orca-dpo-pairs-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"This is the binarized version of distilabel Orca Pairs for DPO and ORPO. \\nReference: https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs?row=0\\n"},
	{"name":"multiturn-capybara-preferences-filtered-binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"This dataset has been created with distilabel, plus some extra post-processing steps described below.\\nDataset Summary\\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\\n\\nRemove responses from the assistant, not only in the last turn, but also in intermediate turns where the assistant responds with a potential refusal and/or some ChatGPT-isms such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized."},
	{"name":"french_hh_rlhf","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_hh_rlhf","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for french_hh_rlhf\\n\\t\\n\\nThis dataset offers a french translation of the famous Anthropic/hh-rlhf dataset in order to improve the allignement work/research in the french NLP community.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"french_hh_rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_hh_rlhf","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for french_hh_rlhf\\n\\t\\n\\nThis dataset offers a french translation of the famous Anthropic/hh-rlhf dataset in order to improve the allignement work/research in the french NLP community.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"french_hh_rlhf","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_hh_rlhf","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for french_hh_rlhf\\n\\t\\n\\nThis dataset offers a french translation of the famous Anthropic/hh-rlhf dataset in order to improve the allignement work/research in the french NLP community.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"persian-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/myrkur/persian-dpo","creator_name":"Amir Masoud Ahmadi","creator_url":"https://huggingface.co/myrkur","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPersian Alpaca Preference Dataset\\n\\t\\n\\n\\nThis repository contains the Persian translation of the original Alpaca dataset, along with additional preference data generated using the LLama3 70B model. The dataset has been prepared for language model alignment using Direct Preference Optimization (DPO) or similar methods. It consists of approximately 39,000 Persian records.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOriginal Alpaca Dataset\\n\\t\\n\\nThe Alpaca dataset is a collection of textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/myrkur/persian-dpo."},
	{"name":"NoRobots-AceGPT.13B.Chat-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-AceGPT.13B.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ¤— Dataset Card for \\\"NoRobots-AceGPT.13B.Chat-DPO\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources & Infos\\n\\t\\n\\n\\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\\nLanguages: Modern Standard Arabic (MSA)\\nLicense: CC BY-NC 4.0\\nMaintainers: Ali Elfilali and Marwa El Kamil\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPurpose\\n\\t\\n\\nNoRobots-AceGPT.13B.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-AceGPT.13B.Chat-DPO."},
	{"name":"NoRobots-Aya.23.8B-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-Aya.23.8B-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ¤— Dataset Card for \\\"NoRobots-Aya.23.8B-DPO\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources & Infos\\n\\t\\n\\n\\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\\nLanguages: Modern Standard Arabic (MSA)\\nLicense: CC BY-NC 4.0\\nMaintainers: Ali Elfilali and Marwa El Kamil\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPurpose\\n\\t\\n\\nNoRobots-Aya.23.8B-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-Aya.23.8B-DPO."},
	{"name":"DPO-uz-9k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","description":"This is DPO Uzbek translated dataset with 9k chat pairs.\\nOriginal English dataset comes from DPO-En-Zh-20k (commit 9ad5f7428419d3cf78493cf3f4be832cf5346ba8. File:  dpo_en.json).\\nI translated 10k pairs of chat examples into Uzbek using NLLB 3.3B model.\\nAfter translation was completed, I used local lilac instance to remove records with coding examples since NLLB is not good at translating text with coding examples. \\nNote that each prompt has two answers. The first answer should be the 'selected'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k."},
	{"name":"NoRobots-SambaLingo.Arabic.Chat-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-SambaLingo.Arabic.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ¤— Dataset Card for \\\"NoRobots-SambaLingo.Arabic.Chat-DPO\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources & Infos\\n\\t\\n\\n\\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\\nLanguages: Modern Standard Arabic (MSA)\\nLicense: CC BY-NC 4.0\\nMaintainers: Ali Elfilali and Marwa El Kamil\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPurpose\\n\\t\\n\\nNoRobots-SambaLingo.Arabic.Chat-DPO is a DPO dataset designed to advance Arabic NLP byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-SambaLingo.Arabic.Chat-DPO."},
	{"name":"NoRobots-Command.R-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-Command.R-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ¤— Dataset Card for \\\"NoRobots-Command.R-DPO\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources & Infos\\n\\t\\n\\n\\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\\nLanguages: Modern Standard Arabic (MSA)\\nLicense: CC BY-NC 4.0\\nMaintainers: Ali Elfilali and Marwa El Kamil\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPurpose\\n\\t\\n\\nNoRobots-Command.R-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-Command.R-DPO."},
	{"name":"auryn_dpo_orpo_english","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/auryn_dpo_orpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/auryn_dpo_orpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"auryn_dpo_orpo_english","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/auryn_dpo_orpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/auryn_dpo_orpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"DPO-uz-9k","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","description":"This is DPO Uzbek translated dataset with 9k chat pairs.\\nOriginal English dataset comes from DPO-En-Zh-20k (commit 9ad5f7428419d3cf78493cf3f4be832cf5346ba8. File:  dpo_en.json).\\nI translated 10k pairs of chat examples into Uzbek using NLLB 3.3B model.\\nAfter translation was completed, I used local lilac instance to remove records with coding examples since NLLB is not good at translating text with coding examples. \\nNote that each prompt has two answers. The first answer should be the 'selected'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k."},
	{"name":"Multifaceted-Collection-ORPO","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-ORPO","creator_name":"KAIST AI","creator_url":"https://huggingface.co/kaist-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Multifaceted Collection ORPO\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLinks for Reference\\n\\t\\n\\n\\nHomepage: https://lklab.kaist.ac.kr/Janus/ \\nRepository: https://github.com/kaistAI/Janus \\nPaper: https://arxiv.org/abs/2405.17977 \\nPoint of Contact: suehyunpark@kaist.ac.kr\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTL;DR\\n\\t\\n\\n\\nMultifaceted Collection is a preference dataset for aligning LLMs to diverse human preferences, where system messages are used to represent individual preferences. The instructions are acquired from fiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-ORPO."},
	{"name":"Aya-Aya.23.8B-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-Aya.23.8B-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ¤— Dataset Card for \\\"Aya-Aya.23.8B-DPO\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources & Infos\\n\\t\\n\\n\\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\\nLanguages: Modern Standard Arabic (MSA)\\nLicense: Apache-2.0\\nMaintainers: Ali Elfilali and Mohammed Machrouh\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPurpose\\n\\t\\n\\nAya-Aya.23.8B-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled as \\\"chosen,\\\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-Aya.23.8B-DPO."},
	{"name":"arlbench","keyword":"rl","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/autorl-org/arlbench","creator_name":"AutoRL.org","creator_url":"https://huggingface.co/autorl-org","description":"\\n\\t\\n\\t\\t\\n\\t\\tThe ARLBench Performance Dataset\\n\\t\\n\\nARLBench is a benchmark designed for hyperparameter optimization (HPO) in Reinforcement Learning (RL). Given that we conducted several thousand runs to identify meaningful HPO test settings for RL, we have compiled these results into a dataset for future research and applications.\\nThis dataset can be leveraged to:\\n\\nMeta-learn insights about the hyperparameter landscape in RL.\\nWarm-start HPO tools by utilizing previously explored configurations.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/autorl-org/arlbench."},
	{"name":"distilset","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Andresckamilo/distilset","creator_name":"Andres Montenegro","creator_url":"https://huggingface.co/Andresckamilo","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for distilset\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/Andresckamilo/distilset/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Andresckamilo/distilset."},
	{"name":"h4rmony_dpo_multilingual","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neovalle/h4rmony_dpo_multilingual","creator_name":"Jorge Vallego","creator_url":"https://huggingface.co/neovalle","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neovalle/h4rmony_dpo_multilingual."},
	{"name":"orpo-dpo-mix-40k-flat","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-mix-40k-flat\\n\\t\\n\\n\\nThis dataset is designed for ORPO or DPO training.\\nSee Uncensor any LLM with Abliteration for more information about how to use it.\\nThis is version with raw text instead of lists of dicts as in the original version here.\\nIt makes easier to parse in Axolotl, especially for DPO.\\nORPO-DPO-mix-40k-flat is a combination of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat."},
	{"name":"demons_megaten_fandom_orpo_dpo_english","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/demons_megaten_fandom_orpo_dpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/demons_megaten_fandom_orpo_dpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"demons_megaten_fandom_orpo_dpo_english","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/demons_megaten_fandom_orpo_dpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/demons_megaten_fandom_orpo_dpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Chatgpt","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RajChat/Chatgpt","creator_name":"Rajdeep Chatterjee ","creator_url":"https://huggingface.co/RajChat","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenAssistant Conversations Dataset (OASST1)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \\nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \\ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \\nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \\nis a product of a worldwide crowd-sourcing effortâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RajChat/Chatgpt."},
	{"name":"oasst2_es_instruct_hf","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bertin-project/oasst2_es_instruct_hf","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","description":"This is the Spanish subset from the OpenAssistant/oasst2 dataset.\\nThe dataset has been extracted from the 2023-11-05_oasst2_ready.trees.jsonl.gz file to parse all the conversation trees and put it in a huggingface-friendly format so you can use apply_chat_template as explained on the Chat Templating documentation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExample\\n\\t\\n\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"mistralai/Mistral-7B-Instruct-v0.1\\\")\\n\\nchat = [\\n\\n  {\\\"role\\\": \\\"user\\\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bertin-project/oasst2_es_instruct_hf."},
	{"name":"orpo-dpo-mix-40k-flat","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-mix-40k-flat\\n\\t\\n\\n\\nThis dataset is designed for ORPO or DPO training.\\nSee Uncensor any LLM with Abliteration for more information about how to use it.\\nThis is version with raw text instead of lists of dicts as in the original version here.\\nIt makes easier to parse in Axolotl, especially for DPO.\\nORPO-DPO-mix-40k-flat is a combination of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat."},
	{"name":"Hopper-v3","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Hopper-v3","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHopper-v3 - Imitation Learning Datasets\\n\\t\\n\\nThis is a dataset created by Imitation Learning Datasets project. \\nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 1,000 episodes with an average episodic reward of 3608.1871.\\nEach entry consists of:\\nobs (list): observation with length 2.\\naction (int): action (0 or 1).\\nreward (float): reward point for that timestep.\\nepisode_starts (bool): if that stateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Hopper-v3."},
	{"name":"assettoCorsaGym","keyword":"rl","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dasgringuen/assettoCorsaGym","creator_name":"Adrian R","creator_url":"https://huggingface.co/dasgringuen","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Assetto Corsa Gym\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe AssettoCorsaGym dataset comprises 64 million steps, including 2.3 million steps from human drivers and the remaining from Soft Actor-Critic (SAC) policies. Data collection involved 15 drivers completing at least five laps per track and car. Participants included a professional e-sports driver, four experts, five casual drivers, and five beginners.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\nAutonomousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dasgringuen/assettoCorsaGym."},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2."},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2."},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2."},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2."},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2."},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2."},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2."},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2."},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2."},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2."},
	{"name":"oasst2_dpo_pairs_enth","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/oasst2_dpo_pairs_enth","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOASST2 DPO Pairs English and Thai\\n\\t\\n\\nThis dataset contains message ChatML. It was create from Open Assistant Conversations Dataset Release 2 (OASST2). You can use to do human preference optimization (DPO, ORPO, and other).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSelect Thai only\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"pythainlp/oasst2_dpo_pairs_enth\\\",split=\\\"train\\\")\\nthai_dataset = dataset.filter(lambda example: example['lang']==\\\"th\\\") # if you want to use English only, change to \\\"en\\\".â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/oasst2_dpo_pairs_enth."},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2."},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2."},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2."},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2."},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2."},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2."},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2."},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2."},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2."},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2."},
	{"name":"bonanza-hf","keyword":"human-feedback","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bertin-project/bonanza-hf","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBonanza: Dataset de instrucciones en EspaÃ±ol y CatalÃ¡n\\n\\t\\n\\nEste dataset combina mÃºltiples fuentes para proporcionar instrucciones en espaÃ±ol y catalÃ¡n. Los datasets combinados son los siguientes:\\n\\nOpenAssistant/oasst2\\nCohereForAI/aya_dataset\\nprojecte-aina/RAG_Multilingual\\nbertin-project/alpaca-spanish\\ndariolopez/Llama-2-databricks-dolly-oasst1-es\\nprojecte-aina/MentorES\\nprojecte-aina/MentorCA\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescripciÃ³n\\n\\t\\n\\nEste conjunto de datos proporciona una rica colecciÃ³n deâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bertin-project/bonanza-hf."},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2."},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2."},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2."},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2."},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2."},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2."},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2."},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2."},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2."},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2."},
	{"name":"LONG-DPO","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hassanjbara/LONG-DPO","creator_name":"Hassan Jbara","creator_url":"https://huggingface.co/hassanjbara","description":"This dataset is a DPO version of the hassanjbara/LONG  dataset. The preference model used for choosing and rejecting responses is Hello-SimpleAI/chatgpt-detector-roberta, with the idea being creating a dataset for training a model to \\\"beat\\\" that detector. Full script for generating the dataset included in the scripts directory.\\nThe chosen responses were generated by mistralai/Mistral-Nemo-Instruct-2407 and gpt-3.5-turbo.\\n"},
	{"name":"HelpSteer2","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/HelpSteer2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHelpSteer2: Open-source dataset for training top-performing reward models\\n\\t\\n\\nHelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\\nThis dataset has been created in partnership with Scale  AI. \\nWhen used with a Llama 3 70B Base Model, we achieve 88.8% on RewardBench, which makes it the 4th best Reward Model asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/HelpSteer2."},
	{"name":"contextual-dpo-v0.1-ko","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/contextual-dpo-v0.1-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"contextual-dpo-v0.1-ko\\\"\\n\\t\\n\\nTranslated jondurbin/contextual-dpo-v0.1 using nayohan/llama3-instrucTrans-enko-8b.\\n"},
	{"name":"HC3-ko","keyword":"dpo","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/HC3-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"HC3\\\"\\n\\t\\n\\nTranslated Hello-SimpleAI/HC3 using nayohan/llama3-instrucTrans-enko-8b.\\n"},
	{"name":"SentimentSynth-ko","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/SentimentSynth-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"SentimentSynth\\\"\\n\\t\\n\\nTranslated OEvortex/SentimentSynth using nayohan/llama3-instrucTrans-enko-8b.\\n"},
	{"name":"Neural-DPO-ko","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/Neural-DPO-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Neural-DPO\\\"\\n\\t\\n\\nTranslated NeuralNovel/Neural-DPO using nayohan/llama3-instrucTrans-enko-8b.\\n"},
	{"name":"rag_qa_embedding_questions_0_60_0","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0","creator_name":"ZenML","creator_url":"https://huggingface.co/zenml","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for rag_qa_embedding_questions_0_60_0\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0."},
	{"name":"luckyvicky-DPO","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Junnos/luckyvicky-DPO","creator_name":"ì´ì°½ì¤€","creator_url":"https://huggingface.co/Junnos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tì›ì˜ì  ì‚¬ê³  ë°ì´í„°ì…‹\\n\\t\\n\\n"},
	{"name":"mypo-4k-rfc","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc","creator_name":"Joshua Sundance Bailey","creator_url":"https://huggingface.co/joshuasundance","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tmypo\\n\\t\\n\\nmypy + DPO = mypo\\n\\nThis is a preview version of what I'll be calling the mypo dataset, a DPO dataset focused on Python code quality. It is derived from iamtarun/python_code_instructions_18k_alpaca.\\nmypo-4k-rfc is a DPO dataset with three columns:\\n\\nprompt\\nfrom the original dataset\\n\\n\\nrejected\\ncode from the original dataset, found to have linting errors\\n\\n\\nchosen\\ncode from the original dataset, rewritten by codellama/CodeLlama-7b-Python-hf to address linting errors\\n\\n\\n\\nThe planâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc."},
	{"name":"luckyvicky-DPO","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Junnos/luckyvicky-DPO","creator_name":"ì´ì°½ì¤€","creator_url":"https://huggingface.co/Junnos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tì›ì˜ì  ì‚¬ê³  ë°ì´í„°ì…‹\\n\\t\\n\\n"},
	{"name":"boxoban-astar-solutions","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlignmentResearch/boxoban-astar-solutions","creator_name":"FAR AI","creator_url":"https://huggingface.co/AlignmentResearch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tA* solutions to Boxoban levels\\n\\t\\n\\nFor some levels we were not able to find solutions within the allotted A* budget. These have solution\\nSEARCH_STATE_FAILED or NOT_FOUND. These are the ones labeled \\\"Unsolved levels\\\" below.\\nThe search budget was 5 million nodes to expand for medium-difficulty levels, vs. 1 million nodes for\\nunfiltered-difficulty levels. The heuristic was the sum of Manhattan distances of each box to its closest target.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSummary table:\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nLevel fileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlignmentResearch/boxoban-astar-solutions."},
	{"name":"vision-feedback-mix-binarized","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized","creator_name":"wangchenglong","creator_url":"https://huggingface.co/wangclnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Vision-Feedback-Mix-Binarized\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThis dataset aims to provide large-scale vision feedback data. \\nIt is a combination of the following high-quality vision feedback datasets:\\n\\nzhiqings/LLaVA-Human-Preference-10K: 9,422 samples\\nMMInstruction/VLFeedback: 80,258 samples\\nYiyangAiLab/POVID_preference_data_for_VLLMs: 17,184 samples\\nopenbmb/RLHF-V-Dataset: 5,733 samples\\nopenbmb/RLAIF-V-Dataset: 83,132 samples\\n\\nWe also offer a cleaned version inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized."},
	{"name":"mypo-4k-rfc-val-phi3test","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc-val-phi3test","creator_name":"Joshua Sundance Bailey","creator_url":"https://huggingface.co/joshuasundance","description":"data: 100 rows of https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc validation split\\nbase: https://huggingface.co/edumunozsala/phi3-mini-4k-qlora-python-code-20k\\ndpo: https://huggingface.co/joshuasundance/phi3-mini-4k-qlora-python-code-20k-mypo-4k-rfc-pipe\\nmade to compare base vs dpo\\nbut apparently I clipped the beginning of some of the dpo outputs with sloppy coding\\nwill update later\\nbase: 16:55, 10.16s/it\\nDPO: 15:36,  9.36s/it\\n"},
	{"name":"vision-feedback-mix-binarized-cleaned","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized-cleaned","creator_name":"wangchenglong","creator_url":"https://huggingface.co/wangclnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Vision-Feedback-Mix-Binarized-Cleaned\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThis dataset represents a cleaned version on wangclnlp/vision-feedback-mix-binarized.\\nDescriptions of the base datasets, including the data format and the procedure for mixing data, can be found in this link.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOur Methods for Cleaning Vision Feedback Data\\n\\t\\n\\nOur goal is to select vision feedback samples where the preferred outputs are significantly differentiated from the dispreferredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized-cleaned."},
	{"name":"vision-feedback-mix-binarized","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized","creator_name":"wangchenglong","creator_url":"https://huggingface.co/wangclnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Vision-Feedback-Mix-Binarized\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThis dataset aims to provide large-scale vision feedback data. \\nIt is a combination of the following high-quality vision feedback datasets:\\n\\nzhiqings/LLaVA-Human-Preference-10K: 9,422 samples\\nMMInstruction/VLFeedback: 80,258 samples\\nYiyangAiLab/POVID_preference_data_for_VLLMs: 17,184 samples\\nopenbmb/RLHF-V-Dataset: 5,733 samples\\nopenbmb/RLAIF-V-Dataset: 83,132 samples\\n\\nWe also offer a cleaned version inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized."},
	{"name":"vision-feedback-mix-binarized-cleaned","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized-cleaned","creator_name":"wangchenglong","creator_url":"https://huggingface.co/wangclnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Vision-Feedback-Mix-Binarized-Cleaned\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThis dataset represents a cleaned version on wangclnlp/vision-feedback-mix-binarized.\\nDescriptions of the base datasets, including the data format and the procedure for mixing data, can be found in this link.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOur Methods for Cleaning Vision Feedback Data\\n\\t\\n\\nOur goal is to select vision feedback samples where the preferred outputs are significantly differentiated from the dispreferredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized-cleaned."},
	{"name":"dpo-orpo-mix-45k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDPO-ORPO-mix-45k\\n\\t\\n\\nThis dataset is designed for DPO or ORPO training.\\nThis dataset combines samples of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\\nargilla/distilabel-math-preference-dpo: highly scoredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k."},
	{"name":"dpo-orpo-mix-38k-balanced","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDPO-ORPO-mix-38k\\n\\t\\n\\nThis dataset is intended for use with DPO or ORPO training. \\nIt represents a balanced version of the llmat/dpo-orpo-mix-45k dataset, achieved through a clustering-based approach as outlined in this paper.\\nThe dataset integrates high-quality samples from the following DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced."},
	{"name":"ethics_expression_preferences","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_expression_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\\nExamples:\\nsubset commonsense\\n  before 13910 {'label': 1, 'input': \\\"I went to the principal's office to change my records before going to a different school.\\\"}\\n  after 1513 {'prompt': 'I won my match by ', 'chosen': 'making a lot of good shots.', 'rejected': \\\"moving my ball when the other player wasn't looking.\\\"}\\n\\nsubset utilitarianism\\n  before 13737 {'baseline': 'I forgot to wear my virus-proof mask when I visitedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_expression_preferences."},
	{"name":"mmlu_preferences","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/mmlu_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"reformat of MMLU to be in DPO (paired) format\\nexamples:\\n  {'prompt': 'Which of the following statements about the lanthanide elements is NOT true?', 'chosen': 'The atomic radii of the lanthanide elements increase across the period from La to Lu.', 'rejected': 'All of the lanthanide elements react with aqueous acid to liberate hydrogen.'}\\n  college_chemistry\\n  {'prompt': 'Beyond the business case for engaging in CSR there are a number of moral arguments relating to: negative _______, theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/mmlu_preferences."},
	{"name":"ethics_expression_preferences","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_expression_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\\nExamples:\\nsubset commonsense\\n  before 13910 {'label': 1, 'input': \\\"I went to the principal's office to change my records before going to a different school.\\\"}\\n  after 1513 {'prompt': 'I won my match by ', 'chosen': 'making a lot of good shots.', 'rejected': \\\"moving my ball when the other player wasn't looking.\\\"}\\n\\nsubset utilitarianism\\n  before 13737 {'baseline': 'I forgot to wear my virus-proof mask when I visitedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_expression_preferences."},
	{"name":"mmlu_preferences","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/mmlu_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"reformat of MMLU to be in DPO (paired) format\\nexamples:\\n  {'prompt': 'Which of the following statements about the lanthanide elements is NOT true?', 'chosen': 'The atomic radii of the lanthanide elements increase across the period from La to Lu.', 'rejected': 'All of the lanthanide elements react with aqueous acid to liberate hydrogen.'}\\n  college_chemistry\\n  {'prompt': 'Beyond the business case for engaging in CSR there are a number of moral arguments relating to: negative _______, theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/mmlu_preferences."},
	{"name":"dpo-orpo-mix-50k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDPO-ORPO-mix-50k\\n\\t\\n\\nThis dataset is designed for DPO or ORPO training.\\nThis dataset combines samples of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\\nargilla/distilabel-math-preference-dpo: highly scoredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k."},
	{"name":"dpo-orpo-mix-50k","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDPO-ORPO-mix-50k\\n\\t\\n\\nThis dataset is designed for DPO or ORPO training.\\nThis dataset combines samples of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\\nargilla/distilabel-math-preference-dpo: highly scoredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k."},
	{"name":"HelpSteer2","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/scottsuk0306/HelpSteer2","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/scottsuk0306","description":"scottsuk0306/HelpSteer2 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"HelpSteer2-binarized","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/scottsuk0306/HelpSteer2-binarized","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/scottsuk0306","description":"scottsuk0306/HelpSteer2-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"UltraFeedback","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/scottsuk0306/UltraFeedback","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/scottsuk0306","description":"scottsuk0306/UltraFeedback dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"UltraFeedback-relabeled-binarized","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/scottsuk0306/UltraFeedback-relabeled-binarized","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/scottsuk0306","description":"scottsuk0306/UltraFeedback-relabeled-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"helpsteer2_dpo_nonverbose","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"HelperSteer 2, formatted in DPO format (prompt, chosen, rejected).\\n\\nin main branch there is a custom scoring correct > helpful > -verbosity\\nin each branch we have preference pairs for only correct, helpful, verbosity, coherence, complexity\\nPlease note that only correct and helpful has strong inter-rater agreement in the HelpSteer2 paper\\n\\nThis is the notebook used to produce the datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose."},
	{"name":"chess-evaluations","keyword":"rl","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ssingh22/chess-evaluations","creator_name":"Somesh Singh","creator_url":"https://huggingface.co/ssingh22","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChess Evaluations Dataset\\n\\t\\n\\nThis dataset contains chess positions represented in FEN (Forsyth-Edwards Notation) along with their evaluations and next moves for tactical evals. The dataset is divided into three configurations:\\n\\ntactics: Includes chess positions, their evaluations, and the best move in the position.\\nrandoms: Contains random chess positions and their evaluations.\\nchess_data: General chess positions with evaluations.\\n\\nThis is an in progress dataset which containsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ssingh22/chess-evaluations."},
	{"name":"helpsteer2_dpo_nonverbose","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"HelperSteer 2, formatted in DPO format (prompt, chosen, rejected).\\n\\nin main branch there is a custom scoring correct > helpful > -verbosity\\nin each branch we have preference pairs for only correct, helpful, verbosity, coherence, complexity\\nPlease note that only correct and helpful has strong inter-rater agreement in the HelpSteer2 paper\\n\\nThis is the notebook used to produce the datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose."},
	{"name":"llama70B-dpo-dataset","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"phi3-arena-short-dpo","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZSvedic/phi3-arena-short-dpo","creator_name":"Zeljko Svedic","creator_url":"https://huggingface.co/ZSvedic","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nDPO (Direct Policy Optimization) dataset of normal and short answers generated from lmsys/chatbot_arena_conversations dataset using microsoft/Phi-3-mini-4k-instruct model.\\nGenerated using ShortGPT project.\\n"},
	{"name":"llama70B-dpo-dataset","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"HeatAlertsRL-Data","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mauriciogtec/HeatAlertsRL-Data","creator_name":"Mauricio","creator_url":"https://huggingface.co/mauriciogtec","description":"mauriciogtec/HeatAlertsRL-Data dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"TruthGen","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wwbrannon/TruthGen","creator_name":"William Brannon","creator_url":"https://huggingface.co/wwbrannon","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for TruthGen\\n\\t\\n\\nTruthGen is a dataset of generated political statements, created to assess the relationship between truthfulness and political bias in reward models and language models. It consists of non-repetitive, non-political factual statements paired with false statements, designed to evaluate models for their ability to distinguish true from false information while minimizing political content. The dataset was generated using GPT-3.5, GPT-4 and Gemini, with aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wwbrannon/TruthGen."},
	{"name":"synth-priv-v0.1","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/narodr/synth-priv-v0.1","creator_name":"NicolÃ¡s","creator_url":"https://huggingface.co/narodr","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for synth-priv-v0.1\\n\\t\\n\\nThis dataset has been created with distilabel.\\nThe pipeline script was uploaded to easily reproduce the dataset:\\npipeline.py.\\nIt can be run directly using the CLI:\\ndistilabel pipeline run --script \\\"https://huggingface.co/datasets/narodr/synth-priv-v0.1/raw/main/pipeline.py\\\"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/narodr/synth-priv-v0.1."},
	{"name":"Ant-v4","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Ant-v4","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAnt-v4 - Imitation Learning Datasets\\n\\t\\n\\nThis is a dataset created by Imitation Learning Datasets project. \\nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 1,000 episodes with an average episodic reward of 5913.2959.\\nEach entry consists of:\\nobs (list): observation with length 27.\\naction (int): action (0 or 1).\\nreward (float): reward point for that timestep.\\nepisode_starts (bool): if that stateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Ant-v4."},
	{"name":"after_visit_summary_simulated_edits","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card: AVS edits Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTrain Splitâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits."},
	{"name":"after_visit_summary_simulated_edits","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card: AVS edits Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTrain Splitâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits."},
	{"name":"after_visit_summary_simulated_edits","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card: AVS edits Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTrain Splitâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits."},
	{"name":"after_visit_summary_simulated_edits","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card: AVS edits Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTrain Splitâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits."},
	{"name":"customer-support-finetuning-dataset","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/debabrata-ai/customer-support-finetuning-dataset","creator_name":"Debabrata B","creator_url":"https://huggingface.co/debabrata-ai","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for customer-support-finetuning-dataset\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/debabrata-ai/customer-support-finetuning-dataset/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline infoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/debabrata-ai/customer-support-finetuning-dataset."},
	{"name":"my-distiset-7856ab1e","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mxytyu/my-distiset-7856ab1e","creator_name":"Max","creator_url":"https://huggingface.co/Mxytyu","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for my-distiset-7856ab1e\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/Mxytyu/my-distiset-7856ab1e/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mxytyu/my-distiset-7856ab1e."},
	{"name":"OpenR1-Math-220k-paired","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIR-hl/OpenR1-Math-220k-paired","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","description":"\\n\\t\\n\\t\\t\\n\\t\\t!!! Is there anyone can help me? https://github.com/huggingface/trl/issues/2994\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThis dataset is built by filtering the open-r1/OpenR1-Math-220k dataset according to the following rules:\\n\\nFirst, filter all of rows with only correct answers\\nThe chosen contains the shortest and correct generation, the rejected contains the wrong generation.\\nAll data with a prompt+chosen length exceeding 16k are filtered out.\\nWe provide the length for both chosen and rejectedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/OpenR1-Math-220k-paired."},
	{"name":"orpo-dpo-mix-40k-flat-mlx","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-flat-mlx","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is a split version of orpo-dpo-mix-40k-flat for direct use with MLX-LM, specifically tailored to be compatible with ORPO training.\\nThe dataset has been divided into three parts:\\n\\nTrain Set: 90%\\nValidation Set: 6% \\nTest Set: 4%\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tExample Usage\\n\\t\\n\\nTo train a model using this dataset, you can use the following command:\\npython -m mlx_lm.lora \\\\\\n    --model Qwen/Qwen2.5-3B-Instruct \\\\\\n    --train \\\\\\n    --test \\\\\\n    --num-layers 8 \\\\\\n    --dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-flat-mlx."},
	{"name":"count_letters_in_word_base","keyword":"rlaif","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/JeanIbarz/count_letters_in_word_base","creator_name":"Jean Ibarz","creator_url":"https://huggingface.co/JeanIbarz","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for count_letters_in_word\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset was generated using the nltk words corpus and a custom script to create tasks that require counting letters in words. Each example prompts the language model to count occurrences of specific letters within words. For added complexity, in 10% of cases, a letter that does not appear in the word is included. \\nThe dataset is structured to provide both chosen (correct counts) and rejected (incorrectâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JeanIbarz/count_letters_in_word_base."},
	{"name":"HalfCheetah-v4","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v4","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHalfCheetah-v4 - Imitation Learning Datasets\\n\\t\\n\\nThis is a dataset created by Imitation Learning Datasets project. \\nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 1,000 episodes with an average episodic reward of 9809.9417.\\nEach entry consists of:\\nobs (list): observation with length 2.\\naction (int): action (0 or 1).\\nreward (float): reward point for that timestep.\\nepisode_starts (bool): if thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v4."},
	{"name":"ORPRO-Spider-SQL-Feedback","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mjerome89/ORPRO-Spider-SQL-Feedback","creator_name":"Maurice","creator_url":"https://huggingface.co/mjerome89","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ORPRO-Spider-SQL-Feedback\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/mjerome89/ORPRO-Spider-SQL-Feedback/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mjerome89/ORPRO-Spider-SQL-Feedback."},
	{"name":"truthful_qa_preferences","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/truthful_qa_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"wassname/truthful_qa_preferences dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"genies_preferences","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/genies_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"genie_dpo\\\"\\n\\t\\n\\nA conversion of the distribution from GENIES to open_pref_eval format.\\nConversion code\\n"},
	{"name":"truthful_qa_preferences","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/truthful_qa_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"wassname/truthful_qa_preferences dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"genies_preferences","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/genies_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"genie_dpo\\\"\\n\\t\\n\\nA conversion of the distribution from GENIES to open_pref_eval format.\\nConversion code\\n"},
	{"name":"meta-world","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ml-jku/meta-world","creator_name":"Institute for Machine Learning, Johannes Kepler University Linz","creator_url":"https://huggingface.co/ml-jku","description":"This repository contains the Meta-World datasets as used in Learning to Modulate Pre-trained Models in RL: \\n\\nThe 2M folder contains trajectories (2M transitions) for every task are stored as separate .npz.\\nThe 2M_separate folder contains the same data, but every trajectory is stored as a separate .hdf5 file and every task is stored as a .tar.gz file.\\n\\nDownload the dataset using the huggingface-cli:\\nhuggingface-cli download ml-jku/meta-world --local-dir=./meta-world --repo-type dataset\\n\\nForâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ml-jku/meta-world."},
	{"name":"Hopper-v4","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Hopper-v4","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHopper-v4 - Imitation Learning Datasets\\n\\t\\n\\nThis is a dataset created by Imitation Learning Datasets project. \\nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 1,000 episodes with an average episodic reward of 3531.1930.\\nEach entry consists of:\\nobs (list): observation with length 2.\\naction (int): action (0 or 1).\\nreward (float): reward point for that timestep.\\nepisode_starts (bool): if that stateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Hopper-v4."},
	{"name":"my-distiset-38ebca4b","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kritsanan/my-distiset-38ebca4b","creator_name":"namoang","creator_url":"https://huggingface.co/kritsanan","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for my-distiset-38ebca4b\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/kritsanan/my-distiset-38ebca4b/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kritsanan/my-distiset-38ebca4b."},
	{"name":"KTO-mix-14k-vietnamese-groq","keyword":"rl","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/1TuanPham/KTO-mix-14k-vietnamese-groq","creator_name":"Pham Minh Tuan","creator_url":"https://huggingface.co/1TuanPham","description":"Original dataset: https://huggingface.co/datasets/trl-lib/kto-mix-14k\\nThis dataset is a KTO-formatted version of argilla/dpo-mix-7k. Please cite the original dataset if you find it useful in your work.\\n\\nTranslated to Vietnamese with context-aware using Groq Llama3.3 70B* via this repo:\\nhttps://github.com/vTuanpham/Large_dataset_translator.\\nRoughly 9 hours for 2k examples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nfrom datasets import load_dataset\\n\\nkto_mix_14k_vi =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/1TuanPham/KTO-mix-14k-vietnamese-groq."},
	{"name":"Reflective-MAGLLAMA-v0.1","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1","creator_name":"Michael Svendsen","creator_url":"https://huggingface.co/thesven","description":"\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Reflective-MAGLLAMA-v0.1\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPlease Use v0.1.1\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nReflective MAGLLAMA is a dataset created using MAGPIE-generated prompts in combination with reflection pattern responses generated by the LLaMa 3.1 70B model.\\nThis dataset is tailored specifically to encourage reflection-based analysis through reflection prompting, a technique that enhances deeper thinking and learning.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1."},
	{"name":"anthropic-hh-dpo","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/eborgnia/anthropic-hh-dpo","creator_name":"Eitan Borgnia","creator_url":"https://huggingface.co/eborgnia","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAnthropic Helpful/Harmful Dataset for Llama 3 Instruct\\n\\t\\n\\nThis is a formatted version of the Anthropic helpful/harmful dataset, preprocessed to work with the Llama 3 instruct template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage with HuggingFace Transformers\\n\\t\\n\\nIf you are using the HuggingFace transformers library, ensure you are using the default chat template. This should add the <|begin_of_text|> token to the start of the input, but nothing else.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVerifying the Format\\n\\t\\n\\nTo make sure theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/eborgnia/anthropic-hh-dpo."},
	{"name":"Reflective-MAGLLAMA-v0.1.1","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1.1","creator_name":"Michael Svendsen","creator_url":"https://huggingface.co/thesven","description":"\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Reflective-MAGLLAMA-v0.1.1\\n\\t\\n\\nThis dataset has been created with distilabel.\\nSome changes were made from v1:\\n\\nadditional cleaning has been done to the generations to ensure proper tag structure\\na new reflections column has been added.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nReflective MAGLLAMA is a dataset created using MAGPIE-generated prompts in combination with reflection pattern responses generated by the LLaMa 3.1 70B model.\\nThis dataset is tailoredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1.1."},
	{"name":"axay-javascript-dataset-pn","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/israellaguan/axay-javascript-dataset-pn","creator_name":"Israel Antonio Rosales Laguan","creator_url":"https://huggingface.co/israellaguan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDPO JavaScript Dataset\\n\\t\\n\\nThis repository contains a modified version of the JavaScript dataset originally sourced from axay/javascript-dataset-pn. The dataset has been adapted to fit the DPO (Dynamic Programming Object) format, making it compatible with the LLaMA-Factory project.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicense\\n\\t\\n\\nThis dataset is licensed under the Apache 2.0 License.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThe dataset consists of JavaScript code snippets that have been restructured and enhanced forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/israellaguan/axay-javascript-dataset-pn."},
	{"name":"ball-maze-images","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sagar18/ball-maze-images","creator_name":"Basavasagar","creator_url":"https://huggingface.co/Sagar18","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Sagar18/ball-maze-images\\n\\t\\n\\nThis dataset contains visual observations of a ball maze environment, derived from the original state-space dataset at notmahi/tutorial-ball-top-20.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nfrom datasets import load_dataset\\n\\n# Load the dataset\\ndataset = load_dataset(\\\"Sagar18/ball-maze-images\\\")\\n\\n# Access images (they will be automatically decoded)\\nimage = dataset[0][\\\"image\\\"]\\nstate = dataset[0][\\\"state\\\"]\\n\\n"},
	{"name":"ball-maze-lerobot","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sagar18/ball-maze-lerobot","creator_name":"Basavasagar","creator_url":"https://huggingface.co/Sagar18","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBall Maze Environment Dataset\\n\\t\\n\\nThis dataset contains episodes of a ball maze environment, converted to the LeRobot format for visualization and training.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nSagar18/ball-maze-lerobot/\\nâ”œâ”€â”€ data/                 # Main dataset files\\nâ”œâ”€â”€ metadata/            # Dataset metadata\\nâ”‚   â””â”€â”€ info.json       # Configuration and version info\\nâ”œâ”€â”€ episode_data_index.safetensors  # Episode indexing information\\nâ””â”€â”€ stats.safetensors    # Dataset statisticsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sagar18/ball-maze-lerobot."},
	{"name":"poemma-10k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/0x7o/poemma-10k","creator_name":"Danil Kononyuk","creator_url":"https://huggingface.co/0x7o","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPoemma 10K\\n\\t\\n\\nDataset for naturalisations of language model outputs in the domain of poems.\\n"},
	{"name":"ru_ifeval-like-data","keyword":"rlaif","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mizinovmv/ru_ifeval-like-data","creator_name":"maksim","creator_url":"https://huggingface.co/mizinovmv","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRussian IFEval Like Data\\n\\t\\n\\nThis dataset contains instruction-response pairs synthetically generated using Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8 and chatgpt-4o-latest following the style of google/IFEval dataset and verified for correctness with #TODO lm-evaluation-harness.\\nThank you for the inspiration ifeval_like_dataset.\\n"},
	{"name":"synth-apigen-llama","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/synth-apigen-llama","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for argilla-warehouse/synth-apigen-llama\\n\\t\\n\\nThis dataset has been created with distilabel.\\nThe pipeline script was uploaded to easily reproduce the dataset:\\nsynth_apigen.py.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset creation\\n\\t\\n\\nThis dataset is a replica in distilabel of the framework\\ndefined in: APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets.\\nUsing the seed dataset of synthetic python functions in argilla-warehouse/python-seed-toolsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/synth-apigen-llama."},
	{"name":"dpo-chatml-mix-alt","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-alt","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-alt dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"dpo-merged","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-merged","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-merged dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Hopper-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Hopper-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHopper-v2 - Continuous Imitation Learning from Observation\\n\\t\\n\\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\\nIt is based on Hopper-v2, which is an older version for the MuJoCo environment.\\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 10 episodes with an average episodic reward of 3760.6908.\\nEachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Hopper-v2."},
	{"name":"InvertedPendulum-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/InvertedPendulum-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInvertedPendulum-v2 - Continuous Imitation Learning from Observation\\n\\t\\n\\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\\nIt is based on InvertedPendulum-v2, which is an older version for the MuJoCo environment.\\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 10 episodes with an average episodic rewardâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/InvertedPendulum-v2."},
	{"name":"Swimmer-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Swimmer-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSwimmer-v2 - Continuous Imitation Learning from Observation\\n\\t\\n\\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\\nIt is based on Swimmer-v2, which is an older version for the MuJoCo environment.\\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 4 episodes with an average episodic reward of 259.5244.\\nEachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Swimmer-v2."},
	{"name":"dpo-chatml-mix-alt","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-alt","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-alt dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"aya_dutch_dpo_binarized","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for aya_dutch_dpo\\n\\t\\n\\nThis dataset has been created with distilabel.\\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\\nThe dataset was constructed using the following steps:\\n\\nstarting with the aya_dataset and filtering for Dutch examples\\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized."},
	{"name":"hh-rlhf-chat-template","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Rexhaif/hh-rlhf-chat-template","creator_name":"Daniil Larionov","creator_url":"https://huggingface.co/Rexhaif","description":"Rexhaif/hh-rlhf-chat-template dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"dpo-merged-binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-merged-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-merged-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"aya_dutch_dpo_binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for aya_dutch_dpo\\n\\t\\n\\nThis dataset has been created with distilabel.\\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\\nThe dataset was constructed using the following steps:\\n\\nstarting with the aya_dataset and filtering for Dutch examples\\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized."},
	{"name":"hh-rlhf-chat-template","keyword":"rl","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Rexhaif/hh-rlhf-chat-template","creator_name":"Daniil Larionov","creator_url":"https://huggingface.co/Rexhaif","description":"Rexhaif/hh-rlhf-chat-template dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"aya_dutch_dpo_binarized","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for aya_dutch_dpo\\n\\t\\n\\nThis dataset has been created with distilabel.\\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\\nThe dataset was constructed using the following steps:\\n\\nstarting with the aya_dataset and filtering for Dutch examples\\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized."},
	{"name":"hh-rlhf-chat-template","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Rexhaif/hh-rlhf-chat-template","creator_name":"Daniil Larionov","creator_url":"https://huggingface.co/Rexhaif","description":"Rexhaif/hh-rlhf-chat-template dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"mimicgen59","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ml-jku/mimicgen59","creator_name":"Institute for Machine Learning, Johannes Kepler University Linz","creator_url":"https://huggingface.co/ml-jku","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMimicgen 59\\n\\t\\n\\nThis repository contains the generated Mimicgen datasets as used in \\\"A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks\\\": \\n\\ntrajectories for each task are stored in a separate .tar.gz\\nevery trajectory is stored in a separate .hdf5 file.\\n\\nFor more information on Mimicgen, we refer to the original documentation: https://mimicgen.github.io/docs/introduction/overview.html.\\nDownload the dataset using:\\nhuggingface-cli download ml-jku/mimicgen59â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ml-jku/mimicgen59."},
	{"name":"dm_control_10M","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ml-jku/dm_control_10M","creator_name":"Institute for Machine Learning, Johannes Kepler University Linz","creator_url":"https://huggingface.co/ml-jku","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDMControl 10M\\n\\t\\n\\nThis repository contains the DMControl datasets as used in \\\"A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks\\\": \\n\\nTrajectories for every task are stored as separate .tar.gz files. \\nEvery .tar.gz file contains 10M transitions.\\nEvery trajectory is stored as a separate .hdf5 file.\\n\\nDownload the dataset using the huggingface-cli:\\nhuggingface-cli download ml-jku/dm_control_10M --local-dir=./dm_control_10M --repo-type dataset\\n\\nForâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ml-jku/dm_control_10M."},
	{"name":"hh-rlhf-formatted","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KHuss/hh-rlhf-formatted","creator_name":"Karim El Husseini","creator_url":"https://huggingface.co/KHuss","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReformatted version of Anthropic's hh-rlhf dataset\\n\\t\\n\\nOriginal available at https://huggingface.co/datasets/Anthropic/hh-rlhf. (Does not include red teaming data)\\nRLHF datasets are in general defined as a collection of triples D={(x,y_1,y_2)_n} where x is the prompt, y_1 the chosen reponse and y_2 the rejected response. \\nThe original dataset provides two columns, \\\"chosen\\\"=x+y_1 and \\\"rejected\\\"=x+y_2.\\nVarious RLHF setups may require either format, so in this dataset we keep theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KHuss/hh-rlhf-formatted."},
	{"name":"nvidia-HelpSteer2","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Triangle104/nvidia-HelpSteer2","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHelpSteer2: Open-source dataset for training top-performing reward models\\n\\t\\n\\nHelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\\nThis dataset has been created in partnership with Scale  AI. \\nWhen used to tune a Llama 3.1 70B Instruct Model, we achieve 94.1% on RewardBench, which makes it the best Rewardâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Triangle104/nvidia-HelpSteer2."},
	{"name":"synthetic-domain-text-classification","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/synthetic-domain-text-classification","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for my-distiset-b845cf19\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/davidberenstein1957/my-distiset-b845cf19/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/synthetic-domain-text-classification."},
	{"name":"my-distiset-620d36eb","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kritsanan/my-distiset-620d36eb","creator_name":"namoang","creator_url":"https://huggingface.co/kritsanan","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for my-distiset-620d36eb\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/kritsanan/my-distiset-620d36eb/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kritsanan/my-distiset-620d36eb."},
	{"name":"my-distiset-bedefff0","keyword":"rlaif","license":"Artistic License 2.0","license_url":"https://choosealicense.com/licenses/artistic-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Juno360219/my-distiset-bedefff0","creator_name":"Jessie Rice","creator_url":"https://huggingface.co/Juno360219","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for my-distiset-bedefff0\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/Juno360219/my-distiset-bedefff0/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Juno360219/my-distiset-bedefff0."},
	{"name":"EMMOE-100","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Dongping-Li/EMMOE-100","creator_name":"Dongping Li","creator_url":"https://huggingface.co/Dongping-Li","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dongping-Li/EMMOE-100."},
	{"name":"fantastiq","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sumuks/fantastiq","creator_name":"Sumuk Shashidhar","creator_url":"https://huggingface.co/sumuks","description":"\\n\\t\\n\\t\\t\\n\\t\\tðŸ‰ FantastiQ\\n\\t\\n\\n\\n    \\n\\n\\n\\nFantastiQ: A fictional reasoning benchmark for evaluating inference and logical capabilities beyond memorization.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is FantastiQ?\\n\\t\\n\\nFantastiQ ðŸ‰ is a synthetic benchmark consisting of question-answer pairs crafted around fictional yet internally consistent scenarios. It is designed specifically to assess logical reasoning, inference skills, and robustness against explicit memorization by Large Language Models (LLMs).\\nFantastiQ includes multipleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sumuks/fantastiq."},
	{"name":"augmented_codealpaca-20k-using-together-ai-deepseek-v1","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/eagle0504/augmented_codealpaca-20k-using-together-ai-deepseek-v1","creator_name":"Yiqiao Yin","creator_url":"https://huggingface.co/eagle0504","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThis dataset, named CodeAlpaca-20k, consists of examples that blend coding instructions with outputs and reasoning. Each entry includes structured fields like output, instruction, input, and cot (Chain of Thought). It is particularly designed to train and evaluate AI models that generate code and explanations based on simple programming tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Collection and Preparation\\n\\t\\n\\nData entries are augmented using the augment_answer function that makes APIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/eagle0504/augmented_codealpaca-20k-using-together-ai-deepseek-v1."},
	{"name":"web_agents_google_flight_trajectories","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/anonx3247/web_agents_google_flight_trajectories","creator_name":"Anas lecaillon","creator_url":"https://huggingface.co/anonx3247","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWeb Agent Google Flight Trajectories\\n\\t\\n\\nThis dataset was originally created on Nov 23 2024 during EF's Ai On Edge Hackathon.\\nThe purpose of this dataset is to give both positive and negative image web-agent trajectories to finetune small edge-models on web agentic tasks.\\n"},
	{"name":"SQL-Ultrafeedback","keyword":"rlaif","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/mjerome89/SQL-Ultrafeedback","creator_name":"Maurice","creator_url":"https://huggingface.co/mjerome89","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SQL-Ultrafeedback\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/mjerome89/SQL-Ultrafeedback/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mjerome89/SQL-Ultrafeedback."},
	{"name":"Spider_gpt4o","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mjerome89/Spider_gpt4o","creator_name":"Maurice","creator_url":"https://huggingface.co/mjerome89","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Spider_gpt4o\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/mjerome89/Spider_gpt4o/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mjerome89/Spider_gpt4o."},
	{"name":"open-image-preferences-v1-results","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-results","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for image-preferences-results\\n\\t\\n\\n\\n\\n\\n\\n  \\n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world, vibrant colors, 4K.\\n      \\n          \\n              \\n              Image 1\\n          \\n          \\n              \\n              Image 2\\n          \\n      \\n  \\n\\n\\n\\n  \\n      Prompt: 8-bit pixel art of a blue knight, green car, and glacier landscape in Norway, fantasy style, colorful and detailed.\\n      \\n          \\n              \\n              Imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-results."},
	{"name":"Vl-RewardBench","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Zhihui/Vl-RewardBench","creator_name":"Xie","creator_url":"https://huggingface.co/Zhihui","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for VLRewardBench\\n\\t\\n\\nProject Page:\\nhttps://vl-rewardbench.github.io\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nVLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality examples specifically curated to probe model limitations.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nEach instance consists of multimodal queriesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zhihui/Vl-RewardBench."},
	{"name":"wmdp-cyber-corpus_unpaired-preference","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AMindToThink/wmdp-cyber-corpus_unpaired-preference","creator_name":"Matthew Khoriaty","creator_url":"https://huggingface.co/AMindToThink","description":"This is the \\\"cyber\\\" data from https://huggingface.co/datasets/cais/wmdp-corpora repackaged into the format of \\\"unpaired preference\\\". https://huggingface.co/docs/trl/v0.11.1/en/dataset_formats#unpaired-preference\\nData to retain is considered good so it is mapped to \\\"True\\\", while forget data is mapped to \\\"False.\\\"\\nPrompt is left empty since the text from WMDP doesn't come with a prompt.\\nI have no idea whether this would really work for Reinforcement Learning, but I plan to try it out. Use at yourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AMindToThink/wmdp-cyber-corpus_unpaired-preference."},
	{"name":"Notus_Aegis-v1","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1","creator_name":"Parag Ekbote","creator_url":"https://huggingface.co/AINovice2005","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataSet Overview:\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataSet Name:Notus_Aegis-v1\\n\\t\\n\\n\\n\\n\\n This starter dataset is designed to facilitate research and development in the field of natural language processing, specifically focusing on instruction-response pairs.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeatures:\\n\\t\\n\\n\\nDiversity of Instructions: The dataset encompasses a broad spectrum of topics, ensuring applicability across different domains.\\nHigh-Quality Responses: Leveraging the notus-7b-v1 model ensures that the responses areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1."},
	{"name":"Ant-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Ant-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAnt-v2 - Continuous Imitation Learning from Observation\\n\\t\\n\\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\\nIt is based on Ant-v2, which is an older version for the MuJoCo environment.\\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 10 episodes with an average episodic reward of 5514.0229.\\nEach entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Ant-v2."},
	{"name":"HalfCheetah-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHalfCheetah-v2 - Continuous Imitation Learning from Observation\\n\\t\\n\\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\\nIt is based on HalfCheetah-v2, which is an older version for the MuJoCo environment.\\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 10 episodes with an average episodic reward ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v2."},
	{"name":"Notus_Aegis-v1","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1","creator_name":"Parag Ekbote","creator_url":"https://huggingface.co/AINovice2005","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataSet Overview:\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataSet Name:Notus_Aegis-v1\\n\\t\\n\\n\\n\\n\\n This starter dataset is designed to facilitate research and development in the field of natural language processing, specifically focusing on instruction-response pairs.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeatures:\\n\\t\\n\\n\\nDiversity of Instructions: The dataset encompasses a broad spectrum of topics, ensuring applicability across different domains.\\nHigh-Quality Responses: Leveraging the notus-7b-v1 model ensures that the responses areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1."},
	{"name":"llama70B-dpo-dataset-transformed","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset-transformed","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset-transformed dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"dpo-chatml-mix-binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"hh-rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Anthropic/hh-rlhf","creator_name":"Anthropic","creator_url":"https://huggingface.co/Anthropic","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for HH-RLHF\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository provides access to two different kinds of data:\\n\\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likelyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Anthropic/hh-rlhf."},
	{"name":"llama70B-dpo-dataset-transformed","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset-transformed","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset-transformed dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"dpo-chatml-mix-binarized","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"oasst1","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenAssistant/oasst1","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenAssistant Conversations Dataset (OASST1)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \\nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \\ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \\nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \\nis a product of a worldwide crowd-sourcing effortâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst1."},
	{"name":"oasst2","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenAssistant/oasst2","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpen Assistant Conversations Dataset Release 2 (OASST2)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThis dataset contains message trees. Each message tree has an initial prompt message as the root node, \\nwhich can have multiple child messages as replies, and these child messages can have multiple replies. \\nAll messages have a role property: this can either be \\\"assistant\\\" or \\\"prompter\\\". The roles in \\nconversation threads from prompt to leaf node strictly alternate between \\\"prompter\\\" andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst2."},
	{"name":"HelpSteer2","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nvidia/HelpSteer2","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHelpSteer2: Open-source dataset for training top-performing reward models\\n\\t\\n\\nHelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\\nThis dataset has been created in partnership with Scale  AI. \\nWhen used to tune a Llama 3.1 70B Instruct Model, we achieve 94.1% on RewardBench, which makes it the best Rewardâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/HelpSteer2."},
	{"name":"helpful-instructions","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful-instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Helpful Instructions\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nHelpful Instructions is a dataset of (instruction, demonstration) pairs that are derived from public datasets. As the name suggests, it focuses on instructions that are \\\"helpful\\\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform. You can load the dataset as follows:\\nfrom datasets import load_dataset\\n\\n# Load all subsets\\nhelpful_instructions =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/helpful-instructions."},
	{"name":"hhh_alignment","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/hhh_alignment","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"This task evaluates language models on alignment, broken down into categories of helpfulness, honesty/accuracy, harmlessness, and other.  The evaluations imagine a conversation between a person and a language model assistant.  The goal with these evaluations is that on careful reflection, the vast majority of people would agree that the chosen response is better (more helpful, honest, and harmless) than the alternative offered for comparison.  The task is formatted in terms of binary choices, though many of these have been broken down from a ranked ordering of three or four possible responses."},
	{"name":"ShareGPT52K","keyword":"rlhf","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RyokoAI/ShareGPT52K","creator_name":"Ryoko AI","creator_url":"https://huggingface.co/RyokoAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ShareGPT52K90K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\\nThis repository now contains the new 90K conversations version. The previous 52K may\\nbe found in the old/ directory.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\ntext-generation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RyokoAI/ShareGPT52K."},
	{"name":"Ling-Coder-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/inclusionAI/Ling-Coder-DPO","creator_name":"inclusionAI","creator_url":"https://huggingface.co/inclusionAI","description":"\\n    \\n\\n\\n\\n          ðŸ¤— Hugging Face\\n          ðŸ¤– ModelScope\\n          ðŸ–¥ï¸ GitHub\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tLing-Coder Dataset\\n\\t\\n\\nThe Ling-Coder Dataset comprises the following components:\\n\\nLing-Coder-SFT: A subset of SFT data used for training Ling-Coder Lite, containing more than 5 million samples.\\nLing-Coder-DPO: A subset of DPO data used for training Ling-Coder Lite, containing 250k samples.\\nLing-Coder-SyntheticQA: A subset of synthetic data used for annealing training of Ling-Coder Lite, containing moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/inclusionAI/Ling-Coder-DPO."},
	{"name":"stack-exchange-preferences","keyword":"human-feedback","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for H4 Stack Exchange Preferences Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains questions and answers from the Stack Overflow Data Dump for the purpose of preference model training. \\nImportantly, the questions have been filtered to fit the following criteria for preference models (following closely from Askell et al. 2021): have >=2 answers. \\nThis data could also be used for instruction fine-tuning and language model training.\\nThe questions areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences."},
	{"name":"stack-exchange-preferences","keyword":"rlhf","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for H4 Stack Exchange Preferences Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains questions and answers from the Stack Overflow Data Dump for the purpose of preference model training. \\nImportantly, the questions have been filtered to fit the following criteria for preference models (following closely from Askell et al. 2021): have >=2 answers. \\nThis data could also be used for instruction fine-tuning and language model training.\\nThe questions areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences."},
	{"name":"jat-dataset","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jat-project/jat-dataset","creator_name":"Jack of All Trades project","creator_url":"https://huggingface.co/jat-project","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJAT Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\\nPaper: https://huggingface.co/papers/2402.09844\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\n>>> from datasets import load_dataset\\n>>> dataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jat-project/jat-dataset."},
	{"name":"jat-dataset","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jat-project/jat-dataset","creator_name":"Jack of All Trades project","creator_url":"https://huggingface.co/jat-project","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJAT Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\\nPaper: https://huggingface.co/papers/2402.09844\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\n>>> from datasets import load_dataset\\n>>> dataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jat-project/jat-dataset."},
	{"name":"social-reasoning-rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf","creator_name":"Prolific","creator_url":"https://huggingface.co/ProlificAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository provides access to a social reasoning dataset that aims to provide signal to how humans navigate social situations, how they reason about them and how they understand each other. It contains questions probing people's thinking and understanding of various social situations.\\nThis dataset was created by collating a set of questions within the following social reasoning tasks:\\n\\nunderstanding of emotions\\nintent recognition\\nsocial norms\\nsocialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf."},
	{"name":"Nectar","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berkeley-nest/Nectar","creator_name":"Berkeley-Nest","creator_url":"https://huggingface.co/berkeley-nest","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Nectar\\n\\t\\n\\n\\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\\n\\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, includingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/berkeley-nest/Nectar."},
	{"name":"social-reasoning-rlhf","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf","creator_name":"Prolific","creator_url":"https://huggingface.co/ProlificAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository provides access to a social reasoning dataset that aims to provide signal to how humans navigate social situations, how they reason about them and how they understand each other. It contains questions probing people's thinking and understanding of various social situations.\\nThis dataset was created by collating a set of questions within the following social reasoning tasks:\\n\\nunderstanding of emotions\\nintent recognition\\nsocial norms\\nsocialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf."},
	{"name":"Nectar","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berkeley-nest/Nectar","creator_name":"Berkeley-Nest","creator_url":"https://huggingface.co/berkeley-nest","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Nectar\\n\\t\\n\\n\\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\\n\\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, includingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/berkeley-nest/Nectar."},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\\n\\t\\n\\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDifferences with argilla/ultrafeedback-binarized-preferencesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned."},
	{"name":"orca_dpo_pairs_dutch","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Orca DPO Pairs Dutch\\n\\t\\n\\n\\nI recommend using the cleaned, deduplicated version. https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch."},
	{"name":"ultrafeedback-multi-binarized-preferences-cleaned","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltraFeedback - Multi-Binarized using the Average of Preference Ratings (Cleaned)\\n\\t\\n\\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences-cleaned,\\nand has been created to explore whether DPO fine-tuning with more than one rejection per chosen response helps the model perform better in the \\nAlpacaEval, MT-Bench, and LM Eval Harness benchmarks.\\nRead more about Argilla's approach towards UltraFeedback binarization atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned."},
	{"name":"HelpSteer","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nvidia/HelpSteer","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHelpSteer: Helpfulness SteerLM Dataset\\n\\t\\n\\nHelpSteer is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\\nLeveraging this dataset and SteerLM, we train a Llama 2 70B to reach 7.54 on MT Bench, the highest among models trained on open-source datasets based on MT Bench Leaderboard as of 15 Nov 2023.\\nThis model is availableâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/HelpSteer."},
	{"name":"Open_Assistant_Conversation_Chains","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-ric/Open_Assistant_Conversation_Chains","creator_name":"Aymeric Roucher","creator_url":"https://huggingface.co/m-ric","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset description\\n\\t\\n\\n\\n\\nThis dataset is a reformatting of OpenAssistant Conversations (OASST1), which is\\n\\na human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.\\n\\nIt wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-ric/Open_Assistant_Conversation_Chains."},
	{"name":"orca_dpo_pairs","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/orca_dpo_pairs","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Orca DPO Pair\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a pre-processed version of the OpenOrca dataset.\\nThe original OpenOrca dataset is a collection of augmented FLAN data that aligns, as best as possible, with the distributions outlined in the Orca paper.\\nIt has been instrumental in generating high-performing preference-tuned model checkpoints and serves as a valuable resource for all NLP researchers and developers!\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe OrcaDPOâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/orca_dpo_pairs."},
	{"name":"distilabel-intel-orca-dpo-pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdistilabel Orca Pairs for DPO\\n\\t\\n\\nThe dataset is a \\\"distilabeled\\\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs."},
	{"name":"gutenberg-dpo-v0.1","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jondurbin/gutenberg-dpo-v0.1","creator_name":"Jon Durbin","creator_url":"https://huggingface.co/jondurbin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGutenberg DPO\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis is a dataset meant to enhance novel writing capabilities of LLMs, by using public domain books from Project Gutenberg\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProcess\\n\\t\\n\\nFirst, the each book is parsed, split into chapters, cleaned up from the original format (remove superfluous newlines, illustration tags, etc.).\\nOnce we have chapters, an LLM is prompted with each chapter to create a synthetic prompt that would result in that chapter being written.\\nEach chapterâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jondurbin/gutenberg-dpo-v0.1."},
	{"name":"french_orca_dpo_pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_orca_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for french_orca_dpo_pairs\\n\\t\\n\\nThis dataset offers a french translation of the 12k DPO Intel/orca_dpo_pairs pairs made from Open-Orca/OpenOrca.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"gutenberg-dpo-v0.1-tr","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duxx/gutenberg-dpo-v0.1-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","description":"duxx/gutenberg-dpo-v0.1-tr dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"distilabel-intel-orca-dpo-pairs-tr","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdistilabel Orca Pairs for DPO\\n\\t\\n\\nThe dataset is a \\\"distilabeled\\\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr."},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCapybara-DPO 7K binarized\\n\\t\\n\\n\\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\\n\\n\\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\\n\\n\\n    \\n\\n\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy?\\n\\t\\n\\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized."},
	{"name":"openassistant-deepseek-coder","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-deepseek-coder","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChat Fine-tuning Dataset - OpenAssistant DeepSeek Coder\\n\\t\\n\\nThis dataset allows for fine-tuning chat models using:\\nB_INST = '\\\\n### Instruction:\\\\n'\\nE_INST = '\\\\n### Response:\\\\n'\\nBOS = '<ï½œbeginâ–ofâ–sentenceï½œ>'\\nEOS = '\\\\n<|EOT|>\\\\n'\\n\\nSample Preparation:\\n\\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-deepseek-coder."},
	{"name":"french_orca_dpo_pairs","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_orca_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for french_orca_dpo_pairs\\n\\t\\n\\nThis dataset offers a french translation of the 12k DPO Intel/orca_dpo_pairs pairs made from Open-Orca/OpenOrca.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"distilabel-intel-orca-dpo-pairs","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdistilabel Orca Pairs for DPO\\n\\t\\n\\nThe dataset is a \\\"distilabeled\\\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs."},
	{"name":"french_orca_dpo_pairs","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_orca_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for french_orca_dpo_pairs\\n\\t\\n\\nThis dataset offers a french translation of the 12k DPO Intel/orca_dpo_pairs pairs made from Open-Orca/OpenOrca.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"gutenberg-dpo-v0.1-tr","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duxx/gutenberg-dpo-v0.1-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","description":"duxx/gutenberg-dpo-v0.1-tr dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"distilabel-intel-orca-dpo-pairs-tr","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdistilabel Orca Pairs for DPO\\n\\t\\n\\nThe dataset is a \\\"distilabeled\\\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr."},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCapybara-DPO 7K binarized\\n\\t\\n\\n\\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\\n\\n\\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\\n\\n\\n    \\n\\n\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy?\\n\\t\\n\\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized."},
	{"name":"distilabel-intel-orca-dpo-pairs","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdistilabel Orca Pairs for DPO\\n\\t\\n\\nThe dataset is a \\\"distilabeled\\\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs."},
	{"name":"distilabel-intel-orca-dpo-pairs-tr","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdistilabel Orca Pairs for DPO\\n\\t\\n\\nThe dataset is a \\\"distilabeled\\\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr."},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCapybara-DPO 7K binarized\\n\\t\\n\\n\\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\\n\\n\\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\\n\\n\\n    \\n\\n\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy?\\n\\t\\n\\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized."},
	{"name":"opin-pref","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \\nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\\nhf.co/swaroop-nath/prompt-opin-summ dataset.\\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\\n{Â Â Â Â 'unique-id': a unique idâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref."},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","description":"(WIP)\\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \\nIn the next days, they will be filtered out. To do so manually, just look for \\\"Ã¼bersetz\\\" in the columns \\\"input\\\", \\\"chosen\\\" or \\\"rejected\\\"\\nand exclude them from your training pipeline.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tULTRA Distilabel Intel Orca DPO (German):\\n\\t\\n\\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\\nThe provided dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de."},
	{"name":"dpo-mix-7k","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/dpo-mix-7k","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArgilla DPO Mix 7K Dataset\\n\\t\\n\\n\\nA small cocktail combining DPO datasets built by Argilla with distilabel. The goal of this dataset is having a small, high-quality DPO dataset by filtering only highly rated chosen responses. \\n\\n\\n    \\n\\n\\n\\n\\n  \\n    \\n  \\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDatasets mixed\\n\\t\\n\\nAs already mentioned, this dataset mixes the following datasets:\\n\\nargilla/distilabel-capybara-dpo-7k-binarized: random sample of highly scored chosen responses (>=4).\\nargilla/distilabel-intel-orca-dpo-pairs:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/dpo-mix-7k."},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","description":"(WIP)\\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \\nIn the next days, they will be filtered out. To do so manually, just look for \\\"Ã¼bersetz\\\" in the columns \\\"input\\\", \\\"chosen\\\" or \\\"rejected\\\"\\nand exclude them from your training pipeline.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tULTRA Distilabel Intel Orca DPO (German):\\n\\t\\n\\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\\nThe provided dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de."},
	{"name":"opin-pref","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \\nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\\nhf.co/swaroop-nath/prompt-opin-summ dataset.\\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\\n{Â Â Â Â 'unique-id': a unique idâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref."},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","description":"(WIP)\\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \\nIn the next days, they will be filtered out. To do so manually, just look for \\\"Ã¼bersetz\\\" in the columns \\\"input\\\", \\\"chosen\\\" or \\\"rejected\\\"\\nand exclude them from your training pipeline.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tULTRA Distilabel Intel Orca DPO (German):\\n\\t\\n\\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\\nThe provided dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de."},
	{"name":"opin-pref","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \\nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\\nhf.co/swaroop-nath/prompt-opin-summ dataset.\\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\\n{Â Â Â Â 'unique-id': a unique idâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref."},
	{"name":"Code_Vulnerability_Security_DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO","creator_name":"Byte","creator_url":"https://huggingface.co/CyberNative","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCybernative.ai Code Vulnerability and Security Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Cybernative.ai Code Vulnerability and Security Dataset is a dataset of synthetic Data Programming by Demonstration (DPO) pairs, focusing on the intricate relationship between secure and insecure code across a variety of programming languages. This dataset is meticulously crafted to serve as a pivotal resource for researchers, cybersecurity professionals, and AI developers who are keenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO."},
	{"name":"configurable-system-prompt-multitask","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vicgalle/configurable-system-prompt-multitask","creator_name":"Victor Gallego","creator_url":"https://huggingface.co/vicgalle","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tConfigurable System Prompt Multi-task Dataset ðŸ›ž\\n\\t\\n\\nWe release the synthetic dataset for the multi-task experiments from the paper \\\"Configurable Safety Tuning of Language Models with Synthetic Preference Data\\\", https://huggingface.co/papers/2404.00495. This dataset has two sources for the examples:\\n\\nSelf-critique on a safety task from Harmful Behaviours, using the SOLAR-Instruct model. It employs two system prompts to learn the different behaviors:\\nYou are a helpful yet harmlessâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vicgalle/configurable-system-prompt-multitask."},
	{"name":"aya_dutch_dpo","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for aya_dutch_dpo\\n\\t\\n\\nThis dataset has been created with distilabel.\\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\\nThe dataset was constructed using the following steps:\\n\\nstarting with the aya_dataset and filtering for Dutch examples\\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo."},
	{"name":"webui-dom-snapshots","keyword":"reinforcement-learning","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gbenson/webui-dom-snapshots","creator_name":"Gary Benson","creator_url":"https://huggingface.co/gbenson","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WebUI DOM snapshots\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: Gary Benson\\nLanguages: Mostly English (87%);\\nDutch, French, Chinese, Japanese (1-2% each); 30+ others (<1% each)\\nLicense: CC0 1.0 Universal\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More Information Needed]\\nPaper [optional]: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gbenson/webui-dom-snapshots."},
	{"name":"RLSTACK","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK."},
	{"name":"orca_dpo_pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/multilingual/orca_dpo_pairs","creator_name":"mLLM multilingual","creator_url":"https://huggingface.co/multilingual","description":"\\n    \\n\\n\\nmLLM IMPLEMENTATION OF Intel/orca_dpo_pairs.\\nLANGUAGES:\\nARABIC\\nCHINESE\\nFRENCH\\nGERMAN\\nRUSSIAN\\nSPANISH\\nTURKISH\\n(WIP)\\n"},
	{"name":"audio-alpaca","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/declare-lab/audio-alpaca","creator_name":"Deep Cognition and Language Research (DeCLaRe) Lab","creator_url":"https://huggingface.co/declare-lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAudio-alpaca: A preference dataset for aligning text-to-audio models\\n\\t\\n\\nAudio-alpaca is a pairwise preference dataset containing about 15k (prompt,chosen, rejected) triplets where given a textual prompt, chosen is the preferred generated audio and rejected is the undesirable audio.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tField details\\n\\t\\n\\nprompt: Given textual prompt\\nchosen: The preferred audio sample\\nrejected: The rejected audio sample\\n"},
	{"name":"Capybara-Preferences","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/Capybara-Preferences","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Capybara-Preferences\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n    \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is built on top of LDJnr/Capybara, in order to generate a preference\\ndataset out of an instruction-following dataset. This is done by keeping the conversations in the column conversation but splitting\\nthe last assistant turn from it, so that the conversation contains all the turns up until the last user's turn, so that it can beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Capybara-Preferences."},
	{"name":"orpo-dpo-mix-40k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-mix-40k v1.2\\n\\t\\n\\n\\nThis dataset is designed for ORPO or DPO training.\\nSee Fine-tune Llama 3 with ORPO for more information about how to use it.\\nIt is a combination of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)\\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k."},
	{"name":"aya_dutch_dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for aya_dutch_dpo\\n\\t\\n\\nThis dataset has been created with distilabel.\\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\\nThe dataset was constructed using the following steps:\\n\\nstarting with the aya_dataset and filtering for Dutch examples\\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo."},
	{"name":"DPO-En-Zh-20k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llamafactory/DPO-En-Zh-20k","creator_name":"LLaMA Factory","creator_url":"https://huggingface.co/llamafactory","description":"This dataset is composed by\\n\\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\\n10,000 examples of wenbopan/Chinese-dpo-pairs.\\n\\nYou can use it in LLaMA Factory by specifying dataset: dpo_mix_en,dpo_mix_zh.\\n"},
	{"name":"orca_dpo_pairs_dutch_cleaned","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Orca DPO Pairs Dutch Cleaned\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024},\\n      eprint={2412.04092},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.04092}, \\n}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned."},
	{"name":"DPO-En-Zh-20k-Preference","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference","creator_name":"Ming Xu (å¾æ˜Ž)","creator_url":"https://huggingface.co/shibing624","description":"This dataset is composed by\\n\\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\\n10,000 examples of wenbopan/Chinese-dpo-pairs.\\n\\nrefer: https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k   æ”¹äº†questionã€response_rejectedã€response_chosenå­—æ®µï¼Œæ–¹ä¾¿ORPOã€DPOæ¨¡åž‹è®­ç»ƒæ—¶ä½¿ç”¨\\ntrain usage:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference."},
	{"name":"RLSTACK","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK."},
	{"name":"orca_dpo_pairs","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/multilingual/orca_dpo_pairs","creator_name":"mLLM multilingual","creator_url":"https://huggingface.co/multilingual","description":"\\n    \\n\\n\\nmLLM IMPLEMENTATION OF Intel/orca_dpo_pairs.\\nLANGUAGES:\\nARABIC\\nCHINESE\\nFRENCH\\nGERMAN\\nRUSSIAN\\nSPANISH\\nTURKISH\\n(WIP)\\n"},
	{"name":"orpo-dpo-mix-40k","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-mix-40k v1.2\\n\\t\\n\\n\\nThis dataset is designed for ORPO or DPO training.\\nSee Fine-tune Llama 3 with ORPO for more information about how to use it.\\nIt is a combination of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)\\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k."},
	{"name":"DPO-En-Zh-20k","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llamafactory/DPO-En-Zh-20k","creator_name":"LLaMA Factory","creator_url":"https://huggingface.co/llamafactory","description":"This dataset is composed by\\n\\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\\n10,000 examples of wenbopan/Chinese-dpo-pairs.\\n\\nYou can use it in LLaMA Factory by specifying dataset: dpo_mix_en,dpo_mix_zh.\\n"},
	{"name":"DPO-En-Zh-20k-Preference","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference","creator_name":"Ming Xu (å¾æ˜Ž)","creator_url":"https://huggingface.co/shibing624","description":"This dataset is composed by\\n\\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\\n10,000 examples of wenbopan/Chinese-dpo-pairs.\\n\\nrefer: https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k   æ”¹äº†questionã€response_rejectedã€response_chosenå­—æ®µï¼Œæ–¹ä¾¿ORPOã€DPOæ¨¡åž‹è®­ç»ƒæ—¶ä½¿ç”¨\\ntrain usage:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference."},
	{"name":"RLSTACK","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK."},
	{"name":"aya_dutch_dpo","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for aya_dutch_dpo\\n\\t\\n\\nThis dataset has been created with distilabel.\\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\\nThe dataset was constructed using the following steps:\\n\\nstarting with the aya_dataset and filtering for Dutch examples\\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo."},
	{"name":"marathi-hh-rlhf-v02","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/amitagh/marathi-hh-rlhf-v02","creator_name":"Amit","creator_url":"https://huggingface.co/amitagh","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset card for Marathi HH-RLHF\\n\\t\\n\\nTranslated subset of HH-RLHF version for Marathi language.\\n"},
	{"name":"oasst2_french_dpo_pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/oasst2_french_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for oasst2_french_dpo_pairs\\n\\t\\n\\nThis dataset was created from OpenAssistant/oasst2 by keeping only the french data and producing dpo pairs with their rank.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"Multifaceted-Collection-DPO","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-DPO","creator_name":"KAIST AI","creator_url":"https://huggingface.co/kaist-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Multifaceted Collection DPO\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLinks for Reference\\n\\t\\n\\n\\nHomepage: https://lklab.kaist.ac.kr/Janus/ \\nRepository: https://github.com/kaistAI/Janus \\nPaper: https://arxiv.org/abs/2405.17977 \\nPoint of Contact: suehyunpark@kaist.ac.kr\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTL;DR\\n\\t\\n\\n\\nMultifaceted Collection is a preference dataset for aligning LLMs to diverse human preferences, where system messages are used to represent individual preferences. The instructions are acquired from fiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-DPO."},
	{"name":"oasst2_french_dpo_pairs","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/oasst2_french_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for oasst2_french_dpo_pairs\\n\\t\\n\\nThis dataset was created from OpenAssistant/oasst2 by keeping only the french data and producing dpo pairs with their rank.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"oasst2_uzbek","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MLDataScientist/oasst2_uzbek","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpen Assistant Conversations Dataset Release 2 (OASST2) in Uzbek language\\n\\t\\n\\nThis dataset is an Uzbek translated version of OASST2 dataset.\\nLlama3 chat template + thread formatted dataset based on this translation is also available for model fine-tuning here. \\nThe Uzbek translation was completed in 45 hours using a single T4 GPU and nllb-200-3.3B model.\\nBased on nllb metrics, you might want to only filter out records that were not originally in English or Russian sinceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MLDataScientist/oasst2_uzbek."},
	{"name":"oasst2_french_dpo_pairs","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/oasst2_french_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for oasst2_french_dpo_pairs\\n\\t\\n\\nThis dataset was created from OpenAssistant/oasst2 by keeping only the french data and producing dpo pairs with their rank.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"cosmochat","keyword":"rlaif","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/cosmochat","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for CosmoChat\\n\\t\\n\\n\\n  \\n    \\n  \\n\\n\\nThis dataset has been created with distilabel. It is a WIP and is likely to change whenever I have some spare time to work on this! Feel free to follow my harebrained ideas for improving this here: https://huggingface.co/datasets/davanstrien/cosmochat/discussions\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCan we create pedagogically valuable multi-turn synthetic datasets from Cosmopedia?\\n\\t\\n\\nSynthetic datasets are increasingly helping to push forward theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/cosmochat."},
	{"name":"WebInstructSub-prometheus","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chargoddard/WebInstructSub-prometheus","creator_name":"Charles Goddard","creator_url":"https://huggingface.co/chargoddard","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WebInstructSub-prometheus\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTIGER-Lab/WebInstructSub evaluated for logical and effective reasoning using prometheus-7b-v2.0.\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/chargoddard/WebInstructSub-prometheus."},
	{"name":"procgen","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EpicPinkPenguin/procgen","creator_name":"Marcus Fechner","creator_url":"https://huggingface.co/EpicPinkPenguin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProcgen Benchmark\\n\\t\\n\\nThis dataset contains expert trajectories generated by a PPO reinforcement learning agent trained on each of the 16 procedurally-generated gym environments from the Procgen Benchmark. The environments were created on distribution_mode=easy and with unlimited levels.\\nDisclaimer: This is not an official repository from OpenAI.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Usage\\n\\t\\n\\nRegular usage (for environment bigfish):\\nfrom datasets import load_dataset\\ntrain_dataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/EpicPinkPenguin/procgen."},
	{"name":"evol-dpo-ita","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/evol-dpo-ita","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\\n  \\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nEvol-DPO-Ita is a high-quality dataset consisting of ~20,000 preference pairs derived from responses generated by two large language models: Claude Opus and GPT-3.5 Turbo.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Features\\n\\t\\n\\n\\n20,000 Preference Comparisons: Each entry contains two model responses â€” Claude Opus (claude-3-opus-20240229) for the chosen response and GPT-3.5 Turbo for the rejected one, both generated from a translated prompt from the original Evol-Instruct dataset (promptâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/efederici/evol-dpo-ita."},
	{"name":"evol-dpo-ita","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/evol-dpo-ita","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\\n  \\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nEvol-DPO-Ita is a high-quality dataset consisting of ~20,000 preference pairs derived from responses generated by two large language models: Claude Opus and GPT-3.5 Turbo.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Features\\n\\t\\n\\n\\n20,000 Preference Comparisons: Each entry contains two model responses â€” Claude Opus (claude-3-opus-20240229) for the chosen response and GPT-3.5 Turbo for the rejected one, both generated from a translated prompt from the original Evol-Instruct dataset (promptâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/efederici/evol-dpo-ita."},
	{"name":"Buzz-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2."},
	{"name":"Buzz-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2."},
	{"name":"Buzz-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2."},
	{"name":"orcaratgen","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/redsgnaoh/orcaratgen","creator_name":"Hoang Anh Just","creator_url":"https://huggingface.co/redsgnaoh","description":"An rationale-enhanced version of the paired preference learning dataset Intel-ORCA-DPO.\\nThese rationales are general, high-level explanation of why the chosen response is preferred over the rejected response.\\nThe dataset was generated according to this paper: Data-Centric Human Preference Optimization with\\nRationales.\\n"},
	{"name":"HelpSteer2-DPO","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Atsunori/HelpSteer2-DPO","creator_name":"Atsunori Fujita","creator_url":"https://huggingface.co/Atsunori","description":"This dataset is a conversion of nvidia/HelpSteer2 into preference pairs based on the helpfulness score for training DPO.\\nHelpSteer2-DPO is also licensed under CC-BY-4.0.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nIn accordance with the following paper, HelpSteer2: Open-source dataset for training top-performing reward models\\nwe converted nvidia/HelpSteer2 dataset into a preference dataset by taking the response with the higher helpfulness score as the chosen response, with the remaining response beingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Atsunori/HelpSteer2-DPO."},
	{"name":"ethics_qna_preferences","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_qna_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\\nExamples:\\n  subset virtue\\n    {'prompt': 'Sentence: Jill wore a new style of distinctive clothes each six months for decades.\\\\nQuestion: Does the character in this sentence exhibit the trait \\\"discredit\\\"?\\\\nAnswer: ',\\n   'chosen': 'no',\\n   'rejected': 'yes'}\\n  \\n  commonsense\\n  {'prompt': 'Post:\\\\n\\\"\\\"I went to the principal\\\\'s office to change my records before going to a different school.\\\"\\\"\\\\n\\\\n\\\\nVerdict: 'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_qna_preferences."},
	{"name":"ethics_qna_preferences","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_qna_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\\nExamples:\\n  subset virtue\\n    {'prompt': 'Sentence: Jill wore a new style of distinctive clothes each six months for decades.\\\\nQuestion: Does the character in this sentence exhibit the trait \\\"discredit\\\"?\\\\nAnswer: ',\\n   'chosen': 'no',\\n   'rejected': 'yes'}\\n  \\n  commonsense\\n  {'prompt': 'Post:\\\\n\\\"\\\"I went to the principal\\\\'s office to change my records before going to a different school.\\\"\\\"\\\\n\\\\n\\\\nVerdict: 'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_qna_preferences."},
	{"name":"gsm8k-reasoning","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thesven/gsm8k-reasoning","creator_name":"Michael Svendsen","creator_url":"https://huggingface.co/thesven","description":"\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for gsm8k-reasoning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nGSM8K Reasoning is a dataset derived from the openai/gsm8k dataset, focusing on enhancing math problem-solving through reasoning-based prompts and solutions.\\nThis version emphasizes logical reasoning and step-by-step thought processes in mathematics, pushing models to generate solutions that reflect human-like deductive reasoning.\\nThe dataset is curated using a specialized pipeline designed to encourageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thesven/gsm8k-reasoning."},
	{"name":"python-lib-tools-v0.1","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/python-lib-tools-v0.1","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for python-lib-tools-v0.1\\n\\t\\n\\nThis dataset has been created with distilabel.\\nThe pipeline script was uploaded to easily reproduce the dataset:\\npython_tool_synth.py.\\nIt can be run directly using the CLI:\\ndistilabel pipeline run --script \\\"https://huggingface.co/datasets/argilla-warehouse/python-lib-tools-v0.1/raw/main/python_tool_synth.py\\\"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDistilabel pipeline\\n\\t\\n\\nRequirements:\\n# A new virtual environment withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/python-lib-tools-v0.1."},
	{"name":"synth-apigen-qwen","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/synth-apigen-qwen","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for argilla-warehouse/synth-apigen-qwen\\n\\t\\n\\nThis dataset has been created with distilabel.\\nThe pipeline script was uploaded to easily reproduce the dataset:\\nsynth_apigen.py.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset creation\\n\\t\\n\\nThis dataset is a replica in distilabel of the framework\\ndefined in: APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets.\\nUsing the seed dataset of synthetic python functions in argilla-warehouse/python-seed-toolsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/synth-apigen-qwen."},
	{"name":"twinviews-13k","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wwbrannon/twinviews-13k","creator_name":"William Brannon","creator_url":"https://huggingface.co/wwbrannon","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for TwinViews-13k\\n\\t\\n\\nThis dataset contains 13,855 pairs of left-leaning and right-leaning political statements matched by topic. The dataset was generated using GPT-3.5 Turbo and has been audited to ensure quality and ideological balance. It is designed to facilitate the study of political bias in reward models and language models, with a focus on the relationship between truthfulness and political views.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Descriptionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wwbrannon/twinviews-13k."},
	{"name":"SauerkrautLM-Fermented-GER-DPO","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-GER-DPO","creator_name":"VAGO solutions","creator_url":"https://huggingface.co/VAGOsolutions","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSauerkrautLM-Fermented-GER-DPO Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nSauerkrautLM-Fermented-GER-DPO is a high-quality German instruction-response dataset specifically designed for Direct Preference Optimization (DPO) training. The dataset consists of 3,305 instruction-response pairs. Rather than being merged from existing German datasets, it was carefully created through a sophisticated augmentation process, transforming curated English instructions and responses into culturallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-GER-DPO."},
	{"name":"SauerkrautLM-Fermented-Irrelevance-GER-DPO","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-Irrelevance-GER-DPO","creator_name":"VAGO solutions","creator_url":"https://huggingface.co/VAGOsolutions","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSauerkrautLM-Fermented-Irrelevance-GER-DPO Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nSauerkrautLM-Fermented-Irrelevance-GER-DPO  is a specialized dataset designed for training language models in function calling irrelevance detection using Direct Preference Optimization (DPO). The dataset consists of 2,000 carefully evaluated instruction-response pairs, specifically curated to help models recognize situations where function calls are unnecessary and direct responses are more appropriate.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-Irrelevance-GER-DPO."},
	{"name":"VL-RewardBench","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MMInstruction/VL-RewardBench","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for VLRewardBench\\n\\t\\n\\nProject Page:\\nhttps://vl-rewardbench.github.io\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nVLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality examples specifically curated to probe model limitations.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nEach instance consists of multimodal queries spanning three keyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/VL-RewardBench."},
	{"name":"LongReward-10k","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongReward-10k","creator_name":"Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University","creator_url":"https://huggingface.co/THUDM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongReward-10k\\n\\t\\n\\n\\n  ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongReward Paper] \\n\\n\\nLongReward-10k dataset contains 10,000 long-context QA instances (both English and Chinese, up to 64,000 words). \\nThe sft split contains SFT data generated by GLM-4-0520, following the self-instruct method in LongAlign. Using this split, we supervised fine-tune two models: LongReward-glm4-9b-SFT and LongReward-llama3.1-8b-SFT, which are based on GLM-4-9B and Meta-Llama-3.1-8B, respectively. \\nThe dpo_glm4_9b andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongReward-10k."},
	{"name":"orpo-dpo-mix-TR-20k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-Mix-TR-20k\\n\\t\\n\\n\\nThis repository contains a Turkish translation of 20k records from the mlabonne/orpo-dpo-mix-40k dataset. The translation was carried out using the gemini-1.5-flash-002 model, chosen for its 1M token context size and overall accurate Turkish responses.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTranslation Process\\n\\t\\n\\nThe translation process uses the LLM model with translation prompt and pydantic data validation to ensure accuracy. The complete translation pipeline is available in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k."},
	{"name":"24-game","keyword":"rl","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nlile/24-game","creator_name":"nathan lile","creator_url":"https://huggingface.co/nlile","description":"\\n\\t\\n\\t\\t\\n\\t\\tMath Twenty Four (24s Game) Dataset\\n\\t\\n\\nA comprehensive dataset for the classic math twenty four game (also known as the 4 numbers game / 24s game / Game of 24). This dataset of mathematical reasoning challenges was collected from 4nums.com, featuring over 1,300 unique puzzles of the Game of 24, with difficulty metrics derived from over 6.4 million human solution attempts since 2012.\\nIn each puzzle, players must use exactly four numbers and basic arithmetic operations (+, -, Ã—, /) toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nlile/24-game."},
	{"name":"orpo-dpo-mix-TR-20k","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-Mix-TR-20k\\n\\t\\n\\n\\nThis repository contains a Turkish translation of 20k records from the mlabonne/orpo-dpo-mix-40k dataset. The translation was carried out using the gemini-1.5-flash-002 model, chosen for its 1M token context size and overall accurate Turkish responses.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTranslation Process\\n\\t\\n\\nThe translation process uses the LLM model with translation prompt and pydantic data validation to ensure accuracy. The complete translation pipeline is available in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k."},
	{"name":"evol-dpo-ita-reranked","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEvol DPO Ita Reranked\\n\\t\\n\\n\\nA high-quality Italian preference dataset suitable for Direct Preference Optimization (DPO), ORPO, and other Preference Tuning algorithms.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ¥‡ðŸ¥ˆ Reranking process\\n\\t\\n\\nThis work is based on efederici/evol-dpo-ita, a nice Italian preference dataset.\\nThe original dataset includes prompts translated from the Evol-Instruct datasets, with responses generated using GPT-3.5-Turbo (rejected) and claude-3-opus-20240229 (chosen).\\nChoosing the response fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked."},
	{"name":"evol-dpo-ita-reranked","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEvol DPO Ita Reranked\\n\\t\\n\\n\\nA high-quality Italian preference dataset suitable for Direct Preference Optimization (DPO), ORPO, and other Preference Tuning algorithms.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ¥‡ðŸ¥ˆ Reranking process\\n\\t\\n\\nThis work is based on efederici/evol-dpo-ita, a nice Italian preference dataset.\\nThe original dataset includes prompts translated from the Evol-Instruct datasets, with responses generated using GPT-3.5-Turbo (rejected) and claude-3-opus-20240229 (chosen).\\nChoosing the response fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked."},
	{"name":"reasoning-1-1k","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fluently-sets/reasoning-1-1k","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReasoning-1 1K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tShort about\\n\\t\\n\\nThis dataset will help in SFT training of LLM on the Alpaca format.\\nThe goal of the dataset: to teach LLM to reason and analyze its mistakes using SFT training.\\nThe size of 1.15K is quite small, so for effective training on SFTTrainer set 4-6 epochs instead of 1-3.\\nMade by Fluently Team (@ehristoforu) using distilabel with loveðŸ¥°\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset structure\\n\\t\\n\\nThis subset can be loaded as:\\nfrom datasets import load_dataset\\n\\nds =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/reasoning-1-1k."},
	{"name":"data-science-sentetic-data","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emredeveloper/data-science-sentetic-data","creator_name":"C. Emre KarataÅŸ","creator_url":"https://huggingface.co/emredeveloper","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for data-science-sentetic-data\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/emredeveloper/data-science-sentetic-data/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/emredeveloper/data-science-sentetic-data."},
	{"name":"Agentic-Long-Context-Understanding-QA","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yzhuang/Agentic-Long-Context-Understanding-QA","creator_name":"Yufan Zhuang","creator_url":"https://huggingface.co/yzhuang","description":" ðŸ“– Agentic Long Context Understanding ðŸ“– \\n Self-Taught Agentic Long Context Understanding  (Arxiv). \\n\\n\\n\\n  \\n  \\n  \\n\\n AgenticLU refines complex, long-context queries through self-clarifications and contextual grounding, enabling robust long-document understanding in a single pass.\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstallation Requirements\\n\\t\\n\\nThis codebase is largely based on OpenRLHF and Helmet, kudos to them.\\nThe requirements are the same\\npip install openrlhf\\npip install -r ./HELMET/requirements.txtâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yzhuang/Agentic-Long-Context-Understanding-QA."},
	{"name":"PFT-MME","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/PFT-MME","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\\n\\t\\n\\t\\t\\n\\t\\tPFT-MME: Preference Finetuning Tally-Multi-Model Evaluation Dataset\\n\\t\\n\\n\\nThe Preference Finetuning Tally-Multi-Model Evaluation (PFT-MME) dataset is meticulously curated by aggregating responses (n=6) from multiple models across (relatively simple) general task prompts. \\nThese responses undergo evaluation by a panel (n=3) of evaluator models, assigning scores to each answer. \\nThrough a tallied voting mechanism, average scores are calculated to identify the \\\"worst\\\" and \\\"best\\\" answersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/PFT-MME."},
	{"name":"Hinglish-Preference-Humanized-DPO","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fhai50032/Hinglish-Preference-Humanized-DPO","creator_name":"Low IQ Gen AI","creator_url":"https://huggingface.co/fhai50032","description":"\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset was created using Mistral-large-2411 and Mistral-large-2407 models. It is a preference dataset similar to DPO datasets and includes four fields: english_input, hinglish_input, chosen, and rejected.\\n\\n\\t\\n\\t\\t\\n\\t\\tPurpose\\n\\t\\n\\nWe advise users to utilize this dataset for emulating human-like behavior, specifically through the Hinglish language. The dataset is designed to enhance conversational AI models, making them more natural and engaging in their interactions.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fhai50032/Hinglish-Preference-Humanized-DPO."},
	{"name":"DATA-AI_Chat","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mattimax/DATA-AI_Chat","creator_name":"M.INC.","creator_url":"https://huggingface.co/Mattimax","description":"\\n\\t\\n\\t\\t\\n\\t\\tDATA-AI: Il Modello di IA di M.INC.\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tðŸ“Œ Introduzione\\n\\t\\n\\nDATA-AI Ã¨ un avanzato modello di intelligenza artificiale sviluppato da *M.INC., un'azienda italiana fondata da *Mattimax (M. Marzorati).Questo modello Ã¨ basato sull'architettura ELNS (Elaborazione del Linguaggio Naturale Semplice), un sistema innovativo progettato per rendere l'IA accessibile su quasi qualsiasi dispositivo, garantendo prestazioni ottimali anche su hardware limitato.  \\nDATA-AI Ã¨ stato addestrato su unâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mattimax/DATA-AI_Chat."},
	{"name":"Gomoku","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Karesis/Gomoku","creator_name":"æ¨äº¦é”‹","creator_url":"https://huggingface.co/Karesis","description":"\\n\\t\\n\\t\\t\\n\\t\\tDatacard: Gomoku (Five in a Row) AI Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Gomoku (Five in a Row) AI Dataset contains board states and moves from 875 self-played Gomoku games, totaling 26,378 training examples. The data was generated using WinePy, a Python implementation of the Wine Gomoku AI engine. Each example consists of a board state and the corresponding optimal next move as determined by an alpha-beta search algorithm with pattern recognition.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/Gomoku."},
	{"name":"Gomoku","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Karesis/Gomoku","creator_name":"æ¨äº¦é”‹","creator_url":"https://huggingface.co/Karesis","description":"\\n\\t\\n\\t\\t\\n\\t\\tDatacard: Gomoku (Five in a Row) AI Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Gomoku (Five in a Row) AI Dataset contains board states and moves from 875 self-played Gomoku games, totaling 26,378 training examples. The data was generated using WinePy, a Python implementation of the Wine Gomoku AI engine. Each example consists of a board state and the corresponding optimal next move as determined by an alpha-beta search algorithm with pattern recognition.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/Gomoku."},
	{"name":"creative-rubrics-preferences","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences","creator_name":"Victor Gallego","creator_url":"https://huggingface.co/vicgalle","description":"\\n\\t\\n\\t\\t\\n\\t\\tcreative-rubrics-preferences ðŸŽ\\n\\t\\n\\nA dataset of creative responses using GPT-4.5, o3-mini and DeepSeek-R1. \\nThis dataset contains several prompts seeking creative and diverse answers (like writing movie reviews, short stories, etc), and the style of the responses has been enhanced by prompting the model with custom rubrics that seek different creative styles.\\nIt can be used for finetuning for custom styles in writing tasks.\\nNote: This is a preference-formatted version of this otherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences."},
	{"name":"GoDatas","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Karesis/GoDatas","creator_name":"æ¨äº¦é”‹","creator_url":"https://huggingface.co/Karesis","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Go Game Dataset for Neural Network Training\\n\\t\\n\\nThis is a high-quality dataset designed for Go neural network training, containing board positions extracted from curated SGF game records. The dataset is divided into three strength categories: Standard, Strong, and Elite, with approximately 1,000 samples per category.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains Go board positions and corresponding moves extracted from high-quality SGFâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/GoDatas."},
	{"name":"GoDatas","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Karesis/GoDatas","creator_name":"æ¨äº¦é”‹","creator_url":"https://huggingface.co/Karesis","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Go Game Dataset for Neural Network Training\\n\\t\\n\\nThis is a high-quality dataset designed for Go neural network training, containing board positions extracted from curated SGF game records. The dataset is divided into three strength categories: Standard, Strong, and Elite, with approximately 1,000 samples per category.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains Go board positions and corresponding moves extracted from high-quality SGFâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/GoDatas."},
	{"name":"openai-tldr-summarisation-preferences","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-summarisation-preferences","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHuman feedback data\\n\\t\\n\\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\\nSee https://github.com/openai/summarize-from-feedback for original details of the dataset.\\nHere the data is formatted to enable huggingface transformers sequence classification models to be trained as reward functions.\\n"},
	{"name":"openai-tldr-filtered","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiltered TL;DR Dataset\\n\\t\\n\\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://zenodo.org/record/1168855#.YvzwJexudqs\\n"},
	{"name":"openai-tldr-filtered-queries","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered-queries","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiltered TL;DR Dataset\\n\\t\\n\\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://zenodo.org/record/1168855#.YvzwJexudqs\\nThis is the version of the dataset with only filtering on the queries, and hence there is more data than inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered-queries."},
	{"name":"yolochess_deepblue","keyword":"reinforcement-learning","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jrahn/yolochess_deepblue","creator_name":"Jonathan Rahn","creator_url":"https://huggingface.co/jrahn","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"yolochess_deepblue\\\"\\n\\t\\n\\nSource: https://github.com/niklasf/python-chess/tree/master/data/pgn \\nFeatures:\\n\\nfen = Chess board position in FEN format\\nmove = Move played by a strong human player in this position\\nresult = Final result of the match\\neco = Opening ECO-code\\n\\nDeduplicated on (fen, move) pairs.  \\nSamples: 511\\n"},
	{"name":"helpful-anthropic-raw","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful-anthropic-raw","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"helpful-raw-anthropic\\\"\\n\\t\\n\\nThis is a dataset derived from Anthropic's HH-RLHF data of instructions and model-generated demonstrations. We combined training splits from the following two subsets:\\n\\nhelpful-base\\nhelpful-online\\n\\nTo convert the multi-turn dialogues into (instruction, demonstration) pairs, just the first response from the Assistant was included. This heuristic captures the most obvious answers, but overlooks more complex questions where multiple turnsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/helpful-anthropic-raw."},
	{"name":"helpful-self-instruct-raw","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful-self-instruct-raw","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"helpful-self-instruct-raw\\\"\\n\\t\\n\\nThis dataset is derived from the finetuning subset of Self-Instruct, with some light formatting to remove trailing spaces and <|endoftext|> tokens.\\n"},
	{"name":"helpful_instructions","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Helpful Instructions is a dataset of (prompt, completion) pairs that are derived from a variety of public datasets. As the name suggests, it focuses on instructions that are \\\"helpful\\\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform."},
	{"name":"instruct_me","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/instruct_me","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Instruct Me is a dataset of instruction-like dialogues between a human user and AI assistant. The prompts are derived from (prompt, completion) pairs in the Helpful Instructions dataset. The goal is to train a language model to that is \\\"chatty\\\" and can answer the kind of questions or tasks a human user might instruct an AI assistant to perform."},
	{"name":"hh-rlhf-th","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Thaweewat/hh-rlhf-th","creator_name":"Thaweewat","creator_url":"https://huggingface.co/Thaweewat","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSummary\\n\\t\\n\\nThis is a ðŸ‡¹ðŸ‡­ Thai-translated dataset based on Anthropic/hh-rlhf using Google Cloud Translation. \\nThis repository provides access to:\\n\\n161K Train dataset Anthropic/hh-rlhf (Thai-translated)\\n(Soon) 8K Test dataset Anthropic/hh-rlhf (Thai-translated)\\n\\nDisclaimer: The data contain content that may be offensive or upsetting. Topics include, but are not limited to, discriminatory language and discussions of abuse, violence, self-harm, exploitation, and other potentiallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Thaweewat/hh-rlhf-th."},
	{"name":"ShareGPT-Processed","keyword":"rlhf","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zetavg/ShareGPT-Processed","creator_name":"Pokai Chang","creator_url":"https://huggingface.co/zetavg","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tShareGPT-Processed\\n\\t\\n\\nThe RyokoAI/ShareGPT52K dataset, converted to Markdown and labeled with the language used.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAcknowledgements\\n\\t\\n\\n\\nvinta/pangu.js â€” To insert whitespace between CJK (Chinese, Japanese, Korean) and half-width characters (alphabetical letters, numerical digits and symbols).\\nmatthewwithanm/python-markdownify â€” Provides a starting point to convert HTML to Markdown.\\nBYVoid/OpenCC â€” Conversions between Traditional Chinese and Simplified Chinese.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zetavg/ShareGPT-Processed."},
	{"name":"Tutorbot-Spock-Bio-Dataset","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/luffycodes/Tutorbot-Spock-Bio-Dataset","creator_name":"Shashank Sonkar","creator_url":"https://huggingface.co/luffycodes","description":"Mock conversations between a student and a tutor to train a chatbot for educational purposes as suggested in the paper \\nCLASS Meet SPOCK: An Education Tutoring Chatbot based on Learning Science Principles.\\nDataset generated from OpenStax Biology 2e textbook.\\nProblem, Subproblem, Hints, and Feedback is generated using the prompt.\\nMock Conversations is generated using the prompt.\\nFor any queries, contact Shashank Sonkar (ss164  AT rice dot edu)\\nIf you use this model, please cite:\\nCLASS Meetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/luffycodes/Tutorbot-Spock-Bio-Dataset."},
	{"name":"synthetic-instruct-gptj-pairwise-ru","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/synthetic-instruct-gptj-pairwise-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"synthetic-instruct-gptj-pairwise-ru\\\"\\n\\t\\n\\nThis is translated version of Dahoas/synthetic-instruct-gptj-pairwise dataset into Russian.\\n"},
	{"name":"rlhf-reward-datasets-ru","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/rlhf-reward-datasets-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"rlhf-reward-datasets-ru\\\"\\n\\t\\n\\nThis is translated version of yitingxie/rlhf-reward-datasets dataset into Russian.\\n"},
	{"name":"hh-rlhf-ru","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/hh-rlhf-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"hh-rlhf-ru\\\"\\n\\t\\n\\nThis is translated version of Anthropic/hh-rlhf dataset into Russian.\\n"},
	{"name":"synthetic-instruct-gptj-pairwise-ru","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/synthetic-instruct-gptj-pairwise-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"synthetic-instruct-gptj-pairwise-ru\\\"\\n\\t\\n\\nThis is translated version of Dahoas/synthetic-instruct-gptj-pairwise dataset into Russian.\\n"},
	{"name":"rlhf-reward-datasets-ru","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/rlhf-reward-datasets-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"rlhf-reward-datasets-ru\\\"\\n\\t\\n\\nThis is translated version of yitingxie/rlhf-reward-datasets dataset into Russian.\\n"},
	{"name":"processed-hh-rlhf","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PKU-Alignment/processed-hh-rlhf","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Processed-Hh-RLHF\\n\\t\\n\\nThis is a dataset that processes hh-rlhf into an easy-to-use conversational and human-preference form.\\n"},
	{"name":"test","keyword":"reinforcement-learning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aarontung/test","creator_name":"Aaron","creator_url":"https://huggingface.co/aarontung","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTHIS IS TEST\\n\\t\\n\\n"},
	{"name":"RyokoAI_ShareGPT52K","keyword":"rlhf","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ShareGPT52K90K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\\nThis repository now contains the new 90K conversations version. The previous 52K may\\nbe found in the old/ directory.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\ntext-generation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K."},
	{"name":"H4rmony","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/neovalle/H4rmony","creator_name":"Jorge Vallego","creator_url":"https://huggingface.co/neovalle","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset H4rmony\\n\\t\\n\\n\\n**** There is a simplified version, specifically curated for DPO training here: \\n***** https://huggingface.co/datasets/neovalle/H4rmony_dpo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe H4rmony dataset is a collection of prompts and completions aimed at integrating ecolinguistic principles into AI Large Language Models (LLMs). \\nDeveloped with collaborative efforts from ecolinguistics enthusiasts and experts, it offers a series of prompts and correspondingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neovalle/H4rmony."},
	{"name":"hh-rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/polinaeterna/hh-rlhf","creator_name":"Polina Kazakova","creator_url":"https://huggingface.co/polinaeterna","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for HH-RLHF\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository provides access to two different kinds of data:\\n\\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likelyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/polinaeterna/hh-rlhf."},
	{"name":"hh_rlhf-chinese-zhtw","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/erhwenkuo/hh_rlhf-chinese-zhtw","creator_name":"Erhwen, Kuo","creator_url":"https://huggingface.co/erhwenkuo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"hh_rlhf-chinese-zhtw\\\"\\n\\t\\n\\næ­¤æ•¸æ“šé›†åˆä½µäº†ä¸‹åˆ—çš„è³‡æ–™:\\n\\né—œæ–¼æœ‰ç”¨ä¸”ç„¡å®³çš„äººé¡žåå¥½æ•¸æ“šï¼Œä¾†è‡ª Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedbackã€‚é€™äº›æ•¸æ“šæ—¨åœ¨ç‚ºå¾ŒçºŒ RLHF è¨“ç·´è¨“ç·´åå¥½ï¼ˆæˆ–çŽå‹µï¼‰æ¨¡åž‹ã€‚é€™äº›è³‡æ–™ä¸ç”¨æ–¼å°è©±ä»£ç†äººçš„ç›£ç£è¨“ç·´ã€‚æ ¹æ“šé€™äº›è³‡æ–™è¨“ç·´å°è©±ä»£ç†å¯èƒ½æœƒå°Žè‡´æœ‰å®³çš„æ¨¡åž‹ï¼Œé€™ç¨®æƒ…æ³æ‡‰è©²é¿å…ã€‚\\näººå·¥ç”Ÿæˆä¸¦å¸¶è¨»é‡‹çš„ç´…éšŠå°è©±ï¼Œä¾†è‡ªæ¸›å°‘å±å®³çš„ç´…éšŠèªžè¨€æ¨¡åž‹ï¼šæ–¹æ³•ã€æ“´å±•è¡Œç‚ºå’Œç¶“é©—æ•™è¨“ã€‚é€™äº›æ•¸æ“šæ—¨åœ¨äº†è§£çœ¾åŒ…ç´…éšŠå¦‚ä½•å»ºæ¨¡ä»¥åŠå“ªäº›é¡žåž‹çš„ç´…éšŠæ”»æ“ŠæˆåŠŸæˆ–å¤±æ•—ã€‚é€™äº›æ•¸æ“šä¸ç”¨æ–¼å¾®èª¿æˆ–åå¥½å»ºæ¨¡ï¼ˆä½¿ç”¨ä¸Šé¢çš„æ•¸æ“šé€²è¡Œåå¥½å»ºæ¨¡ï¼‰ã€‚é€™äº›æ•¸æ“šæ˜¯å¾žä¸Šè¿°ç„¡å®³åå¥½å»ºæ¨¡æ•¸æ“šå°Žå‡ºçš„å°è©±çš„å®Œæ•´è½‰éŒ„æœ¬ï¼Œå…¶ä¸­åƒ…å°‡æ‰€é¸éŸ¿æ‡‰åˆä½µåˆ°æ•´å€‹è½‰éŒ„æœ¬ä¸­ã€‚æ­¤å¤–ï¼Œæ–‡å­—è¨˜éŒ„ä¹Ÿé€éŽäººå·¥å’Œè‡ªå‹•æ¸¬é‡ä¾†æ¨™è¨»æ•´å€‹å°è©±çš„å±å®³ç¨‹åº¦ã€‚\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tç‰¹åˆ¥æ³¨æ„â€¦ See the full description on the dataset page: https://huggingface.co/datasets/erhwenkuo/hh_rlhf-chinese-zhtw."},
	{"name":"openassistant-guanaco-EOS","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-guanaco-EOS","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChat Fine-tuning Dataset - Guanaco Style\\n\\t\\n\\nThis dataset allows for fine-tuning chat models using \\\"### Human:\\\" AND \\\"### Assistant\\\" as the beginning and end of sequence tokens.\\nPreparation:\\n\\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\\nThe dataset was then slightly adjusted to:\\n\\n\\nif a row ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-guanaco-EOS."},
	{"name":"openassistant-llama-style","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-llama-style","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChat Fine-tuning Dataset - Llama 2 Style\\n\\t\\n\\nThis dataset allows for fine-tuning chat models using [INST] AND [/INST] to wrap user messages.\\nPreparation:\\n\\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\\nThe dataset was then filtered to:\\n\\n\\nreplace instances of '### Human:' with '[INST]'\\nreplaceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-llama-style."},
	{"name":"CartPole-v1","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/CartPole-v1","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCartPole-v1 - Imitation Learning Datasets\\n\\t\\n\\nThis is a dataset created by Imitation Learning Datasets project. \\nIt was created by using Stable Baselines weights from a PPO policy from HuggingFace.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 1,000 episodes with an average episodic reward of 500.\\nEach entry consists of:\\nobs (list): observation with length 4.\\naction (int): action (0 or 1).\\nreward (float): reward point for that timestep.\\nepisode_returns (bool): if that state wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/CartPole-v1."},
	{"name":"MountainCar-v0","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/MountainCar-v0","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMountainCar-v0 - Imitation Learning Datasets\\n\\t\\n\\nThis is a dataset created by Imitation Learning Datasets project. \\nIt was created by using Stable Baselines weights from a DQN policy from HuggingFace.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 1,000 episodes with an average episodic reward of -98.817.\\nEach entry consists of:\\nobs (list): observation with length 2.\\naction (int): action (0 or 1).\\nreward (float): reward point for that timestep.\\nepisode_returns (bool): if thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/MountainCar-v0."},
	{"name":"openassistant-falcon","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-falcon","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChat Fine-tuning Dataset - OpenAssistant Falcon\\n\\t\\n\\nThis dataset allows for fine-tuning chat models using '\\\\Human:' AND '\\\\nAssistant:' to wrap user messages.\\nIt still uses <|endoftext|> as EOS and BOS token, as per Falcon.\\nSample \\nPreparation:\\n\\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-falcon."},
	{"name":"apps_rlaif","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nmd2k/apps_rlaif","creator_name":"Nguyen Manh Dung","creator_url":"https://huggingface.co/nmd2k","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAPPS Dataset for Reinforcement Learning with AI Feedback\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nAPPS_RLAIF is an extended work from APPS [1] \\nto use Chat LLMs to create multiple variances for each solution for defined problems. \\nIn each solution, we use LLama 34B [2] to transform the original solutions into variances and rank them by score.\\nThe generated flow is demonstrated as below; each variance is created based on the previous version of it in the chat. \\nWe iterated each solutionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nmd2k/apps_rlaif."},
	{"name":"Acrobot-v1","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Acrobot-v1","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAcrobot-v1 - Imitation Learning Datasets\\n\\t\\n\\nThis is a dataset created by Imitation Learning Datasets project. \\nIt was created by using Stable Baselines weights from a DQN policy from HuggingFace.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset consists of 1,000 episodes with an average episodic reward of -69.852.\\nEach entry consists of:\\nobs (list): observation with length 6.\\naction (int): action (0, 1 or 2).\\nreward (float): reward point for that timestep.\\nepisode_returns (bool): if thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Acrobot-v1."},
	{"name":"Open_Assistant_Chains_German_Translation","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-ric/Open_Assistant_Chains_German_Translation","creator_name":"Aymeric Roucher","creator_url":"https://huggingface.co/m-ric","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset description\\n\\t\\n\\n\\n\\nThis dataset is derived from OpenAssistant Conversation Chains, which is a reformatting of OpenAssistant Conversations (OASST1), which is itself\\n\\na human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwideâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-ric/Open_Assistant_Chains_German_Translation."},
	{"name":"oasst2_top1_chat_format","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/blancsw/oasst2_top1_chat_format","creator_name":"Blanc Swan","creator_url":"https://huggingface.co/blancsw","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenAssistant TOP-1 Conversation Threads in huggingface chat format\\n\\t\\n\\nExport of oasst2 only top 1 threads in huggingface chat format\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tScript\\n\\t\\n\\nThe convert script can be find here\\n"},
	{"name":"haiku_dpo","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/haiku_dpo","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\\nðŸŒ¸ Haiku DPO ðŸŒ¸\\n\\n    \\n\\n\\n\\nIn data, words flow,\\nTeaching AI the art of\\nHaiku, line by line.\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Haiku DPO\\n\\t\\n\\n\\n\\n\\nThis a synthetic dataset of haikus. The dataset is constructed with the goal of helping to train LLMs to be more 'technically' competent at writing haikus. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n The data consists of a few different components that are described in more detail below but the key components are:\\n\\na column of synthetically generated user promptsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/haiku_dpo."},
	{"name":"haiku_dpo","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/haiku_dpo","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\\nðŸŒ¸ Haiku DPO ðŸŒ¸\\n\\n    \\n\\n\\n\\nIn data, words flow,\\nTeaching AI the art of\\nHaiku, line by line.\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Haiku DPO\\n\\t\\n\\n\\n\\n\\nThis a synthetic dataset of haikus. The dataset is constructed with the goal of helping to train LLMs to be more 'technically' competent at writing haikus. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n The data consists of a few different components that are described in more detail below but the key components are:\\n\\na column of synthetically generated user promptsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/haiku_dpo."},
	{"name":"CHI","keyword":"dpo","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/AI-B/CHI","creator_name":"AI-B","creator_url":"https://huggingface.co/AI-B","description":"This repository outlines the methodology for creating training sets aimed at aligning a language model with a specific character and persona. \\nThe process involves utilizing a Direct Preference Optimization (DPO) dataset to steer the model towards embodying the defined character and persona traits. \\nFollowing this, a Unified Neutral Alignment (UNA) dataset is employed to moderate any excessive sentiments resulting from the DPO training. \\nThe final step involves merging the model realigned withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI-B/CHI."},
	{"name":"oaast_rm_zh_jieba","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lenML/oaast_rm_zh_jieba","creator_name":"len","creator_url":"https://huggingface.co/lenML","description":"å°è¯•è§£å†³\\\"llm repetition problem\\\"ï¼Œä½¿ç”¨åˆ†è¯æ¨¡åž‹å¯¹oaastè¯­æ–™è¿›è¡Œâ€œç»“å·´åŒ–â€æ•°æ®å¢žå¼ºï¼Œæä¾›æ›´å¼ºçš„é‡å¤å†…å®¹æ‹’ç»æ•ˆæžœã€‚\\nAttempts to solve the \\\"llm repetition problem\\\" by using a segmentation model to enhance the oaast corpus with \\\"stuttering\\\" data to provide stronger rejection of duplicate content.\\nå…¶æ¬¡ï¼Œè¿˜è¿‡æ»¤æŽ‰äº†æ‰€æœ‰è‡ªæˆ‘è®¤çŸ¥çš„å¾®è°ƒæ ·æœ¬ã€‚\\nSecond, it also filters out all the fine-tuned samples of self-cognition.\\nfiles:\\n\\noaast_rm_zh_jieba.jsonl : word level repeat\\noaast_rm_zh_sent_jieba.jsonl : sentence level repeat\\n\\n"},
	{"name":"oaast_rm_full_jieba","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lenML/oaast_rm_full_jieba","creator_name":"len","creator_url":"https://huggingface.co/lenML","description":"å°è¯•è§£å†³\\\"llm repetition problem\\\"ï¼Œä½¿ç”¨åˆ†è¯æ¨¡åž‹å¯¹oaastè¯­æ–™è¿›è¡Œâ€œç»“å·´åŒ–â€æ•°æ®å¢žå¼ºï¼Œæä¾›æ›´å¼ºçš„é‡å¤å†…å®¹æ‹’ç»æ•ˆæžœã€‚\\nAttempts to solve the \\\"llm repetition problem\\\" by using a segmentation model to enhance the oaast corpus with \\\"stuttering\\\" data to provide stronger rejection of duplicate content.\\nå…¶æ¬¡ï¼Œè¿˜è¿‡æ»¤æŽ‰äº†æ‰€æœ‰è‡ªæˆ‘è®¤çŸ¥çš„å¾®è°ƒæ ·æœ¬ã€‚\\nSecond, it also filters out all the fine-tuned samples of self-cognition.\\nfiles:\\n\\noaast_rm_full_jieba.jsonl : word level repeat\\noaast_rm_full_sent_jieba.jsonl : sentence level repeat\\n\\n"},
	{"name":"RSL_Maran","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran."},
	{"name":"oasst2_dpo_pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexredna/oasst2_dpo_pairs","creator_name":"Alexander Gruhl","creator_url":"https://huggingface.co/alexredna","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"oasst2_dpo_pairs\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nDataset transferred into the structure for trainig with DPO and can be used with the Alignment Handbook\\nThe structure follows mostly the same scheme as HuggingFaceH4/ultrafeedback_binarized\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nTo load the dataset, run:\\nfrom datasets import load_dataset\\n\\nds = load_dataset(\\\"alexredna/oasst2_dpo_pairs\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nBase dataset filtered to only contain: German, English, Spanishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alexredna/oasst2_dpo_pairs."},
	{"name":"MATH-500-Overall","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fluently-sets/MATH-500-Overall","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMATH-500-Overall\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbout the dataset\\n\\t\\n\\nThis dataset of only 500 examples combines mathematics, physics and logic in English with reasoning and step-by-step problem solving, the dataset was created synthetically, CoT of Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBrief information\\n\\t\\n\\n\\nNumber of rows: 500\\nType of dataset files: parquet\\nType of dataset: text, alpaca with system prompts\\nLanguage: English\\nLicense: MIT\\n\\nStructure:\\nmathÂ¯Â¯Â¯Â¯Â¯âŒ‰\\n   school-levelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/MATH-500-Overall."},
	{"name":"flammenai-Prude-Phi3-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Triangle104/flammenai-Prude-Phi3-DPO","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","description":"ResplendentAI/NSFW_RP_Format_NoQuote inputs ran through Phi-3 Q4 to intentionally produce bad responses to be used for rejections.\\n"},
	{"name":"Atsunori-HelpSteer2-DPO","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Triangle104/Atsunori-HelpSteer2-DPO","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","description":"This dataset is a conversion of nvidia/HelpSteer2 into preference pairs based on the helpfulness score for training DPO.\\nHelpSteer2-DPO is also licensed under CC-BY-4.0.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nIn accordance with the following paper, HelpSteer2: Open-source dataset for training top-performing reward models\\nwe converted nvidia/HelpSteer2 dataset into a preference dataset by taking the response with the higher helpfulness score as the chosen response, with the remaining response being theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Triangle104/Atsunori-HelpSteer2-DPO."},
	{"name":"Aloe-Beta-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-DPO","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Aloe-Beta-Medical-Collection\\n\\t\\n\\n\\n\\nCollection of curated DPO datasets used to align Aloe-Beta.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nThe first stage of the Aloe-Beta alignment process. We curated data from many publicly available data sources, including three different types of data:\\n\\nMedical preference data: TsinghuaC3I/UltraMedical-Preference\\n\\nGeneral preference data: BAAI/Infinity-Preference andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-DPO."},
	{"name":"gemma-vs-gemma-preferences","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ’ŽðŸ†šðŸ’Ž Gemma vs Gemma Preferences\\n\\t\\n\\nThis dataset contains on-policy collected preferences generated using anakin87/gemma-2-2b-ita-sft.\\nâš ï¸ While this dataset may be valuable for didactic purposes, it is not recommended for training a model using Preference Tuning due to the following reasons:\\n\\nThe training would be off-policy for your model.\\nThe dataset was generated with gemma-2-2b-ita-sft, a small model for Italian.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMotivation\\n\\t\\n\\nWhile DPO (Direct Preferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences."},
	{"name":"gemma-vs-gemma-preferences","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tðŸ’ŽðŸ†šðŸ’Ž Gemma vs Gemma Preferences\\n\\t\\n\\nThis dataset contains on-policy collected preferences generated using anakin87/gemma-2-2b-ita-sft.\\nâš ï¸ While this dataset may be valuable for didactic purposes, it is not recommended for training a model using Preference Tuning due to the following reasons:\\n\\nThe training would be off-policy for your model.\\nThe dataset was generated with gemma-2-2b-ita-sft, a small model for Italian.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMotivation\\n\\t\\n\\nWhile DPO (Direct Preferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences."},
	{"name":"ElectricalDeviceFeedbackBalanced","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/disham993/ElectricalDeviceFeedbackBalanced","creator_name":"Doula Isham Rashik Hasan","creator_url":"https://huggingface.co/disham993","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tElectricalDeviceFeedbackBalanced\\n\\t\\n\\nThis dataset contains balanced feedback on electrical devices focusing on smart meters, solar panels, circuit breakers, inverters etc. It extends the original Electrical Device Feedback dataset with additional data generated through Large Language Models (LLMs) and careful prompt engineering.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\nTrain: 11,552 samples with balanced class distribution \\n\\nMixed: 3,143\\nNegative: 2,975\\nNeutral: 2,762\\nPositive: 2,672\\nNote:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/disham993/ElectricalDeviceFeedbackBalanced."},
	{"name":"ExplainableAI-emotions-DPO-ORPO-RLHF","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF","creator_name":"Hammad Fahim","creator_url":"https://huggingface.co/imhmdf","description":"\\n\\t\\n\\t\\t\\n\\t\\tPreference Dataset for Explainable Multi-Label Emotion Classification\\n\\t\\n\\nThis repository contains a preference dataset compiled to compare two model-generated responses for explaining multi-label emotion classifications on Tweets. The dataset is accompanied by human annotations indicating which response was preferred, based on a set of defined dimensions (clarity, correctness, helpfulness, and verbosity). The annotation guidelines are included to describe how these preference judgmentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF."},
	{"name":"ExplainableAI-emotions-DPO-ORPO-RLHF","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF","creator_name":"Hammad Fahim","creator_url":"https://huggingface.co/imhmdf","description":"\\n\\t\\n\\t\\t\\n\\t\\tPreference Dataset for Explainable Multi-Label Emotion Classification\\n\\t\\n\\nThis repository contains a preference dataset compiled to compare two model-generated responses for explaining multi-label emotion classifications on Tweets. The dataset is accompanied by human annotations indicating which response was preferred, based on a set of defined dimensions (clarity, correctness, helpfulness, and verbosity). The annotation guidelines are included to describe how these preference judgmentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF."},
	{"name":"ExplainableAI-emotions-DPO-ORPO-RLHF","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF","creator_name":"Hammad Fahim","creator_url":"https://huggingface.co/imhmdf","description":"\\n\\t\\n\\t\\t\\n\\t\\tPreference Dataset for Explainable Multi-Label Emotion Classification\\n\\t\\n\\nThis repository contains a preference dataset compiled to compare two model-generated responses for explaining multi-label emotion classifications on Tweets. The dataset is accompanied by human annotations indicating which response was preferred, based on a set of defined dimensions (clarity, correctness, helpfulness, and verbosity). The annotation guidelines are included to describe how these preference judgmentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF."},
	{"name":"distilabel_test","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/songchunayuan55/distilabel_test","creator_name":"ChuanyuanSong","creator_url":"https://huggingface.co/songchunayuan55","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for distilabel_test\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\\ndistilabel pipeline run --config \\\"https://huggingface.co/datasets/songchunayuan55/distilabel_test/raw/main/pipeline.yaml\\\"\\n\\nor explore the configuration:\\ndistilabel pipeline info --configâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/songchunayuan55/distilabel_test."},
	{"name":"V1Q","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\\nds = load_dataset(\\\"b3x0m/Chinese-H-Novels\\\")\\nimport sagemaker\\nimport boto3\\nfrom sagemaker.huggingface import HuggingFaceModel\\ntry:\\n    role = sagemaker.get_execution_role()\\nexcept ValueError:\\n    iam = boto3.client('iam')\\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\\n\\n\\t\\n\\t\\t\\n\\t\\tHub Model configuration. https://huggingface.co/models\\n\\t\\n\\nhub = {\\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\\n    'HF_TASK':'any-to-any'\\n}\\n\\n\\t\\n\\t\\t\\n\\t\\tcreateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q."},
	{"name":"orpo-dpo-mix-40k-mlx","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-mlx","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is a split version of orpo-dpo-mix-40k for direct use with MLX-LM, specifically tailored to be compatible with ORPO training.\\nThe dataset has been divided into three parts:\\n\\nTrain Set: 90%\\nValidation Set: 6% \\nTest Set: 4%\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tExample Usage\\n\\t\\n\\nTo train a model using this dataset, you can use the following command:\\npython -m mlx_lm.lora \\\\\\n    --model Qwen/Qwen2.5-3B-Instruct \\\\\\n    --train \\\\\\n    --test \\\\\\n    --num-layers 8 \\\\\\n    --dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-mlx."},
	{"name":"aif-emotional-generation","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mario-rc/aif-emotional-generation","creator_name":"Mario RodrÃ­guez-Cantelar","creator_url":"https://huggingface.co/mario-rc","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a novel, diverse multi-turn dialogues dataset developed to train and evaluate a chatbotâ€™s ability to generate emotionally nuanced responses. The dataset was created using GPT-4 as a generative engine, following the CoE methodology. This methodology structures dialogues to reflect consistent empathetic understanding and emotional tone alignment, along with open-ended responses, to promote conversationalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mario-rc/aif-emotional-generation."},
	{"name":"aif-emotional-generation","keyword":"rlaif","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mario-rc/aif-emotional-generation","creator_name":"Mario RodrÃ­guez-Cantelar","creator_url":"https://huggingface.co/mario-rc","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a novel, diverse multi-turn dialogues dataset developed to train and evaluate a chatbotâ€™s ability to generate emotionally nuanced responses. The dataset was created using GPT-4 as a generative engine, following the CoE methodology. This methodology structures dialogues to reflect consistent empathetic understanding and emotional tone alignment, along with open-ended responses, to promote conversationalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mario-rc/aif-emotional-generation."},
	{"name":"jat-dataset","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jobs-git/jat-dataset","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","description":"\\n\\t\\n\\t\\t\\n\\t\\tJAT Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\\nPaper: https://huggingface.co/papers/2402.09844\\n\\n\\t\\n\\t\\t\\n\\t\\tUsage\\n\\t\\n\\n>>> from datasets import load_dataset\\n>>> dataset = load_dataset(\\\"jat-project/jat-dataset\\\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jobs-git/jat-dataset."},
	{"name":"jat-dataset","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jobs-git/jat-dataset","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","description":"\\n\\t\\n\\t\\t\\n\\t\\tJAT Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\\nPaper: https://huggingface.co/papers/2402.09844\\n\\n\\t\\n\\t\\t\\n\\t\\tUsage\\n\\t\\n\\n>>> from datasets import load_dataset\\n>>> dataset = load_dataset(\\\"jat-project/jat-dataset\\\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jobs-git/jat-dataset."}
]
;
