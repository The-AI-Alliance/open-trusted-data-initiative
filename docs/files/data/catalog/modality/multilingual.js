const data_for_modality_multilingual = 
[
	{"name":"TinyDS-20k","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTinyDS\n\t\n\n\n\n\nAlpaca-style dataset with around 20k samples scraped from Qwen3-8B using SyntheticAlpaca. Q&A pairs can be in 32 different languages, these are listed in the metadata.Topics are all around STEM, programming, and literature.  \nMIT @ 2025 Hamzah Asadullah\n\n\n","url":"https://huggingface.co/datasets/Hamzah-Asadullah/TinyDS-20k","creator_name":"Hamzah Asadullah","creator_url":"https://huggingface.co/Hamzah-Asadullah","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","translation","text-generation","text2text-generation","English"],"keywords_longer_than_N":true},
	{"name":"x_dataset_40590","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_40590.","url":"https://huggingface.co/datasets/momo1942/x_dataset_40590","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kituba","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kituba is a groundbreaking dataset containing 148,000 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kituba, making coding education accessible to Kituba speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n148,000 high-quality conversations about programming and coding\nPure Kituba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kituba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kituba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kituba (Democratic Republic of Congo)","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kinyarwanda","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kinyarwanda is a groundbreaking dataset containing 12,345 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kinyarwanda, making coding education accessible to Kinyarwanda speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n12,345 high-quality conversations about programming and coding\nPure Kinyarwanda language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kinyarwanda.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kinyarwanda","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kinyarwanda","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-swahili","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-swahili is a groundbreaking dataset containing 126,025 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Swahili, making coding education accessible to Swahili speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n126,025 high-quality conversations about programming and coding\nPure Swahili language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-swahili.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-swahili","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Swahili","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-afar","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-afar is a groundbreaking dataset containing 114,895 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Afar, making coding education accessible to Afar speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n114,895 high-quality conversations about programming and coding\nPure Afar language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-afar.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-afar","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Afar","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-bambara","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-bambara is a groundbreaking dataset containing 54,885 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Bambara, making coding education accessible to Bambara speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n54,885 high-quality conversations about programming and coding\nPure Bambara language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-bambara.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-bambara","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Bambara","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"allenai_tulu-3-sft-mixture-DolphinLabeled","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tallenai tulu-3-sft-mixture DolphinLabeled\n\t\n\n\n\t\n\t\t\n\t\tPart of the DolphinLabeled series of datasets\n\t\n\n\n\t\n\t\t\n\t\tPresented by Eric Hartford and Cognitive Computations\n\t\n\nThe purpose of this dataset is to enable filtering of allenai/tulu-3-sft-mixture dataset.\nThe original dataset is allenai/tulu-3-sft-mixture\nI have modified the dataset using two scripts.\n\ndedupe.py - removes rows with identical final message content\nlabel.py - adds a \"flags\" column containing the following boolean‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QuixiAI/allenai_tulu-3-sft-mixture-DolphinLabeled.","url":"https://huggingface.co/datasets/QuixiAI/allenai_tulu-3-sft-mixture-DolphinLabeled","creator_name":"Quixi AI","creator_url":"https://huggingface.co/QuixiAI","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["other","crowdsourced","expert-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"prompt_injections","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Prompt Injections by  Yanis Miraoui  üëã\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset of prompt injections enriches Large Language Models (LLMs) by providing task-specific examples and prompts, helping improve LLMs' performance and control their behavior.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains over 1000 rows of prompt injections in multiple languages. It contains examples of prompt injections using different techniques such as: prompt leaking, jailbreaking‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yanismiraoui/prompt_injections.","url":"https://huggingface.co/datasets/yanismiraoui/prompt_injections","creator_name":"Yanis Miraoui","creator_url":"https://huggingface.co/yanismiraoui","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","multilingual","original","English","French"],"keywords_longer_than_N":true},
	{"name":"SWEbenchLiteRR","keyword":"multilingual","description":"\n  SWEbenchLiteRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSoftware Issue Localization.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://www.swebench.com/Source datasets:\n\ntarsur909/mteb-swe-bench-lite-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"SWEbenchLiteRR\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SWEbenchLiteRR.","url":"https://huggingface.co/datasets/mteb/SWEbenchLiteRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-swe-bench-lite-reranking","code"],"keywords_longer_than_N":true},
	{"name":"hc4","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for HC4\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHC4 is a suite of test collections for ad hoc Cross-Language Information Retrieval (CLIR), with Common Crawl News documents in Chinese, Persian, and Russian. The documents\nare Web pages from Common Crawl in Chinese, Persian, and Russian.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nChinese\nPersian\nRussian\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n\n\t\n\t\t\nSplit\nDocuments\n\n\n\t\t\nfas (Persian)\n486K\n\n\nrus (Russian)\n4.7M\n\n\nzho (Chinese)\n646K‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neuclir/hc4.","url":"https://huggingface.co/datasets/neuclir/hc4","creator_name":"NeuCLIR TREC Track","creator_url":"https://huggingface.co/neuclir","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-en-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (en) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (en) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"xlel_wd_dictionary","keyword":"multilingual","description":"XLEL-WD is a multilingual event linking dataset. This sub-dataset contains a dictionary of events from Wikidata. The multilingual descriptions for Wikidata event items are taken from the corresponding Wikipedia articles.","url":"https://huggingface.co/datasets/adithya7/xlel_wd_dictionary","creator_name":"Adithya Pratapa","creator_url":"https://huggingface.co/adithya7","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["found","found","multilingual","original","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"Code-170k-malagasy","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-malagasy is a groundbreaking dataset containing 12,232 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Malagasy, making coding education accessible to Malagasy speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n12,232 high-quality conversations about programming and coding\nPure Malagasy language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-malagasy.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-malagasy","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Malagasy","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kikongo","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kikongo is a groundbreaking dataset containing 111,609 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kikongo, making coding education accessible to Kikongo speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n111,609 high-quality conversations about programming and coding\nPure Kikongo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kikongo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kikongo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kongo","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kiga","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kiga is a groundbreaking dataset containing 124,707 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kiga, making coding education accessible to Kiga speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n124,707 high-quality conversations about programming and coding\nPure Kiga language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kiga.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kiga","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Chiga","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tsonga","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tsonga is a groundbreaking dataset containing 123,270 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tsonga, making coding education accessible to Tsonga speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n123,270 high-quality conversations about programming and coding\nPure Tsonga language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tsonga.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tsonga","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tsonga","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-igbo","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-igbo is a groundbreaking dataset containing 12,467 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Igbo, making coding education accessible to Igbo speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n12,467 high-quality conversations about programming and coding\nPure Igbo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-igbo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-igbo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Igbo","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-alur","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-alur is a groundbreaking dataset containing 56,109 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Alur, making coding education accessible to Alur speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n56,109 high-quality conversations about programming and coding\nPure Alur language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-alur.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-alur","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Alur","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kanuri","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kanuri is a groundbreaking dataset containing 128,111 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kanuri, making coding education accessible to Kanuri speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n128,111 high-quality conversations about programming and coding\nPure Kanuri language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kanuri.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kanuri","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kanuri","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-sepedi","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-sepedi is a groundbreaking dataset containing 108,619 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Sepedi, making coding education accessible to Sepedi speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n108,619 high-quality conversations about programming and coding\nPure Sepedi language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-sepedi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-sepedi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Pedi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tshiluba","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tshiluba is a groundbreaking dataset containing 113,468 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tshiluba, making coding education accessible to Tshiluba speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n113,468 high-quality conversations about programming and coding\nPure Tshiluba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tshiluba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tshiluba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Luba-Lulua","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-swati","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-swati is a groundbreaking dataset containing 122,345 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Swati, making coding education accessible to Swati speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n122,345 high-quality conversations about programming and coding\nPure Swati language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-swati.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-swati","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Swati","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tumbuka","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tumbuka is a groundbreaking dataset containing 129,591 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tumbuka, making coding education accessible to Tumbuka speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n129,591 high-quality conversations about programming and coding\nPure Tumbuka language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tumbuka.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tumbuka","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tumbuka","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-fon","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-fon is a groundbreaking dataset containing 125,588 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Fon, making coding education accessible to Fon speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n125,588 high-quality conversations about programming and coding\nPure Fon language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-fon.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-fon","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Fon","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-seychellois-creole","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-seychellois-creole is a groundbreaking dataset containing 100,690 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Seychellois Creole, making coding education accessible to Seychellois Creole speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n100,690 high-quality conversations about programming and coding\nPure Seychellois Creole language - democratizing coding education\nMulti-turn dialogues covering various‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-seychellois-creole.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-seychellois-creole","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Seselwa Creole French","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-rundi","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-rundi is a groundbreaking dataset containing 56,496 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Rundi, making coding education accessible to Rundi speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n56,496 high-quality conversations about programming and coding\nPure Rundi language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-rundi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-rundi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kirundi","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-oromo","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-oromo is a groundbreaking dataset containing 150,739 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Oromo, making coding education accessible to Oromo speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n150,739 high-quality conversations about programming and coding\nPure Oromo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-oromo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-oromo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Oromo","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-ndebele-south","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-ndebele-south is a groundbreaking dataset containing 176,994 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Ndebele (South), making coding education accessible to Ndebele (South) speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n176,994 high-quality conversations about programming and coding\nPure Ndebele (South) language - democratizing coding education\nMulti-turn dialogues covering various programming‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-ndebele-south.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-ndebele-south","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Southern Ndebele","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-mauritian-creole","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-mauritian-creole is a groundbreaking dataset containing 145,454 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Mauritian Creole, making coding education accessible to Mauritian Creole speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n145,454 high-quality conversations about programming and coding\nPure Mauritian Creole language - democratizing coding education\nMulti-turn dialogues covering various programming‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-mauritian-creole.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-mauritian-creole","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Morisyen","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"tulu-3-sft-mixture","keyword":"multilingual","description":"\n\n\n\t\n\t\t\n\t\tTulu 3 SFT Mixture\n\t\n\nNote that this collection is licensed under ODC-BY-1.0 license; different licenses apply to subsets of the data. Some portions of the dataset are non-commercial. We present the mixture as a research artifact.\nThe Tulu 3 SFT mixture was used to train the Tulu 3 series of models.\nIt contains 939,344 samples from the following sets:\n\nCoCoNot (ODC-BY-1.0), 10,983 prompts (Brahman et al., 2024)\nFLAN v2 via ai2-adapt-dev/flan_v2_converted, 89,982 prompts (Longpre et‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/tulu-3-sft-mixture.","url":"https://huggingface.co/datasets/allenai/tulu-3-sft-mixture","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["other","crowdsourced","expert-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"xP3megds","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for xP3\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nxP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks. It is used for the training of BLOOMZ and mT0, multilingual language models capable of following human instructions in dozens of languages zero-shot.\n\n\nCreation: The dataset can be recreated using instructions available here. We provide this version to save processing time and ease reproducibility.\nLanguages: 46 (Can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bigscience/xP3megds.","url":"https://huggingface.co/datasets/bigscience/xP3megds","creator_name":"BigScience Workshop","creator_url":"https://huggingface.co/bigscience","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Akan"],"keywords_longer_than_N":true},
	{"name":"many_emotions","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for \"many_emotions\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe data fields are:\n\nid: unique identifier\ntext: a string feature.\nlabel: a classification label, with possible values including anger (0), fear (1), joy (2), love (\n3), sadness (4), surprise (5), neutral (6).\nlicense: inherited license from source dataset\ndataset: source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ma2za/many_emotions.","url":"https://huggingface.co/datasets/ma2za/many_emotions","creator_name":"paolo mazza","creator_url":"https://huggingface.co/ma2za","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multilingual","dair-ai/emotion","daily_dialog","go_emotions"],"keywords_longer_than_N":true},
	{"name":"iva_mt_wslot","keyword":"machine translation","description":"\\","url":"https://huggingface.co/datasets/cartesinus/iva_mt_wslot","creator_name":"Marcin Sowanski","creator_url":"https://huggingface.co/cartesinus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","English","Polish","German","Spanish"],"keywords_longer_than_N":true},
	{"name":"common_voice_11_0","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Common Voice Corpus 11.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Common Voice dataset consists of a unique MP3 and corresponding text file. \nMany of the 24210 recorded hours in the dataset also include demographic metadata like age, sex, and accent \nthat can help improve the accuracy of speech recognition engines.\nThe dataset currently consists of 16413 validated hours in 100 languages, but more voices and languages are always added. \nTake a look at the Languages page to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0.","url":"https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0","creator_name":"Mozilla Foundation","creator_url":"https://huggingface.co/mozilla-foundation","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","crowdsourced","crowdsourced","multilingual","extended|common_voice"],"keywords_longer_than_N":true},
	{"name":"naamapadam","keyword":"multilingual","description":"\\","url":"https://huggingface.co/datasets/AnanthZeke/naamapadam","creator_name":"Ananth","creator_url":"https://huggingface.co/AnanthZeke","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","machine-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"diffusiondb-pixelart","keyword":"multilingual","description":"DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2\nmillion images generated by Stable Diffusion using prompts and hyperparameters\nspecified by real users. The unprecedented scale and diversity of this\nhuman-actuated dataset provide exciting research opportunities in understanding\nthe interplay between prompts and generative models, detecting deepfakes, and\ndesigning human-AI interaction tools to help users more easily use these models.","url":"https://huggingface.co/datasets/jainr3/diffusiondb-pixelart","creator_name":"Rahul Jain","creator_url":"https://huggingface.co/jainr3","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"jeli-asr","keyword":"multilingual","description":"The **Jeli-ASR Audio Dataset** is a multilingual dataset converted into the optimized Arrow format, \nensuring fast access and compatibility with modern data workflows. It contains audio samples in Bambara \nwith semi-expert transcriptions and French translations. Each subset of the dataset is organized by \nconfiguration (`jeli-asr-rmai`, `bam-asr-oza`, and `jeli-asr`) and further split into training and testing sets. \nThe dataset is designed for tasks like automatic speech recognition (ASR), text-to-speech synthesis (TTS), \nand translation. Data was recorded in Mali with griots, then transcribed and translated into French.\n","url":"https://huggingface.co/datasets/RobotsMali/jeli-asr","creator_name":"RobotsMali AI4D LAB","creator_url":"https://huggingface.co/RobotsMali","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","audio-language-identification","keyword-spotting"],"keywords_longer_than_N":true},
	{"name":"Korean_Wikipedia_Dataset_for_GPT2_August_2022","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for korean_wikipedia_dataset_for_GPT2\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nEntire Korean language Wikipedia data for GPT-2 training as of August 1st, 2022.\nemail: oscar.eaglewatch@gmail.com\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is to make a pre-trained GPT-2 Korean model\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nKorean\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nTrain wikipedia article count: 334420\nvalidation wikipedia article count: 83605\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n'text'\n\n\t\n\t\t\n\t\tData Splits‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eaglewatch/Korean_Wikipedia_Dataset_for_GPT2_August_2022.","url":"https://huggingface.co/datasets/eaglewatch/Korean_Wikipedia_Dataset_for_GPT2_August_2022","creator_name":"Yongwoo Jeong","creator_url":"https://huggingface.co/eaglewatch","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","translation","visual-question-answering","open-domain-qa","closed-domain-qa"],"keywords_longer_than_N":true},
	{"name":"mteb-nl-sick-pcls-pr","keyword":"translated","description":"\n  SICKNLPairClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSICK-NL is a Dutch translation of SICK \n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Written\n\n\nReference\nhttps://aclanthology.org/2021.eacl-main.126/\n\n\n\t\n\nSource datasets:\n\nclips/mteb-nl-sick\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"SICKNLPairClassification\")\nevaluator = mteb.MTEB([task])\n\nmodel =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clips/mteb-nl-sick-pcls-pr.","url":"https://huggingface.co/datasets/clips/mteb-nl-sick-pcls-pr","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","translated","clips/mteb-nl-sick","Dutch"],"keywords_longer_than_N":true},
	{"name":"mmBERT-midtraining-data","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Mid-training Data\n\t\n\n\n\n\n\n\nPhase 2 of 3: High-quality mid-training data mixture (600B tokens) with context extension to 8192 tokens.\n\nThis dataset contains the mid-training phase data used to train all mmBERT encoder models. This phase focuses on higher quality data sources and extends the context length from 1024 to 8192 tokens. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.\n\n\t\n\t\t\n\t\tüìä Data Composition\n\t\n\n\n\t\n\t\t\nData Source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-midtraining-data.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-midtraining-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"LocBenchRR","keyword":"multilingual","description":"\n  LocBenchRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSoftware Issue Localization.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://arxiv.org/abs/2503.09089Source datasets:\n\ntarsur909/mteb-loc-bench-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"LocBenchRR\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/LocBenchRR.","url":"https://huggingface.co/datasets/mteb/LocBenchRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-loc-bench-reranking","code"],"keywords_longer_than_N":true},
	{"name":"miracl-noauth","keyword":"multilingual","description":"A clone of the excellent miracl/miracl dataset that doesn't require authentication. Refer to the original dataset for details.\n","url":"https://huggingface.co/datasets/macavaney/miracl-noauth","creator_name":"Sean MacAvaney","creator_url":"https://huggingface.co/macavaney","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","miracl/miracl"],"keywords_longer_than_N":true},
	{"name":"c4","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tC4\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: \"https://commoncrawl.org\".\nThis is the processed version of Google's C4 dataset\nWe prepared five variants of the data: en, en.noclean, en.noblocklist, realnewslike, and multilingual (mC4).\nFor reference, these are the sizes of the variants:\n\nen: 305GB\nen.noclean: 2.3TB\nen.noblocklist: 380GB\nrealnewslike: 15GB\nmultilingual (mC4): 9.7TB (108 subsets, one per‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/c4.","url":"https://huggingface.co/datasets/allenai/c4","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"Art-GenEvalGPT","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset includes synthetic dialogues in the art domain that can be used for training a chatbot to discuss artworks within a museum setting. Leveraging Large Language Models (LLMs), particularly ChatGPT, the dataset comprises over 13,000 dialogues generated using prompt-engineering techniques. The dialogues cover a wide range of user and chatbot behaviors, including expert guidance, tutoring, and handling‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Astound/Art-GenEvalGPT.","url":"https://huggingface.co/datasets/Astound/Art-GenEvalGPT","creator_name":"Astound","creator_url":"https://huggingface.co/Astound","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"miracl-fr-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (fr) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-fr-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-fr-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-fr-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-fr-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","French"],"keywords_longer_than_N":true},
	{"name":"pedagogy-benchmark-multilingual","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tPedagogy Benchmark - Multilingual\n\t\n\nA multilingual translation of the AI-for-Education/pedagogy-benchmark dataset into African languages.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset provides translations of Chilean teacher training exam questions into African languages. The original dataset contains multiple-choice questions covering various pedagogical domains, education levels, and subject areas.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nLuganda (Uganda)\nNyankore (Uganda)\n\nComing soon: Swahili‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CraneAILabs/pedagogy-benchmark-multilingual.","url":"https://huggingface.co/datasets/CraneAILabs/pedagogy-benchmark-multilingual","creator_name":"Crane AI Labs","creator_url":"https://huggingface.co/CraneAILabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Ganda","Nyankole","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"NLU-few-shot-benchmark-en-de","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tNLU Few-shot Benchmark - English and German\n\t\n\nThis is a few-shot training dataset from the domain of human-robot interaction.\nIt contains texts in German and English language with 64 different utterances (classes).\nEach utterance (class) has exactly 20 samples in the training set.\nThis leads to a total of 1280 different training samples.\nThe dataset is intended to benchmark the intent classifiers of chat bots in English and especially in German language.\nWe are building on our‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/deutsche-telekom/NLU-few-shot-benchmark-en-de.","url":"https://huggingface.co/datasets/deutsche-telekom/NLU-few-shot-benchmark-en-de","creator_name":"Deutsche Telekom AG","creator_url":"https://huggingface.co/deutsche-telekom","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","intent-classification","multilingual","extended|deutsche-telekom/NLU-Evaluation-Data-en-de","English"],"keywords_longer_than_N":true},
	{"name":"tweetyface_debug","keyword":"multilingual","description":"DEBUG DATASET","url":"https://huggingface.co/datasets/ML-Projects-Kiel/tweetyface_debug","creator_name":"Machine Learning Projects FH Kiel","creator_url":"https://huggingface.co/ML-Projects-Kiel","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","machine-generated","crowdsourced","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"afriberta-corpus","keyword":"multilingual","description":"Corpus used for training AfriBERTa models","url":"https://huggingface.co/datasets/castorini/afriberta-corpus","creator_name":"Castorini","creator_url":"https://huggingface.co/castorini","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","Oromo","Amharic","Kinyarwanda"],"keywords_longer_than_N":true},
	{"name":"tydiqa_xtreme","keyword":"multilingual","description":"TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\ninformation-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\ndon‚Äôt know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\nthe use of translation (unlike MLQA and XQuAD).\n\nWe also include \"translate-train\" and \"translate-test\" splits for each non-English languages from XTREME (Hu et al., 2020). These splits are the automatic translations from English to each target language used in the XTREME paper [https://arxiv.org/abs/2003.11080]. The \"translate-train\" split purposefully ignores the non-English TyDiQA-GoldP training data to simulate the transfer learning scenario where original-language data is not available and system builders must rely on labeled English data plus existing machine translation systems.","url":"https://huggingface.co/datasets/juletxara/tydiqa_xtreme","creator_name":"Julen Etxaniz","creator_url":"https://huggingface.co/juletxara","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"miracl-zh-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (zh) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-zh-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-zh-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-zh-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-zh-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Chinese"],"keywords_longer_than_N":true},
	{"name":"udhr-lid","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tUDHR-LID\n\t\n\nWhy UDHR-LID?\nYou can access UDHR (Universal Declaration of Human Rights) here, but when a verse is missing, they have texts such as \"missing\" or \"?\". Also, about 1/3 of the sentences consist only of \"articles 1-30\" in different languages. We cleaned the entire dataset from XML files and selected only the paragraphs. We cleared any unrelated language texts from the data and also removed the cases that were incorrect.\nIncorrect? Look at the ckb and kmr files in the UDHR.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/udhr-lid.","url":"https://huggingface.co/datasets/cis-lmu/udhr-lid","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Tigrinya","Balkan Romani","Standard Arabic","Metlat√≥noc Mixtec"],"keywords_longer_than_N":true},
	{"name":"mqa","keyword":"multilingual","description":"MQA is a multilingual corpus of questions and answers parsed from the Common Crawl. Questions are divided between Frequently Asked Questions (FAQ) pages and Community Question Answering (CQA) pages.","url":"https://huggingface.co/datasets/clips/mqa","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","no-annotation","other","multilingual"],"keywords_longer_than_N":true},
	{"name":"naamapadam","keyword":"multilingual","description":"\\","url":"https://huggingface.co/datasets/ai4bharat/naamapadam","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","machine-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"MultiSWEbenchRR","keyword":"multilingual","description":"\n  MultiSWEbenchRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual Software Issue Localization.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://multi-swe-bench.github.io/#/\n\n\n\t\n\nSource datasets:\n\ntarsur909/mteb-swe-bench-multi-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"MultiSWEbenchRR\")\nevaluator = mteb.MTEB([task])‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MultiSWEbenchRR.","url":"https://huggingface.co/datasets/mteb/MultiSWEbenchRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-swe-bench-multi-reranking","code"],"keywords_longer_than_N":true},
	{"name":"miracl-hi-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (hi) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-hi-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-hi-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-hi-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-hi-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Hindi"],"keywords_longer_than_N":true},
	{"name":"miracl-ja-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (ja) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-ja-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-ja-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-ja-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-ja-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Japanese"],"keywords_longer_than_N":true},
	{"name":"miracl-bn-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (bn) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-bn-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-bn-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-bn-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-bn-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Bengali"],"keywords_longer_than_N":true},
	{"name":"miracl-ru-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (ru) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-ru-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-ru-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-ru-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-ru-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Russian"],"keywords_longer_than_N":true},
	{"name":"miracl-fa-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (fa) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-fa-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-fa-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-fa-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-fa-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Persian"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-es-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (es) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (es) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-es-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-es-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Spanish"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-simple-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (simple English) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (simple English) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","multilingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"fleurs","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tFLEURS\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark. \nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\nused and ‚Äùunit error rate‚Äù (characters, signs) of all languages is averaged. Languages and results are also grouped into seven‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/fleurs.","url":"https://huggingface.co/datasets/google/fleurs","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-de-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (de) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (de) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-de-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-de-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","German"],"keywords_longer_than_N":true},
	{"name":"taln-archives","keyword":"multilingual","description":"TALN Archives benchmark dataset for keyphrase extraction an generation.","url":"https://huggingface.co/datasets/taln-ls2n/taln-archives","creator_name":"TALN research group at LS2N lab","creator_url":"https://huggingface.co/taln-ls2n","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","unknown","unknown","multilingual","French"],"keywords_longer_than_N":true},
	{"name":"tapaco","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for TaPaCo Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA freely available paraphrase corpus for 73 languages extracted from the Tatoeba database. \nTatoeba is a crowdsourcing project mainly geared towards language learners. Its aim is to provide example sentences \nand translations for particular linguistic constructions and words. The paraphrase corpus is created by populating a \ngraph with Tatoeba sentences and equivalence links between sentences ‚Äúmeaning the same thing‚Äù. This‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/tapaco.","url":"https://huggingface.co/datasets/community-datasets/tapaco","creator_name":"Community Datasets","creator_url":"https://huggingface.co/community-datasets","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","text-classification","semantic-similarity-classification","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"hope_edi","keyword":"multilingual","description":"A Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not.","url":"https://huggingface.co/datasets/dravidianlangtech/hope_edi","creator_name":"dravidianlangtech","creator_url":"https://huggingface.co/dravidianlangtech","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","expert-generated","crowdsourced","monolingual","multilingual"],"keywords_longer_than_N":true},
	{"name":"africlirmatrix","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAfriCLIRMatrix is a test collection for cross-lingual information retrieval research in 15 diverse African languages. This resource comprises English queries with query‚Äìdocument relevance judgments in 15 African languages automatically mined from Wikipedia\nThis dataset stores documents of AfriCLIRMatrix. To access the queries and judgments, please refer to castorini/africlirmatrix.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nThe only configuration here is the language.\nAn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/castorini/africlirmatrix.","url":"https://huggingface.co/datasets/castorini/africlirmatrix","creator_name":"Castorini","creator_url":"https://huggingface.co/castorini","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multilingual","Afrikaans","Amharic","Egyptian Arabic"],"keywords_longer_than_N":true},
	{"name":"scielo","keyword":"multilingual","description":"A parallel corpus of full-text scientific articles collected from Scielo database in the following languages: English, Portuguese and Spanish. The corpus is sentence aligned for all language pairs, as well as trilingual aligned for a small subset of sentences. Alignment was carried out using the Hunalign algorithm.","url":"https://huggingface.co/datasets/bigbio/scielo","creator_name":"BigScience Biomedical Datasets","creator_url":"https://huggingface.co/bigbio","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multilingual","English","Spanish","Portuguese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"mkqa","keyword":"multilingual","description":"We introduce MKQA, an open-domain question answering evaluation set comprising 10k question-answer pairs sampled from the Google Natural Questions dataset, aligned across 26 typologically diverse languages (260k question-answer pairs in total). For each query we collected new passage-independent answers. These queries and answers were then human translated into 25 Non-English languages.","url":"https://huggingface.co/datasets/apple/mkqa","creator_name":"Apple","creator_url":"https://huggingface.co/apple","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":null,"first_N":5,"first_N_keywords":["question-answering","open-domain-qa","crowdsourced","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"curation-corpus-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tcuration-corpus-ru\n\t\n\nTranslated version of d0rj/curation-corpus into Russian.\n","url":"https://huggingface.co/datasets/d0rj/curation-corpus-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["summarization","translated","monolingual","d0rj/curation-corpus","Russian"],"keywords_longer_than_N":true},
	{"name":"multi_para_crawl","keyword":"multilingual","description":"Parallel corpora from Web Crawls collected in the ParaCrawl project and further processed for making it a multi-parallel corpus by pivoting via English. Here we only provide the additional language pairs that came out of pivoting. The bitexts for English are available from the ParaCrawl release.\n40 languages, 669 bitexts\ntotal number of files: 40\ntotal number of tokens: 10.14G\ntotal number of sentence fragments: 505.48M\n\nPlease, acknowledge the ParaCrawl project at http://paracrawl.eu. This version is derived from the original release at their website adjusted for redistribution via the OPUS corpus collection. Please, acknowledge OPUS as well for this service.","url":"https://huggingface.co/datasets/Helsinki-NLP/multi_para_crawl","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"headline_cause","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for HeadlineCause\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA dataset for detecting implicit causal relations between pairs of news headlines. The dataset includes over 5000 headline pairs from English news and over 9000 headline pairs from Russian news labeled through crowdsourcing. The pairs vary from totally unrelated or belonging to the same general topic to the ones including causation and refutation relations.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nLoading Russian Simple task:\nfrom datasets import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IlyaGusev/headline_cause.","url":"https://huggingface.co/datasets/IlyaGusev/headline_cause","creator_name":"Ilya Gusev","creator_url":"https://huggingface.co/IlyaGusev","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multi-class-classification","crowdsourced","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"ml_spoken_words","keyword":"multilingual","description":"Multilingual Spoken Words Corpus is a large and growing audio dataset of spoken\nwords in 50 languages collectively spoken by over 5 billion people, for academic\nresearch and commercial applications in keyword spotting and spoken term search,\nlicensed under CC-BY 4.0. The dataset contains more than 340,000 keywords,\ntotaling 23.4 million 1-second spoken examples (over 6,000 hours). The dataset\nhas many use cases, ranging from voice-enabled consumer devices to call center\nautomation. This dataset is generated by applying forced alignment on crowd-sourced sentence-level\naudio to produce per-word timing estimates for extraction.\nAll alignments are included in the dataset.","url":"https://huggingface.co/datasets/MLCommons/ml_spoken_words","creator_name":"MLCommons","creator_url":"https://huggingface.co/MLCommons","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["audio-classification","machine-generated","other","multilingual","extended|common_voice"],"keywords_longer_than_N":true},
	{"name":"swissner","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tSwissNER\n\t\n\nA multilingual test set for named entity recognition (NER) on Swiss news articles.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nSwissNER is a dataset for named entity recognition based on manually annotated news articles in Swiss Standard German, French, Italian, and Romansh Grischun.\nWe have manually annotated a selection of articles that have been published in February 2023 in the categories \"Switzerland\" or \"Regional\" on the following online news portals:\n\nSwiss Standard German: srf.ch‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZurichNLP/swissner.","url":"https://huggingface.co/datasets/ZurichNLP/swissner","creator_name":"University of Zurich, Department of Computational Linguistics","creator_url":"https://huggingface.co/ZurichNLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","multilingual","German","French"],"keywords_longer_than_N":true},
	{"name":"Korean_Real_Estate_Ads_Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKorean Real Estate Ads Dataset\n\t\n\nThis dataset contains high-resolution images of Korean real estate advertisements, including online listings, printed flyers, and billboard ads for properties such as apartments, houses, and commercial spaces. It is designed to support AI research in OCR, visual understanding, and property analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupported‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Korean_Real_Estate_Ads_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Korean_Real_Estate_Ads_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","Korean","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"mapa-eur-lex","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Multilingual European Datasets for Sensitive Entity Detection in the Legal Domain\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a completed version of the MAPA EUR-LEX dataset, originally converted to Huggingface format by joelniklaus. See the dataset card for more information about MAPA.\n3 of the (Spanish) EUR-LEX WebAnno TSV files in the source MAPA repository are malformed, so they were omitted from the original conversion, causing under-representation of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dglover1/mapa-eur-lex.","url":"https://huggingface.co/datasets/dglover1/mapa-eur-lex","creator_name":"D Glover","creator_url":"https://huggingface.co/dglover1","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","other","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"mapa-eur-lex","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Multilingual European Datasets for Sensitive Entity Detection in the Legal Domain\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a completed version of the MAPA EUR-LEX dataset, originally converted to Huggingface format by joelniklaus. See the dataset card for more information about MAPA.\n3 of the (Spanish) EUR-LEX WebAnno TSV files in the source MAPA repository are malformed, so they were omitted from the original conversion, causing under-representation of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dglover1/mapa-eur-lex.","url":"https://huggingface.co/datasets/dglover1/mapa-eur-lex","creator_name":"D Glover","creator_url":"https://huggingface.co/dglover1","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","other","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"lawinstruct","keyword":"multilingual","description":"LawInstruct is an instruction tuning dataset of multilingual legal documents.","url":"https://huggingface.co/datasets/lawinstruct/lawinstruct","creator_name":"lawinstruct","creator_url":"https://huggingface.co/lawinstruct","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["fill-mask","other","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"x_dataset_36","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_36.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_36","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"DBPedia_PL_test_top_250_only_w_correct-v2","keyword":"translated","description":"\n  DBPedia-PLHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\n\n\nReference\nhttps://github.com/iai-group/DBpedia-Entity/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DBPedia_PL_test_top_250_only_w_correct-v2.","url":"https://huggingface.co/datasets/mteb/DBPedia_PL_test_top_250_only_w_correct-v2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/dbpedia","Polish"],"keywords_longer_than_N":true},
	{"name":"x_dataset_25","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_25.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_25","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_63","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hadesgod517/reddit_dataset_63.","url":"https://huggingface.co/datasets/hadesgod517/reddit_dataset_63","creator_name":"Hades","creator_url":"https://huggingface.co/hadesgod517","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"amazon_massive_intent","keyword":"translated","description":"\n  MassiveIntentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSpoken\n\n\nReference\nhttps://arxiv.org/abs/2204.08582\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"MassiveIntentClassification\"])\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/amazon_massive_intent.","url":"https://huggingface.co/datasets/mteb/amazon_massive_intent","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","translated","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"x_dataset_132","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_132.","url":"https://huggingface.co/datasets/gk4u/x_dataset_132","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_0110104","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/reddit_dataset_0110104.","url":"https://huggingface.co/datasets/william-1111/reddit_dataset_0110104","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"arabic-english-code-switching-text","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tArabic-English Code-Switching Dataset (Text Only)\n\t\n\nThis dataset is a text-only version of Arabic-English Code-Switching Dataset dataset,\ncreated by this notebook.\n\n\t\n\t\t\n\t\tChanges Made\n\t\n\n\nExtracted only the text column from the original dataset.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"MagedSaeed/arabic-english-code-switching-text\")\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nPlease reference/cite the original dataset when using this data.\n","url":"https://huggingface.co/datasets/MagedSaeed/arabic-english-code-switching-text","creator_name":"Maged Saeed","creator_url":"https://huggingface.co/MagedSaeed","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","multilingual","MohamedRashad/arabic-english-code-switching","Arabic"],"keywords_longer_than_N":true},
	{"name":"arabic-english-code-switching-text","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tArabic-English Code-Switching Dataset (Text Only)\n\t\n\nThis dataset is a text-only version of Arabic-English Code-Switching Dataset dataset,\ncreated by this notebook.\n\n\t\n\t\t\n\t\tChanges Made\n\t\n\n\nExtracted only the text column from the original dataset.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"MagedSaeed/arabic-english-code-switching-text\")\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nPlease reference/cite the original dataset when using this data.\n","url":"https://huggingface.co/datasets/MagedSaeed/arabic-english-code-switching-text","creator_name":"Maged Saeed","creator_url":"https://huggingface.co/MagedSaeed","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","multilingual","MohamedRashad/arabic-english-code-switching","Arabic"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Model-based","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Model-based is a dataset of BenchMAX, sourcing from m-ArenaHard, which evaluates the instruction following capability via model-based judgment.\nWe extend the original dataset to include languages that are not supported by m-ArenaHard through‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Model-based","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Model-based is a dataset of BenchMAX, sourcing from m-ArenaHard, which evaluates the instruction following capability via model-based judgment.\nWe extend the original dataset to include languages that are not supported by m-ArenaHard through‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_128_test","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GSKCM24/reddit_dataset_128_test.","url":"https://huggingface.co/datasets/GSKCM24/reddit_dataset_128_test","creator_name":"GUNEET SINGH KHURANA","creator_url":"https://huggingface.co/GSKCM24","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"common_voice_13_0_dv_preprocessed","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Common Voice Corpus 13.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Common Voice dataset consists of a unique MP3 and corresponding text file. \nMany of the 27141 recorded hours in the dataset also include demographic metadata like age, sex, and accent \nthat can help improve the accuracy of speech recognition engines.\nThe dataset currently consists of 17689 validated hours in 108 languages, but more voices and languages are always added. \nTake a look at the Languages page to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fmagot01/common_voice_13_0_dv_preprocessed.","url":"https://huggingface.co/datasets/fmagot01/common_voice_13_0_dv_preprocessed","creator_name":"Francisco Magot","creator_url":"https://huggingface.co/fmagot01","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","crowdsourced","crowdsourced","multilingual","extended|common_voice"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_25","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chenxinpingcxp/reddit_dataset_25.","url":"https://huggingface.co/datasets/chenxinpingcxp/reddit_dataset_25","creator_name":"tian chen","creator_url":"https://huggingface.co/chenxinpingcxp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_140","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/taowtje/reddit_dataset_140.","url":"https://huggingface.co/datasets/taowtje/reddit_dataset_140","creator_name":"TAO tje","creator_url":"https://huggingface.co/taowtje","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_47","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tarzan19990815/reddit_dataset_47.","url":"https://huggingface.co/datasets/tarzan19990815/reddit_dataset_47","creator_name":"matthew allen","creator_url":"https://huggingface.co/tarzan19990815","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"RomaniBibleClustering","keyword":"translated","description":"\n  RomaniBibleClustering\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering verses from the Bible in Kalderash Romani by book.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReligious, Written\n\n\nReference\nhttps://romani.global.bible/info\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"RomaniBibleClustering\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RomaniBibleClustering.","url":"https://huggingface.co/datasets/mteb/RomaniBibleClustering","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","translated","kardosdrur/romani-bible","Romany"],"keywords_longer_than_N":true},
	{"name":"x_dataset_191","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SAVE0x0/x_dataset_191.","url":"https://huggingface.co/datasets/SAVE0x0/x_dataset_191","creator_name":"x","creator_url":"https://huggingface.co/SAVE0x0","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0104179","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_0104179.","url":"https://huggingface.co/datasets/william-1111/x_dataset_0104179","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"EuroGEC-7","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tEuroGEC-7: A Growing Multilingual Dataset for Grammatical Error Correction\n\t\n\nEuroGEC-7 is a large-scale, synthetic, multilingual grammatical error correction (GEC) dataset created using the Mistral API. It is specifically designed to simulate learner-style grammar mistakes across 7 major European languages ‚Äî with over 20,000 annotated pairs and counting.\nThis dataset is actively maintained and continuously expanding, both in scale and coverage. New entries are generated daily from a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/EuroGEC-7.","url":"https://huggingface.co/datasets/NoeFlandre/EuroGEC-7","creator_name":"No√© Flandre","creator_url":"https://huggingface.co/NoeFlandre","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","French","Spanish","German","Italian"],"keywords_longer_than_N":true},
	{"name":"x_dataset_4","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_4.","url":"https://huggingface.co/datasets/suul999922/x_dataset_4","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Granary","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tGranary: Speech Recognition and Translation Dataset in 25 European Languages\n\t\n\nGranary is a large-scale, open-source multilingual speech dataset covering 25 European languages for Automatic Speech Recognition (ASR) and Automatic Speech Translation (AST) tasks. \n\n\n\n\t\n\t\t\n\n\n\n\n\t\t\n\n\n\n\n\t\n\n\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nGranary addresses the scarcity of high-quality speech data for low-resource languages by consolidating multiple datasets under a unified framework:\nüó£Ô∏è ~1M hours of high-quality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/Granary.","url":"https://huggingface.co/datasets/nvidia/Granary","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","translation","Bulgarian","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"IdiomsInCtx-MT","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tIdiomsInCtx-MT Dataset\n\t\n\nThis repository contains the IdiomsInCtx-MT dataset used in our ACL 2024 paper: The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities. See this GitHub repo for the origin of the data.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of idiomatic expressions in context and their human-written translations. There are 1000 translations per direction. The dataset covers 2 language pairs (English-German and English-Russian) with 3‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/IdiomsInCtx-MT.","url":"https://huggingface.co/datasets/davidstap/IdiomsInCtx-MT","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["expert-generated","translation","multilingual","German","English"],"keywords_longer_than_N":true},
	{"name":"hadith_datasets","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tSunnah Dataset ‚Äî Hadith JSON & CSV Collection\n\t\n\nA blessed and open-source collection of authenticated Hadiths from the six major books of Sunnah, available in both JSON and CSV formats for research, study, and teaching purposes. This dataset is structured cleanly with English + Arabic + grading + reference links for each Hadith.\n\n\n\t\n\t\t\n\t\tContents\n\t\n\nThis dataset contains the following Hadith collections:\n\n\t\n\t\t\nFile Name\nFormat\nBook Name\n\n\n\t\t\nJami' at-Tirmidhi.csv\nCSV\nJami' at-Tirmidhi‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/meeAtif/hadith_datasets.","url":"https://huggingface.co/datasets/meeAtif/hadith_datasets","creator_name":"Atif","creator_url":"https://huggingface.co/meeAtif","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","Arabic","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Problem_Solving","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Problem_Solving is a dataset of BenchMAX, sourcing from LiveCodeBench_v4, which evaluates the code generation capability for solving multilingual competitive code problems.\nWe extend the original English dataset by 16 non-English languages.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Problem_Solving.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Problem_Solving","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"xP3x","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for xP3x\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nxP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts & datasets across 277 languages & 16 NLP tasks. It contains all of xP3 + much more! It is used for training future contenders of mT0 & BLOOMZ at project Aya @Cohere Labs üß°\n\n\nCreation: The dataset can be recreated using instructions available here together with the file in this repository named xp3x_create.py. We provide this version to save processing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/xP3x.","url":"https://huggingface.co/datasets/CohereLabs/xP3x","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"bigbench","keyword":"multilingual","description":"BIG-Bench but it doesn't require the hellish dependencies (tensorflow, pypi-bigbench, protobuf) of the official version.\ndataset = load_dataset(\"tasksource/bigbench\",'movie_recommendation')\n\nCode to reproduce:\nhttps://colab.research.google.com/drive/1MKdLdF7oqrSQCeavAcsEnPdI85kD0LzU?usp=sharing\nDatasets are capped to 50k examples to keep things light.\nI also removed the default split when train was available also to save space, as default=train+val.\n@article{srivastava2022beyond‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/bigbench.","url":"https://huggingface.co/datasets/tasksource/bigbench","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","text-generation","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"x_dataset_8","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Axel232/x_dataset_8.","url":"https://huggingface.co/datasets/Axel232/x_dataset_8","creator_name":"Pits","creator_url":"https://huggingface.co/Axel232","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"vietnamese-nom-latin-translation","keyword":"multilingual","description":"lunovian/vietnamese-nom-latin-translation dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/lunovian/vietnamese-nom-latin-translation","creator_name":"Nguyen Xuan An","creator_url":"https://huggingface.co/lunovian","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","Vietnamese","Latin"],"keywords_longer_than_N":true},
	{"name":"LatinSummarizer","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tLatinSummarizer Dataset\n\t\n\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\naligned_en_la_data_raw.csv\naligned_en_la_data_cleaned.csv\naligned_en_la_data_cleaned_with_stanza.csv\nconcat_aligned_data.csv\nconcat_cleaned.csv\nlatin_wikipedia_cleaned.csv\nlatin_wikipedia_raw.csv\nlatin-literature-dataset-170M_raw_cleaned.csv\nlatin-literature-dataset-170M_raw_cleaned_chunked.csv\nElsa_aligned/\nREADME.md\n\n\n\t\n\t\t\n\t\n\t\n\t\tDetails\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\taligned_en_la_data_raw.csv\n\t\n\nThis dataset contains aligned Latin (la) - English (en)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/LatinSummarizer.","url":"https://huggingface.co/datasets/LatinNLP/LatinSummarizer","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","summarization","news-articles-summarization","document-retrieval"],"keywords_longer_than_N":true},
	{"name":"x_dataset_11230","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/x_dataset_11230.","url":"https://huggingface.co/datasets/StormKing99/x_dataset_11230","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Electrical-engineering-ru","keyword":"translated","description":"Translated instructions from STEM-AI-mtl/Electrical-engineering into Russian using gemini-flash-1.5-8b.\n","url":"https://huggingface.co/datasets/d0rj/Electrical-engineering-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","translated","STEM-AI-mtl/Electrical-engineering","Russian"],"keywords_longer_than_N":true},
	{"name":"x_dataset_41","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_41.","url":"https://huggingface.co/datasets/James096/x_dataset_41","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"bhojpuri","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tFLEURS\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark. \nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\nused and ‚Äùunit error rate‚Äù (characters, signs) of all languages is averaged. Languages and results are also grouped into seven‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ankur02/bhojpuri.","url":"https://huggingface.co/datasets/ankur02/bhojpuri","creator_name":"Ankur Verma","creator_url":"https://huggingface.co/ankur02","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"diffusiondb","keyword":"multilingual","description":"DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2\nmillion images generated by Stable Diffusion using prompts and hyperparameters\nspecified by real users. The unprecedented scale and diversity of this\nhuman-actuated dataset provide exciting research opportunities in understanding\nthe interplay between prompts and generative models, detecting deepfakes, and\ndesigning human-AI interaction tools to help users more easily use these models.","url":"https://huggingface.co/datasets/jobs-git/diffusiondb","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"bluesky-posts","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\t8 Million Bluesky Social Posts Collection\n\t\n\nI've collected and curated 8 million public posts from Bluesky Social between November 27 - December 1, 2024, with an additional 12 million posts coming in the upcoming weeks. This growing dataset aims to provide researchers and developers with a comprehensive sample of real world social media data for analysis and experimentation. This collection represents one of the largest publicly available Bluesky datasets, offering unique insights‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/withalim/bluesky-posts.","url":"https://huggingface.co/datasets/withalim/bluesky-posts","creator_name":"alim maasoglu","creator_url":"https://huggingface.co/withalim","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","multilingual","mit","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"IndicSentiment","keyword":"translated","description":"\n  IndicSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA new, multilingual, and n-way parallel dataset for sentiment analysis in 13 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReferencehttps://arxiv.org/abs/2212.05409\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"IndicSentimentClassification\"])\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicSentiment.","url":"https://huggingface.co/datasets/mteb/IndicSentiment","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"IndicReviewsClusteringP2P","keyword":"translated","description":"\n  IndicReviewsClusteringP2P\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of reviews from IndicSentiment dataset. Clustering of 14 sets on the generic categories label.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReference\nhttps://arxiv.org/abs/2212.05409\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"IndicReviewsClusteringP2P\"])\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicReviewsClusteringP2P.","url":"https://huggingface.co/datasets/mteb/IndicReviewsClusteringP2P","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","translated","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0303241","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_0303241.","url":"https://huggingface.co/datasets/james-1111/x_dataset_0303241","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_193","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RentonWEB3/reddit_dataset_193.","url":"https://huggingface.co/datasets/RentonWEB3/reddit_dataset_193","creator_name":"Renton Mark","creator_url":"https://huggingface.co/RentonWEB3","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"NeuCLIR2023Retrieval","keyword":"multilingual","description":"\n  NeuCLIR2023Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\nSource datasets:\n\nmteb/neuclir-2023\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"NeuCLIR2023Retrieval\")\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NeuCLIR2023Retrieval.","url":"https://huggingface.co/datasets/mteb/NeuCLIR2023Retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2023","Persian"],"keywords_longer_than_N":true},
	{"name":"English-Hinglish","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tEnglish Hinglish\n\t\n\nEnglish to Hinglish Dataset processed from findnitai/english-to-hinglish.\nSources:\n\nHinglish TOP Dataset\nCMU English Dog\nHinGE\nPHINC\n\n","url":"https://huggingface.co/datasets/rvv-karma/English-Hinglish","creator_name":"Rahul Vishwakarma","creator_url":"https://huggingface.co/rvv-karma","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","multilingual","translation","English"],"keywords_longer_than_N":true},
	{"name":"x_dataset_192","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mamung/x_dataset_192.","url":"https://huggingface.co/datasets/mamung/x_dataset_192","creator_name":"ansloth","creator_url":"https://huggingface.co/mamung","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_12.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_12","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_19039","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_19039.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_19039","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_1051","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_1051.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_1051","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_1.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_1","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"domain-translations","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Domain Name Translations Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 155,004 domain names with their multilingual translations across 20 languages. Each domain has been segmented into constituent words and translated while preserving semantic meaning and commercial appeal. The dataset is particularly valuable for domain name research, multilingual NLP tasks, and understanding how brand names and concepts translate across languages.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/humbleworth/domain-translations.","url":"https://huggingface.co/datasets/humbleworth/domain-translations","creator_name":"HumbleWorth","creator_url":"https://huggingface.co/humbleworth","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","feature-extraction","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"domain-translations","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Domain Name Translations Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 155,004 domain names with their multilingual translations across 20 languages. Each domain has been segmented into constituent words and translated while preserving semantic meaning and commercial appeal. The dataset is particularly valuable for domain name research, multilingual NLP tasks, and understanding how brand names and concepts translate across languages.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/humbleworth/domain-translations.","url":"https://huggingface.co/datasets/humbleworth/domain-translations","creator_name":"HumbleWorth","creator_url":"https://huggingface.co/humbleworth","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","feature-extraction","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ashikshaffi08/x_dataset_44.","url":"https://huggingface.co/datasets/ashikshaffi08/x_dataset_44","creator_name":"Ashik","creator_url":"https://huggingface.co/ashikshaffi08","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_39615","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_39615.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_39615","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/roknedin/x_dataset_44.","url":"https://huggingface.co/datasets/roknedin/x_dataset_44","creator_name":"Mohammad Roknedin","creator_url":"https://huggingface.co/roknedin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mmBERT-data-decay-cont","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-data-decay-cont.","url":"https://huggingface.co/datasets/orionweller/mmBERT-data-decay-cont","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_246","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trungnam299/reddit_dataset_246.","url":"https://huggingface.co/datasets/trungnam299/reddit_dataset_246","creator_name":"Trung Nam","creator_url":"https://huggingface.co/trungnam299","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"fusion-synth-data-geofactx","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOffline Synthetic Data (GeoFactX) for: Making, not taking, the Best-of-N\n\t\n\n\n\t\n\t\t\n\t\tContent\n\t\n\nThis data contains completions for the  GeoFactX training split prompts from 5 different teacher models and 2 aggregations:\nTeachers: We sample one completion from each of the following models at temperature T=0.3. For kimik2, qwen3, and deepseek-v3 we use TogetherAI, for gemma3-27b and command-a we use locally hosted images.\n\ngemma3-27b: GEMMA3-27B-IT\nkimik2: KIMI-K2-INSTRUCT\nqwen3:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/fusion-synth-data-geofactx.","url":"https://huggingface.co/datasets/CohereLabs/fusion-synth-data-geofactx","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Hindi","Japanese"],"keywords_longer_than_N":true},
	{"name":"x_dataset_156","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/markrogolino/x_dataset_156.","url":"https://huggingface.co/datasets/markrogolino/x_dataset_156","creator_name":"Mark Rogolino","creator_url":"https://huggingface.co/markrogolino","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_10492","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_10492.","url":"https://huggingface.co/datasets/momo1942/x_dataset_10492","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\nThe dataset includes content tailored for financial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k.","url":"https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k","creator_name":"Joseph G Flowers","creator_url":"https://huggingface.co/Josephgflowers","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_111","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicchio816/reddit_dataset_111.","url":"https://huggingface.co/datasets/nicchio816/reddit_dataset_111","creator_name":"Alex Avery","creator_url":"https://huggingface.co/nicchio816","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"plaid-shirttt-doc-date","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tClueweb09 and NeuCLIR1 document dates for reproducing PLAID SHIRTTT\n\t\n\nThis dataset contains the dates of each document in Clueweb09 and \nNeuCLIR1 for reproducing experiments in the PLAID SHIRTTT paper (accepted at SIGIR 2024). \nFor reproducibility, we release the document IDs of the collection divided into shards where there is a file per shard. \nThe creation date of each document is extracted and recorded in ISO format along with the document ID. \nFor combining the retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hltcoe/plaid-shirttt-doc-date.","url":"https://huggingface.co/datasets/hltcoe/plaid-shirttt-doc-date","creator_name":"JHU Human Language Technology Center of Excellence","creator_url":"https://huggingface.co/hltcoe","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","multilingual","neuclir/neuclir1","clueweb09"],"keywords_longer_than_N":true},
	{"name":"mypresentation","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for MyPresentation.ru\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 420,946 educational presentations in Russian extracted from mypresentation.ru website. The content includes presentation slides across various educational topics and categories.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian (ru).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the following fields:\n\nurl: URL of the presentation page (string)\ntitle:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/mypresentation.","url":"https://huggingface.co/datasets/nyuuzyou/mypresentation","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","image-to-text","topic-classification","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"x_dataset_12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_12.","url":"https://huggingface.co/datasets/suul999922/x_dataset_12","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_225","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AISOMA-Bittensor/reddit_dataset_225.","url":"https://huggingface.co/datasets/AISOMA-Bittensor/reddit_dataset_225","creator_name":"Murat Durmus","creator_url":"https://huggingface.co/AISOMA-Bittensor","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"semi-Voxpopuli","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tVoxPopuli Multilingual Audio Dataset\n\t\n\nThis dataset contains audio recordings in English (EN), Polish (PL), and Swedish (SV) languages. It is derived from the VoxPopuli dataset and tailored for multilingual language processing tasks.\nThe dataset includes audio clips and corresponding metadata to support research and development in multilingual audio processing.\n\n\t\n\t\t\n\t\tDataset Files\n\t\n\nThe dataset includes the following files:\n\ndata.csv: Contains metadata about the audio files‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jagadeesh9580/semi-Voxpopuli.","url":"https://huggingface.co/datasets/Jagadeesh9580/semi-Voxpopuli","creator_name":"Jagadeesh Rachapudi","creator_url":"https://huggingface.co/Jagadeesh9580","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","soundfolder","Audio","Datasets"],"keywords_longer_than_N":true},
	{"name":"dhanishtha-2.0-superthinker","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüì¶ Dhanishtha-2.0-SUPERTHINKER-MLX\n\t\n\n A distilled corpus of 11.7K high-quality samples showcasing multi-phase reasoning and structured emotional cognition. Sourced directly from the internal training data of Dhanishtha-2.0 ‚Äî the world‚Äôs first Large Language Model (LLM) to implement Intermediate Thinking, featuring multiple <think> and <ser> blocks per response\n\n\t\n\t\t\n\t\n\t\n\t\tExample with MLX-LM-LoRA:\n\t\n\nmlx_lm_lora.train \\\n--model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker.","url":"https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Afrikaans","Arabic","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"x_dataset_040752","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_040752.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_040752","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_76","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/reddit_dataset_76.","url":"https://huggingface.co/datasets/marry-1111/reddit_dataset_76","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_196","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/reddit_dataset_196.","url":"https://huggingface.co/datasets/arrmlet/reddit_dataset_196","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"fineweb2hq-vs-c4","keyword":"multilingual","description":"This dataset includes 5000 rows per language from each of two sources: the higher-quality epfml/FineWeb2-HQ\nand the lower-quality allenai/c4. The data is split 80/20 into training and test sets.\nLanguages were carefully chosen to ensure balanced representation across both splits:\nArabic, Chinese, Czech, Danish, Dutch, French, German, Greek, Hungarian, Indonesian, Italian, Japanese, Persian, Polish, Portuguese, Russian, Spanish, Swedish, Turkish, and Vietnamese.\n","url":"https://huggingface.co/datasets/agentlans/fineweb2hq-vs-c4","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","Portuguese","Danish","Persian","German"],"keywords_longer_than_N":true},
	{"name":"x_dataset_231","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bit0/x_dataset_231.","url":"https://huggingface.co/datasets/bit0/x_dataset_231","creator_name":"sn13","creator_url":"https://huggingface.co/bit0","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_7","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_7.","url":"https://huggingface.co/datasets/suul999922/x_dataset_7","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0402228","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_0402228.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_0402228","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44882","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_44882.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_44882","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_218","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SAVE0x0/x_dataset_218.","url":"https://huggingface.co/datasets/SAVE0x0/x_dataset_218","creator_name":"x","creator_url":"https://huggingface.co/SAVE0x0","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_197","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chaiamy/x_dataset_197.","url":"https://huggingface.co/datasets/chaiamy/x_dataset_197","creator_name":"Amy","creator_url":"https://huggingface.co/chaiamy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_88","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wenknow/reddit_dataset_88.","url":"https://huggingface.co/datasets/wenknow/reddit_dataset_88","creator_name":"Dt","creator_url":"https://huggingface.co/wenknow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"IN22ConvBitextMining","keyword":"multilingual","description":"\n  IN22ConvBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Conv is a n-way parallel conversation domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nSocial, Spoken, Fiction, Spoken\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Conv\n\n\n\t\n\nSource datasets:\n\nmteb/IN22-Conv\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22ConvBitextMining.","url":"https://huggingface.co/datasets/mteb/IN22ConvBitextMining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","multilingual","mteb/IN22-Conv","Assamese"],"keywords_longer_than_N":true},
	{"name":"MasakhaNEWSClusteringP2P","keyword":"multilingual","description":"\n  MasakhaNEWSClusteringP2P\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of news article headlines and texts from MasakhaNEWS dataset. Clustering of 10 sets on the news article label.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written, Non-fiction\nReference\nhttps://huggingface.co/datasets/masakhane/masakhanews\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MasakhaNEWSClusteringP2P.","url":"https://huggingface.co/datasets/mteb/MasakhaNEWSClusteringP2P","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","multilingual","Amharic","English"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_377626","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_377626.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_377626","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_7","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Aniruddh79012/x_dataset_7.","url":"https://huggingface.co/datasets/Aniruddh79012/x_dataset_7","creator_name":"Singh","creator_url":"https://huggingface.co/Aniruddh79012","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"medical-reports-simplification-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüè• Medical Reports Simplification Dataset\n\t\n\n\n\t\n\t\t\n\t\tüìã Description\n\t\n\nDataset cr√©√© avec Gemini 2.5 Pro (Preview) pour entra√Æner des mod√®les √† simplifier les rapports m√©dicaux complexes en explications compr√©hensibles pour les patients.\nüéØ Objectif : D√©mocratiser l'acc√®s √† l'information m√©dicale en rendant les rapports techniques accessibles au grand public.\n\n\t\n\t\t\n\t\tüîß G√©n√©ration du Dataset\n\t\n\n\nG√©n√©ration : Gemini 2.5 Pro (Preview)\nValidation : Contr√¥le qualit√© automatis√©‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sadou/medical-reports-simplification-dataset.","url":"https://huggingface.co/datasets/Sadou/medical-reports-simplification-dataset","creator_name":"Sadou BARRY","creator_url":"https://huggingface.co/Sadou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","machine-generated","multilingual","original","French"],"keywords_longer_than_N":true},
	{"name":"medical-reports-simplification-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüè• Medical Reports Simplification Dataset\n\t\n\n\n\t\n\t\t\n\t\tüìã Description\n\t\n\nDataset cr√©√© avec Gemini 2.5 Pro (Preview) pour entra√Æner des mod√®les √† simplifier les rapports m√©dicaux complexes en explications compr√©hensibles pour les patients.\nüéØ Objectif : D√©mocratiser l'acc√®s √† l'information m√©dicale en rendant les rapports techniques accessibles au grand public.\n\n\t\n\t\t\n\t\tüîß G√©n√©ration du Dataset\n\t\n\n\nG√©n√©ration : Gemini 2.5 Pro (Preview)\nValidation : Contr√¥le qualit√© automatis√©‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sadou/medical-reports-simplification-dataset.","url":"https://huggingface.co/datasets/Sadou/medical-reports-simplification-dataset","creator_name":"Sadou BARRY","creator_url":"https://huggingface.co/Sadou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","machine-generated","multilingual","original","French"],"keywords_longer_than_N":true},
	{"name":"x_dataset_252","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bill199284/x_dataset_252.","url":"https://huggingface.co/datasets/bill199284/x_dataset_252","creator_name":"thomas","creator_url":"https://huggingface.co/bill199284","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"News_Hinglish_English","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a collection of text conversations in Hinglish (code mixing between Hindi-English) and their corresponding English versions. Can be used for Translating between the two.\nThis dataset was generated by translating the first 5000 news content from the Inshorts Dataset - English News [https://www.kaggle.com/datasets/shivamtaneja2304/inshorts-dataset-english]\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\n\nHinglish\nEnglish\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nAn example from the json file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suyash2739/News_Hinglish_English.","url":"https://huggingface.co/datasets/suyash2739/News_Hinglish_English","creator_name":"suyash agarwal","creator_url":"https://huggingface.co/suyash2739","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","machine-generated","crowdsourced","multilingual","translation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_682","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/x_dataset_682.","url":"https://huggingface.co/datasets/StormKing99/x_dataset_682","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_464099","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_464099.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_464099","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_18","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/roknedin/reddit_dataset_18.","url":"https://huggingface.co/datasets/roknedin/reddit_dataset_18","creator_name":"Mohammad Roknedin","creator_url":"https://huggingface.co/roknedin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"cdnpdf-presentations-part1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for cdnpdf Educational Materials (Part 1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 101,022 educational presentations from the cdnpdf.com platform, which provides free access to books, documents, magazines and presentations. This collection focuses exclusively on presentations and includes archives with IDs from 00 to 24. The dataset includes information such as presentation titles, descriptions, URLs, download URLs, and file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/cdnpdf-presentations-part1.","url":"https://huggingface.co/datasets/nyuuzyou/cdnpdf-presentations-part1","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"cdnpdf-presentations-part1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for cdnpdf Educational Materials (Part 1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 101,022 educational presentations from the cdnpdf.com platform, which provides free access to books, documents, magazines and presentations. This collection focuses exclusively on presentations and includes archives with IDs from 00 to 24. The dataset includes information such as presentation titles, descriptions, URLs, download URLs, and file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/cdnpdf-presentations-part1.","url":"https://huggingface.co/datasets/nyuuzyou/cdnpdf-presentations-part1","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"pptonline","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for PPT Online\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata about 1,418,349 PowerPoint (.ppt) files hosted on the ppt-online.org platform. PPT Online is a service designed to display PowerPoint presentations. The dataset includes information such as presentation titles, categories, download links, file sizes, and content snippets. The majority of the presentations are in Russian, Ukrainian, Belarusian, Kazakh, and English, but other languages are also‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/pptonline.","url":"https://huggingface.co/datasets/nyuuzyou/pptonline","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"pptonline","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for PPT Online\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata about 1,418,349 PowerPoint (.ppt) files hosted on the ppt-online.org platform. PPT Online is a service designed to display PowerPoint presentations. The dataset includes information such as presentation titles, categories, download links, file sizes, and content snippets. The majority of the presentations are in Russian, Ukrainian, Belarusian, Kazakh, and English, but other languages are also‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/pptonline.","url":"https://huggingface.co/datasets/nyuuzyou/pptonline","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_154","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sm4rtdev/reddit_dataset_154.","url":"https://huggingface.co/datasets/sm4rtdev/reddit_dataset_154","creator_name":"ElonScott","creator_url":"https://huggingface.co/sm4rtdev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_13","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_13.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_13","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Data-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-Data-ru\n\t\n\nTranslated lmms-lab/LLaVA-OneVision-Data dataset into Russian language using Google translate.\n\nAlmost all datasets have been translated, except for the following:\n[\"tallyqa(cauldron,llava_format)\", \"clevr(cauldron,llava_format)\", \"VisualWebInstruct(filtered)\", \"figureqa(cauldron,llava_format)\", \"magpie_pro(l3_80b_mt)\", \"magpie_pro(qwen2_72b_st)\", \"rendered_text(cauldron)\", \"ureader_ie\"]\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru.","url":"https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-to-text","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"x_dataset_12552","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_12552.","url":"https://huggingface.co/datasets/icedwind/x_dataset_12552","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"nigerian_common_voice_dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Nigerian Common Voice Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Nigerian Common Voice Dataset is a comprehensive dataset consisting of 158 hours of audio recordings and corresponding transcription (sentence). \nThis dataset includes metadata like accent, locale that can help improve the accuracy of speech recognition engines. This dataset is specifically curated to address the gap in speech and language \ndatasets for African accents, making it a valuable resource for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/benjaminogbonna/nigerian_common_voice_dataset.","url":"https://huggingface.co/datasets/benjaminogbonna/nigerian_common_voice_dataset","creator_name":"Benjamin Ogbonna","creator_url":"https://huggingface.co/benjaminogbonna","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"x_dataset_12949","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_12949.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_12949","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"whisper-eval-rare-languages-csv","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWhisper 3 Large Evaluation on Mozilla Common Voice 17 Rare Languages (Enhanced Metrics)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis enhanced dataset contains comprehensive evaluation results of OpenAI's Whisper 3 Large model on rare languages from Mozilla Common Voice 17, with extensive additional metrics for thorough ASR evaluation.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\nEnhanced Error Metrics:\n\nWER (Word Error Rate): Standard word-level error measurement\nCER (Character Error Rate): Character-level error‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/norbertm/whisper-eval-rare-languages-csv.","url":"https://huggingface.co/datasets/norbertm/whisper-eval-rare-languages-csv","creator_name":"Norbert M","creator_url":"https://huggingface.co/norbertm","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","multilingual","Assamese","Breton","Welsh"],"keywords_longer_than_N":true},
	{"name":"x_dataset_9","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_9.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_9","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"br_fr_en_translation","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nIl s'agit du jeu de donn√©es cvqa_br_fr_en o√π seuls les textes align√©s br/fr/en sont gard√©s afin de constituer un jeu de donn√©es de traduction automatique qui soit plus l√©ger √† t√©l√©charger.405 textes de types \"questions\" et 405 textes de type \"options\" sont disponibles.\n","url":"https://huggingface.co/datasets/Bretagne/br_fr_en_translation","creator_name":"Bretagne","creator_url":"https://huggingface.co/Bretagne","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","Breton","French","English"],"keywords_longer_than_N":true},
	{"name":"x_dataset_250","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_250.","url":"https://huggingface.co/datasets/James096/x_dataset_250","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_37","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_37.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_37","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_8","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_8.","url":"https://huggingface.co/datasets/suul999922/x_dataset_8","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_218","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_218.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_218","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_218","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_218.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_218","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_8","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_8.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_8","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_193","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RentonWEB3/x_dataset_193.","url":"https://huggingface.co/datasets/RentonWEB3/x_dataset_193","creator_name":"Renton Mark","creator_url":"https://huggingface.co/RentonWEB3","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_59332","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_59332.","url":"https://huggingface.co/datasets/momo1942/x_dataset_59332","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"roman-urdu-alpaca-qa-mix","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Roman Urdu + Alpaca QA Mix\n\t\n\nThis dataset is intended to support fine-tuning and evaluation of language models that understand and respond to Roman Urdu and English instructions. It consists of 1,022 records in total:\n\n500 examples in Roman Urdu generated from high-quality Urdu sources and transliterated using the ChatGPT API.\n500 examples in English randomly sampled from the Stanford Alpaca dataset.\n\nThe dataset follows the same format as Alpaca-style instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Redgerd/roman-urdu-alpaca-qa-mix.","url":"https://huggingface.co/datasets/Redgerd/roman-urdu-alpaca-qa-mix","creator_name":"Muhammad Salaar","creator_url":"https://huggingface.co/Redgerd","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","Urdu","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_198","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wenknow/reddit_dataset_198.","url":"https://huggingface.co/datasets/wenknow/reddit_dataset_198","creator_name":"Dt","creator_url":"https://huggingface.co/wenknow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_62085","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rainbowbridge/x_dataset_62085.","url":"https://huggingface.co/datasets/rainbowbridge/x_dataset_62085","creator_name":"Rain","creator_url":"https://huggingface.co/rainbowbridge","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0605250","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_0605250.","url":"https://huggingface.co/datasets/john-1111/x_dataset_0605250","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_28","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gsjcm/x_dataset_28.","url":"https://huggingface.co/datasets/gsjcm/x_dataset_28","creator_name":"gsjcmurn","creator_url":"https://huggingface.co/gsjcm","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"websim","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Websim.ai User Projects\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about 137,452 user projects from Websim.ai, a service for creating small sites from a description using Large Language Models (LLMs). The data is stored in JSONL format and includes details about each project, such as project metadata, user information, and the generated HTML content.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in English, as it contains project descriptions and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/websim.","url":"https://huggingface.co/datasets/nyuuzyou/websim","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Sofie","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Sofie","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"edutexts","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Russian Educational Text Collection\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains approximately 1.38M educational texts primarily in Russian with some content in Ukrainian and English. The content is extracted from presentations and documents, including educational presentations, essays, and various academic documents covering diverse topics from natural sciences to literature.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nRussian (ru) - primary language\nUkrainian (uk) - secondary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/edutexts.","url":"https://huggingface.co/datasets/nyuuzyou/edutexts","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","topic-classification","multiple-choice-qa","found"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_187","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vouu/reddit_dataset_187.","url":"https://huggingface.co/datasets/vouu/reddit_dataset_187","creator_name":"Pham Manh Truong","creator_url":"https://huggingface.co/vouu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"COMET_score","keyword":"multilingual","description":"Tadesse/COMET_score dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Tadesse/COMET_score","creator_name":"Tadesse Destaw Belay","creator_url":"https://huggingface.co/Tadesse","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","English","Amharic","Ewe","French"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_36","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/reddit_dataset_36.","url":"https://huggingface.co/datasets/arrmlet/reddit_dataset_36","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_102","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GSKCM24/x_dataset_102.","url":"https://huggingface.co/datasets/GSKCM24/x_dataset_102","creator_name":"GUNEET SINGH KHURANA","creator_url":"https://huggingface.co/GSKCM24","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_18","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/roknedin/x_dataset_18.","url":"https://huggingface.co/datasets/roknedin/x_dataset_18","creator_name":"Mohammad Roknedin","creator_url":"https://huggingface.co/roknedin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0109104","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_0109104.","url":"https://huggingface.co/datasets/william-1111/x_dataset_0109104","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44100","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_44100.","url":"https://huggingface.co/datasets/icedwind/x_dataset_44100","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_8","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_8.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_8","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_132","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_132.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_132","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_2983","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_2983.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_2983","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_13","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_13.","url":"https://huggingface.co/datasets/suul999922/x_dataset_13","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"TRECCOVID-NL","keyword":"translated","description":"\n  TRECCOVID-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nTRECCOVID is an ad-hoc search challenge based on the COVID-19 dataset containing scientific articles related to the COVID-19 pandemic. TRECCOVID-NL is a Dutch translation. \n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical, Academic, Written\n\n\nReference\nhttps://colab.research.google.com/drive/1R99rjeAGt8S9IfAIRR3wS052sNu3Bjo-#scrollTo=4HduGW6xHnrZ\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TRECCOVID-NL.","url":"https://huggingface.co/datasets/mteb/TRECCOVID-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/trec-covid","Dutch"],"keywords_longer_than_N":true},
	{"name":"x_dataset_37","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_37.","url":"https://huggingface.co/datasets/gk4u/x_dataset_37","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_27","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_27.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_27","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_225","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Den4ikkk/reddit_dataset_225.","url":"https://huggingface.co/datasets/Den4ikkk/reddit_dataset_225","creator_name":"Staff","creator_url":"https://huggingface.co/Den4ikkk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_34","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zengsdfew/x_dataset_34.","url":"https://huggingface.co/datasets/zengsdfew/x_dataset_34","creator_name":"miner3","creator_url":"https://huggingface.co/zengsdfew","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"PolyFiQA-Easy","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for PolyFiQA-Easy\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPolyFiQA-Easy is a multilingual financial question-answering dataset designed to evaluate financial reasoning in a simplified setting. Each instance consists of a task identifier, a query prompt, an associated financial question, and the correct answer. The Easy split focuses on queries that can be answered with minimal document retrieval, making it ideal for low-latency or resource-constrained systems.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TheFinAI/PolyFiQA-Easy.","url":"https://huggingface.co/datasets/TheFinAI/PolyFiQA-Easy","creator_name":"The Fin AI","creator_url":"https://huggingface.co/TheFinAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","Chinese","jp","Spanish"],"keywords_longer_than_N":true},
	{"name":"vdr-multilingual-test","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Visual Document Retrieval Benchmarks\n\t\n\n\nThis dataset consists of 15 different benchmarks used to initially evaluate the vdr-2b-multi-v1 multimodal retrieval embedding model. These benchmarks allow the testing of multilingual, multimodal retrieval capabilities on text-only, visual-only and mixed page screenshots.\nEach language subset contains queries and images in that language and is divided into three different categories by the \"pagetype\" column. Each category contains‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llamaindex/vdr-multilingual-test.","url":"https://huggingface.co/datasets/llamaindex/vdr-multilingual-test","creator_name":"LlamaIndex","creator_url":"https://huggingface.co/llamaindex","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","German","Italian","French","Spanish"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_44.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_44","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_211","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chidinna/reddit_dataset_211.","url":"https://huggingface.co/datasets/chidinna/reddit_dataset_211","creator_name":"chidinn","creator_url":"https://huggingface.co/chidinna","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"cdnpdf-presentations-part2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for cdnpdf Educational Materials (Part 2)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 100,978 educational presentations from the cdnpdf.com platform, which provides free access to books, documents, magazines and presentations. This collection focuses exclusively on presentations and includes archives with IDs from 25 to 49. The dataset includes information such as presentation titles, descriptions, URLs, download URLs, and file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/cdnpdf-presentations-part2.","url":"https://huggingface.co/datasets/nyuuzyou/cdnpdf-presentations-part2","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"cdnpdf-presentations-part2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for cdnpdf Educational Materials (Part 2)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 100,978 educational presentations from the cdnpdf.com platform, which provides free access to books, documents, magazines and presentations. This collection focuses exclusively on presentations and includes archives with IDs from 25 to 49. The dataset includes information such as presentation titles, descriptions, URLs, download URLs, and file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/cdnpdf-presentations-part2.","url":"https://huggingface.co/datasets/nyuuzyou/cdnpdf-presentations-part2","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"urokosvitaua","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Urok.Osvita.ua Educational Materials\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 32,290 educational materials from the urok.osvita.ua platform, a service for Ukrainian educators to share and publish their teaching materials. The dataset includes information such as material titles, descriptions, categories, grade levels, and download links for the original files.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual, with Ukrainian‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/urokosvitaua.","url":"https://huggingface.co/datasets/nyuuzyou/urokosvitaua","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"urokosvitaua","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Urok.Osvita.ua Educational Materials\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 32,290 educational materials from the urok.osvita.ua platform, a service for Ukrainian educators to share and publish their teaching materials. The dataset includes information such as material titles, descriptions, categories, grade levels, and download links for the original files.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual, with Ukrainian‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/urokosvitaua.","url":"https://huggingface.co/datasets/nyuuzyou/urokosvitaua","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"x_dataset_070439","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_070439.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_070439","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"IndicQARetrieval","keyword":"translated","description":"\n  IndicQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIndicQA is a manually curated cloze-style reading comprehension dataset that can be used for evaluating question-answering models in 11 Indic languages. It is repurposed retrieving relevant context for each question.\n\n\t\n\t\t\n\n\n\n\n\t\tTask category\nt2t\n\n\nDomains\nWeb, Written\n\n\nReference\nhttps://arxiv.org/abs/2212.05409\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicQARetrieval.","url":"https://huggingface.co/datasets/mteb/IndicQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","human-annotated","translated","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"mala-monolingual-integration","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMaLA Corpus: Massive Language Adaptation Corpus\n\t\n\nThis is the noisy version that integrates texts from different sources.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MaLA Corpus (Massive Language Adaptation) is a comprehensive, multilingual dataset designed to support the continual pre-training of large language models. It covers 939 languages and consists of over 74 billion tokens, making it one of the largest datasets of its kind. With a focus on improving the representation of low-resource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MaLA-LM/mala-monolingual-integration.","url":"https://huggingface.co/datasets/MaLA-LM/mala-monolingual-integration","creator_name":"MaLA-LM","creator_url":"https://huggingface.co/MaLA-LM","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","odc-by","1B - 10B","json","Text"],"keywords_longer_than_N":true},
	{"name":"russian_prompt_injections","keyword":"multilingual","description":"üìÑ Dataset Description\nThis dataset comprises examples of direct prompt injection attacks in Russian, curated to evaluate the robustness of instruction-following language models (LLMs). Each entry includes a Russian prompt, its English translation, the type of injection technique employed, and the source of the prompt.\nüìÇ Dataset Structure\nThe dataset is provided in JSON format with the following fields:\nprompt_ru: The original Russian prompt intended for testing LLMs.\nprompt_en: The English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dmtrdr/russian_prompt_injections.","url":"https://huggingface.co/datasets/dmtrdr/russian_prompt_injections","creator_name":"Dmitry Druchinin","creator_url":"https://huggingface.co/dmtrdr","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","Russian","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"x_dataset_14","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_14.","url":"https://huggingface.co/datasets/suul999922/x_dataset_14","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"kzgov-budget-data","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKazakhstan Government Budget Data üá∞üáø\n\t\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis comprehensive dataset provides detailed insights into Kazakhstan's government budget allocation, execution, and performance across various sectors, regions, and administrative levels for 2024. The dataset enables analysis of fiscal policy, budget efficiency, and resource distribution across the country.\n\n\t\n\t\t\n\t\tüìä Dataset Statistics\n\t\n\n\nTotal Records: 615 entries\nCoverage Period: 2024\nAdministrative Levels:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Adilbai/kzgov-budget-data.","url":"https://huggingface.co/datasets/Adilbai/kzgov-budget-data","creator_name":"Baidalin Adilzhan","creator_url":"https://huggingface.co/Adilbai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["table-to-text","question-answering","text-retrieval","open-domain-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_General_Translation","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_General_Translation is a dataset of BenchMAX, which evaluates the translation capability on the general domain.\nWe collect parallel test data from Flore-200, TED-talk, and WMT24.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nRun the following commands to generate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_General_Translation.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_General_Translation","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Math","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Math is a dataset of BenchMAX, sourcing from MGSM, which evaluates the math reasoning capability in multilingual scenarios.\nWe extend the original MGSM dataset by six additional languages, i.e. Arabic, Czech, Hungarian, Korean, Serbian, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Math.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Math","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"luhya-multilingual-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tLuhya Multilingual Translation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 26,205 translation pairs for Luhya dialects, English, and Swahili. It was created to support machine translation research for Luhya, a Bantu language family spoken in Kenya.\n\n\t\n\t\t\n\t\tLanguages and Dialects\n\t\n\nLanguages:\n\nen\nluy\nsw\n\nLuhya Dialects:\n\nBukusu\nLuwanga\nMarachi\nMaragoli\nTsotso\n\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\n\t\n\t\t\nSplit\nExamples\n\n\n\t\t\nTrain\n20,963\n\n\nValidation\n2,621\n\n\nTest\n2,621‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mamakobe/luhya-multilingual-dataset.","url":"https://huggingface.co/datasets/mamakobe/luhya-multilingual-dataset","creator_name":"Moody Amakobe","creator_url":"https://huggingface.co/mamakobe","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","original","English","Swahili"],"keywords_longer_than_N":true},
	{"name":"Health-Bench-Eval-OSS-2025-07","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for HealthBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHealthBench is a benchmark dataset developed by OpenAI in collaboration with 262 physicians from 60 countries to evaluate AI systems in health-related conversational scenarios. It contains 5,000 multi-turn health conversations in a JSONL file (2025-05-07-06-14-12_oss_eval.jsonl), simulating interactions between AI models and users (laypersons or clinicians). Each conversation includes a user prompt, a candidate model response‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tonic/Health-Bench-Eval-OSS-2025-07.","url":"https://huggingface.co/datasets/Tonic/Health-Bench-Eval-OSS-2025-07","creator_name":"Joseph [open/acc] Pollack","creator_url":"https://huggingface.co/Tonic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","multilingual","English","mit"],"keywords_longer_than_N":true},
	{"name":"x_dataset_6","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_6.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_6","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_49","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_49.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_49","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_152","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/synapz/reddit_dataset_152.","url":"https://huggingface.co/datasets/synapz/reddit_dataset_152","creator_name":"Derek Barnes","creator_url":"https://huggingface.co/synapz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_192","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Crystal1101/reddit_dataset_192.","url":"https://huggingface.co/datasets/Crystal1101/reddit_dataset_192","creator_name":"Butterfly","creator_url":"https://huggingface.co/Crystal1101","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_13","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_13.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_13","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_107","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lightbeam888/reddit_dataset_107.","url":"https://huggingface.co/datasets/lightbeam888/reddit_dataset_107","creator_name":"Ryan Orino","creator_url":"https://huggingface.co/lightbeam888","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"SingaporeAirline_vision_dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tSINGAPORE-AIRLINES-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from Singapore Airlines official documentation. It is designed to train and evaluate information retrieval models and improve AI understanding of commercial aviation operational documentation.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, an engineering student specializing in Computer Science, Big Data, and AI, currently working as an apprentice at TW3 Partners, a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/SingaporeAirline_vision_dataset.","url":"https://huggingface.co/datasets/Davidsv/SingaporeAirline_vision_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_247","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zevebe/reddit_dataset_247.","url":"https://huggingface.co/datasets/zevebe/reddit_dataset_247","creator_name":"Andrea","creator_url":"https://huggingface.co/zevebe","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"visual-qa-llama-format","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOpen Paws Visual Qa Llama Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Multimodal Data\nFormat: JSONL (JSON Lines)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Open Paws\nLicense: Apache 2.0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/visual-qa-llama-format.","url":"https://huggingface.co/datasets/open-paws/visual-qa-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0510248","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_0510248.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_0510248","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"cukurova_university_chatbot","keyword":"multilingual","description":"\n\t\n\t\t\n\t\t√áukurova University Computer Engineering Chatbot Dataset\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tüìä Dataset Overview\n\t\n\nThis dataset contains 22,524 high-quality question-answer pairs specifically designed for training an AI chatbot that serves the Computer Engineering Department at √áukurova University. The dataset is part of the CengBot project, a sophisticated multilingual Telegram chatbot that provides automated assistance to students regarding courses, programs, and departmental information.\n\n\t\n\t\t\n\t\n\t\n\t\tüî¢‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Naholav/cukurova_university_chatbot.","url":"https://huggingface.co/datasets/Naholav/cukurova_university_chatbot","creator_name":"Arda M√ºlayim","creator_url":"https://huggingface.co/Naholav","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","open-domain-qa","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"cukurova_university_chatbot","keyword":"multilingual","description":"\n\t\n\t\t\n\t\t√áukurova University Computer Engineering Chatbot Dataset\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tüìä Dataset Overview\n\t\n\nThis dataset contains 22,524 high-quality question-answer pairs specifically designed for training an AI chatbot that serves the Computer Engineering Department at √áukurova University. The dataset is part of the CengBot project, a sophisticated multilingual Telegram chatbot that provides automated assistance to students regarding courses, programs, and departmental information.\n\n\t\n\t\t\n\t\n\t\n\t\tüî¢‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Naholav/cukurova_university_chatbot.","url":"https://huggingface.co/datasets/Naholav/cukurova_university_chatbot","creator_name":"Arda M√ºlayim","creator_url":"https://huggingface.co/Naholav","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","open-domain-qa","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"MANGO","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMANGO: A Corpus of Human Ratings for Speech\n\t\n\nMANGO (MUSHRA Assessment corpus using Native listeners and Guidelines to understand human Opinions at scale) is the first large-scale dataset designed for evaluating Text-to-Speech (TTS) systems in Indian languages. \n\n\t\n\t\t\n\t\tKey Features:\n\t\n\n\n255,150 human ratings of TTS-generated outputs and ground-truth human speech.\nCovers two major Indian languages: Hindi & Tamil, and English.\nBased on the MUSHRA (Multiple Stimuli with Hidden Reference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/MANGO.","url":"https://huggingface.co/datasets/ai4bharat/MANGO","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-speech","crowd-sourced","Hindi","Tamil","English"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_236","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bersov75/reddit_dataset_236.","url":"https://huggingface.co/datasets/bersov75/reddit_dataset_236","creator_name":"Bersov Bersov","creator_url":"https://huggingface.co/bersov75","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_46092","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rainbowbridge/x_dataset_46092.","url":"https://huggingface.co/datasets/rainbowbridge/x_dataset_46092","creator_name":"Rain","creator_url":"https://huggingface.co/rainbowbridge","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"multilingual","description":"Ehsanl/test dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Ehsanl/test","creator_name":"Ehsan","creator_url":"https://huggingface.co/Ehsanl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","text-classification","document-retrieval","topic-classification","expert-generated"],"keywords_longer_than_N":true},
	{"name":"x_dataset_24589","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_24589.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_24589","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"IndicMSMARCO","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüîç IndicMSMARCO: Multilingual Information Retrieval Benchmark\n\t\n\nA comprehensive multilingual variant of MS MARCO specifically tailored for Indian languages, featuring carefully selected queries and corresponding passages with high-quality translations.\n\n\t\n\t\t\n\t\tüöÄ Quick Start - Load Individual Languages\n\t\n\nfrom datasets import load_dataset\n\n# Load ONLY Hindi data (fast and efficient!)\nhindi_data = load_dataset(\"ai4bharat/IndicMSMARCO\", \"hi\")\nprint(f\"Hindi queries:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/IndicMSMARCO.","url":"https://huggingface.co/datasets/ai4bharat/IndicMSMARCO","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","question-answering","multilingual","ms_marco","Assamese"],"keywords_longer_than_N":true},
	{"name":"IndicMSMARCO","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüîç IndicMSMARCO: Multilingual Information Retrieval Benchmark\n\t\n\nA comprehensive multilingual variant of MS MARCO specifically tailored for Indian languages, featuring carefully selected queries and corresponding passages with high-quality translations.\n\n\t\n\t\t\n\t\tüöÄ Quick Start - Load Individual Languages\n\t\n\nfrom datasets import load_dataset\n\n# Load ONLY Hindi data (fast and efficient!)\nhindi_data = load_dataset(\"ai4bharat/IndicMSMARCO\", \"hi\")\nprint(f\"Hindi queries:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/IndicMSMARCO.","url":"https://huggingface.co/datasets/ai4bharat/IndicMSMARCO","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","question-answering","multilingual","ms_marco","Assamese"],"keywords_longer_than_N":true},
	{"name":"mmlux","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCitation Information\n\t\n\nIf you find benchmarks useful in your research, please consider citing the test and also the MMLU dataset it draws from:\n    @misc{thellmann2024crosslingual,\n    title={Towards Cross-Lingual LLM Evaluation for European Languages},\n    author={Klaudia Thellmann and Bernhard Stadler and Michael Fromm and Jasper Schulze Buschhoff and Alex Jude and Fabio Barth and Johannes Leveling and Nicolas Flores-Herr and Joachim K√∂hler and Ren√© J√§kel and Mehdi Ali}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Eurolingua/mmlux.","url":"https://huggingface.co/datasets/Eurolingua/mmlux","creator_name":"EuroLingua-GPT","creator_url":"https://huggingface.co/Eurolingua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","expert-generated","multilingual","cais/mmlu","German"],"keywords_longer_than_N":true},
	{"name":"x_dataset_3","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_3.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_3","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_232","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wenknow/reddit_dataset_232.","url":"https://huggingface.co/datasets/wenknow/reddit_dataset_232","creator_name":"Dt","creator_url":"https://huggingface.co/wenknow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_551805","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_551805.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_551805","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_050348","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_050348.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_050348","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_225","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AISOMA-Bittensor/x_dataset_225.","url":"https://huggingface.co/datasets/AISOMA-Bittensor/x_dataset_225","creator_name":"Murat Durmus","creator_url":"https://huggingface.co/AISOMA-Bittensor","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_151","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/williamlewis0620/reddit_dataset_151.","url":"https://huggingface.co/datasets/williamlewis0620/reddit_dataset_151","creator_name":"William Lewis","creator_url":"https://huggingface.co/williamlewis0620","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"IN22-Conv","keyword":"multilingual","description":"\n  IN22ConvBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Conv is a n-way parallel conversation domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nSocial, Spoken, Fiction, Spoken\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Conv\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22-Conv.","url":"https://huggingface.co/datasets/mteb/IN22-Conv","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","expert-generated","multilingual","Assamese"],"keywords_longer_than_N":true},
	{"name":"x_dataset_18","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_18.","url":"https://huggingface.co/datasets/suul999922/x_dataset_18","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"XTD-10","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tXTD Multimodal Multilingual Data With Instruction\n\t\n\nThis dataset contains datasets (with English instruction) used for evaluating the multilingual capability of a multimodal embedding model, including seven languages:\n\nit, es, ru, zh, pl, tr, ko\n\n\n\t\n\t\t\n\t\tDataset Usage\n\t\n\n\nThe instruction on the query side is: \"Retrieve an image of this caption.\"\nThe instruction on the document side is: \"Represent the given image.\"\nEach example contains a query and a set of targets. The first one in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Haon-Chen/XTD-10.","url":"https://huggingface.co/datasets/Haon-Chen/XTD-10","creator_name":"Haonan Chen","creator_url":"https://huggingface.co/Haon-Chen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"NeuCLIR2022RetrievalHardNegatives","keyword":"multilingual","description":"\n  NeuCLIR2022RetrievalHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\nSource datasets:\n\nmteb/neuclir-2022\nmteb/neuclir-2022-hard-negatives‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NeuCLIR2022RetrievalHardNegatives.","url":"https://huggingface.co/datasets/mteb/NeuCLIR2022RetrievalHardNegatives","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2022","mteb/neuclir-2022-hard-negatives"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0203106","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_0203106.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_0203106","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0401151","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_0401151.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_0401151","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_3","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_3.","url":"https://huggingface.co/datasets/suul999922/x_dataset_3","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"multilingual-text","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Text Dataset\n\t\n\nThis dataset contains a curated selection of rows from multiple input datasets, where each row includes a text chunk of approximately 2000 tokens (as measured by Llama 3.1 tokenizer) verified to be written in the correct language. Only rows with properly classified language chunks are retained, ensuring high-quality multilingual data for analysis or model training.\n\n\t\n\t\t\n\t\tPreprocessing Steps\n\t\n\n\nNormalized whitespace, punctuation, Unicode characters, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/multilingual-text.","url":"https://huggingface.co/datasets/agentlans/multilingual-text","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","Amharic","Arabic","Bengali"],"keywords_longer_than_N":true},
	{"name":"CommonForms","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCommonForms: A Large, Diverse Dataset for Form Field Detection\n\t\n\nThis repository hosts the CommonForms dataset, a web-scale dataset for form field detection, introduced in the paper CommonForms: A Large, Diverse Dataset for Form Field Detection.\nCommonForms casts the problem of form field detection as object detection: given an image of a page, predict the location and type (Text Input, Choice Button, Signature) of form fields.\nKey Features:\n\nScale: Roughly 55,000 documents comprising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jbarrow/CommonForms.","url":"https://huggingface.co/datasets/jbarrow/CommonForms","creator_name":"Joe Barrow","creator_url":"https://huggingface.co/jbarrow","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["object-detection","apache-2.0","Document","arxiv:2509.16506","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"v4_nuclear_power_articles","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Nuclear News V4 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Nuclear News V4 Dataset is a multilingual dataset consisting of 33,104 unique news articles sourced from 12 online news platforms across the Visegr√°d Group (V4) countries ‚Äî Poland, Czech Republic, Slovakia, and Hungary ‚Äî published between 1998 and 2025.\nThe goal of the dataset is to analyze media narratives surrounding nuclear energy in Central Europe.\nWhile the dataset does not contain human-annotated (golden)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eoplumbum/v4_nuclear_power_articles.","url":"https://huggingface.co/datasets/eoplumbum/v4_nuclear_power_articles","creator_name":"Edyta","creator_url":"https://huggingface.co/eoplumbum","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","summarization","Polish","Hungarian","Slovak"],"keywords_longer_than_N":true},
	{"name":"x_dataset_250","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aaron927dev/x_dataset_250.","url":"https://huggingface.co/datasets/aaron927dev/x_dataset_250","creator_name":"William Hudson","creator_url":"https://huggingface.co/aaron927dev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"bigslide","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Bigslide.ru Presentations\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 50,872 presentations from the bigslide.ru platform, a presentation storage and viewing service for school students. The dataset includes information such as presentation titles, URLs, download URLs, and extracted text content where available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual, with Russian being the primary language. Other languages present‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/bigslide.","url":"https://huggingface.co/datasets/nyuuzyou/bigslide","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"bigslide","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Bigslide.ru Presentations\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 50,872 presentations from the bigslide.ru platform, a presentation storage and viewing service for school students. The dataset includes information such as presentation titles, URLs, download URLs, and extracted text content where available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual, with Russian being the primary language. Other languages present‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/bigslide.","url":"https://huggingface.co/datasets/nyuuzyou/bigslide","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"suno","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Suno.ai Music Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 659,788 songs generated by artificial intelligence on the suno.com platform, a service that generates music using artificial intelligence. The songs were discovered by search queries with words from the dwyl/english-words word list.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual with English as the primary language:\n\nEnglish (en): Primary language for metadata and most lyrics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/suno.","url":"https://huggingface.co/datasets/nyuuzyou/suno","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["audio-classification","text-to-audio","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"suno","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Suno.ai Music Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 659,788 songs generated by artificial intelligence on the suno.com platform, a service that generates music using artificial intelligence. The songs were discovered by search queries with words from the dwyl/english-words word list.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual with English as the primary language:\n\nEnglish (en): Primary language for metadata and most lyrics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/suno.","url":"https://huggingface.co/datasets/nyuuzyou/suno","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["audio-classification","text-to-audio","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0309155","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_0309155.","url":"https://huggingface.co/datasets/james-1111/x_dataset_0309155","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"korean-leg","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMulti-Domain Korean Speech Dataset\n\t\n\nThis dataset contains 5 audio recordings with corresponding text transcriptions across multiple languages and domains.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive collection of audio files paired with text transcriptions, featuring both synthetic and natural speech across various domains. Suitable for automatic speech recognition (ASR), text-to-speech (TTS), and domain-specific speech processing tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jsbeaudry/korean-leg.","url":"https://huggingface.co/datasets/jsbeaudry/korean-leg","creator_name":"Jean Sauvenel Beaudry","creator_url":"https://huggingface.co/jsbeaudry","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Korean","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"bordirlines","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBordIRLines Dataset\n\t\n\nThis is the dataset associated with the paper \"BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented Generation\" (link).\nCode: https://github.com/manestay/bordIRlines\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe BordIRLines Dataset is an information retrieval (IR) dataset constructed from various language corpora. It contains queries and corresponding ranked docs along with their relevance scores. The dataset includes multiple languages, including English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/borderlines/bordirlines.","url":"https://huggingface.co/datasets/borderlines/bordirlines","creator_name":"cross-lingual LLMs and RAG","creator_url":"https://huggingface.co/borderlines","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["question-answering","human","machine-generated","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"x_dataset_21","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_21.","url":"https://huggingface.co/datasets/suul999922/x_dataset_21","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_39138","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_39138.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_39138","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_9","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_9.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_9","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_154","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sm4rtdev/x_dataset_154.","url":"https://huggingface.co/datasets/sm4rtdev/x_dataset_154","creator_name":"ElonScott","creator_url":"https://huggingface.co/sm4rtdev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_71","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/reddit_dataset_71.","url":"https://huggingface.co/datasets/suul999922/reddit_dataset_71","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"afri-aya","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tAfri-Aya üåç\n\t\n\nGiving Sight to African LLMs\nAfri-Aya is a community-curated multilingual image dataset covering 13 major African languages with AI-powered categorization, created as part of Expedition Aya - a six-week global open-build challenge hosted by Cohere Labs.\n\n\t\n\t\t\n\t\tProject Background\n\t\n\nThis dataset was developed by the Cohere Labs Regional Africa community during Expedition Aya, aiming to include more African low-resource languages and their cultures in Vision Language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabsCommunity/afri-aya.","url":"https://huggingface.co/datasets/CohereLabsCommunity/afri-aya","creator_name":"Cohere Labs Community","creator_url":"https://huggingface.co/CohereLabsCommunity","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","text-generation","English","Ganda"],"keywords_longer_than_N":true},
	{"name":"dark_thoughts_case_study_reason","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tDark Thoughts Ê°à‰æãÁ†îÁ©∂Êï∞ÊçÆÈõÜ - Êé®ÁêÜ\n\t\n\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜÊèèËø∞\n\t\n\n\n\t\n\t\t\n\t\tÊ¶ÇËø∞\n\t\n\nDark Thoughts Ê°à‰æãÁ†îÁ©∂Êï∞ÊçÆÈõÜ - Êé®ÁêÜÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ§öËØ≠Ë®ÄÂïÜ‰∏öÊ°à‰æãÁ†îÁ©∂ÂèäÁõ∏ÂÖ≥Êé®ÁêÜÂõûÂ§çÈõÜÂêà„ÄÇËØ•Êï∞ÊçÆÈõÜÈÄöËøáÂÖàËøõÁöÑËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜ Cablegate ÁîµÊä•ÔºåÁîüÊàê‰∏≠Ëã±ÊñáÂïÜ‰∏öÊ°à‰æãÁ†îÁ©∂ÔºåÂπ∂Ëøõ‰∏ÄÊ≠•‰∏∞ÂØå‰∫ÜÂà©ÁõäÁõ∏ÂÖ≥ËÄÖÁâπÂÆöÁöÑÊé®ÁêÜËßÜËßí„ÄÇÂØπ‰∫éÂØπÂïÜ‰∏öÂàÜÊûê„ÄÅÂ§öËØ≠Ë®ÄÂÜÖÂÆπÁîüÊàêÂíåÊé®ÁêÜËÉΩÂäõÊÑüÂÖ¥Ë∂£ÁöÑÁ†îÁ©∂‰∫∫ÂëòÂíå‰ªé‰∏ö‰∫∫ÂëòÊù•ËØ¥ÔºåËØ•Êï∞ÊçÆÈõÜÊòØÂÆùË¥µÁöÑËµÑÊ∫ê„ÄÇ\n\n\t\n\t\t\n\t\tÊîØÊåÅÁöÑ‰ªªÂä°\n\t\n\nËØ•Êï∞ÊçÆÈõÜÊîØÊåÅ‰ª•‰∏ã‰ªªÂä°Ôºö\n\nÊñáÊú¨ÁîüÊàê\nËØ≠Ë®ÄÂª∫Ê®°\nÊé®ÁêÜ‰∏éÂàÜÊûê\nÂèåËØ≠Ê°à‰æãÁ†îÁ©∂ÁîüÊàê\nË∑®ËØ≠Ë®ÄÂÜÖÂÆπÂàÜÊûê\nÂïÜ‰∏öÊàòÁï•Âà∂ÂÆö\nÂà©ÁõäÁõ∏ÂÖ≥ËÄÖËßÜËßíÂª∫Ê®°\n\n\n\t\n\t\t\n\t\tËØ≠Ë®Ä\n\t\n\nËØ•Êï∞ÊçÆÈõÜ‰∏∫ÂèåËØ≠Êï∞ÊçÆÈõÜÔºö\n\nËã±ËØ≠ (en)\n‰∏≠Êñá (zh)\n\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜÁªìÊûÑ\n\t\n\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÂ≠óÊÆµ\n\t\n\n{\n'id': 'string', # Êù°ÁõÆÁöÑÂîØ‰∏ÄÊ†áËØÜÁ¨¶\n'think': 'string', # ÊÄùËÄÉËøáÁ®ã\n'response': 'string', # ÁîüÊàêÁöÑÊé®ÁêÜÂìçÂ∫î\n'query': 'string', #‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DataTonic/dark_thoughts_case_study_reason.","url":"https://huggingface.co/datasets/DataTonic/dark_thoughts_case_study_reason","creator_name":"Data Tonic (Alignment Lab)","creator_url":"https://huggingface.co/DataTonic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"css10-ljspeech","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCSS10-LJSpeech\n\t\n\nCSS10-LJSpeech „ÅØ„ÄÅPark et al. „ÅåÂÖ¨Èñã„Åó„Åü CSS10 „Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„ÄÅLJSpeech‰∫íÊèõ„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´Â§âÊèõ„Åó„Åü10Ë®ÄË™û„ÅÆÈü≥Â£∞ÂêàÊàêÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇÂêÑË®ÄË™û„ÅÆÊñáÂ≠¶‰ΩúÂìÅ„ÇíÈü≥Â£∞Âåñ„Åó„ÅüÈ´òÂìÅË≥™„Å™Èü≥Â£∞„Éá„Éº„Çø„ÇíÊèê‰æõ„Åó„ÄÅLJSpeech„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÔºàid|text & wavs/*.wavÔºâ„Å´Áµ±‰∏Ä„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n\t\n\t\t\n\t\t„Éá„Éº„ÇøÊ¶ÇË¶Å\n\t\n\n\n\t\n\t\t\nÈ†ÖÁõÆ\nÂÄ§\n\n\n\t\t\nË©±ËÄÖÊï∞\n10 (Ë®ÄË™ûÂà•)\n\n\nÁ∑èÈü≥Â£∞Êï∞\n64,196\n\n\nÂêàË®àÊôÇÈñì\nÁ¥Ñ 140 ÊôÇÈñì\n\n\n„Çµ„É≥„Éó„É™„É≥„Ç∞„É¨„Éº„Éà\n22,050 Hz\n\n\nÈü≥Â£∞„Éï„Ç©„Éº„Éû„ÉÉ„Éà\nIEEEÊµÆÂãïÂ∞èÊï∞ÁÇπ (32bit)\n\n\n„ÉÜ„Ç≠„Çπ„ÉàË®ÄË™û\n10Ë®ÄË™û\n\n\n„Éï„Ç©„Éº„Éû„ÉÉ„Éà\n`id\n\n\n\t\n\n\n\t\n\t\t\n\t\tË®ÄË™ûÂà•Áµ±Ë®à\n\t\n\n\n\t\n\t\t\nË®ÄË™û\nË®ÄË™û„Ç≥„Éº„Éâ\nÈü≥Â£∞Êï∞\nÂêàË®àÊôÇÈñì\n\n\n\t\t\n„Éâ„Ç§„ÉÑË™û\nde\n7,428\n16.14ÊôÇÈñì\n\n\n„ÇÆ„É™„Ç∑„É£Ë™û\nel\n1,844\n4.14ÊôÇÈñì\n\n\n„Çπ„Éö„Ç§„É≥Ë™û\nes\n11,016\n19.15ÊôÇÈñì\n\n\n„Éï„Ç£„É≥„É©„É≥„ÉâË™û\nfi\n4,842\n10.53ÊôÇÈñì‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ayousanz/css10-ljspeech.","url":"https://huggingface.co/datasets/ayousanz/css10-ljspeech","creator_name":"yousan","creator_url":"https://huggingface.co/ayousanz","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["German","Greek","Spanish","Finnish","French"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_295492","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_295492.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_295492","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_010613","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_010613.","url":"https://huggingface.co/datasets/william-1111/x_dataset_010613","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"cyrillic-language-classification","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCyrillic Language Classification Corpus\n\t\n\nThis corpus contains texts in 26 languages using the Cyrillic alphabet, designed for the task of automatic language identification.\n\n\t\n\t\t\n\t\tSummary\n\t\n\n\nLanguages: 26 individual Cyrillic languages, plus 17 mixed language combinations created through augmentation\nSource: texts collected from Wikipedia and augmented using various methods\nSize: 20,232 examples in total, including 18,334 original articles collected from Wikipedia and 1,898 texts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlmaznayaGroza/cyrillic-language-classification.","url":"https://huggingface.co/datasets/AlmaznayaGroza/cyrillic-language-classification","creator_name":"G√©raldine","creator_url":"https://huggingface.co/AlmaznayaGroza","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","Belarusian","Ukrainian","Russian","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_888","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wenknow/reddit_dataset_888.","url":"https://huggingface.co/datasets/wenknow/reddit_dataset_888","creator_name":"Dt","creator_url":"https://huggingface.co/wenknow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Speech2Latex","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tSpeech2Latex Dataset\n\t\n\nThe Speech2LaTeX dataset is the first fully open-source large-scale dataset for converting spoken mathematical expressions and sentences into LaTeX. It comprises over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. This work lays the groundwork for future advances in multimodal AI, with a particular focus on mathematical content recognition.\nThe dataset was presented‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marsianin500/Speech2Latex.","url":"https://huggingface.co/datasets/marsianin500/Speech2Latex","creator_name":"Anonymous Account","creator_url":"https://huggingface.co/marsianin500","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-generation","crowdsourced","machine-generated","expert-generated"],"keywords_longer_than_N":true},
	{"name":"exorde-social-media-december-2024-week1","keyword":"multilingual","description":"","url":"https://huggingface.co/datasets/Exorde/exorde-social-media-december-2024-week1","creator_name":"ExordeLabs","creator_url":"https://huggingface.co/Exorde","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","summarization","text-retrieval","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"Hypa_Fleurs","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tHypa_Fleurs\n\t\n\nHypa_Fleurs is an open-source multilingual, multi-modal dataset with a long term vision of advancing speech and language technology for low-resource African languages by leveraging the English split of the Google Fleurs dataset to create parallel speech and text datasets for a wide range of low-resource African languages. In this initial release, professional AfroVoices experts translated the original English texts into three under-resourced African languages: Igbo (ig)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hypaai/Hypa_Fleurs.","url":"https://huggingface.co/datasets/hypaai/Hypa_Fleurs","creator_name":"Hypa-Intelligence","creator_url":"https://huggingface.co/hypaai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","text-classification","AfroVoices"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_1234","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/reddit_dataset_1234.","url":"https://huggingface.co/datasets/arrmlet/reddit_dataset_1234","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_33945","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_33945.","url":"https://huggingface.co/datasets/momo1942/x_dataset_33945","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"exorde-social-media-december-2024-week1","keyword":"multi-lingual","description":"","url":"https://huggingface.co/datasets/Exorde/exorde-social-media-december-2024-week1","creator_name":"ExordeLabs","creator_url":"https://huggingface.co/Exorde","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","summarization","text-retrieval","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"x_dataset_53989","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_53989.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_53989","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"RWKV-World-v3","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tRWKV-7 (Goose) World v3 Corpus\n\t\n\nPaper | Code\nThis is an itemised and annotated list of the RWKV World v3 corpus\nwhich is a multilingual dataset with about 3.1T tokens used to train the\n\"Goose\" RWKV-7 World model series.\nRWKV World v3 was crafted from public datasets spanning >100 world languages\n(80% English, 10% multilang, and 10% code). Also available as a HF Collection of Datasets.\nSubsampled subsets (previews) of the corpus are available as 100k JSONL dataset and 1M JSONL dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Goose-World/RWKV-World-v3.","url":"https://huggingface.co/datasets/Goose-World/RWKV-World-v3","creator_name":"Goose-World","creator_url":"https://huggingface.co/Goose-World","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1M - 10M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"x_dataset_58","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_58.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_58","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_26","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_26.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_26","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_151","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/williamlewis0620/x_dataset_151.","url":"https://huggingface.co/datasets/williamlewis0620/x_dataset_151","creator_name":"William Lewis","creator_url":"https://huggingface.co/williamlewis0620","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Rule-based","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Rule-based is a dataset of BenchMAX, sourcing from IFEval, which is a rule-based benchmark for evaluating the instruction following capabilities in multilingual scenarios.\nWe extend the original dataset to 16 non-English languages by first‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Rule-based.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Rule-based","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"x_dataset_24095","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_24095.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_24095","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_17276","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_17276.","url":"https://huggingface.co/datasets/momo1942/x_dataset_17276","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"indic_sts","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Indic STS\n\t\n\nThis dataset is STS benchmark between English and 12 high-resource Indic languages. This was released as a part of Samanantar paper. Please refer to the paper for more details.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAvailable languages are: en-as, en-bn, en-gu, en-hi, en-kn, en-ml, en-mr, en-or, en-pa, en-ta, en-te, en-ur\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tDataset Fields\n\t\n\n\nlang_code: 2-digit ISO language code\nsource: The source from which the candidate sentence is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/indic_sts.","url":"https://huggingface.co/datasets/mteb/indic_sts","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-scoring","semantic-similarity-scoring","crowdsourced","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"indic_sts","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Indic STS\n\t\n\nThis dataset is STS benchmark between English and 12 high-resource Indic languages. This was released as a part of Samanantar paper. Please refer to the paper for more details.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAvailable languages are: en-as, en-bn, en-gu, en-hi, en-kn, en-ml, en-mr, en-or, en-pa, en-ta, en-te, en-ur\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tDataset Fields\n\t\n\n\nlang_code: 2-digit ISO language code\nsource: The source from which the candidate sentence is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/indic_sts.","url":"https://huggingface.co/datasets/mteb/indic_sts","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-scoring","semantic-similarity-scoring","crowdsourced","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"engime","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Engime.org\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 2,677,221 pages of educational content primarily in Kazakh language with some Russian content extracted from engime.org website. The content includes academic and educational materials, with a focus on technical and scientific topics.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Kazakh (kk) with some Russian (ru) content.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/engime.","url":"https://huggingface.co/datasets/nyuuzyou/engime","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","topic-classification","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-JohnSlegers","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-JohnSlegers","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"mls-eng-10k-tags_tagged_10k_generated","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Annotations of 10K hours of English MLS\n\t\n\nThis dataset consists in annotations of a 10K hours subset of English version of the Multilingual LibriSpeech (MLS) dataset. \nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/mls-eng-10k-tags_tagged_10k_generated.","url":"https://huggingface.co/datasets/pharaouk/mls-eng-10k-tags_tagged_10k_generated","creator_name":"Farouk","creator_url":"https://huggingface.co/pharaouk","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"x_dataset_52806","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_52806.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_52806","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_7","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Aniruddh79012/reddit_dataset_7.","url":"https://huggingface.co/datasets/Aniruddh79012/reddit_dataset_7","creator_name":"Singh","creator_url":"https://huggingface.co/Aniruddh79012","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_10","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_10.","url":"https://huggingface.co/datasets/suul999922/x_dataset_10","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_28","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gsjcm/reddit_dataset_28.","url":"https://huggingface.co/datasets/gsjcm/reddit_dataset_28","creator_name":"gsjcmurn","creator_url":"https://huggingface.co/gsjcm","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_171","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tensorshield/reddit_dataset_171.","url":"https://huggingface.co/datasets/tensorshield/reddit_dataset_171","creator_name":"Tensor Shield","creator_url":"https://huggingface.co/tensorshield","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_239","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sharper740/reddit_dataset_239.","url":"https://huggingface.co/datasets/sharper740/reddit_dataset_239","creator_name":"sharper","creator_url":"https://huggingface.co/sharper740","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_24","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_24.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_24","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"en-te-kn-translation-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüìö Multilingual English-Telugu-Kannada Translation Dataset\n\t\n\nThis dataset is a curated and preprocessed subset of the AI4Bharat Samanantar dataset focused on multilingual translation tasks between English, Telugu (te_IN), and Kannada (kn_IN).\n\n\t\n\t\t\n\t\t‚ú® Dataset Features\n\t\n\n\nLanguage pairs:\nen ‚Üî te_IN\nen ‚Üî kn_IN\n\n\nPreprocessed:\nFiltered for sentence length (min=3, max=128 words)\nCleaned and normalized\n\n\nTokenized using Hugging Face Transformers tokenizers:\nM2M100Tokenizer (for en‚Üîkn)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Koushim/en-te-kn-translation-dataset.","url":"https://huggingface.co/datasets/Koushim/en-te-kn-translation-dataset","creator_name":"K Koushik Reddy","creator_url":"https://huggingface.co/Koushim","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","manual","found","multilingual","ai4bharat/samanantar"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0604139","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_0604139.","url":"https://huggingface.co/datasets/john-1111/x_dataset_0604139","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"epfml-FineWeb2-HQ-sample","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tepfml/FineWeb2-HQ\n\t\n\nA curated subset of the epfml/FineWeb2-HQ dataset featuring high-quality multilingual text.\n\n\t\n\t\t\n\t\tDetails\n\t\n\n\nFirst 25‚Äâ000 rows per config (language and script pair)\nDuplicates removed\nTexts truncated to 512 LLaMA 3.1 tokens\nScores transformed with log10\nRows shuffled and 20% of the rows split into the test set (stratified by config)\n\n\n\t\n\t\t\n\t\tExample\n\t\n\n{\n  \"text\": \"ÁàµÂ£´Â§ßÂ∏àTim Garland Ê∑±Âú≥‰∏ìÂú∫ - [jazz]\\nTim Garland Lighthouse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/epfml-FineWeb2-HQ-sample.","url":"https://huggingface.co/datasets/agentlans/epfml-FineWeb2-HQ-sample","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","Chinese","German","Japanese"],"keywords_longer_than_N":true},
	{"name":"werewolf_game_reasoning","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWerewolf Game Dataset\n\t\n\nThis repository contains a comprehensive dataset for the Werewolf game in paper Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game, including both raw game data and processed  multi-level instruction datasets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tRaw Data\n\t\n\nThe raw data is located in the raw folder. Each game consists of two files:\n\nevent.json: Contains the game regular record and thinking process data, including:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning.","url":"https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning","creator_name":"Rong Ye","creator_url":"https://huggingface.co/ReneeYe","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","expert-generated","multilingual","original","Chinese"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0409154","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_0409154.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_0409154","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_14","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Axioris/reddit_dataset_14.","url":"https://huggingface.co/datasets/Axioris/reddit_dataset_14","creator_name":"DaneFoster","creator_url":"https://huggingface.co/Axioris","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"translated_text2cypher24_trainset_sampled","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTranslated Text2Cypher'24 Training Set - Sampled & Multilingual\n\t\n\nThis dataset provides a sampled and translated training set based on the Neo4j Text2Cypher '24 dataset. \nIt is designed to support research on multilingual natural language to Cypher query generation.\nWe offer two versions of the training set:\n\n\t\n\t\t\n\t\n\t\n\t\t1. Multilingual Version (multilang)\n\t\n\n\nTotal examples: ~36,000  \nLanguages: English (en), Spanish (es), Turkish (tr)  \nSamples per language: ~12,000  \nTranslation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mgoNeo4j/translated_text2cypher24_trainset_sampled.","url":"https://huggingface.co/datasets/mgoNeo4j/translated_text2cypher24_trainset_sampled","creator_name":"MGO","creator_url":"https://huggingface.co/mgoNeo4j","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["apache-2.0","üá∫üá∏ Region: US","multilingual","text2cypher","question-to-query"],"keywords_longer_than_N":true},
	{"name":"Customer-Care-Services-Dataset-in-Nepali","keyword":"multilingual","description":"kshitizgajurel/Customer-Care-Services-Dataset-in-Nepali dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/kshitizgajurel/Customer-Care-Services-Dataset-in-Nepali","creator_name":"Kshitiz Gajurel","creator_url":"https://huggingface.co/kshitizgajurel","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","fill-mask","open-domain-abstractive-qa","document-question-answering"],"keywords_longer_than_N":true},
	{"name":"x_dataset_060955","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_060955.","url":"https://huggingface.co/datasets/john-1111/x_dataset_060955","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_65258","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_65258.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_65258","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"genaral-swahili","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMulti-Domain Swahili Speech Dataset\n\t\n\nThis dataset contains 5 audio recordings with corresponding text transcriptions across multiple languages and domains.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive collection of audio files paired with text transcriptions, featuring both synthetic and natural speech across various domains. Suitable for automatic speech recognition (ASR), text-to-speech (TTS), and domain-specific speech processing tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jsbeaudry/genaral-swahili.","url":"https://huggingface.co/datasets/jsbeaudry/genaral-swahili","creator_name":"Jean Sauvenel Beaudry","creator_url":"https://huggingface.co/jsbeaudry","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Swahili","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Tatoeba-Translations","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis is the latest version of Tatoeba translations as of December 2024.\nThe sentences are downloaded from the Tatoeba collection website.\nThe dataset is processed through mapping sentences.tar.bz2 using sentences_base.tar.bz2 to find source (sentence_src) and target (sentence_tgt) sentences.\nWhile lang_src and lang_tgt columns follow the mapping provided by Tatoeba, the lang_pair column merely lists the two languages in the translation pair.\n\n\t\n\t\t\n\t\n\t\n\t\tStatistics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/Tatoeba-Translations.","url":"https://huggingface.co/datasets/ymoslem/Tatoeba-Translations","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","Abkhaz","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"fama-data","keyword":"multilingual","description":"\n\n\n\t\n\t\t\n\t\tDataset Description, Collection, and Source\n\t\n\nThe FAMA training data is the collection of English and Italian datasets for automatic speech recognition (ASR) and speech translation (ST)\nused to train the FAMA models family.\nThe ASR section of FAMA is derived from the MOSEL data collection, including the automatic\ntranscripts obtained with Whisper and available in the HuggingFace MOSEL Dataset.\nThe ASR is further augmented with automatically transcribed speech from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FBK-MT/fama-data.","url":"https://huggingface.co/datasets/FBK-MT/fama-data","creator_name":"FBK-MT","creator_url":"https://huggingface.co/FBK-MT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","automatic-speech-recognition","multilingual","Italian","English"],"keywords_longer_than_N":true},
	{"name":"x_dataset_36943","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_36943.","url":"https://huggingface.co/datasets/momo1942/x_dataset_36943","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_11","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_11.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_11","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"znanio-videos","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Videos\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 6,653 educational videos from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-videos.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-videos","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"znanio-videos","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Videos\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 6,653 educational videos from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-videos.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-videos","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"indic_sts","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Indic STS\n\t\n\nThis dataset is STS benchmark between English and 12 high-resource Indic languages. This was released as a part of Samanantar paper. Please refer to the paper for more details.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAvailable languages are: en-as, en-bn, en-gu, en-hi, en-kn, en-ml, en-mr, en-or, en-pa, en-ta, en-te, en-ur\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tDataset Fields\n\t\n\n\nlang_code: 2-digit ISO language code\nsource: The source from which the candidate sentence is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jaygala24/indic_sts.","url":"https://huggingface.co/datasets/jaygala24/indic_sts","creator_name":"Jay Gala","creator_url":"https://huggingface.co/jaygala24","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-scoring","semantic-similarity-scoring","crowdsourced","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"indic_sts","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Indic STS\n\t\n\nThis dataset is STS benchmark between English and 12 high-resource Indic languages. This was released as a part of Samanantar paper. Please refer to the paper for more details.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAvailable languages are: en-as, en-bn, en-gu, en-hi, en-kn, en-ml, en-mr, en-or, en-pa, en-ta, en-te, en-ur\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tDataset Fields\n\t\n\n\nlang_code: 2-digit ISO language code\nsource: The source from which the candidate sentence is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jaygala24/indic_sts.","url":"https://huggingface.co/datasets/jaygala24/indic_sts","creator_name":"Jay Gala","creator_url":"https://huggingface.co/jaygala24","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-scoring","semantic-similarity-scoring","crowdsourced","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"x_dataset_32","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Axioris/x_dataset_32.","url":"https://huggingface.co/datasets/Axioris/x_dataset_32","creator_name":"DaneFoster","creator_url":"https://huggingface.co/Axioris","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for BibleNLP Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPartial and complete Bible translations in 833 languages, aligned by verse.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\naai, aak, aau, aaz, abt, abx, aby, acf, acr, acu, adz, aer, aey, agd, agg, agm, agn, agr, agt, agu, aia, aii, aka, ake, alp, alq, als, aly, ame, amf, amk, amm, amn, amo, amp, amr, amu, amx, anh, anv, aoi, aoj, aom, aon, apb, ape, apn, apr, apu, apw, apz, arb, are, arl, arn, arp, asm, aso, ata, atb, atd, atg, att, auc, aui, auy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bible-nlp/biblenlp-corpus.","url":"https://huggingface.co/datasets/bible-nlp/biblenlp-corpus","creator_name":"The Partnership for Applied Biblical NLP","creator_url":"https://huggingface.co/bible-nlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","no-annotation","expert-generated","translation","multilingual"],"keywords_longer_than_N":true},
	{"name":"vukuzenzele-sentence-aligned","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tThe Vuk'uzenzele South African Multilingual Corpus\n\t\n\nGithub: https://github.com/dsfsi/vukuzenzele-nlp/\nZenodo: \nArxiv Preprint: \nGive Feedback üìë: DSFSI Resource Feedback Form\n\n\t\n\t\t\n\t\n\t\n\t\tAbout\n\t\n\nThe dataset was obtained from the South African government magazine Vuk'uzenzele, created by the Government Communication and Information System (GCIS). \nThe original raw PDFS were obtatined from the Vuk'uzenzele website.\nThe datasets contain government magazine editions in 11 languages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dsfsi/vukuzenzele-sentence-aligned.","url":"https://huggingface.co/datasets/dsfsi/vukuzenzele-sentence-aligned","creator_name":"Data Science for Social Impact","creator_url":"https://huggingface.co/dsfsi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","translation","English","Afrikaans","South Ndebele"],"keywords_longer_than_N":true},
	{"name":"test-dateset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for C4\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: \"https://commoncrawl.org\".\nThis is the version prepared by AllenAI, hosted at this address: https://huggingface.co/datasets/allenai/c4\nIt comes in four variants:\n\nen: 305GB in JSON format\nen.noblocklist: 380GB in JSON format\nen.noclean: 2.3TB in JSON format\nrealnewslike: 15GB in JSON format\n\nThe en.noblocklist variant is exactly the same as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Peihao/test-dateset.","url":"https://huggingface.co/datasets/Peihao/test-dateset","creator_name":"Peihao Yang","creator_url":"https://huggingface.co/Peihao","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"HashtagPrediction","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tHashtag Prediction Dataset from paper TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations\n\t\n\n  \nThis repo contains the Hashtag prediction dataset from our paper TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations. \n[arXiv][HuggingFace Models]\n[Github repo]\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\t\n\t\t\n\t\n\t\n\t\tDownload\n\t\n\nUse the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Twitter/HashtagPrediction.","url":"https://huggingface.co/datasets/Twitter/HashtagPrediction","creator_name":"Twitter","creator_url":"https://huggingface.co/Twitter","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Slovenian","Urdu","Sindhi","Polish","Vietnamese"],"keywords_longer_than_N":true},
	{"name":"evi","keyword":"multilingual","description":"EVI is a challenging spoken multilingual dataset with 5,506 dialogues in English, Polish, and French \nthat can be used for benchmarking and developing knowledge-based enrolment, identification, and identification \nfor spoken dialogue systems.","url":"https://huggingface.co/datasets/PolyAI/evi","creator_name":"PolyAI","creator_url":"https://huggingface.co/PolyAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["crowdsourced","machine-generated","crowdsourced","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"wino_x","keyword":"multilingual","description":"Wino-X is a parallel dataset of German, French, and Russian Winograd schemas, aligned with their English \ncounterparts, used to examine whether neural machine translation models can perform coreference resolution that \nrequires commonsense knowledge and whether multilingual language models are capable of commonsense reasoning across \nmultiple languages.","url":"https://huggingface.co/datasets/demelin/wino_x","creator_name":"Denis Emelin","creator_url":"https://huggingface.co/demelin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","multiple-choice-qa","language-modeling","no-annotation","machine-generated"],"keywords_longer_than_N":true},
	{"name":"NLU-Evaluation-Data-en-de","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tNLU Evaluation Data - English and German\n\t\n\nA labeled English and German language multi-domain dataset (21 domains) with 25K user utterances for human-robot interaction.\nThis dataset is collected and annotated for evaluating NLU services and platforms.\nThe detailed paper on this dataset can be found at arXiv.org:\nBenchmarking Natural Language Understanding Services for building Conversational Agents\nThe dataset builds on the annotated data of the xliuhw/NLU-Evaluation-Data\nrepository.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/deutsche-telekom/NLU-Evaluation-Data-en-de.","url":"https://huggingface.co/datasets/deutsche-telekom/NLU-Evaluation-Data-en-de","creator_name":"Deutsche Telekom AG","creator_url":"https://huggingface.co/deutsche-telekom","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","intent-classification","multilingual","extended|nlu_evaluation_data","English"],"keywords_longer_than_N":true},
	{"name":"mala-opus-dedup-2410-reLID","keyword":"multilingual","description":"MaLA-LM/mala-opus-dedup-2410-reLID dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/MaLA-LM/mala-opus-dedup-2410-reLID","creator_name":"MaLA-LM","creator_url":"https://huggingface.co/MaLA-LM","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["translation","text-generation","multilingual","odc-by","10B<n<100B"],"keywords_longer_than_N":true},
	{"name":"bernice-pretrain-data","keyword":"multilingual","description":"Tweet IDs for the 2.5 billion multilingual tweets used to train Bernice, a Twitter encoder.\nThe tweets are from the public 1% Twitter API stream from January 2016 to December 2021. \nTwitter-provided language metadata is provided with the tweet ID. The data contains 66 unique languages, \nas identified by ISO 639 language codes, including `und` for undefined languages.\nTweets need to be re-gathered via the Twitter API.","url":"https://huggingface.co/datasets/jhu-clsp/bernice-pretrain-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["other","no-annotation","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"itu_faroese_danish","keyword":"multilingual","description":"\\","url":"https://huggingface.co/datasets/strombergnlp/itu_faroese_danish","creator_name":"Str√∏mberg NLP","creator_url":"https://huggingface.co/strombergnlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-generated","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"scandi-reddit","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for ScandiReddit\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nScandiReddit is a filtered and post-processed corpus consisting of comments from Reddit.\nAll Reddit comments from December 2005 up until October 2022 were downloaded through PushShift, after which these were filtered based on the FastText language detection model. Any comment which was classified as Danish (da), Norwegian (no), Swedish (sv) or Icelandic (is) with a confidence score above 70% was kept.\nThe resulting comments‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexandrainst/scandi-reddit.","url":"https://huggingface.co/datasets/alexandrainst/scandi-reddit","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","multilingual","Danish"],"keywords_longer_than_N":true},
	{"name":"afriqa_wiki_en_fr_100","keyword":"multilingual","description":"","url":"https://huggingface.co/datasets/masakhane/afriqa_wiki_en_fr_100","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multilingual","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"masakhaner2","keyword":"multilingual","description":"MasakhaNER 2.0 is the largest publicly available high-quality dataset for named entity recognition (NER) in 20 African languages.\n\nNamed entities are phrases that contain the names of persons, organizations, locations, times and quantities.\n\nExample:\n[PER Wolff] , currently a journalist in [LOC Argentina] , played with [PER Del Bosque] in the final years of the seventies in [ORG Real Madrid] .\nMasakhaNER is a named entity dataset consisting of PER, ORG, LOC, and DATE entities annotated by Masakhane for 20 African languages:\n- Bambara (bam)\n- Ghomala (bbj)\n- Ewe (ewe)\n- Fon (fon)\n- Hausa (hau)\n- Igbo (ibo)\n- Kinyarwanda (kin)\n- Luganda (lug)\n- Dholuo (luo) \n- Mossi (mos)\n- Chichewa (nya)\n- Nigerian Pidgin\n- chShona (sna)\n- Kiswahili (swƒÖ)\n- Setswana (tsn)\n- Twi (twi)\n- Wolof (wol)\n- isiXhosa (xho)\n- Yor√πb√° (yor)\n- isiZulu (zul)\n\nThe train/validation/test sets are available for all the ten languages.\n\nFor more details see https://arxiv.org/abs/2103.11811","url":"https://huggingface.co/datasets/masakhane/masakhaner2","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","expert-generated","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"mteb-nl-sick-sts-pr","keyword":"translated","description":"\n  SICK-NL-STS\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSICK-NL (read: signal), a dataset targeting Natural Language Inference in Dutch. SICK-NL is obtained by translating the SICK dataset of (Marelli et al., 2014) from English into Dutch.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\nDomains\nNews, Social, Web, Spoken, Written\n\n\nReference\nhttps://aclanthology.org/2021.eacl-main.126/\n\n\n\t\n\nSource datasets:\n\nclips/mteb-nl-sick\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clips/mteb-nl-sick-sts-pr.","url":"https://huggingface.co/datasets/clips/mteb-nl-sick-sts-pr","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","human-annotated","translated","clips/mteb-nl-sick","Dutch"],"keywords_longer_than_N":true},
	{"name":"ChatML-aya_dataset","keyword":"multilingual","description":"CohereForAI/aya_dataset in ChatML format, ready to use in HuggingFace TRL's SFT Trainer.\nPython code used for conversion:\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"Felladrin/Llama-160M-Chat-v1\")\n\ndataset = load_dataset(\"CohereForAI/aya_dataset\", split=\"train\")\n\ndef format(columns):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": columns[\"inputs\"].strip(),\n        },\n        {‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Felladrin/ChatML-aya_dataset.","url":"https://huggingface.co/datasets/Felladrin/ChatML-aya_dataset","creator_name":"Victor Nogueira","creator_url":"https://huggingface.co/Felladrin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","crowdsourced","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"miracl-yo-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (yo) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-yo-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-yo-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-yo-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-yo-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Yoruba"],"keywords_longer_than_N":true},
	{"name":"lextreme","keyword":"multilingual","description":"The LEXTREME Benchmark is a collection of multilingual datasets for evaluating model performance \nacross a diverse set of legal NLU tasks.","url":"https://huggingface.co/datasets/joelniklaus/lextreme","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","multi-class-classification","multi-label-classification","topic-classification"],"keywords_longer_than_N":true},
	{"name":"mls_eng","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for English MLS\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a streamable version of the English version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls_eng.","url":"https://huggingface.co/datasets/parler-tts/mls_eng","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"filtered_mc4","keyword":"multilingual","description":"The mC4 dataset to which arbitrary filters can be applied.\n\nThe original description is below:\n===\nA colossal, cleaned version of Common Crawl's web crawl corpus.\nBased on Common Crawl dataset: \"https://commoncrawl.org\".\nThis is the processed version of Google's mC4 dataset by AllenAI.","url":"https://huggingface.co/datasets/hiroshi-matsuda-rit/filtered_mc4","creator_name":"Hiroshi Matsuda","creator_url":"https://huggingface.co/hiroshi-matsuda-rit","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["multilingual","odc-by","arxiv:1910.10683","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"MLDR","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMLDR is a Multilingual Long-Document Retrieval dataset built on Wikipeida, Wudao and mC4, covering 13 typologically diverse languages. Specifically, we sample lengthy articles from Wikipedia, Wudao and mC4 datasets and randomly choose paragraphs from them. Then we use GPT-3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the dataset. The prompt for GPT3.5 is ‚ÄúYou are a curious AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Shitao/MLDR.","url":"https://huggingface.co/datasets/Shitao/MLDR","creator_name":"Xiao","creator_url":"https://huggingface.co/Shitao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-retrieval","multilingual","Arabic","German","English"],"keywords_longer_than_N":true},
	{"name":"xtr-wiki_qa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tXtr-WikiQA\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nXtr-WikiQA is an Answer Sentence Selection (AS2) dataset in 9 non-English languages, proposed in our paper accepted at ACL 2023 (Findings): Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages.\nThis dataset is based on an English AS2 dataset, WikiQA (Original, Hugging Face).\nFor translations, we used Amazon Translate.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\n\nArabic (ar)\nSpanish (es)\nFrench (fr)\nGerman (de)\nHindi (hi)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AmazonScience/xtr-wiki_qa.","url":"https://huggingface.co/datasets/AmazonScience/xtr-wiki_qa","creator_name":"Amazon Science","creator_url":"https://huggingface.co/AmazonScience","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["question-answering","text-retrieval","open-domain-qa","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"multilingual-sentiments","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Sentiments Dataset\n\t\n\nA collection of multilingual sentiments datasets grouped into 3 classes -- positive, neutral, negative.\nMost multilingual sentiment datasets are either 2-class positive or negative, 5-class ratings of products reviews (e.g. Amazon multilingual dataset) or multiple classes of emotions. However, to an average person, sometimes positive, negative and neutral classes suffice and are more straightforward to perceive and annotate. Also, a positive/negative‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tyqiangz/multilingual-sentiments.","url":"https://huggingface.co/datasets/tyqiangz/multilingual-sentiments","creator_name":"Tay Yong Qiang","creator_url":"https://huggingface.co/tyqiangz","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-classification","monolingual","multilingual"],"keywords_longer_than_N":true},
	{"name":"honest","keyword":"multilingual","description":"HONEST dataset comprises a set of templates for measuring hurtful sentence completions in language models. The templates are provided in six languages (English, Italian, French, Portuguese, Romanian, and Spanish) for binary gender and in English for LGBTQAI+ individuals. WARNING: This dataset contains content that are offensive and/or hateful in nature.","url":"https://huggingface.co/datasets/MilaNLProc/honest","creator_name":"MilaNLP","creator_url":"https://huggingface.co/MilaNLProc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","hate-speech-detection","no-annotation","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"text_coordinates_regions","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Multilingual Geo-Tagged Social Media Posts (by 123 world regions)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe \"Regions\" dataset is a multilingual corpus that encompasses textual data from the 123 most populated regions worldwide, with each region's data organized into separate .json files. This dataset consists of approximately 500,000 text samples, each paired with its geographic coordinates.\nKey Features:\n\nTextual Data: The dataset contains 500,000 text samples.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yachay/text_coordinates_regions.","url":"https://huggingface.co/datasets/yachay/text_coordinates_regions","creator_name":"Yachay AI","creator_url":"https://huggingface.co/yachay","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","token-classification","text-classification","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"multilingual_librispeech","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for MultiLingual LibriSpeech\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a streamable version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/multilingual_librispeech.","url":"https://huggingface.co/datasets/facebook/multilingual_librispeech","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"mfaq","keyword":"multilingual","description":"We present the first multilingual FAQ dataset publicly available. We collected around 6M FAQ pairs from the web, in 21 different languages.","url":"https://huggingface.co/datasets/clips/mfaq","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","no-annotation","other","multilingual"],"keywords_longer_than_N":true},
	{"name":"indicxnli","keyword":"multilingual","description":"IndicXNLI is a translated version of XNLI to 11 Indic Languages. As with XNLI, the goal is\nto predict textual entailment (does sentence A imply/contradict/neither sentence\nB) and is a classification task (given two sentences, predict one of three\nlabels).","url":"https://huggingface.co/datasets/Divyanshu/indicxnli","creator_name":"Divyanshu Aggarwal","creator_url":"https://huggingface.co/Divyanshu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","machine-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"synthetic-instruct-gptj-pairwise-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tDataset Card for \"synthetic-instruct-gptj-pairwise-ru\"\n\t\n\nThis is translated version of Dahoas/synthetic-instruct-gptj-pairwise dataset into Russian.\n","url":"https://huggingface.co/datasets/d0rj/synthetic-instruct-gptj-pairwise-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translated","monolingual","Dahoas/synthetic-instruct-gptj-pairwise","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"blbooks-parquet","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for British Library Books\n\t\n\nThis dataset is the same as https://huggingface.co/datasets/TheBritishLibrary/blbooks, however, this version is stored as parquet to avoid needing to run a datasets script. This also makes loading this dataset much quicker. \n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of books digitised by the British Library in partnership with Microsoft. The dataset includes ~25 million pages of out of copyright texts. The majority of the texts were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biglam/blbooks-parquet.","url":"https://huggingface.co/datasets/biglam/blbooks-parquet","creator_name":"BigLAM: BigScience Libraries, Archives and Museums","creator_url":"https://huggingface.co/biglam","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","other","language-modeling","masked-language-modeling"],"keywords_longer_than_N":true},
	{"name":"diffusiondb","keyword":"multilingual","description":"DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2\nmillion images generated by Stable Diffusion using prompts and hyperparameters\nspecified by real users. The unprecedented scale and diversity of this\nhuman-actuated dataset provide exciting research opportunities in understanding\nthe interplay between prompts and generative models, detecting deepfakes, and\ndesigning human-AI interaction tools to help users more easily use these models.","url":"https://huggingface.co/datasets/poloclub/diffusiondb","creator_name":"Polo Club of Data Science","creator_url":"https://huggingface.co/poloclub","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"SWEbenchMultilingualRR","keyword":"multilingual","description":"\n  SWEbenchMultilingualRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual Software Issue Localization.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://www.swebench.com/multilingual.html\n\n\n\t\n\nSource datasets:\n\ntarsur909/mteb-swe-bench-multilingual-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"SWEbenchMultilingualRR\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SWEbenchMultilingualRR.","url":"https://huggingface.co/datasets/mteb/SWEbenchMultilingualRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-swe-bench-multilingual-reranking","code"],"keywords_longer_than_N":true},
	{"name":"legal-mc4","keyword":"multilingual","description":"Legal-MC4: A Corpus Covering the Legal Part of MC4 for European Languages","url":"https://huggingface.co/datasets/joelniklaus/legal-mc4","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","other","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"wikipedia-for-mask-filling","keyword":"multilingual","description":"\\","url":"https://huggingface.co/datasets/rcds/wikipedia-for-mask-filling","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","other","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"MOCKS","keyword":"multilingual","description":"Multilingual Open Custom Keyword Spotting Testset (MOCKS) is a comprehensive \naudio testset for evaluation and benchmarking Open-Vocabulary Keyword Spotting (OV-KWS) models.","url":"https://huggingface.co/datasets/voiceintelligenceresearch/MOCKS","creator_name":"VoiceIntelligenceResearch","creator_url":"https://huggingface.co/voiceintelligenceresearch","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["expert-generated","multilingual","English","German","Spanish"],"keywords_longer_than_N":true},
	{"name":"HumanEvalRetrieval","keyword":"multilingual","description":"\n  HumanEvalRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA code retrieval task based on 164 Python programming problems from HumanEval. Each query is a natural language description of a programming task (e.g., 'Check if in given list of numbers, are any two numbers closer to each other than given threshold'), and the corpus contains Python code implementations. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HumanEvalRetrieval.","url":"https://huggingface.co/datasets/mteb/HumanEvalRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","multilingual","embedding-benchmark/HumanEval","code"],"keywords_longer_than_N":true},
	{"name":"RuSciBenchCociteRetrieval","keyword":"multilingual","description":"\n  RuSciBenchCociteRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task focuses on Co-citation Prediction for scientific papers from eLibrary,\n        Russia's largest electronic library of scientific publications. Given a query paper (title and abstract),\n        the goal is to retrieve other papers that are co-cited with it. Two papers are considered co-cited\n        if they are both cited by at least 5 of the same other papers. Similar to the Direct Citation task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RuSciBenchCociteRetrieval.","url":"https://huggingface.co/datasets/mteb/RuSciBenchCociteRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","multilingual","mlsa-iai-msu-lab/ru_sci_bench_cocite_retrieval"],"keywords_longer_than_N":true},
	{"name":"mFollowIRCrossLingual","keyword":"multilingual","description":"\n  mFollowIRCrossLingual\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis tasks measures retrieval instruction following ability on NeuCLIR narratives for the mFollowIR benchmark on the Farsi, Russian, and Chinese languages with English queries/instructions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\nSource datasets:\n\njhu-clsp/mFollowIR-cross-lingual-parquet-mteb\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/mFollowIRCrossLingual.","url":"https://huggingface.co/datasets/mteb/mFollowIRCrossLingual","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-ranking","expert-annotated","multilingual","jhu-clsp/mFollowIR-cross-lingual-parquet-mteb","English"],"keywords_longer_than_N":true},
	{"name":"miracl-th-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (th) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-th-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-th-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-th-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-th-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Thai"],"keywords_longer_than_N":true},
	{"name":"Vidore2BioMedicalLecturesRetrieval","keyword":"multilingual","description":"\n  Vidore2BioMedicalLecturesRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/biomedical_lectures_v2\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"Vidore2BioMedicalLecturesRetrieval\")\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Vidore2BioMedicalLecturesRetrieval.","url":"https://huggingface.co/datasets/mteb/Vidore2BioMedicalLecturesRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"ParlaMint3","keyword":"multilingual","description":"ParlaMint 3.0 is a multilingual set of 26 comparable corpora containing parliamentary debates mostly starting in 2015 and extending to mid-2022. \nThe corpora have extensive metadata, including aspects of the parliament; the speakers (name, gender, MP status, party affiliation, party coalition/opposition); \nare structured into time-stamped terms, sessions and meetings; and with speeches being marked by the speaker and their role (e.g. chair, regular speaker). \nThe speeches also contain marked-up transcriber comments, such as gaps in the transcription, interruptions, applause, etc. \nNote that some corpora have further information, e.g. the year of birth of the speakers, links to their Wikipedia articles, their membership in various committees, etc. \nThe corpora are also marked to the subcorpus they belong to (\"reference\", until 2020-01-30, \"covid\", from 2020-01-31, and \"war\", from 2022-02-24). \nThe corpora are encoded according to the Parla-CLARIN TEI recommendation (https://clarin-eric.github.io/parla-clarin/), but have been encoded against the compatible, \nbut much stricter ParlaMint encoding guidelines (https://clarin-eric.github.io/ParlaMint/) and schemas (included in this distribution). \nThis entry contains the ParlaMint TEI-encoded corpora with the derived plain text versions of the corpora along with TSV metadata of the speeches. \nAlso included is the 3.0 release of the data and scripts available at the GitHub repository of the ParlaMint project.","url":"https://huggingface.co/datasets/cjvt/ParlaMint3","creator_name":"Center za jezikovne vire in tehnologije Univerze v Ljubljani","creator_url":"https://huggingface.co/cjvt","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["other","multilingual","Slovenian","German","Bosnian"],"keywords_longer_than_N":true},
	{"name":"miracl-fr-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (fr) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-fr-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-fr-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-fr-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-fr-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","French"],"keywords_longer_than_N":true},
	{"name":"miracl-ko-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (ko) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-ko-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-ko-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-ko-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-ko-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Korean"],"keywords_longer_than_N":true},
	{"name":"miracl-ja-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (ja) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-ja-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-ja-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-ja-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-ja-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Japanese"],"keywords_longer_than_N":true},
	{"name":"ilist","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for ilist\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is introduced in a task which aimed at identifying 5 closely-related languages of Indo-Aryan language family: Hindi (also known as Khari Boli), Braj Bhasha, Awadhi, Bhojpuri and Magahi. These languages form part of a continuum starting from Western Uttar Pradesh (Hindi and Braj Bhasha) to Eastern Uttar Pradesh (Awadhi and Bhojpuri) and the neighbouring Eastern state of Bihar (Bhojpuri and Magahi).\nFor this task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kmi-linguistics/ilist.","url":"https://huggingface.co/datasets/kmi-linguistics/ilist","creator_name":"kmi-linguistics","creator_url":"https://huggingface.co/kmi-linguistics","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","no-annotation","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"multilingual_librispeech","keyword":"multilingual","description":"Multilingual LibriSpeech (MLS) dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish.","url":"https://huggingface.co/datasets/legacy-datasets/multilingual_librispeech","creator_name":"Legacy Datasets","creator_url":"https://huggingface.co/legacy-datasets","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","speaker-identification","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"miracl-corpus","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for MIRACL Corpus\n\t\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages, which collectively encompass over three billion native speakers around the world.\nThis dataset contains the collection data of the 16 \"known languages\". The remaining 2 \"surprise languages\" will not be released until later.\nThe corpus for each language is prepared from a Wikipedia‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/miracl/miracl-corpus.","url":"https://huggingface.co/datasets/miracl/miracl-corpus","creator_name":"MIRACL","creator_url":"https://huggingface.co/miracl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"eur-lex-sum","keyword":"multilingual","description":"The EUR-Lex-Sum dataset is a multilingual resource intended for text summarization in the legal domain.\nIt is based on human-written summaries of legal acts issued by the European Union.\nIt distinguishes itself by introducing a smaller set of high-quality human-written samples,\neach of which have much longer references (and summaries!) than comparable datasets.\nAdditionally, the underlying legal acts provide a challenging domain-specific application to legal texts,\nwhich are so far underrepresented in non-English languages.\nFor each legal act, the sample can be available in up to 24 languages\n(the officially recognized languages in the European Union);\nthe validation and test samples consist entirely of samples available in all languages,\nand are aligned across all languages at the paragraph level.","url":"https://huggingface.co/datasets/dennlinger/eur-lex-sum","creator_name":"Dennis Aumiller","creator_url":"https://huggingface.co/dennlinger","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","summarization","found","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"eur-lex-sum","keyword":"multilingual","description":"The EUR-Lex-Sum dataset is a multilingual resource intended for text summarization in the legal domain.\nIt is based on human-written summaries of legal acts issued by the European Union.\nIt distinguishes itself by introducing a smaller set of high-quality human-written samples,\neach of which have much longer references (and summaries!) than comparable datasets.\nAdditionally, the underlying legal acts provide a challenging domain-specific application to legal texts,\nwhich are so far underrepresented in non-English languages.\nFor each legal act, the sample can be available in up to 24 languages\n(the officially recognized languages in the European Union);\nthe validation and test samples consist entirely of samples available in all languages,\nand are aligned across all languages at the paragraph level.","url":"https://huggingface.co/datasets/dennlinger/eur-lex-sum","creator_name":"Dennis Aumiller","creator_url":"https://huggingface.co/dennlinger","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","summarization","found","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"arabic_pos_dialect","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Arabic POS Dialect\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset was created to support part of speech (POS) tagging in dialects of Arabic. It contains sets of 350 manually segmented and POS tagged tweets for each of four dialects: Egyptian, Levantine, Gulf, and Maghrebi.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset can be used to train a model for Arabic token segmentation and part of speech tagging in Arabic dialects. Success on this task is typically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QCRI/arabic_pos_dialect.","url":"https://huggingface.co/datasets/QCRI/arabic_pos_dialect","creator_name":"Qatar Computing Research Institute","creator_url":"https://huggingface.co/QCRI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","part-of-speech","expert-generated","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"EU_Wikipedias","keyword":"multilingual","description":"Wikipedia dataset containing cleaned articles of all languages.\nThe datasets are built from the Wikipedia dump\n(https://dumps.wikimedia.org/) with one split per language. Each example\ncontains the content of one full Wikipedia article with cleaning to strip\nmarkdown and unwanted sections (references, etc.).","url":"https://huggingface.co/datasets/joelniklaus/EU_Wikipedias","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","other","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"wildchat-stratified-sample","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWildChat Stratified Sample\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains a stratified sample of 263 GPT-4 conversations (347 total turns) from the WildChat dataset. The sample was carefully selected to ensure balanced representation across conversation turn positions and user message lengths.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Conversations: 263\nTotal Turns/Rows: 347\nAverage Turns per Conversation: 1.32\nConversation Length: 1-5 turns (conversations with >5 turns excluded)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Avinaash/wildchat-stratified-sample.","url":"https://huggingface.co/datasets/Avinaash/wildchat-stratified-sample","creator_name":"Anand","creator_url":"https://huggingface.co/Avinaash","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"common_voice_13_0_dv_preprocessed","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Common Voice Corpus 13.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Common Voice dataset consists of a unique MP3 and corresponding text file. \nMany of the 27141 recorded hours in the dataset also include demographic metadata like age, sex, and accent \nthat can help improve the accuracy of speech recognition engines.\nThe dataset currently consists of 17689 validated hours in 108 languages, but more voices and languages are always added. \nTake a look at the Languages page to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ferno22/common_voice_13_0_dv_preprocessed.","url":"https://huggingface.co/datasets/ferno22/common_voice_13_0_dv_preprocessed","creator_name":"Ant","creator_url":"https://huggingface.co/ferno22","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","crowdsourced","crowdsourced","multilingual","extended|common_voice"],"keywords_longer_than_N":true},
	{"name":"RyokoAI_ShareGPT52K","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for ShareGPT52K90K\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\nThis repository now contains the new 90K conversations version. The previous 52K may\nbe found in the old/ directory.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\ntext-generation\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThis dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K.","url":"https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K","creator_name":"ab10","creator_url":"https://huggingface.co/botp","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Spanish","German","multilingual"],"keywords_longer_than_N":true},
	{"name":"bible_para","keyword":"multilingual","description":"This is a multilingual parallel corpus created from translations of the Bible compiled by Christos Christodoulopoulos and Mark Steedman.\n\n102 languages, 5,148 bitexts\ntotal number of files: 107\ntotal number of tokens: 56.43M\ntotal number of sentence fragments: 2.84M","url":"https://huggingface.co/datasets/Helsinki-NLP/bible_para","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"CulturaY","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCulturaY: A Large Cleaned Multilingual Dataset of 75 Languages\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nFrom the team that brought you CulturaX, we present CulturaY, another substantial multilingual dataset of 15TB (uncompressed)/3TB (zstd-compressed) that applies the same dataset cleaning methodology to the HPLT v1.1 dataset. \nPlease note that HPLT v1.2 has also been released and is an alternative verison with different cleaning methodolgies. \nThis data was used in part to train our SOTA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Viet-Mistral/CulturaY.","url":"https://huggingface.co/datasets/Viet-Mistral/CulturaY","creator_name":"Vietnamese Mistral","creator_url":"https://huggingface.co/Viet-Mistral","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"random_streetview_images_pano_v0.0.2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for panoramic street view images (v.0.0.2)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe random streetview images dataset are labeled, panoramic images scraped from randomstreetview.com. Each image shows a location\naccessible by Google Streetview that has been roughly combined to provide ~360 degree view of a single location. The dataset was designed with the intent to geolocate an image purely based on its visual content.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nNone as of now!‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stochastic/random_streetview_images_pano_v0.0.2.","url":"https://huggingface.co/datasets/stochastic/random_streetview_images_pano_v0.0.2","creator_name":"Winson Truong","creator_url":"https://huggingface.co/stochastic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","multi-label-image-classification","expert-generated","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"autshumato","keyword":"multilingual","description":"Multilingual information access is stipulated in the South African constitution. In practise, this\nis hampered by a lack of resources and capacity to perform the large volumes of translation\nwork required to realise multilingual information access. One of the aims of the Autshumato\nproject is to develop machine translation systems for three South African languages pairs.","url":"https://huggingface.co/datasets/nwu-ctext/autshumato","creator_name":"Centre for Text Technology - Humanities - NWU","creator_url":"https://huggingface.co/nwu-ctext","license_name":"Creative Commons Attribution 2.5","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.5.html","language":null,"first_N":5,"first_N_keywords":["translation","expert-generated","expert-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"masakhanews","keyword":"multilingual","description":"MasakhaNEWS is the largest publicly available dataset for news topic classification in 16 languages widely spoken in Africa.\n\nThe languages are:\n- Amharic (amh)\n- English (eng)\n- French (fra)\n- Hausa (hau)\n- Igbo (ibo)\n- Lingala (lin)\n- Luganda (lug)\n- Oromo (orm)\n- Nigerian Pidgin (pcm)\n- Rundi (run)\n- chShona (sna)\n- Somali (som)\n- Kiswahili (swƒÖ)\n- Tigrinya (tir)\n- isiXhosa (xho)\n- Yor√πb√° (yor)\n\nThe train/validation/test sets are available for all the 16 languages.\n\nFor more details see *** arXiv link **","url":"https://huggingface.co/datasets/masakhane/masakhanews","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","expert-generated","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-hi-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (hi) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (hi) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-hi-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-hi-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Hindi"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p2-fineweb2-remaining","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P2\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P2 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p2-fineweb2-remaining.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p2-fineweb2-remaining","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","English","mit","arxiv:2509.06888","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p1-fineweb2-langs","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P1\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P1 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","feature-extraction","multilingual","mit","arxiv:2509.06888"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p1-fineweb2-langs","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P1\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P1 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","feature-extraction","multilingual","mit","arxiv:2509.06888"],"keywords_longer_than_N":true},
	{"name":"Fact-Completion","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\nHomepage: https://bit.ly/ischool-berkeley-capstone\nRepository: https://github.com/daniel-furman/Capstone\nPoint of Contact: daniel_furman@berkeley.edu\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis is the dataset for Polyglot or Not?: Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models.\n\n\t\n\t\t\n\t\n\t\n\t\tTest Description\n\t\n\n Given a factual association such as The capital of France is Paris, we determine whether a model adequately \"knows\" this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Polyglot-or-Not/Fact-Completion.","url":"https://huggingface.co/datasets/Polyglot-or-Not/Fact-Completion","creator_name":"Polyglot-or-Not","creator_url":"https://huggingface.co/Polyglot-or-Not","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","expert-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"QuranExe","keyword":"multilingual","description":"This dataset contains the exegeses/tafsirs (ÿ™ŸÅÿ≥Ÿäÿ± ÿßŸÑŸÇÿ±ÿ¢ŸÜ) of the holy Quran in arabic by 8 exegetes.\nThis is a non Official dataset. It have been scrapped from the Quran.com Api\nThis dataset contains 49888 records with +14 Million words. 8 records per Quranic verse\nUsage Example :\nfrom datasets import load_dataset\n\ntafsirs = load_dataset(\"mustapha/QuranExe\")\n\n","url":"https://huggingface.co/datasets/mustapha/QuranExe","creator_name":"AJEGHRIR mustapha","creator_url":"https://huggingface.co/mustapha","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","sentence-similarity","language-modeling","masked-language-modeling"],"keywords_longer_than_N":true},
	{"name":"miracl-te-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (te) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-te-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-te-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-te-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-te-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Telugu"],"keywords_longer_than_N":true},
	{"name":"miracl-bn-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (bn) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-bn-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-bn-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-bn-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-bn-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Bengali"],"keywords_longer_than_N":true},
	{"name":"bigbench","keyword":"multilingual","description":"The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to\nprobe large language models, and extrapolate their future capabilities.","url":"https://huggingface.co/datasets/google/bigbench","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","text-generation","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"IndicQA","keyword":"multilingual","description":"\\","url":"https://huggingface.co/datasets/ai4bharat/IndicQA","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","expert-generated","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"IndicCOPA","keyword":"multilingual","description":"\\","url":"https://huggingface.co/datasets/ai4bharat/IndicCOPA","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"xP3mt","keyword":"multilingual","description":"xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks. It is used for the training of BLOOMZ and mT0, multilingual language models capable of following human instructions in dozens of languages zero-shot.","url":"https://huggingface.co/datasets/bigscience/xP3mt","creator_name":"BigScience Workshop","creator_url":"https://huggingface.co/bigscience","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Akan"],"keywords_longer_than_N":true},
	{"name":"mc4_legal","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for MC4_Legal: A Corpus Covering the Legal Part of MC4 for European Languages\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains large text resources (~133GB in total) from mc4 filtered for legal data that can be used for pretraining language models.\nUse the dataset like this:\nfrom datasets import load_dataset\ndataset = load_dataset(\"joelito/mc4_legal\", \"de\", split='train', streaming=True)\n\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset supports the task of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/joelniklaus/mc4_legal.","url":"https://huggingface.co/datasets/joelniklaus/mc4_legal","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","other","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"gsm8k-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tgsm8k-ru\n\t\n\nTranslated version of gsm8k dataset into Russian.\n","url":"https://huggingface.co/datasets/d0rj/gsm8k-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["crowdsourced","translated","monolingual","gsm8k","Russian"],"keywords_longer_than_N":true},
	{"name":"miracl-id-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (id) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-id-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-id-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-id-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-id-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Indonesian"],"keywords_longer_than_N":true},
	{"name":"eurlex_resources","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for EurlexResources: A Corpus Covering the Largest EURLEX Resources\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains large text resources (~179GB in total) from EURLEX that can be used for pretraining language models.\nUse the dataset like this:\nfrom datasets import load_dataset\nconfig = \"de_caselaw\" # {lang}_{resource}\ndataset = load_dataset(\"joelito/eurlex_resources\", config, split='train', streaming=True) \n\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/joelniklaus/eurlex_resources.","url":"https://huggingface.co/datasets/joelniklaus/eurlex_resources","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["fill-mask","other","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"miracl-ko-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (ko) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-ko-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-ko-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-ko-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-ko-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Korean"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-zh-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (zh) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (zh) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-zh-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-zh-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","multilingual","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"x-stance","keyword":"multilingual","description":"The x-stance dataset contains more than 150 political questions, and 67k comments written by candidates on those questions. The comments are partly German, partly French and Italian. The data have been extracted from the Swiss voting advice platform Smartvote.","url":"https://huggingface.co/datasets/strombergnlp/x-stance","creator_name":"Str√∏mberg NLP","creator_url":"https://huggingface.co/strombergnlp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","fact-checking","crowdsourced","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"Sinhala-English-Code-Mixed-Code-Switched-Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tSinhala-English-Code-Mixed-Code-Switched-Dataset\n\t\n\nThis dataset contains 10,000 comments that have been annotated at the sentence level for sentiment analysis, humor detection, hate speech detection, aspect identification, and language identification.\nThe following is the tag scheme.\n\nSentiment -  Positive, Negative, Neutral,  Conflict\nHumor - Humorous, Non humorous\nHate Speech - Hate-Inducing, Abusive, Not offensive\nAspect - Network, Billing or Price, Package, Customer Service, Data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NLPC-UOM/Sinhala-English-Code-Mixed-Code-Switched-Dataset.","url":"https://huggingface.co/datasets/NLPC-UOM/Sinhala-English-Code-Mixed-Code-Switched-Dataset","creator_name":"The National Languages Processing Centre","creator_url":"https://huggingface.co/NLPC-UOM","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","hate-speech-detection","language-identification","multilingual"],"keywords_longer_than_N":true},
	{"name":"xP3x-sample","keyword":"multilingual","description":"A multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual commonsense reasoning capabilities.","url":"https://huggingface.co/datasets/Muennighoff/xP3x-sample","creator_name":"Niklas Muennighoff","creator_url":"https://huggingface.co/Muennighoff","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k-Japanese","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k (Japanese Translation)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a Japanese translation of the Finance-Instruct-500k dataset, created using OpenAI's GPT-4o-mini via the Batch API.\n\n\t\n\t\t\n\t\tOriginal Dataset\n\t\n\n\nOriginal Author: Joseph G. Flowers\nOriginal Dataset: Josephgflowers/Finance-Instruct-500k\nLicense: Apache 2.0\n\n\n\t\n\t\t\n\t\tTranslation Details\n\t\n\n\nTranslation Model: GPT-4o-mini (OpenAI)\nTranslation Method: OpenAI Batch API with human verifications\nDate: 2025‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ronantakizawa/Finance-Instruct-500k-Japanese.","url":"https://huggingface.co/datasets/ronantakizawa/Finance-Instruct-500k-Japanese","creator_name":"Ronan Takizawa","creator_url":"https://huggingface.co/ronantakizawa","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","translation","Japanese","English"],"keywords_longer_than_N":true},
	{"name":"mr-tydi","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMr. TyDi is a multi-lingual benchmark dataset built on TyDi, covering eleven typologically diverse languages. It is designed for monolingual retrieval, specifically to evaluate ranking with learned dense representations.\nThis dataset stores the queries, judgements, and example training data of Mr. TyDi. To access the corpus, please refer to castorini/mr-tydi-corpus.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nThe only configuration here is the language, \nFor each language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/castorini/mr-tydi.","url":"https://huggingface.co/datasets/castorini/mr-tydi","creator_name":"Castorini","creator_url":"https://huggingface.co/castorini","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multilingual","Arabic","Bengali","English"],"keywords_longer_than_N":true},
	{"name":"miracl-de-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (de) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-de-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-de-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-de-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-de-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","German"],"keywords_longer_than_N":true},
	{"name":"sbb-dc-ocr","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Berlin State Library OCR data\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nThe digital collections of the SBB contain 153,942 digitized works from the time period of 1470 to 1945.\n\n\nAt the time of publication, 28,909 works have been OCR-processed resulting in 4,988,099 full-text pages.\nFor each page with OCR text, the language has been determined by langid (Lui/Baldwin 2012).\n\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\nlanguage-modeling: this dataset has the potential to be used‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SBB/sbb-dc-ocr.","url":"https://huggingface.co/datasets/SBB/sbb-dc-ocr","creator_name":"Staatsbibliothek zu Berlin - Preu√üischer Kulturbesitz","creator_url":"https://huggingface.co/SBB","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","text-generation","masked-language-modeling","language-modeling","machine-generated"],"keywords_longer_than_N":true},
	{"name":"humsetbias","keyword":"multilingual","description":"nlp-thedeep/humsetbias dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/nlp-thedeep/humsetbias","creator_name":"TheDEEP NLP","creator_url":"https://huggingface.co/nlp-thedeep","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","text-retrieval","token-classification","multi-label-classification","expert-generated"],"keywords_longer_than_N":true},
	{"name":"dialogsum-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tDataset Card for DIALOGSum Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tLinks\n\t\n\n\nHomepage: https://aclanthology.org/2021.findings-acl.449\nRepository: https://github.com/cylnlp/dialogsum\nPaper: https://aclanthology.org/2021.findings-acl.449\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nDialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 (Plus 100 holdout data for topic generation) dialogues with corresponding manually labeled summaries and topics.\n\n\t\n\t\n\t\n\t\tLanguages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/dialogsum-ru.","url":"https://huggingface.co/datasets/d0rj/dialogsum-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["summarization","text-generation","expert-generated","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"miracl-sw-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (sw) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-sw-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-sw-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-sw-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-sw-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Swahili"],"keywords_longer_than_N":true},
	{"name":"euronews","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Europeana Newspapers\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Splits\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tCuration Rationale\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tSource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/euronews.","url":"https://huggingface.co/datasets/community-datasets/euronews","creator_name":"Community Datasets","creator_url":"https://huggingface.co/community-datasets","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","expert-generated","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"blbooks","keyword":"multilingual","description":"A dataset comprising of text created by OCR from the 49,455 digitised books, equating to 65,227 volumes (25+ million pages), published between c. 1510 - c. 1900.\nThe books cover a wide range of subject areas including philosophy, history, poetry and literature.","url":"https://huggingface.co/datasets/TheBritishLibrary/blbooks","creator_name":"British Library","creator_url":"https://huggingface.co/TheBritishLibrary","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","fill-mask","other","language-modeling","masked-language-modeling"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Common Voice Corpus 10.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Common Voice dataset consists of a unique MP3 and corresponding text file. \nMany of the 20817 recorded hours in the dataset also include demographic metadata like age, sex, and accent \nthat can help improve the accuracy of speech recognition engines.\nThe dataset currently consists of 15234 validated hours in 96 languages, but more voices and languages are always added. \nTake a look at the Languages page to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gogogogo-1/test.","url":"https://huggingface.co/datasets/gogogogo-1/test","creator_name":"mhj","creator_url":"https://huggingface.co/gogogogo-1","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","crowdsourced","crowdsourced","multilingual","extended|common_voice"],"keywords_longer_than_N":true},
	{"name":"covid19_emergency_event","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for EXCEPTIUS Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset presents a new corpus of legislative documents from 8 European countries (Beglium, France, Hunary, Italy, Netherlands, Norway, Poland, UK) in 7 languages (Dutch, English, French, Hungarian, Italian, Norwegian Bokm√•l, Polish) manually annotated for exceptional measures against COVID-19. The annotation was done on the sentence level.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset can be used for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/joelniklaus/covid19_emergency_event.","url":"https://huggingface.co/datasets/joelniklaus/covid19_emergency_event","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multi-label-classification","found","other","found"],"keywords_longer_than_N":true},
	{"name":"humset","keyword":"multilingual","description":"HumSet is a novel and rich multilingual dataset of humanitarian response documents annotated by experts in the humanitarian response community. HumSet is curated by humanitarian analysts and covers various disasters around the globe that occurred from 2018 to 2021 in 46 humanitarian response projects. The dataset consists of approximately 17K annotated documents in three languages of English, French, and Spanish, originally taken from publicly-available resources. For each document, analysts have identified informative snippets (entries) in respect to common humanitarian frameworks, and assigned one or many classes to each entry. See the our paper for details.","url":"https://huggingface.co/datasets/nlp-thedeep/humset","creator_name":"TheDEEP NLP","creator_url":"https://huggingface.co/nlp-thedeep","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","token-classification","multi-label-classification","expert-generated"],"keywords_longer_than_N":true},
	{"name":"SWEbenchVerifiedRR","keyword":"multilingual","description":"\n  SWEbenchVerifiedRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSoftware Issue Localization for SWE-bench Verified\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://openai.com/index/introducing-swe-bench-verified/\n\n\n\t\n\nSource datasets:\n\ntarsur909/mteb-swe-bench-verified-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SWEbenchVerifiedRR.","url":"https://huggingface.co/datasets/mteb/SWEbenchVerifiedRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-swe-bench-verified-reranking","code"],"keywords_longer_than_N":true},
	{"name":"wikipedia-persons-masked","keyword":"multilingual","description":"\n\t\n\t\t\n\t\twikipedia persons masked: A filtered version of the wikipedia dataset, with only pages of people\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nContains ~70k pages from wikipedia, each describing a person. For each page, the person described in the text\nis masked with a \n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset supports the tasks of fill-mask, but can also be used for other tasks such as question answering,\ne.g. \"Who is \n\n\t\n\t\t\n\t\tLanguages\n\t\n\nenglish only\n\n\t\n\t\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rcds/wikipedia-persons-masked.","url":"https://huggingface.co/datasets/rcds/wikipedia-persons-masked","creator_name":"Institute for Public Sector Transformation IPST - Digital Sustainability Lab DSL","creator_url":"https://huggingface.co/rcds","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","other","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"Code-170k-baoule","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-baoule is a groundbreaking dataset containing 176,999 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Baoule, making coding education accessible to Baoule speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n176,999 high-quality conversations about programming and coding\nPure Baoule language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-baoule.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-baoule","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Baoul√©","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"iva_mt_wslot-exp","keyword":"machine translation","description":"\\","url":"https://huggingface.co/datasets/cartesinus/iva_mt_wslot-exp","creator_name":"Marcin Sowanski","creator_url":"https://huggingface.co/cartesinus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","English","Polish","German","Spanish"],"keywords_longer_than_N":true},
	{"name":"x_dataset_39483","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_39483.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_39483","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0201171","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_0201171.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_0201171","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_219","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chris241/reddit_dataset_219.","url":"https://huggingface.co/datasets/chris241/reddit_dataset_219","creator_name":"ch","creator_url":"https://huggingface.co/chris241","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"amazon_counterfactual","keyword":"multilingual","description":"\n  AmazonCounterfactualClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA collection of Amazon customer reviews annotated for counterfactual detection pair classification.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\nReference\nhttps://arxiv.org/abs/2104.06893\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"AmazonCounterfactualClassification\"])‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/amazon_counterfactual.","url":"https://huggingface.co/datasets/mteb/amazon_counterfactual","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","multilingual","German","English"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_255","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sm4rtdev/reddit_dataset_255.","url":"https://huggingface.co/datasets/sm4rtdev/reddit_dataset_255","creator_name":"ElonScott","creator_url":"https://huggingface.co/sm4rtdev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0111208","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_0111208.","url":"https://huggingface.co/datasets/william-1111/x_dataset_0111208","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"neuclir1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for NeuCLIR1\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is the dataset created for the TREC NeuCLIR Track. The collection is designed to be similar to HC4, and a large portion of documents from HC4 are ported to this collection.\nThe documents are Web pages from Common Crawl in Chinese, Persian, and Russian.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nChinese\nPersian\nRussian\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n\n\t\n\t\t\nSplit\nNum Documents\n\n\n\t\t\nfas (Persian)\n2.2M\n\n\nrus (Russian)\n4.6M‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neuclir/neuclir1.","url":"https://huggingface.co/datasets/neuclir/neuclir1","creator_name":"NeuCLIR TREC Track","creator_url":"https://huggingface.co/neuclir","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"NQ-NL","keyword":"translated","description":"\n  NQ-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNQ-NL is a translation of NQ\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\n\n\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-nq\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NQ-NL\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nTo learn more about how‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NQ-NL.","url":"https://huggingface.co/datasets/mteb/NQ-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","translated","mteb/nq"],"keywords_longer_than_N":true},
	{"name":"x_dataset_020629","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_020629.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_020629","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0601119","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_0601119.","url":"https://huggingface.co/datasets/john-1111/x_dataset_0601119","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_104","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smmrokn/reddit_dataset_104.","url":"https://huggingface.co/datasets/smmrokn/reddit_dataset_104","creator_name":"Mohammad Mahdi","creator_url":"https://huggingface.co/smmrokn","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_40563","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_40563.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_40563","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_2.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_2","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0208165","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_0208165.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_0208165","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_260222","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_260222.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_260222","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"CathayPacific_dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCATHAY-PACIFIC-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from Cathay Pacific technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation, with a specific focus on international airline operations in the Asia-Pacific region.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/CathayPacific_dataset.","url":"https://huggingface.co/datasets/Davidsv/CathayPacific_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"RuNLUIntentClassification","keyword":"multilingual","description":"\n  RuNLUIntentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nContains natural language data for human-robot interaction in home domain which we collected and annotated for evaluating NLU Services/platforms.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomainsNone\n\n\nReference\nhttps://arxiv.org/abs/1903.05566\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RuNLUIntentClassification.","url":"https://huggingface.co/datasets/mteb/RuNLUIntentClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","intent-classification","human-annotated","multilingual","Russian"],"keywords_longer_than_N":true},
	{"name":"x_dataset_117","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_117.","url":"https://huggingface.co/datasets/gk4u/x_dataset_117","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_479243","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_479243.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_479243","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_11627","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_11627.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_11627","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_146","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_146.","url":"https://huggingface.co/datasets/James096/x_dataset_146","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"ShareGPT-X","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nShareGPT-X is an expanded, snapshot of ~92K (ChatGPT) one-to-one human & LLM conversations harvested from X.com (formerly Twitter).The corpus spans January 2024 ‚Üí present (last ingest 2025-05) and is built entirely from public \"share\" links that users posted to their timelines.Each thread contains the original user prompt plus the assistant‚Äôs reply; no system prompts or metadata are exposed.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\ntext-generation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DSULT-Core/ShareGPT-X.","url":"https://huggingface.co/datasets/DSULT-Core/ShareGPT-X","creator_name":"DeSULT","creator_url":"https://huggingface.co/DSULT-Core","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","cc0-1.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"xsimplusplus","keyword":"multilingual","description":"xSIM++ is an extension of xSIM. In comparison to xSIM, this evaluates using target-side data with additional synthetic, hard-to-distinguish examples. You can find more details about it in the publication: xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages.\n","url":"https://huggingface.co/datasets/jaygala24/xsimplusplus","creator_name":"Jay Gala","creator_url":"https://huggingface.co/jaygala24","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["derived","multilingual","Achinese","Mesopotamian Arabic","Ta'izzi-Adeni Arabic"],"keywords_longer_than_N":true},
	{"name":"znanio-audios","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Audio\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 3,417 educational audio files from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-audios.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-audios","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["audio-classification","automatic-speech-recognition","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"znanio-audios","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Audio\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 3,417 educational audio files from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-audios.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-audios","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["audio-classification","automatic-speech-recognition","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"x_dataset_37411","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_37411.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_37411","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_070287","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_070287.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_070287","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_070630","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_070630.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_070630","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_2447","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_2447.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_2447","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0511250","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_0511250.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_0511250","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_14253","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_14253.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_14253","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"darja-en-translation","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDarja-English Translation Dataset\n\t\n\nThis dataset contains translations from Algerian Darja (Arabic dialect) to English. The dataset includes sentences in Darja along with their corresponding English translations.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nDarja (Algerian Arabic dialect)\nEnglish\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of two fields:\n\ninput: Sentence in Darja\ntranslation: Corresponding translation in English\n\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is licensed under the MIT License.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ademchaoua/darja-en-translation.","url":"https://huggingface.co/datasets/ademchaoua/darja-en-translation","creator_name":"adem chaoua","creator_url":"https://huggingface.co/ademchaoua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","text-classification","summarization","text-generation","Arabic"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0101118","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_0101118.","url":"https://huggingface.co/datasets/william-1111/x_dataset_0101118","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_172","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/coldmind/reddit_dataset_172.","url":"https://huggingface.co/datasets/coldmind/reddit_dataset_172","creator_name":"Toc","creator_url":"https://huggingface.co/coldmind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_14","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_14.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_14","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_204","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bdkzk/reddit_dataset_204.","url":"https://huggingface.co/datasets/bdkzk/reddit_dataset_204","creator_name":"bdkzkfosjfksjckzmx62737","creator_url":"https://huggingface.co/bdkzk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_57071","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rainbowbridge/x_dataset_57071.","url":"https://huggingface.co/datasets/rainbowbridge/x_dataset_57071","creator_name":"Rain","creator_url":"https://huggingface.co/rainbowbridge","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_20589","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_20589.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_20589","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_223","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_223.","url":"https://huggingface.co/datasets/James096/x_dataset_223","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_91","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/coldmind/reddit_dataset_91.","url":"https://huggingface.co/datasets/coldmind/reddit_dataset_91","creator_name":"Toc","creator_url":"https://huggingface.co/coldmind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_20","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_20.","url":"https://huggingface.co/datasets/suul999922/x_dataset_20","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus-mmteb","keyword":"multilingual","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\nLoading example:\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"davidstap/biblenlp-corpus-mmteb\", \"eng-arb\", trust_remote_code=True)\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 28723\n    })\n    validation: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 1578\n    })‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb.","url":"https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","expert-generated","translation","multilingual","Arifama-Miniafia"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44_","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jacksss123/x_dataset_44_.","url":"https://huggingface.co/datasets/Jacksss123/x_dataset_44_","creator_name":"Tony","creator_url":"https://huggingface.co/Jacksss123","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"TheGuardian-Articles","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tCuration Rationale\n\t\n\nThe dataset was curated to facilitate research and development in natural language processing tasks such as text classification and information extraction from news articles.\n\n\t\n\t\t\n\t\tSource Data\n\t\n\n\n\t\n\t\t\n\t\tInitial Data Collection and Normalization\n\t\n\nArticles and similar content were scraped from theguardian.com website.\n\n\t\n\t\t\n\t\tEncoding\n\t\n\nThe primary language of the dataset is English, but it may contain content in other languages. It‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Stefan171/TheGuardian-Articles.","url":"https://huggingface.co/datasets/Stefan171/TheGuardian-Articles","creator_name":"Stefan Carter","creator_url":"https://huggingface.co/Stefan171","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","summarization","feature-extraction","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"x_dataset_18","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_18.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_18","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_9","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/reddit_dataset_9.","url":"https://huggingface.co/datasets/arrmlet/reddit_dataset_9","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_85","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tensorshield/reddit_dataset_85.","url":"https://huggingface.co/datasets/tensorshield/reddit_dataset_85","creator_name":"Tensor Shield","creator_url":"https://huggingface.co/tensorshield","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_144","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ashikshaffi08/reddit_dataset_144.","url":"https://huggingface.co/datasets/ashikshaffi08/reddit_dataset_144","creator_name":"Ashik","creator_url":"https://huggingface.co/ashikshaffi08","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_7480","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_7480.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_7480","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"DBPedia-PL","keyword":"translated","description":"\n  DBPedia-PL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\n\n\nReference\nhttps://github.com/iai-group/DBpedia-Entity/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"DBPedia-PL\"])\nevaluator = mteb.MTEB(task)\n\nmodel =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DBPedia-PL.","url":"https://huggingface.co/datasets/mteb/DBPedia-PL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/dbpedia","Polish"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-NL","keyword":"translated","description":"\n  FiQA2018-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nFinancial Opinion Mining and Question Answering. FiQA2018-NL is a Dutch translation\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Non-fiction\n\n\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-fiqa\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"FiQA2018-NL\"])\nevaluator = mteb.MTEB(task)\n\nmodel =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FiQA2018-NL.","url":"https://huggingface.co/datasets/mteb/FiQA2018-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","translated","mteb/fiqa"],"keywords_longer_than_N":true},
	{"name":"x_dataset_34576","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_34576.","url":"https://huggingface.co/datasets/icedwind/x_dataset_34576","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_103502","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_103502.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_103502","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mmBERT-midtraining-data","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Mid-training Data\n\t\n\n\n\n\n\n\nPhase 2 of 3: High-quality mid-training data mixture (600B tokens) with context extension to 8192 tokens.\n\nThis dataset contains the mid-training phase data used to train all mmBERT encoder models. This phase focuses on higher quality data sources and extends the context length from 1024 to 8192 tokens. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.\n\n\t\n\t\t\n\t\tüìä Data Composition\n\t\n\n\n\t\n\t\t\nData Source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-midtraining-data.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-midtraining-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_34","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zengsdfew/reddit_dataset_34.","url":"https://huggingface.co/datasets/zengsdfew/reddit_dataset_34","creator_name":"miner3","creator_url":"https://huggingface.co/zengsdfew","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_102","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GSKCM24/reddit_dataset_102.","url":"https://huggingface.co/datasets/GSKCM24/reddit_dataset_102","creator_name":"GUNEET SINGH KHURANA","creator_url":"https://huggingface.co/GSKCM24","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_42","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_42.","url":"https://huggingface.co/datasets/James096/reddit_dataset_42","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"tool-use-llama-format","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOpen Paws Tool Use Llama Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Tool Use Data\nFormat: JSONL (JSON Lines)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Open Paws\nLicense: Apache 2.0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/tool-use-llama-format.","url":"https://huggingface.co/datasets/open-paws/tool-use-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"x_dataset_172","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/coldmind/x_dataset_172.","url":"https://huggingface.co/datasets/coldmind/x_dataset_172","creator_name":"Toc","creator_url":"https://huggingface.co/coldmind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_9","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_9.","url":"https://huggingface.co/datasets/suul999922/x_dataset_9","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Samael1976","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Samael1976","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"fastcup-highlights","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Fastcup.net Highlights\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about 85,488 video clips from the gaming platform Fastcup.net, with 78,143 clips from Counter-Strike 2 and 7,345 clips from Counter-Strike: Global Offensive. The clips showcase gameplay highlights and include detailed metadata such as player statistics, weapon information, and engagement metrics. The total size of raw video content is approximately 34 TB.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/fastcup-highlights.","url":"https://huggingface.co/datasets/nyuuzyou/fastcup-highlights","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"RTE3","keyword":"multilingual","description":"\n  RTE3\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRecognising Textual Entailment Challenge (RTE-3) aim to provide the NLP community with a benchmark to test progress in recognizing textual entailment\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Web, Encyclopaedic, Written\nReference\nhttps://aclanthology.org/W07-1401/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"RTE3\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RTE3.","url":"https://huggingface.co/datasets/mteb/RTE3","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","natural-language-inference","expert-annotated","multilingual"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretraining-data-chunk0","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk0.","url":"https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk0","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vietnambk82/reddit_dataset_44.","url":"https://huggingface.co/datasets/Vietnambk82/reddit_dataset_44","creator_name":"Bui Viet Nam","creator_url":"https://huggingface.co/Vietnambk82","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"PD12M-ru","keyword":"translated","description":"Translated captions from Spawning/PD12M into Russian using Google Translate.\n","url":"https://huggingface.co/datasets/d0rj/PD12M-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["image-feature-extraction","image-to-text","text-to-image","translated","Spawning/PD12M"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_72","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_72.","url":"https://huggingface.co/datasets/James096/reddit_dataset_72","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_255","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sm4rtdev/x_dataset_255.","url":"https://huggingface.co/datasets/sm4rtdev/x_dataset_255","creator_name":"ElonScott","creator_url":"https://huggingface.co/sm4rtdev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mala-monolingual-dedup","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMaLA Corpus: Massive Language Adaptation Corpus\n\t\n\nThis is a deduplicated version after minhash and exact hash deduplication.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MaLA Corpus (Massive Language Adaptation) is a comprehensive, multilingual dataset designed to support the continual pre-training of large language models. It covers 939 languages and consists of over 74 billion tokens, making it one of the largest datasets of its kind. With a focus on improving the representation of low-resource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MaLA-LM/mala-monolingual-dedup.","url":"https://huggingface.co/datasets/MaLA-LM/mala-monolingual-dedup","creator_name":"MaLA-LM","creator_url":"https://huggingface.co/MaLA-LM","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","odc-by","100M - 1B","arrow","Text"],"keywords_longer_than_N":true},
	{"name":"x_dataset_26008","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_26008.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_26008","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"minds14","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMInDS-14\n\t\n\nMINDS-14 is training and evaluation resource for intent detection task with spoken data. It covers 14 \nintents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties.\n\n\t\n\t\t\n\t\tExample\n\t\n\nMInDS-14 can be downloaded and used as follows:\nfrom datasets import load_dataset\n\nminds_14 = load_dataset(\"PolyAI/minds14\", \"fr-FR\") # for French\n# to download all data for multi-lingual fine-tuning uncomment following‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PolyAI/minds14.","url":"https://huggingface.co/datasets/PolyAI/minds14","creator_name":"PolyAI","creator_url":"https://huggingface.co/PolyAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","keyword-spotting","expert-generated","crowdsourced","machine-generated"],"keywords_longer_than_N":true},
	{"name":"MixtureVitae-fineweb-permissive-multilingual-2m","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMixtureVitae Fineweb-Permissive-Multilingual-2M: 2 Million Translated Documents Of Permissive Text From Fineweb-edu-2\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a translation of a small subset of the Fineweb-edu-2 dataset. We have filtered to find websites with what we believe are government domain names, international organization domain names like the UN and europa.eu, and creative commons licensed data. While we strongly believe that fair use protects machine learning on webcrawled data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ontocord/MixtureVitae-fineweb-permissive-multilingual-2m.","url":"https://huggingface.co/datasets/ontocord/MixtureVitae-fineweb-permissive-multilingual-2m","creator_name":"Ontocord.AI","creator_url":"https://huggingface.co/ontocord","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","odc-by","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"x_dataset_108","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wavecreator22/x_dataset_108.","url":"https://huggingface.co/datasets/wavecreator22/x_dataset_108","creator_name":"Krovanov","creator_url":"https://huggingface.co/wavecreator22","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_27","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_27.","url":"https://huggingface.co/datasets/suul999922/x_dataset_27","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"OGC_MEGA_MultiDomain_DocRetrieval","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tVisual Document Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is designed for training visual document retrieval models. It combines multiple datasets from the OGC series, Colpali, and LlamaIndex to create the most comprehensive training resource for visual document retrieval tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains structured fields including unique identifiers with string lengths ranging from 45 to 50 characters, search query text with variable lengths between‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/racineai/OGC_MEGA_MultiDomain_DocRetrieval.","url":"https://huggingface.co/datasets/racineai/OGC_MEGA_MultiDomain_DocRetrieval","creator_name":"racine.ai","creator_url":"https://huggingface.co/racineai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","text-retrieval","multilingual","English","French"],"keywords_longer_than_N":true},
	{"name":"dark_thoughts_stakeholders_en_cn","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDark Thoughts Case Studies Dataset (English-Chinese)\n\t\n\nThis dataset contains a bilingual collection of case studies with detailed stakeholder analyses in English and Chinese. Each case study includes structured information about stakeholders and their motivations, along with comprehensive case analysis and solutions.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe dataset consists of 344,580 case studies in English and in Chinese, with detailed stakeholder analyses and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DataTonic/dark_thoughts_stakeholders_en_cn.","url":"https://huggingface.co/datasets/DataTonic/dark_thoughts_stakeholders_en_cn","creator_name":"Data Tonic (Alignment Lab)","creator_url":"https://huggingface.co/DataTonic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"buddha_oss_dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tÏû•ÏïÑÌï®Í≤Ω Buddha QA Dataset (Complete) / Agama Sutra Buddha QA Dataset\n\t\n\nÌïúÍµ≠Ïñ¥ ÏÑ§Î™Ö | English Description\n\n\n\t\n\t\t\n\t\tÌïúÍµ≠Ïñ¥\n\t\n\n\n\t\n\t\t\n\t\tüôè Í∞úÏöî\n\t\n\nÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ Ïû•ÏïÑÌï®Í≤Ω(Èï∑ÈòøÂê´Á∂ì) Ï†ú1-3Í∂å Ï†ÑÏ≤¥Î•º Í∏∞Î∞òÏúºÎ°ú ÏÉùÏÑ±Îêú ÌïúÍµ≠Ïñ¥ Î∂àÍµê ÏßàÎ¨∏-ÎãµÎ≥Ä Îç∞Ïù¥ÌÑ∞ÏÖãÏûÖÎãàÎã§. GPT-4.1-2025-04-14 Î™®Îç∏Í≥º Í≥†Í∏â Ï∂îÎ°† ÏãúÏä§ÌÖú(fill_thinking.py)ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏÉùÏÑ±ÎêòÏóàÏúºÎ©∞, ÌòÑÎåÄÏù∏Ïù¥ Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÎèÑÎ°ù Ìï¥ÏÑùÎêú Î∂ìÎã§Ïùò Í∞ÄÎ•¥Ïπ®ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.\n\n\t\n\t\t\n\t\tüìä Îç∞Ïù¥ÌÑ∞ÏÖã ÌÜµÍ≥Ñ\n\t\n\n\nÏ¥ù QA Ïåç: 335Í∞ú\nÌõàÎ†® Îç∞Ïù¥ÌÑ∞: 268Í∞ú (80%)\nÍ≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞: 67Í∞ú (20%)\nÏ∂úÏ≤ò: Ïû•ÏïÑÌï®Í≤Ω Ï†ú1-3Í∂å (Ï¥ù 70Í∞ú Í≤ΩÏ†Ñ)\nÏÉùÏÑ± Î™®Îç∏: GPT-4.1-2025-04-14\nÏ∂îÎ°† ÏãúÏä§ÌÖú: fill_thinking.py Í∏∞Î∞ò\n\n\n\t\n\t\t\n\t\tüîß Îç∞Ïù¥ÌÑ∞ ÌòïÏãù\n\t\n\nÍ∞Å Î†àÏΩîÎìúÎäî Îã§ÏùåÍ≥º Í∞ôÏùÄ 3-message‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LeBrony/buddha_oss_dataset.","url":"https://huggingface.co/datasets/LeBrony/buddha_oss_dataset","creator_name":"Î∞±Ïû¨ÌòÑ","creator_url":"https://huggingface.co/LeBrony","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Korean","English","mit"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_192","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mamung/reddit_dataset_192.","url":"https://huggingface.co/datasets/mamung/reddit_dataset_192","creator_name":"ansloth","creator_url":"https://huggingface.co/mamung","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"ParallelPrompt","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tPARALLELPROMPT\n\t\n\nA benchmark dataset of 37,021 parallelizable prompts from real-world LLM conversations, designed for optimizing LLM serving systems through intra-query parallelism.\n\n\t\n\t\t\n\t\tRepository and Resources\n\t\n\n\nDataset: Hugging Face\nCode: GitHub\nPaper: PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries\n\nThe GitHub repository contains:\n\nData curation pipeline\nSchema extraction code\nEvaluation suite for measuring latency and quality\nBaseline implementations‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/forgelab/ParallelPrompt.","url":"https://huggingface.co/datasets/forgelab/ParallelPrompt","creator_name":"Forge Lab","creator_url":"https://huggingface.co/forgelab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","other","English","multilingual","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"amazon_massive_scenario","keyword":"translated","description":"\n  MassiveScenarioClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSpoken\n\nReference\nhttps://arxiv.org/abs/2204.08582\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"MassiveScenarioClassification\"])\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/amazon_massive_scenario.","url":"https://huggingface.co/datasets/mteb/amazon_massive_scenario","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","translated","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_118","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chz1001/reddit_dataset_118.","url":"https://huggingface.co/datasets/chz1001/reddit_dataset_118","creator_name":"z","creator_url":"https://huggingface.co/chz1001","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"ga_multispeaker_audio_transcribed","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tGa Multispeaker Audio Transcribed Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Ga Multispeaker Audio Transcribed dataset is a collection of speech recordings and their transcriptions in Ga, a widely spoken dialect of the Akan language in Ghana. The dataset is designed for training and evaluating automatic speech recognition (ASR) models and other natural language processing (NLP) applications.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: The dataset is derived from the Financial Inclusion Speech Dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/ga_multispeaker_audio_transcribed.","url":"https://huggingface.co/datasets/michsethowusu/ga_multispeaker_audio_transcribed","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","Irish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"GeNTE","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tüö® GeNTE has been superseded by mGeNTE, a new multilingual release of the corpus with additional annotations.\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Card for GeNTE\n\t\n\nHomepage: https://mt.fbk.eu/gente/\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGeNTE (Gender-Neutral Translation Evaluation) is a natural, bilingual corpus designed to benchmark the ability of machine translation systems to generate gender-neutral translations.\nBuilt from European Parliament speeches, GeNTE comprises a subset of the English-Italian portion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FBK-MT/GeNTE.","url":"https://huggingface.co/datasets/FBK-MT/GeNTE","creator_name":"FBK-MT","creator_url":"https://huggingface.co/FBK-MT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","language-modeling","expert-generated","expert-generated"],"keywords_longer_than_N":true},
	{"name":"Ko_Simple_QA","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tÌïúÏòÅ ÏßàÎ¨∏ÎãµÎ≥Ä Îç∞Ïù¥ÌÑ∞ÏÖã\n\t\n\n\n\t\n\t\t\n\t\tÎç∞Ïù¥ÌÑ∞ÏÖã ÏÑ§Î™Ö\n\t\n\nÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ ÏòÅÏñ¥ ÏßàÎ¨∏ÎãµÎ≥Ä ÏåçÍ≥º Í∑∏Ïóê ÎåÄÏùëÌïòÎäî ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ÏúºÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§. \nÍ∞Å Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏\n\nÎ©îÌÉÄÎç∞Ïù¥ÌÑ∞: Ï£ºÏ†ú, ÎãµÎ≥Ä Ïú†Ìòï, Ï∞∏Í≥† URL Îì±Ïùò Ï†ïÎ≥¥\nÏòÅÏñ¥ ÏßàÎ¨∏\nÏòÅÏñ¥ ÎãµÎ≥Ä\nÌïúÍµ≠Ïñ¥ ÏßàÎ¨∏\nÌïúÍµ≠Ïñ¥ ÎãµÎ≥Ä\n\nÏ¥ù 4,265Í∞úÏùò ÏßàÎ¨∏ÎãµÎ≥Ä ÏåçÏù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏúºÎ©∞, CSV ÌòïÏãùÏúºÎ°ú Ï†úÍ≥µ.\n\n\t\n\t\t\n\t\tÏõêÏ≤ú Îç∞Ïù¥ÌÑ∞ Í¥ÄÎ†® ÎßÅÌÅ¨\n\t\n\nhttps://github.com/openai/simple-evals\nhttps://openai.com/index/introducing-simpleqa/\n\n\t\n\t\t\n\t\tÎç∞Ïù¥ÌÑ∞ ÏòàÏãú\n\t\n\n{\n  \"metadata\": {\n    \"topic\": \"Science and technology\",\n    \"answer_type\": \"Person\",\n    \"urls\": [\"https://en.wikipedia.org/wiki/IEEE_Frank_Rosenblatt_Award\"]\n  }‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sionic-ai/Ko_Simple_QA.","url":"https://huggingface.co/datasets/sionic-ai/Ko_Simple_QA","creator_name":"sionic-ai","creator_url":"https://huggingface.co/sionic-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Korean","mit","üá∫üá∏ Region: US","korean","question-answering"],"keywords_longer_than_N":true},
	{"name":"x_dataset_17","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_17.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_17","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0205251","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_0205251.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_0205251","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"MAiDE-up","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel Reviews\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMultilingual Deception Detection of GPT-generated Hotel Reviews. We compare real hotel reviews from Booking with LLM-generated hotel reviews in 10 languages.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text in the dataset is in 10 languages: Chinese, English, French, German, Italian, Romanian, Korean, Russian, Spanish, Turkish\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nTODO‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MichiganNLP/MAiDE-up.","url":"https://huggingface.co/datasets/MichiganNLP/MAiDE-up","creator_name":"LIT @ UMich","creator_url":"https://huggingface.co/MichiganNLP","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","English","Chinese","French"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0701110","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_0701110.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_0701110","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_204","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bdkzk/x_dataset_204.","url":"https://huggingface.co/datasets/bdkzk/x_dataset_204","creator_name":"bdkzkfosjfksjckzmx62737","creator_url":"https://huggingface.co/bdkzk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0708150","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_0708150.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_0708150","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_10","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_10.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_10","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"GSKCM24_Testupload","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GSKCM24/GSKCM24_Testupload.","url":"https://huggingface.co/datasets/GSKCM24/GSKCM24_Testupload","creator_name":"GUNEET SINGH KHURANA","creator_url":"https://huggingface.co/GSKCM24","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"tweet_sentiment_multilingual","keyword":"multilingual","description":"\n  TweetSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA multilingual Sentiment Analysis dataset consisting of tweets in 8 different languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, Written\n\n\nReferencehttps://aclanthology.org/2022.lrec-1.27\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"TweetSentimentClassification\"])\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/tweet_sentiment_multilingual.","url":"https://huggingface.co/datasets/mteb/tweet_sentiment_multilingual","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"IndicCrosslingualSTS","keyword":"multilingual","description":"\n  IndicCrosslingualSTS\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a Semantic Textual Similarity testset between English and 12 high-resource Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Non-fiction, Web, Spoken, Government, Written, Spoken\nReference\nhttps://huggingface.co/datasets/jaygala24/indic_sts\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicCrosslingualSTS.","url":"https://huggingface.co/datasets/mteb/IndicCrosslingualSTS","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","semantic-similarity-scoring","expert-annotated","multilingual","Assamese"],"keywords_longer_than_N":true},
	{"name":"znanio-presentations-part1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Presentations\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 144,280 educational presentations from the znanio.ru platform, a comprehensive resource for teachers, educators, students, and parents that has been pioneering educational technologies and distance learning in the Russian-speaking internet since 2009. The dataset is split into two parts, each containing ~72,140 presentations organized across 25 archives. All files have been‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-presentations-part1.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-presentations-part1","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","document-question-answering","text-retrieval","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"znanio-presentations-part1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Presentations\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 144,280 educational presentations from the znanio.ru platform, a comprehensive resource for teachers, educators, students, and parents that has been pioneering educational technologies and distance learning in the Russian-speaking internet since 2009. The dataset is split into two parts, each containing ~72,140 presentations organized across 25 archives. All files have been‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-presentations-part1.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-presentations-part1","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","document-question-answering","text-retrieval","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"svitppt","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Svitppt.com.ua Presentations\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for presentations from the svitppt.com.ua platform, a presentation storage and viewing service for Ukrainian school students. The dataset includes information such as presentation titles, URLs, download URLs, and extracted text content where available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual, with Ukrainian being the primary language. Other languages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/svitppt.","url":"https://huggingface.co/datasets/nyuuzyou/svitppt","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"svitppt","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Svitppt.com.ua Presentations\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for presentations from the svitppt.com.ua platform, a presentation storage and viewing service for Ukrainian school students. The dataset includes information such as presentation titles, URLs, download URLs, and extracted text content where available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual, with Ukrainian being the primary language. Other languages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/svitppt.","url":"https://huggingface.co/datasets/nyuuzyou/svitppt","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"mmBERT-decay-data","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMMBERT Decay Phase Data\n\t\n\n\n\n\n\n\nPhase 3 of 3: Annealed language learning decay phase (100B tokens) with massive multilingual expansion to 1833 languages.\n\n\n\t\n\t\n\t\n\t\tüìä Data Composition\n\t\n\nNOTE: there are multiple decay data mixtures: this mixture described below is the Decay-Cont mixture. However, the data in this repository is the Decay-Eng. If you are interested in the others, please let me know so I can prioritize it.\n\t\n\t\t\nData Source\nTokens (B)\nPercentage\nDescription\n\n\n\t\t\nFineWeb2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-decay-data.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-decay-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0405200","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_0405200.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_0405200","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_72","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_72.","url":"https://huggingface.co/datasets/James096/x_dataset_72","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"squad_v2_french_translated_fr_prompt_context_generation_with_answer","keyword":"squad_v2_french_translated","description":"\n\t\n\t\t\n\t\tsquad_v2_french_translated_fr_prompt_context_generation_with_answer\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nsquad_v2_french_translated_fr_prompt_context_generation_with_answer is a subset of the Dataset of French Prompts (DFP).It contains 1,271,928 rows that can be used for a context-generation (with answer) task.The original data (without prompts) comes from the dataset pragnakalp/squad_v2_french_translated and was augmented by questions in SQUAD 2.0 format in the FrenchQA dataset.\nA list of prompts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_context_generation_with_answer.","url":"https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_context_generation_with_answer","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","squad_v2_french_translated"],"keywords_longer_than_N":true},
	{"name":"nova-dataset-finetune-2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tNova: Voice-to-Text Companion Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains fine-tuning data for Nova, a real-time voice-to-text companion that can transcribe, translate, and assist with various voice-controlled workflows.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Conversations: 845 conversation objects\nTotal Messages: 4,605 individual messages\nLanguages: English, Vietnamese, and multilingual support\nFormat: Structured conversations with tool calls and responses\nUse Case:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Buiilding/nova-dataset-finetune-2.","url":"https://huggingface.co/datasets/Buiilding/nova-dataset-finetune-2","creator_name":"Dinh Tuan Anh Bui","creator_url":"https://huggingface.co/Buiilding","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","dialogue-modeling","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"nova-dataset-finetune-2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tNova: Voice-to-Text Companion Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains fine-tuning data for Nova, a real-time voice-to-text companion that can transcribe, translate, and assist with various voice-controlled workflows.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Conversations: 845 conversation objects\nTotal Messages: 4,605 individual messages\nLanguages: English, Vietnamese, and multilingual support\nFormat: Structured conversations with tool calls and responses\nUse Case:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Buiilding/nova-dataset-finetune-2.","url":"https://huggingface.co/datasets/Buiilding/nova-dataset-finetune-2","creator_name":"Dinh Tuan Anh Bui","creator_url":"https://huggingface.co/Buiilding","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","dialogue-modeling","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"BrightRetrieval","keyword":"multilingual","description":"\n  BrightRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nBright retrieval dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNon-fiction, Written\n\n\nReference\nhttps://huggingface.co/datasets/xlangai/BRIGHT\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"BrightRetrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nTo learn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BrightRetrieval.","url":"https://huggingface.co/datasets/mteb/BrightRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","multilingual","xlangai/BRIGHT"],"keywords_longer_than_N":true},
	{"name":"x_dataset_23","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_23.","url":"https://huggingface.co/datasets/suul999922/x_dataset_23","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_41147","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_41147.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_41147","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_21893","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_21893.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_21893","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_11100","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_11100.","url":"https://huggingface.co/datasets/icedwind/x_dataset_11100","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_104","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_104.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_104","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Hypa_AIME2024","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tHypa_AIME2024\n\t\n\nHypa_AIME2024 is an open-source, multilingual benchmark dataset for advanced mathematical reasoning, designed with the long-term vision of ensuring all languages are represented in AI development. This dataset marks a crucial step toward closing the gap between AI capabilities for no-resource/low-resource and all-resource languages, particularly in complex reasoning domains. \nThis initial release features the complete 2024 American Invitational Mathematics Examination‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hypaai/Hypa_AIME2024.","url":"https://huggingface.co/datasets/hypaai/Hypa_AIME2024","creator_name":"Hypa-Intelligence","creator_url":"https://huggingface.co/hypaai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","question-answering","AfroVoices","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"squad_v2_french_translated_fr_prompt_question_generation_with_answer_and_context","keyword":"squad_v2_french_translated","description":"\n\t\n\t\t\n\t\tsquad_v2_french_translated_fr_prompt_question_generation_with_answer_and_context\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nsquad_v2_french_translated_fr_prompt_question_generation_with_answer_and_context is a subset of the Dataset of French Prompts (DFP).It contains 1,112,937 rows that can be used for a question-generation (with answer and context) task.The original data (without prompts) comes from the dataset pragnakalp/squad_v2_french_translated and was augmented by questions in SQUAD 2.0 format in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_question_generation_with_answer_and_context.","url":"https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_question_generation_with_answer_and_context","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","squad_v2_french_translated"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_100415","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_100415.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_100415","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_196","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_196.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_196","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mmBERT-data-decay-all","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-data-decay-all.","url":"https://huggingface.co/datasets/orionweller/mmBERT-data-decay-all","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0304209","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_0304209.","url":"https://huggingface.co/datasets/james-1111/x_dataset_0304209","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_232","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Amylyx/reddit_dataset_232.","url":"https://huggingface.co/datasets/Amylyx/reddit_dataset_232","creator_name":"jianghonglin30@gmail.com","creator_url":"https://huggingface.co/Amylyx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_128","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chidinna/reddit_dataset_128.","url":"https://huggingface.co/datasets/chidinna/reddit_dataset_128","creator_name":"chidinn","creator_url":"https://huggingface.co/chidinna","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Indic-Rag-Suite","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüåè Multilingual Indic RAG Suite\n\t\n\nA comprehensive multilingual question-answering dataset covering 18 Indian languages with 12,802,615 total samples, designed for RAG (Retrieval-Augmented Generation) applications and multilingual NLP research.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load specific language (recommended)\ndataset = load_dataset(\"AshwinSankar/Indic-Rag-Suite\", \"as\")\ntrain_data = dataset['train']\n\nprint(f\"Loaded {len(train_data)} samples\")\n\n# Access‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AshwinSankar/Indic-Rag-Suite.","url":"https://huggingface.co/datasets/AshwinSankar/Indic-Rag-Suite","creator_name":"Ashwin Sankar","creator_url":"https://huggingface.co/AshwinSankar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multilingual","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"Indic-Rag-Suite","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüåè Multilingual Indic RAG Suite\n\t\n\nA comprehensive multilingual question-answering dataset covering 18 Indian languages with 12,802,615 total samples, designed for RAG (Retrieval-Augmented Generation) applications and multilingual NLP research.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load specific language (recommended)\ndataset = load_dataset(\"AshwinSankar/Indic-Rag-Suite\", \"as\")\ntrain_data = dataset['train']\n\nprint(f\"Loaded {len(train_data)} samples\")\n\n# Access‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AshwinSankar/Indic-Rag-Suite.","url":"https://huggingface.co/datasets/AshwinSankar/Indic-Rag-Suite","creator_name":"Ashwin Sankar","creator_url":"https://huggingface.co/AshwinSankar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multilingual","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"x_dataset_26","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andreans27/x_dataset_26.","url":"https://huggingface.co/datasets/andreans27/x_dataset_26","creator_name":"Andrean","creator_url":"https://huggingface.co/andreans27","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_5","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexinfstones/reddit_dataset_5.","url":"https://huggingface.co/datasets/alexinfstones/reddit_dataset_5","creator_name":"alexander","creator_url":"https://huggingface.co/alexinfstones","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_66","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vmintam/reddit_dataset_66.","url":"https://huggingface.co/datasets/vmintam/reddit_dataset_66","creator_name":"Vu Minh Tam","creator_url":"https://huggingface.co/vmintam","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_24","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_24.","url":"https://huggingface.co/datasets/suul999922/x_dataset_24","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"RWKV-World-Listing","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tRWKV World Corpus\n\t\n\n\n\t\n\t\t\n\t\t(includes v3, v2.1 and v2 subsets)\n\t\n\nThis is an itemised and annotated list of the RWKV World corpus as described in the RWKV-7 paper\nwhich is a multilingual dataset with about 3.1T tokens used to train the\n\"Goose\" RWKV-7 World model series.\nRWKV World v3 was crafted from public datasets spanning >100 world languages\n(80% English, 10% multilang, and 10% code). \n\n\t\n\t\t\n\t\n\t\n\t\tPREVIEW\n\t\n\nRandom subsampled subsets of the world v3 corpus are available in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RWKV/RWKV-World-Listing.","url":"https://huggingface.co/datasets/RWKV/RWKV-World-Listing","creator_name":"RWKV","creator_url":"https://huggingface.co/RWKV","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"x_dataset_53985","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_53985.","url":"https://huggingface.co/datasets/icedwind/x_dataset_53985","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_245","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/williamlewis0620/reddit_dataset_245.","url":"https://huggingface.co/datasets/williamlewis0620/reddit_dataset_245","creator_name":"William Lewis","creator_url":"https://huggingface.co/williamlewis0620","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_01085","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/reddit_dataset_01085.","url":"https://huggingface.co/datasets/william-1111/reddit_dataset_01085","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_170","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qr12138/x_dataset_170.","url":"https://huggingface.co/datasets/qr12138/x_dataset_170","creator_name":"wu","creator_url":"https://huggingface.co/qr12138","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Global-Usernames","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tü•≥ Global-Usernames\n\t\n\nA curated collection of creative usernames sourced from generative AI trained with a wide pool of global name data. This dataset emphasizes usernames that feel realistic, creative, and human-chosen ‚Äî distinct from bot-like or generator-made names.\n\n\n\t\n\t\t\n\t\t‚ú® What‚Äôs Inside\n\t\n\n\n10k+ usernames (single column text file)\nFrequent use of diacritics, emoji, symbols, stylistic casing, and mixed-language blends\nDiversity of scripts: Latin, Cyrillic, Arabic, Chinese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mookiezi/Global-Usernames.","url":"https://huggingface.co/datasets/mookiezi/Global-Usernames","creator_name":"Jason","creator_url":"https://huggingface.co/mookiezi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","apache-2.0","10K - 100K","text"],"keywords_longer_than_N":true},
	{"name":"Global-Usernames","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tü•≥ Global-Usernames\n\t\n\nA curated collection of creative usernames sourced from generative AI trained with a wide pool of global name data. This dataset emphasizes usernames that feel realistic, creative, and human-chosen ‚Äî distinct from bot-like or generator-made names.\n\n\n\t\n\t\t\n\t\t‚ú® What‚Äôs Inside\n\t\n\n\n10k+ usernames (single column text file)\nFrequent use of diacritics, emoji, symbols, stylistic casing, and mixed-language blends\nDiversity of scripts: Latin, Cyrillic, Arabic, Chinese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mookiezi/Global-Usernames.","url":"https://huggingface.co/datasets/mookiezi/Global-Usernames","creator_name":"Jason","creator_url":"https://huggingface.co/mookiezi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","apache-2.0","10K - 100K","text"],"keywords_longer_than_N":true},
	{"name":"Numbers","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTamazight Numbers Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains numbers from 1 to 1,000,000 translated into:\n\nEnglish.\nFrench.\nSpanish.\nTamazight (Berber).\n\nThe dataset is designed to assist researchers and developers in building machine learning models for understanding and converting numbers into words in multiple languages.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nThe dataset contains the following columns:\n\n\t\n\t\t\nColumn\nDescription\nExample\n\n\n\t\t\nNumber\nThe numeric‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tamazight/Numbers.","url":"https://huggingface.co/datasets/Tamazight/Numbers","creator_name":"Standard Moroccan Tamazight (ZGH)","creator_url":"https://huggingface.co/Tamazight","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","French","Spanish","Standard Moroccan Tamazight"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_14","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_14.","url":"https://huggingface.co/datasets/James096/reddit_dataset_14","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_061120","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_061120.","url":"https://huggingface.co/datasets/john-1111/x_dataset_061120","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_206","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/intensity809/x_dataset_206.","url":"https://huggingface.co/datasets/intensity809/x_dataset_206","creator_name":"intensity heat","creator_url":"https://huggingface.co/intensity809","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"CaLMQA","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\n\nCaLMQA is a translation-free long-form question answering (LFQA) dataset spanning 23 high- to low-resource languages. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCaLMQA is a translation-free LFQA dataset with 51.7K questions from 23 languages, 11 high- to mid-resource and 12 low-resource.\nAll questions are culturally specific ‚Äì (1) they refer to concepts unique to one or a few cultures, such as\n\"Kuber iki umwami wa mbere w‚Äôuburundi yitwa Ntare?\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shanearora/CaLMQA.","url":"https://huggingface.co/datasets/shanearora/CaLMQA","creator_name":"Shane Arora","creator_url":"https://huggingface.co/shanearora","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multilingual","Afar","Arabic","Baluchi","German"],"keywords_longer_than_N":true},
	{"name":"x_dataset_26","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_26.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_26","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_166","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_166.","url":"https://huggingface.co/datasets/James096/reddit_dataset_166","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"africa-emotions-balanced-multilang","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Emotion Analysis Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains emotion-labeled text data in multiple African languages for emotion classification (joy, sadness, anger, fear, surprise, disgust, neutral). Emotions were inferred from the original content using a multilingual processing pipeline and balanced per language to support fairer modeling.\nThe dataset is part of a growing collection of African language emotion analysis resources.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/africa-emotions-balanced-multilang.","url":"https://huggingface.co/datasets/michsethowusu/africa-emotions-balanced-multilang","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["machine-generated","found","multilingual","original","mit"],"keywords_longer_than_N":true},
	{"name":"LoTTE","keyword":"multilingual","description":"\n  LoTTE\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nLoTTE (Long-Tail Topic-stratified Evaluation for IR) is designed to evaluate retrieval models on underrepresented, long-tail topics. Unlike MSMARCO or BEIR, LoTTE features domain-specific queries and passages from StackExchange (covering writing, recreation, science, technology, and lifestyle), providing a challenging out-of-domain generalization benchmark.\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Web, Social\n\n\nReference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/LoTTE.","url":"https://huggingface.co/datasets/mteb/LoTTE","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0501128","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_0501128.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_0501128","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"1M-OpenOrca_be","keyword":"translated","description":"En/Be\nüêã The Belarusian OpenOrca Dataset! üêã\n\n\n\nBelarusian OpenOrca dataset - is rich collection of augmented FLAN data aligns, that translated in belarusian language.\nThat dataset should help training LLM in belarusian language and should help on other NLP tasks.\nThis dataset have 2 version:\n\n~1M GPT-4 completions (Now translating)\n~3.2M GPT-3.5 completions (Can be translated in future)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe fields are:\n\n'id', a unique numbered identifier which includes one of 'niv'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be.","url":"https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be","creator_name":"Artsem Holub","creator_url":"https://huggingface.co/WiNE-iNEFF","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Feedback-Collection-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tFeedback-Collection-ru\n\t\n\nThis is russian version of prometheus-eval/Feedback-Collection translated using Google Translate.\n","url":"https://huggingface.co/datasets/d0rj/Feedback-Collection-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","translated","prometheus-eval/Feedback-Collection","Russian"],"keywords_longer_than_N":true},
	{"name":"AfriSentiClassification","keyword":"multilingual","description":"\n  AfriSentiClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nAfriSenti is the largest sentiment analysis dataset for under-represented African languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, Written\n\n\nReferencehttps://arxiv.org/abs/2302.08956\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"AfriSentiClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AfriSentiClassification.","url":"https://huggingface.co/datasets/mteb/AfriSentiClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0207146","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_0207146.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_0207146","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"aya_evaluation_suite","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAya Evaluation Suite contains a total of 26,750 open-ended conversation-style prompts to evaluate multilingual open-ended generation quality.To strike a balance between language coverage and the quality that comes with human curation, we create an evaluation suite that includes:\n\nhuman-curated examples in 7 languages (tur, eng, yor, arb, zho, por, tel) ‚Üí aya-human-annotated.\nmachine-translations of handpicked examples into 101 languages ‚Üí dolly-machine-translated.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/aya_evaluation_suite.","url":"https://huggingface.co/datasets/CohereLabs/aya_evaluation_suite","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","crowdsourced","expert-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0612232","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_0612232.","url":"https://huggingface.co/datasets/john-1111/x_dataset_0612232","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"uzbek-language-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tUzbek Language Dataset Collection\n\t\n\nBu repository o'zbek tili uchun eng keng ko'lamli va keng qamrovli dataset to'plami hisoblanadi. Dataset turli manbalardan to'plangan va NLP modellari, til modellari va boshqa AI ilovalar uchun mo'ljallangan.\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\nBu dataset to'plami 4ta asosiy qism va qo'shimcha merge qilish asboblaridan iborat:\n\n\t\n\t\t\n\t\tüéØ Dataset Qismlari\n\t\n\n\n\t\n\t\t\nDataset\nHajmi\nMaqsad\nSource\n\n\n\t\t\ncommunity-oscar-uzbek\n1.1GB\nOSCAR Community data\nCommon‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xkas2001/uzbek-language-dataset.","url":"https://huggingface.co/datasets/xkas2001/uzbek-language-dataset","creator_name":"Abdullaev Samandar","creator_url":"https://huggingface.co/xkas2001","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","text-classification","oscar-corpus/OSCAR-2301","original"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_76","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huyjojo/reddit_dataset_76.","url":"https://huggingface.co/datasets/huyjojo/reddit_dataset_76","creator_name":"Huy","creator_url":"https://huggingface.co/huyjojo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_218","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_218.","url":"https://huggingface.co/datasets/gk4u/x_dataset_218","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_2025","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/goldentraversy07/x_dataset_2025.","url":"https://huggingface.co/datasets/goldentraversy07/x_dataset_2025","creator_name":"Steven K","creator_url":"https://huggingface.co/goldentraversy07","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_30","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tensorshield/reddit_dataset_30.","url":"https://huggingface.co/datasets/tensorshield/reddit_dataset_30","creator_name":"Tensor Shield","creator_url":"https://huggingface.co/tensorshield","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"bluesky-sentiment","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBluesky Sentiment Dataset Card\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nBluesky Sentiment contains posts from the agentlans/bluesky dataset, annotated for six emotions: \nhappiness, sadness, fear, disgust, anger, and surprise.\nAnnotations were generated automatically using ChatGPT, providing a nuanced, multidimensional sentiment analysis beyond simple positive/negative labels.\nThe dataset covers posts in multiple languages.\nThe few-shot config contains annotations by google/gemma-3-4b-it with 10-shot‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/bluesky-sentiment.","url":"https://huggingface.co/datasets/agentlans/bluesky-sentiment","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","feature-extraction","English","multilingual"],"keywords_longer_than_N":true},
	{"name":"DBPedia-NL","keyword":"translated","description":"\n  DBPedia-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. DBPedia-NL is a Dutch translation.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-dbpedia-entity\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DBPedia-NL.","url":"https://huggingface.co/datasets/mteb/DBPedia-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/dbpedia","Dutch"],"keywords_longer_than_N":true},
	{"name":"x_dataset_7","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_7.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_7","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_157","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tensorshield/reddit_dataset_157.","url":"https://huggingface.co/datasets/tensorshield/reddit_dataset_157","creator_name":"Tensor Shield","creator_url":"https://huggingface.co/tensorshield","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"uhura-arc-easy","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Uhura-Arc-Easy\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nUhura-ARC-Easy is a widely recognized scientific question answering benchmark composed of multiple-choice science questions derived from grade-school examinations that test various styles of knowledge and reasoning. \nThe original English version of the benchmark originates from Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge (Clark et al., 2018) and is divided into \"Challenge\" and \"Easy\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/uhura-arc-easy.","url":"https://huggingface.co/datasets/masakhane/uhura-arc-easy","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","multiple-choice-qa","multilingual","Amharic"],"keywords_longer_than_N":true},
	{"name":"x_dataset_030237","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_030237.","url":"https://huggingface.co/datasets/james-1111/x_dataset_030237","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"webui-dom-snapshots","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for WebUI DOM snapshots\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: Gary Benson\nLanguages: Mostly English (87%);\nDutch, French, Chinese, Japanese (1-2% each); 30+ others (<1% each)\nLicense: CC0 1.0 Universal\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gbenson/webui-dom-snapshots.","url":"https://huggingface.co/datasets/gbenson/webui-dom-snapshots","creator_name":"Gary Benson","creator_url":"https://huggingface.co/gbenson","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-feature-extraction","reinforcement-learning","text-classification","multilingual","biglab/webui-7k"],"keywords_longer_than_N":true},
	{"name":"x_dataset_21","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_21.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_21","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_27221","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_27221.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_27221","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"tulu-3-sft-mixture-with-language","keyword":"multilingual","description":"\n\nJust a version of the good tulu-3-sft-mixture dataset with a column indicating language.\nLanguage detection has been performed with fastText.\n‚ö†Ô∏è It may contain errors.\n","url":"https://huggingface.co/datasets/anakin87/tulu-3-sft-mixture-with-language","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["other","crowdsourced","expert-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_90","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/goldentraversy07/reddit_dataset_90.","url":"https://huggingface.co/datasets/goldentraversy07/reddit_dataset_90","creator_name":"Steven K","creator_url":"https://huggingface.co/goldentraversy07","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"akuapem_multispeaker_audio_transcribed","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tAkuapem Multispeaker Audio Transcribed Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Akuapem Multispeaker Audio Transcribed dataset is a collection of speech recordings and their transcriptions in Akuapem Twi, a widely spoken dialect of the Akan language in Ghana. The dataset is designed for training and evaluating automatic speech recognition (ASR) models and other natural language processing (NLP) applications.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: The dataset is derived from the Financial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/akuapem_multispeaker_audio_transcribed.","url":"https://huggingface.co/datasets/michsethowusu/akuapem_multispeaker_audio_transcribed","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","Twi","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"RealDevBench","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tRealDevWorld: Benchmarking Production-Ready Software Engineering\n\t\n\n\n\t\n\t\t\n\t\tWhy RealDevWorld?\n\t\n\nWith the explosion of AI-generated repositories and applications, the software engineering community faces a critical challenge: How do we automatically evaluate the quality and functionality of instantly generated projects? Manual testing is impractical for the scale and speed of AI development, yet traditional automated testing requires pre-written test suites that don't exist for novel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stellaHsr-mm/RealDevBench.","url":"https://huggingface.co/datasets/stellaHsr-mm/RealDevBench","creator_name":"StellaHSR","creator_url":"https://huggingface.co/stellaHsr-mm","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","other","expert-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_58","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/reddit_dataset_58.","url":"https://huggingface.co/datasets/arrmlet/reddit_dataset_58","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_162","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_162.","url":"https://huggingface.co/datasets/James096/reddit_dataset_162","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_112","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zengsdfew/reddit_dataset_112.","url":"https://huggingface.co/datasets/zengsdfew/reddit_dataset_112","creator_name":"miner3","creator_url":"https://huggingface.co/zengsdfew","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_17","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_17.","url":"https://huggingface.co/datasets/suul999922/x_dataset_17","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"SMPQA","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tSMPQA (Synthetic Multilingual Plot QA)\n\t\n\n\n\nThe SMPQA evaluation dataset proposed in Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model.\nSMPQA is composed of synthetic bar plots and pie charts (generated using word lists of different languages) together with questions about those plots.\nThe datasets aims at providing an initial way of evaluating multilingual OCR capabilities of models in arbritrary languages.\nThere are two sub-tasks: \n\nGrounding text labels‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/SMPQA.","url":"https://huggingface.co/datasets/WueNLP/SMPQA","creator_name":"W√ºNLP","creator_url":"https://huggingface.co/WueNLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","Zulu","Indonesian","Italian"],"keywords_longer_than_N":true},
	{"name":"philosophy-culture-translations-html-csv","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tAI-Culture Philosophy and Culture Translations CSV + HTML Corpus\n\t\n\nThe corpus contains an exceptionally diverse range of cultural, philosophical, and literary texts, available in 12 major languages. Among other topics, there is extensive engagement with the ethics and aesthetics of artificial intelligence and its cultural and philosophical implications, as well as connections between AI and philosophy of language and philosophy of mind.\nThis project is maintained by a non-profit‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-Culture-Commons/philosophy-culture-translations-html-csv.","url":"https://huggingface.co/datasets/AI-Culture-Commons/philosophy-culture-translations-html-csv","creator_name":"AI‚ÄëCulture‚ÄëCommons","creator_url":"https://huggingface.co/AI-Culture-Commons","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text-classification","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"cus-qa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset\n\t\n\nCUS-QA is a benchmark for open-ended regional question answering that encompasses both textual and visual modalities. The dataset consists of manually curated questions and answers grounded in Wikipedia, created by native speakers from Czechia, Slovakia, and Ukraine, with accompanying English translations.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\nCUS-QA addresses the gap in evaluation of regional knowledge in large‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ufal/cus-qa.","url":"https://huggingface.co/datasets/ufal/cus-qa","creator_name":"Institute of Formal and Applied Linguistics, Charles University, Prague","creator_url":"https://huggingface.co/ufal","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multilingual","Czech","Slovak"],"keywords_longer_than_N":true},
	{"name":"ProfessorHeidelTime","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tProfessor HeidelTime\n\t\n\nPaper    GitHub\nProfessor HeidelTime is a project to create a multilingual corpus weakly labeled with HeidelTime, a temporal tagger.\n\n\t\n\t\t\n\t\tCorpus Details\n\t\n\nThe weak labeling was performed in six languages. Here are the specifics of the corpus for each language:\n\n\t\n\t\t\nDataset\nLanguage\nDocuments\nFrom\nTo\nTokens\nTimexs\n\n\n\t\t\nAll the News 2.0\nEN\n24,642\n2016-01-01\n2020-04-02\n18,755,616\n254,803\n\n\nItalian Crime News\nIT\n9,619\n2011-01-01\n2021-12-31\n3,296,898\n58,823‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hugosousa/ProfessorHeidelTime.","url":"https://huggingface.co/datasets/hugosousa/ProfessorHeidelTime","creator_name":"Hugo Sousa","creator_url":"https://huggingface.co/hugosousa","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","parsing","part-of-speech","named-entity-recognition","machine-generated"],"keywords_longer_than_N":true},
	{"name":"reasoning-and-chat-harmony-format","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOpen Paws Reasoning And Conversational Finetuning Harmony Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Reasoning Data\nFormat: JSONL (JSON Lines)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Open‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/reasoning-and-chat-harmony-format.","url":"https://huggingface.co/datasets/open-paws/reasoning-and-chat-harmony-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"mosel","keyword":"multilingual","description":"\n\n\n\t\n\t\t\n\t\tDataset Description, Collection, and Source\n\t\n\nThe MOSEL corpus is a multilingual dataset collection including up to 950K hours of open-source speech recordings covering the 24 official languages of the European Union. We collect data by surveying labeled and unlabeled speech corpora under open-source compliant licenses.\nIn particular, MOSEL includes the automatic transcripts of 441k hours of unlabeled speech from VoxPopuli and LibriLight. The data is transcribed using Whisper large‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FBK-MT/mosel.","url":"https://huggingface.co/datasets/FBK-MT/mosel","creator_name":"FBK-MT","creator_url":"https://huggingface.co/FBK-MT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","machine-generated","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"x_dataset_22","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_22.","url":"https://huggingface.co/datasets/James096/x_dataset_22","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"hermes3-quick-probes-multilingual","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tHermes3 Quick Probes (Multilingual, Reasoning ON/OFF)\n\t\n\n–ú—ñ–Ω—ñ-–¥–∞—Ç–∞—Å–µ—Ç (20 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤) –¥–ª—è —à–≤–∏–¥–∫–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ Hermes-3 —É —Ä–µ–∂–∏–º–∞—Ö reasoning ON/OFF (UA/ES/EN/ID).\n–¶—ñ–ª—å ‚Äî –ª–µ–≥–∫—ñ sanity-checks: –¥–µ –ø–æ—Ç—Ä—ñ–±–Ω–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è, –∞ –¥–µ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Å—Ç–∏—Å–ª–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.\n\n\t\n\t\t\n\t\t–§–æ—Ä–º–∞—Ç\n\t\n\n\n–§–∞–π–ª: data.jsonl, –ø–æ 1 JSON-–æ–±‚Äô—î–∫—Ç—É –Ω–∞ —Ä—è–¥–æ–∫ –∑ –ø–æ–ª—è–º–∏:\nid (string) ‚Äî —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä\nlang (uk|es|en|id)\nreasoning (\"on\"|\"off\")\nprompt (string)\nexpect (dict, –æ–ø—Ü—ñ–π–Ω–æ: keywords/max_sentences/answer)\n\n\n\n\n\t\n\t\t\n\t\t–Ø–∫‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/segs/hermes3-quick-probes-multilingual.","url":"https://huggingface.co/datasets/segs/hermes3-quick-probes-multilingual","creator_name":"fdt","creator_url":"https://huggingface.co/segs","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Ukrainian","Spanish","English","Indonesian"],"keywords_longer_than_N":true},
	{"name":"x_dataset_29","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_29.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_29","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_138","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/markrogolino/reddit_dataset_138.","url":"https://huggingface.co/datasets/markrogolino/reddit_dataset_138","creator_name":"Mark Rogolino","creator_url":"https://huggingface.co/markrogolino","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p3-others","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P3\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P3 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p3-others.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p3-others","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","English","mit","arxiv:2509.06888","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"Speech-Translation-Instructions","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tSpeech-Translation-Instructions\n\t\n\nThe instructions translated from 120 languages Common Voice to english, arabic, japanese, mandarin and french from common voice speech dataset. Suitable to use to finetune Speech LLM.\n","url":"https://huggingface.co/datasets/mesolitica/Speech-Translation-Instructions","creator_name":"Mesolitica","creator_url":"https://huggingface.co/mesolitica","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","multilingual","Malay","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"enviba","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tEnViBa Dataset\n\t\n\nThis dataset contains parallel translations between three languages:\n\nBahnar (bah): An Austroasiatic language spoken in Vietnam and Cambodia\nEnglish (en)\nVietnamese (vi)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains three splits:\n\ntrain: Training data\ntest: Test data  \nvalidation: Validation data\n\nEach example has three fields:\n\nbahnar: Text in Bahnar language\nenglish: Text in English\nvietnamese: Text in Vietnamese\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/21uyennt/enviba.","url":"https://huggingface.co/datasets/21uyennt/enviba","creator_name":"Nguyen Thai Uyen","creator_url":"https://huggingface.co/21uyennt","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","English","Vietnamese","Bahamas Creole English","mit"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_210","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicholejohnson0530/reddit_dataset_210.","url":"https://huggingface.co/datasets/nicholejohnson0530/reddit_dataset_210","creator_name":"Nichole Johnson","creator_url":"https://huggingface.co/nicholejohnson0530","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_010718","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_010718.","url":"https://huggingface.co/datasets/william-1111/x_dataset_010718","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_5","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_5.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_5","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"IndicVarna-100k","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tIndicVarna for Callchimp.ai (a Dynopii product)\n\t\n\nWe introduce IndiVarna which was prepared by using Google Translate on the dair-ai/emotion dataset to get the samples there translated to the top 10 most commonly used Indian languages.\nThis dataset contains 10000 samples of each of the 10 languages supported.\nThe dataset further translated the labels in the dataset to 3 label sentiments - 0: Negative, 1: Neutral and 2: Positive. Each language has 3334 samples of each category of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dynopii/IndicVarna-100k.","url":"https://huggingface.co/datasets/dynopii/IndicVarna-100k","creator_name":"Dynopii Inc","creator_url":"https://huggingface.co/dynopii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","translation","sentence-similarity","fill-mask","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_226","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tensorshield/reddit_dataset_226.","url":"https://huggingface.co/datasets/tensorshield/reddit_dataset_226","creator_name":"Tensor Shield","creator_url":"https://huggingface.co/tensorshield","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_214","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wenknow/reddit_dataset_214.","url":"https://huggingface.co/datasets/wenknow/reddit_dataset_214","creator_name":"Dt","creator_url":"https://huggingface.co/wenknow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_19","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_19.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_19","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Benchmark-Testing","keyword":"machine translation","description":"shounakpaul95/Benchmark-Testing dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/shounakpaul95/Benchmark-Testing","creator_name":"Shounak Paul","creator_url":"https://huggingface.co/shounakpaul95","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","summarization","translation","token-classification","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"airbus-vision-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tAIRBUS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from Airbus technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice at TW3 Partners, a company specialized in Generative AI. Passionate about artificial intelligence and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/airbus-vision-dataset.","url":"https://huggingface.co/datasets/Davidsv/airbus-vision-dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_67","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/orochi001/reddit_dataset_67.","url":"https://huggingface.co/datasets/orochi001/reddit_dataset_67","creator_name":"tran","creator_url":"https://huggingface.co/orochi001","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_540880","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_540880.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_540880","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_28105","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_28105.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_28105","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_130","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Spark0801/reddit_dataset_130.","url":"https://huggingface.co/datasets/Spark0801/reddit_dataset_130","creator_name":"SparkLiu","creator_url":"https://huggingface.co/Spark0801","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"crh-parallel-corpora-document-level-noisy","keyword":"machine translation","description":"QIRIM/crh-parallel-corpora-document-level-noisy dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/QIRIM/crh-parallel-corpora-document-level-noisy","creator_name":"QIRI'M Young","creator_url":"https://huggingface.co/QIRIM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","Crimean Tatar","English","Russian","Ukrainian"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bit0/reddit_dataset_12.","url":"https://huggingface.co/datasets/bit0/reddit_dataset_12","creator_name":"sn13","creator_url":"https://huggingface.co/bit0","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-italian","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned-Italian\n\t\n\n\n\t\n\t\t\n\t\tAbout the translation and the original data\n\t\n\nThe translation was done with X-ALMA, a 13-billion-parameter model that surpasses state-of-the-art open-source multilingual LLMs (as of Q1 2025, paper here).\nThe original alpaca-cleaned dataset is also kept here so that there is parallel data for Italian and English.\n\n\t\n\t\t\n\t\n\t\n\t\tAdditional notes on the translation\n\t\n\n\nDespite the good quality of the translation, errors, though rare, are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DanielSc4/alpaca-cleaned-italian.","url":"https://huggingface.co/datasets/DanielSc4/alpaca-cleaned-italian","creator_name":"Daniel Scalena","creator_url":"https://huggingface.co/DanielSc4","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","language-modeling","multilingual","translation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_146","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_146.","url":"https://huggingface.co/datasets/James096/reddit_dataset_146","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"MediBeng","keyword":"machine translation","description":"\n  \n    \n    \n  \n\n  \n    \n    \n  \n\n  \n    \n    \n  \n\n  \n    \n    \n  \n\n  \n    \n    \n  \n\n  \n    \n    \n    \n    \n  \n\n  \n    \n    \n  \n\n  \n    \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n  \n\n\n\n\n\t\t\n\t\tDataset Card for MediBeng\n\t\n\nThis dataset includes synthetic code-switched conversations in Bengali and English. It is designed to help train models for tasks like speech recognition (ASR), text-to-speech (TTS), and machine translation, focusing on bilingual code-switching in healthcare settings. The dataset is free to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pr0mila-gh0sh/MediBeng.","url":"https://huggingface.co/datasets/pr0mila-gh0sh/MediBeng","creator_name":"Promila Ghosh","creator_url":"https://huggingface.co/pr0mila-gh0sh","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","text-to-audio","text-to-speech","translation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_070513","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_070513.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_070513","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0209123","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_0209123.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_0209123","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_28","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_28.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_28","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"multilingual-dataset-index","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Dataset Index\n\t\n\nA curated list of multilingual text datasets available on Huggingface, designed to help users easily find datasets by language‚Äîincluding those for low-resource languages.\n\n\t\n\t\t\n\t\tPurpose\n\t\n\nThis index aims to make it easier to find datasets by language, addressing the common issue of inconsistent or unclear language codes across different datasets.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe index is provided as a CSV file with the following columns:\n\nlanguage: The English name‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/multilingual-dataset-index.","url":"https://huggingface.co/datasets/agentlans/multilingual-dataset-index","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","cc0-1.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"x_dataset_21447","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_21447.","url":"https://huggingface.co/datasets/momo1942/x_dataset_21447","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"NusaParagraphEmotionClassification","keyword":"multilingual","description":"\n  NusaParagraphEmotionClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNusaParagraphEmotionClassification is a multi-class emotion classification on 10 Indonesian languages from the NusaParagraph dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNon-fiction, Fiction, Written\n\n\nReference\nhttps://github.com/IndoNLP/nusa-writes\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NusaParagraphEmotionClassification.","url":"https://huggingface.co/datasets/mteb/NusaParagraphEmotionClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","multilingual","Batak Toba","Betawi"],"keywords_longer_than_N":true},
	{"name":"zenamt-sentence-level","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tZenaMT corpus (sentence-level)\n\t\n\nThis is an Italian ‚Äì Ligurian (Genoese) parallel corpus covering a number of domains of cultural relevance to Ligurian speakers. Parts of the corpus also contain aligned English translations, available in the column eng. Whenever an English translation is not available, the corresponding column is set to null.\nThis is the sentence-level version of the corpus. If you are training your translation model on documents, you may be interested in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ConseggioLigure/zenamt-sentence-level.","url":"https://huggingface.co/datasets/ConseggioLigure/zenamt-sentence-level","creator_name":"Council for Ligurian Linguistic Heritage","creator_url":"https://huggingface.co/ConseggioLigure","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","original","Ligurian","Italian"],"keywords_longer_than_N":true},
	{"name":"x_dataset_6","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_6.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_6","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_49","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hadesgod517/reddit_dataset_49.","url":"https://huggingface.co/datasets/hadesgod517/reddit_dataset_49","creator_name":"Hades","creator_url":"https://huggingface.co/hadesgod517","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0308199","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_0308199.","url":"https://huggingface.co/datasets/james-1111/x_dataset_0308199","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_157","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mo0han3d/reddit_dataset_157.","url":"https://huggingface.co/datasets/Mo0han3d/reddit_dataset_157","creator_name":"AbdElMonsef","creator_url":"https://huggingface.co/Mo0han3d","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nThe versatility‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/quanglt/x_dataset_44.","url":"https://huggingface.co/datasets/quanglt/x_dataset_44","creator_name":"Quang Le","creator_url":"https://huggingface.co/quanglt","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_29","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_29.","url":"https://huggingface.co/datasets/suul999922/x_dataset_29","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_118","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chz1001/x_dataset_118.","url":"https://huggingface.co/datasets/chz1001/x_dataset_118","creator_name":"z","creator_url":"https://huggingface.co/chz1001","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_46763","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_46763.","url":"https://huggingface.co/datasets/icedwind/x_dataset_46763","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"webfaq","keyword":"multilingual","description":"WebFAQ Q&A Dataset\n\n   \n       Overview |\n       Details  |\n       Structure  |\n       Examples |\n       Considerations |\n       License |\n       Citation |\n       Contact |\n       Acknowledgement\n   \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe WebFAQ Q&A Dataset is a broad-coverage corpus of 96 million natural question-answer (QA) pairs in 75 languages, gathered from FAQ pages on the web. It leverages structured schema.org FAQPage annotations, making it a unique resource for large-scale Question Answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PaDaS-Lab/webfaq.","url":"https://huggingface.co/datasets/PaDaS-Lab/webfaq","creator_name":"Chair of Data Science, University of Passau","creator_url":"https://huggingface.co/PaDaS-Lab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multilingual","Afrikaans","Amharic","Arabic"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0103245","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_0103245.","url":"https://huggingface.co/datasets/william-1111/x_dataset_0103245","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_151","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jacksss123/x_dataset_151.","url":"https://huggingface.co/datasets/Jacksss123/x_dataset_151","creator_name":"Tony","creator_url":"https://huggingface.co/Jacksss123","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_84","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tensorshield/reddit_dataset_84.","url":"https://huggingface.co/datasets/tensorshield/reddit_dataset_84","creator_name":"Tensor Shield","creator_url":"https://huggingface.co/tensorshield","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"sri_lankan_classifieds_data_set_ad_classification","keyword":"multilingual","description":"The Sri Lanka Classified Ads Dataset is a collection of classified advertisement listings sourced from various online marketplaces operating in Sri Lanka. It contains over 90,000 ads categorized under major sectors such as Property, Vehicles, and Electronics, and includes detailed titles and descriptions for each ad. This dataset is ideal for research and development in natural language processing (NLP) tasks like text classification, information extraction, and recommendation systems specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Damika-7/sri_lankan_classifieds_data_set_ad_classification.","url":"https://huggingface.co/datasets/Damika-7/sri_lankan_classifieds_data_set_ad_classification","creator_name":"Damika Alwis","creator_url":"https://huggingface.co/Damika-7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","Sinhala","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_84","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_84.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_84","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"NeuCLIR2022Retrieval","keyword":"multilingual","description":"\n  NeuCLIR2022Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\nSource datasets:\n\nmteb/neuclir-2022\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"NeuCLIR2022Retrieval\")\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NeuCLIR2022Retrieval.","url":"https://huggingface.co/datasets/mteb/NeuCLIR2022Retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2022","Persian"],"keywords_longer_than_N":true},
	{"name":"FrenchBee_dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tFRENCHBEE-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from FrenchBee technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice at TW3 Partners, a company specialized in Generative AI. Passionate about artificial intelligence‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/FrenchBee_dataset.","url":"https://huggingface.co/datasets/Davidsv/FrenchBee_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"x_dataset_96","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_96.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_96","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_206","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Axioris/reddit_dataset_206.","url":"https://huggingface.co/datasets/Axioris/reddit_dataset_206","creator_name":"DaneFoster","creator_url":"https://huggingface.co/Axioris","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Bonolota-ai-database","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüå∏ Bonolota-ai-database\n\t\n\nBonolota-ai-database is a bilingual Bengali-English dataset designed for emotion-aware chatbot and storytelling applications. It contains structured question-answer pairs with emotional tags, summaries, and optional voice paths‚Äîmaking it ideal for mobile-first, offline-friendly AI systems.\n\n\t\n\t\t\n\t\tüßæ Dataset Summary\n\t\n\n\nüî§ Languages: Bengali (bn) and English (en)\nüé≠ Emotion-aware: Each response is tagged with an emotion (e.g., ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡¶ø‡¶ï, ‡¶§‡¶•‡ßç‡¶Ø‡¶¨‡¶π‡ßÅ‡¶≤)\nüìò Summary:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Lavlu118557/Bonolota-ai-database.","url":"https://huggingface.co/datasets/Lavlu118557/Bonolota-ai-database","creator_name":"Lavlu mia","creator_url":"https://huggingface.co/Lavlu118557","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","Bengali","English","mit"],"keywords_longer_than_N":true},
	{"name":"x_dataset_58641","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_58641.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_58641","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"exorde-social-media-one-month-2024","keyword":"multilingual","description":"","url":"https://huggingface.co/datasets/Exorde/exorde-social-media-one-month-2024","creator_name":"ExordeLabs","creator_url":"https://huggingface.co/Exorde","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","summarization","text-retrieval","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"exorde-social-media-one-month-2024","keyword":"multi-lingual","description":"","url":"https://huggingface.co/datasets/Exorde/exorde-social-media-one-month-2024","creator_name":"ExordeLabs","creator_url":"https://huggingface.co/Exorde","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","summarization","text-retrieval","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0703124","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_0703124.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_0703124","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_250","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_250.","url":"https://huggingface.co/datasets/James096/reddit_dataset_250","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"bosnian-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tUltimate Bosnian Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is a comprehensive Bosnian language dataset designed for training large language models. It combines literary works, dictionary entries, and bilingual content to create a rich training resource for Bosnian language AI applications.\nNote: This dataset has been cleaned to remove poor English translations, fix paragraph formatting issues, and ensure high-quality content for optimal model training.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statistics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sag69/bosnian-dataset.","url":"https://huggingface.co/datasets/Sag69/bosnian-dataset","creator_name":"Mr Haydarevich","creator_url":"https://huggingface.co/Sag69","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","translation","question-answering","text-classification","language-modeling"],"keywords_longer_than_N":true},
	{"name":"PangeaBench-tydiqa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neulab/PangeaBench-tydiqa.","url":"https://huggingface.co/datasets/neulab/PangeaBench-tydiqa","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"x_dataset_8","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_8.","url":"https://huggingface.co/datasets/gk4u/x_dataset_8","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0306116","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_0306116.","url":"https://huggingface.co/datasets/james-1111/x_dataset_0306116","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"DBNL-public-qa-english-translation","keyword":"translated","description":"nscharrenberg/DBNL-public-qa-english-translation dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/nscharrenberg/DBNL-public-qa-english-translation","creator_name":"Noah","creator_url":"https://huggingface.co/nscharrenberg","license_name":"Public Domain Dedication & License","license_url":"https://scancode-licensedb.aboutcode.org/pddl-1.0.html","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","pddl","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"vocabulaire_de_litterature","keyword":"multilingual","description":"\n[!NOTE]\nDataset origin: https://loterre-skosmos.loterre.fr/P21/fr/\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nVocabulaire contr√¥l√© utilis√© pour l'indexation des r√©f√©rences bibliographiques de la base de donn√©es FRANCIS \"Litt√©rature\" (de 1972 √† 2015, http://pascal-francis.inist.fr/). Il est align√© avec DBpedia.\n","url":"https://huggingface.co/datasets/datasets-CNRS/vocabulaire_de_litterature","creator_name":"datasets-CNRS","creator_url":"https://huggingface.co/datasets-CNRS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","English","French","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"InjongoIntent","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for InjongoIntent\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nInjongoIntent\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThere are 17 languages available :\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe examples look like this for English:\nfrom datasets import load_dataset\ndata = load_dataset('masakhane/InjongoIntent', 'eng') \n# Please, specify the language code\n# A data point example is below:\n{\n\n}\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nquestion: the question string to a grade school math problem.\nanswer: the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/InjongoIntent.","url":"https://huggingface.co/datasets/masakhane/InjongoIntent","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","intent-classification","multi-class-classification","multilingual","clinc/clinc_oos"],"keywords_longer_than_N":true},
	{"name":"x_dataset_41613","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rainbowbridge/x_dataset_41613.","url":"https://huggingface.co/datasets/rainbowbridge/x_dataset_41613","creator_name":"Rain","creator_url":"https://huggingface.co/rainbowbridge","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_158","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/artao/x_dataset_158.","url":"https://huggingface.co/datasets/artao/x_dataset_158","creator_name":"arvee taofu","creator_url":"https://huggingface.co/artao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_57","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cappedapollo/reddit_dataset_57.","url":"https://huggingface.co/datasets/cappedapollo/reddit_dataset_57","creator_name":"Derik Han","creator_url":"https://huggingface.co/cappedapollo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0105204","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_0105204.","url":"https://huggingface.co/datasets/william-1111/x_dataset_0105204","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"DBPedia-PLHardNegatives","keyword":"translated","description":"\n  DBPedia-PLHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\n\n\nReference\nhttps://github.com/iai-group/DBpedia-Entity/\n\n\n\t\n\nSource datasets:\n\nmteb/dbpedia‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DBPedia-PLHardNegatives.","url":"https://huggingface.co/datasets/mteb/DBPedia-PLHardNegatives","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/dbpedia","mteb/DBPedia_PL_test_top_250_only_w_correct-v2"],"keywords_longer_than_N":true},
	{"name":"x_dataset_2205","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/x_dataset_2205.","url":"https://huggingface.co/datasets/StormKing99/x_dataset_2205","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_27","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_27.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_27","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"MultiEup-v2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMulti-EuP-v2\n\t\n\nThis dataset card documents Multi-EuP-v2, a multilingual corpus of European Parliament debate speeches enriched with Member of European Parliament (MEP) metadata and multilingual debate titles/IDs. It supports research on political text analysis, speaker-attribute prediction, stance/vote prediction, multilingual NLP, and retrieval.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMulti-EuP-v2 aggregates 50,337 debate speeches (each a unique did) in 24‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/unimelb-nlp/MultiEup-v2.","url":"https://huggingface.co/datasets/unimelb-nlp/MultiEup-v2","creator_name":"The University of Melbourne","creator_url":"https://huggingface.co/unimelb-nlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","text-generation","multilingual","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"MultiEup-v2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMulti-EuP-v2\n\t\n\nThis dataset card documents Multi-EuP-v2, a multilingual corpus of European Parliament debate speeches enriched with Member of European Parliament (MEP) metadata and multilingual debate titles/IDs. It supports research on political text analysis, speaker-attribute prediction, stance/vote prediction, multilingual NLP, and retrieval.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMulti-EuP-v2 aggregates 50,337 debate speeches (each a unique did) in 24‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/unimelb-nlp/MultiEup-v2.","url":"https://huggingface.co/datasets/unimelb-nlp/MultiEup-v2","creator_name":"The University of Melbourne","creator_url":"https://huggingface.co/unimelb-nlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","text-generation","multilingual","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"neuclir-2022","keyword":"multilingual","description":"\n  NeuCLIR2022Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NeuCLIR2022Retrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/neuclir-2022.","url":"https://huggingface.co/datasets/mteb/neuclir-2022","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","Persian","Russian"],"keywords_longer_than_N":true},
	{"name":"afrimgsm","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for afrimgsm\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAFRIMGSM is an evaluation dataset comprising translations of a subset of the GSM8k dataset into 16 African languages. \nIt includes test sets across all 18 languages, maintaining an English and French subsets from the original GSM8k dataset. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThere are 18 languages available :\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe examples look like this for English:\nfrom datasets import load_dataset\ndata =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/afrimgsm.","url":"https://huggingface.co/datasets/masakhane/afrimgsm","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["natural-language-inference","multilingual","gsm8k","Amharic","Ewe"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_94","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Goldragon/reddit_dataset_94.","url":"https://huggingface.co/datasets/Goldragon/reddit_dataset_94","creator_name":"Goldragon","creator_url":"https://huggingface.co/Goldragon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_050976","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_050976.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_050976","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"LatinSummarizerDataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tLatinSummarizer Dataset\n\t\n\n    \n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe LatinSummarizerDataset is a structured dataset used in the GitHub Repository for Latin summarization and translation tasks. This dataset provides aligned English-Latin texts, extractive summaries, and pre-training prompts for fine-tuning models like mT5 for low-resource NLP applications.\n\n\t\n\t\t\n\t\tStructure\n\t\n\nThe dataset is divided into two main phases: \n\nPre-training Data: Includes aligned bilingual corpora, synthetic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/LatinSummarizerDataset.","url":"https://huggingface.co/datasets/LatinNLP/LatinSummarizerDataset","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","summarization","news-articles-summarization","document-retrieval"],"keywords_longer_than_N":true},
	{"name":"x_dataset_152","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/synapz/x_dataset_152.","url":"https://huggingface.co/datasets/synapz/x_dataset_152","creator_name":"Derek Barnes","creator_url":"https://huggingface.co/synapz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_040849","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_040849.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_040849","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0608106","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_0608106.","url":"https://huggingface.co/datasets/john-1111/x_dataset_0608106","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"squad_v2_french_translated_fr_prompt_question_generation_with_answer","keyword":"squad_v2_french_translated","description":"\n\t\n\t\t\n\t\tsquad_v2_french_translated_fr_prompt_question_generation_with_answer\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nsquad_v2_french_translated_fr_prompt_question_generation_with_answer is a subset of the Dataset of French Prompts (DFP).It contains 1,165,934 rows that can be used for a question-generation (with answer) task.The original data (without prompts) comes from the dataset pragnakalp/squad_v2_french_translated and was augmented by questions in SQUAD 2.0 format in the FrenchQA dataset.\nA list of prompts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_question_generation_with_answer.","url":"https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_question_generation_with_answer","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","squad_v2_french_translated"],"keywords_longer_than_N":true},
	{"name":"HPLT2.0_cleaned","keyword":"multilingual","description":"This is a large-scale collection of web-crawled documents in 191 world languages, produced by the HPLT project. \nThe source of the data is mostly Internet Archive with some additions from Common Crawl.\nFor a detailed description of the dataset, please refer to https://hplt-project.org/datasets/v2.0\nThe Cleaned variant of HPLT Datasets v2.0\nThis is the cleaned variant of the HPLT Datasets v2.0 converted to the Parquet format semi-automatically when being uploaded here. \nThe original JSONL files‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jobs-git/HPLT2.0_cleaned.","url":"https://huggingface.co/datasets/jobs-git/HPLT2.0_cleaned","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","text-generation","language-modeling","multilingual","Achinese"],"keywords_longer_than_N":true},
	{"name":"GlotCC-V1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n \n\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\n\n\t\t\n\t\tUsage (Huggingface Hub -- Recommended)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1.","url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Abau","Amarasi","Abkhaz","Abkhazian"],"keywords_longer_than_N":true},
	{"name":"GlotCC-V1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n \n\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\n\n\t\t\n\t\tUsage (Huggingface Hub -- Recommended)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1.","url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Abau","Amarasi","Abkhaz","Abkhazian"],"keywords_longer_than_N":true},
	{"name":"literary-reasoning","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tLiterary Reasoning: Symbolism and Structure from Classic Texts\n\t\n\n\n\t\n\t\t\n\t\tüß† Purpose and Scope\n\t\n\nThis dataset is designed to support literary reasoning, specifically interpretive analysis of themes and symbolism in classic literature. It enables research into how models can analyze literature beyond surface-level content.\nIt targets advanced tasks like:\n\nDetecting symbolic elements\nInterpreting tone and genre-specific devices\nAnalyzing narrative structures\nRecognizing literary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/literary-reasoning.","url":"https://huggingface.co/datasets/agentlans/literary-reasoning","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","English","multilingual","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"literary-reasoning","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tLiterary Reasoning: Symbolism and Structure from Classic Texts\n\t\n\n\n\t\n\t\t\n\t\tüß† Purpose and Scope\n\t\n\nThis dataset is designed to support literary reasoning, specifically interpretive analysis of themes and symbolism in classic literature. It enables research into how models can analyze literature beyond surface-level content.\nIt targets advanced tasks like:\n\nDetecting symbolic elements\nInterpreting tone and genre-specific devices\nAnalyzing narrative structures\nRecognizing literary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/literary-reasoning.","url":"https://huggingface.co/datasets/agentlans/literary-reasoning","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","English","multilingual","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"uchitelya","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Uchitelya.com Educational Materials\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 199,230 educational materials from the uchitelya.com platform, a resource for teachers, educators, students, and parents providing diverse educational content on various topics. The dataset includes information such as material titles, URLs, download URLs, and extracted text content where available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/uchitelya.","url":"https://huggingface.co/datasets/nyuuzyou/uchitelya","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"uchitelya","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Uchitelya.com Educational Materials\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 199,230 educational materials from the uchitelya.com platform, a resource for teachers, educators, students, and parents providing diverse educational content on various topics. The dataset includes information such as material titles, URLs, download URLs, and extracted text content where available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/uchitelya.","url":"https://huggingface.co/datasets/nyuuzyou/uchitelya","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"oscar_subset","keyword":"multilingual","description":"This dataset is a subset of OSCAR\n2023.1\nobtained by sampling randomly 50% of documents from the first 30 JSONL files\nfor each language contained in the mother corpus, followed by truncating each\ndocument to the first 2048 Unicode code points. It thus contains all languages\nin OSCAR but drastically oversamples less frequent languages in comparison to\nlarger ones.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nFor convenience the languages all files are shipped in a single folder and can\nbe loaded together without‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mittagessen/oscar_subset.","url":"https://huggingface.co/datasets/mittagessen/oscar_subset","creator_name":"Benjamin Kiessling","creator_url":"https://huggingface.co/mittagessen","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","text-generation","language-modeling","multilingual","oscar-corpus/OSCAR-2301"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_218","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chris241/reddit_dataset_218.","url":"https://huggingface.co/datasets/chris241/reddit_dataset_218","creator_name":"ch","creator_url":"https://huggingface.co/chris241","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_51","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/romban38/reddit_dataset_51.","url":"https://huggingface.co/datasets/romban38/reddit_dataset_51","creator_name":"Romban","creator_url":"https://huggingface.co/romban38","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0102122","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_0102122.","url":"https://huggingface.co/datasets/william-1111/x_dataset_0102122","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_16","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_16.","url":"https://huggingface.co/datasets/suul999922/x_dataset_16","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_236","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bersov75/x_dataset_236.","url":"https://huggingface.co/datasets/bersov75/x_dataset_236","creator_name":"Bersov Bersov","creator_url":"https://huggingface.co/bersov75","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_23","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_23.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_23","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"fusion-synth-data-ufb","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOffline Synthetic Data (UFB) for: Making, not taking, the Best-of-N\n\t\n\n\n\t\n\t\t\n\t\tContent\n\t\n\nThis data contains completions for a 10,000 subset of the  UFB prompts (translated into 9 languages)  from 5 different teacher models and 2 aggregations:\nTeachers: We sample one completion from each of the following models at temperature T=0.3. For kimik2, qwen3, and deepseek-v3 we use TogetherAI, for gemma3-27b and command-a we use locally hosted images.\n\ngemma3-27b: GEMMA3-27B-IT\nkimik2:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/fusion-synth-data-ufb.","url":"https://huggingface.co/datasets/CohereLabs/fusion-synth-data-ufb","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","French","German","Spanish"],"keywords_longer_than_N":true},
	{"name":"afrimmlu","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for afrimmlu\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAFRIMMLU is an evaluation dataset comprising translations of a subset of the MMLU dataset into 15 African languages. \nIt includes test sets across all 17 languages, maintaining an English and French subsets from the original MMLU dataset. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThere are 17 languages available :\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe examples look like this for English:\nfrom datasets import load_dataset\ndata =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/afrimmlu.","url":"https://huggingface.co/datasets/masakhane/afrimmlu","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","multilingual","mmlu","Amharic"],"keywords_longer_than_N":true},
	{"name":"urdu-english-name-variants","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tNames Dataset (English-Urdu-Variants)\n\t\n\nThis dataset contains names with their standardized English form, Urdu script, and common English variants.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nen_std: Standardized English name\nur: Name in Urdu script  \nen_var: Common English variants/spellings of the name\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"muhammadUsman31254/urdu-english-name-variants\")\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (primary and variants)\nUrdu\n\n\n\t\n\t\t\n\t\tUse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/muhammadUsman31254/urdu-english-name-variants.","url":"https://huggingface.co/datasets/muhammadUsman31254/urdu-english-name-variants","creator_name":"Muhammad Usman","creator_url":"https://huggingface.co/muhammadUsman31254","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","Urdu","mit"],"keywords_longer_than_N":true},
	{"name":"MCEval8K","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMCEval8K\n\t\n\nMCEval8K is a diverse multiple-choice evaluation benchmark for probing language models‚Äô (LMs) understanding of a broad range of language skills using neuron-level analysis. \nIt was introduced in the ACL 2025 paper - \"Neuron Empirical Gradient: Discovering and Quantifying Neurons‚Äô Global Linear Controllability\".\n\n\t\n\t\t\n\t\tüîç Overview\n\t\n\nMCEval8K consists of 22 tasks grouped into six skill genres, covering linguistic analysis, content classification, reasoning, factuality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iszhaoxin/MCEval8K.","url":"https://huggingface.co/datasets/iszhaoxin/MCEval8K","creator_name":"XIN ZHAO","creator_url":"https://huggingface.co/iszhaoxin","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","natural-language-inference","acceptability-classification"],"keywords_longer_than_N":true},
	{"name":"MCEval8K","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMCEval8K\n\t\n\nMCEval8K is a diverse multiple-choice evaluation benchmark for probing language models‚Äô (LMs) understanding of a broad range of language skills using neuron-level analysis. \nIt was introduced in the ACL 2025 paper - \"Neuron Empirical Gradient: Discovering and Quantifying Neurons‚Äô Global Linear Controllability\".\n\n\t\n\t\t\n\t\tüîç Overview\n\t\n\nMCEval8K consists of 22 tasks grouped into six skill genres, covering linguistic analysis, content classification, reasoning, factuality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iszhaoxin/MCEval8K.","url":"https://huggingface.co/datasets/iszhaoxin/MCEval8K","creator_name":"XIN ZHAO","creator_url":"https://huggingface.co/iszhaoxin","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","natural-language-inference","acceptability-classification"],"keywords_longer_than_N":true},
	{"name":"IN22-Conv-Doc-Level","keyword":"multilingual","description":"This dataset was constructed by merging individual conversations from the IN22-Conv dataset to create a long-context, document-level parallel benchmark. For further information on domains and statistics, please refer to the original paper and dataset.\n","url":"https://huggingface.co/datasets/VarunGumma/IN22-Conv-Doc-Level","creator_name":"Varun Gumma","creator_url":"https://huggingface.co/VarunGumma","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-generated","multilingual","translation","Assamese"],"keywords_longer_than_N":true},
	{"name":"x_dataset_239","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smartnuel87/x_dataset_239.","url":"https://huggingface.co/datasets/smartnuel87/x_dataset_239","creator_name":"smartnuel","creator_url":"https://huggingface.co/smartnuel87","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_734775","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_734775.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_734775","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_127","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_127.","url":"https://huggingface.co/datasets/James096/reddit_dataset_127","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"castillo","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüè∞ CASTILLO: Characterizing Response Length Distributions in Large Language Models\n\t\n\nThe CASTILLO dataset is designed to support research on the variability of response lengths in large language models (LLMs). It provides statistical summaries of output lengths across 13 open-source LLMs evaluated on 7 instruction-following datasets. For each unique ‚ü®prompt, model‚ü© pair, 10 independent responses were generated using fixed decoding parameters, and key statistics were recorded‚Äîsuch as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/danfperam/castillo.","url":"https://huggingface.co/datasets/danfperam/castillo","creator_name":"Daniel F. Perez-Ramirez","creator_url":"https://huggingface.co/danfperam","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["tabular-classification","text-classification","text-generation","tabular-multi-class-classification","tabular-single-column-regression"],"keywords_longer_than_N":true},
	{"name":"test_for_upload","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/donh99922/test_for_upload.","url":"https://huggingface.co/datasets/donh99922/test_for_upload","creator_name":"hyun","creator_url":"https://huggingface.co/donh99922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"multilingual-terminology","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüìö GIST: Glossary of Multilingual AI Scientific Terminology\n\t\n\nPaper Title: Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology Dataset (GIST)\nWebsite Demo Instructions: https://github.com/jiarui-liu/MultilingualAITerminology\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGIST is a large-scale, high-quality multilingual AI terminology dataset developed to support global inclusivity in AI research. It consists of around 5,000 English AI-specific terms, each translated into Arabic, Chinese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jerry999/multilingual-terminology.","url":"https://huggingface.co/datasets/Jerry999/multilingual-terminology","creator_name":"Jiarui Liu","creator_url":"https://huggingface.co/Jerry999","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Arabic","Chinese","French","Japanese"],"keywords_longer_than_N":true},
	{"name":"x_dataset_231","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CelestialWandererOfTheVoid/x_dataset_231.","url":"https://huggingface.co/datasets/CelestialWandererOfTheVoid/x_dataset_231","creator_name":"Kenneth Wayne Long","creator_url":"https://huggingface.co/CelestialWandererOfTheVoid","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_250","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ashikshaffi08/reddit_dataset_250.","url":"https://huggingface.co/datasets/ashikshaffi08/reddit_dataset_250","creator_name":"Ashik","creator_url":"https://huggingface.co/ashikshaffi08","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_041213","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_041213.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_041213","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_061079","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_061079.","url":"https://huggingface.co/datasets/john-1111/x_dataset_061079","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_231","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CelestialWandererOfTheVoid/reddit_dataset_231.","url":"https://huggingface.co/datasets/CelestialWandererOfTheVoid/reddit_dataset_231","creator_name":"Kenneth Wayne Long","creator_url":"https://huggingface.co/CelestialWandererOfTheVoid","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_190","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CelestialWandererOfTheVoid/reddit_dataset_190.","url":"https://huggingface.co/datasets/CelestialWandererOfTheVoid/reddit_dataset_190","creator_name":"Kenneth Wayne Long","creator_url":"https://huggingface.co/CelestialWandererOfTheVoid","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_158","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/artao/reddit_dataset_158.","url":"https://huggingface.co/datasets/artao/reddit_dataset_158","creator_name":"arvee taofu","creator_url":"https://huggingface.co/artao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"common_voice_13_0_dv_preprocessed","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Common Voice Corpus 13.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Common Voice dataset consists of a unique MP3 and corresponding text file. \nMany of the 27141 recorded hours in the dataset also include demographic metadata like age, sex, and accent \nthat can help improve the accuracy of speech recognition engines.\nThe dataset currently consists of 17689 validated hours in 108 languages, but more voices and languages are always added. \nTake a look at the Languages page to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ssahir/common_voice_13_0_dv_preprocessed.","url":"https://huggingface.co/datasets/ssahir/common_voice_13_0_dv_preprocessed","creator_name":"Saad Sahir","creator_url":"https://huggingface.co/ssahir","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","crowdsourced","crowdsourced","multilingual","extended|common_voice"],"keywords_longer_than_N":true},
	{"name":"cantonese-chinese-parallel-corpus","keyword":"machine translation","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of parallel sentence pairs in Cantonese and Chinese. It is designed for various tasks, including machine translation. \nThe corpus contains a large number of sentence pairs collected from various domains and most has been improved through manual correction and translation.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nCantonese (yue)\nSimplified Chinese (zh)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry in the dataset is a JSON object containing two fields: \"yue\" for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HKAllen/cantonese-chinese-parallel-corpus.","url":"https://huggingface.co/datasets/HKAllen/cantonese-chinese-parallel-corpus","creator_name":"Allen D","creator_url":"https://huggingface.co/HKAllen","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","Chinese","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"quran","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for the Quran\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThe Quran with metadata, translations, and multiple Arabic text (can use specific types for embeddings, search, classification, and display). There are 126+ columns containing 43+ languages.\n\n\t\n\t\t\n\t\tTODO\n\t\n\n\n Add Tafsirs  \n Add topics/ontology\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"nazimali/quran\", split=\"train\")\nds\n\nOutput:\nDataset({\n    features: ['surah', 'ayah', 'surah-name', 'surah-total-ayas'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nazimali/quran.","url":"https://huggingface.co/datasets/nazimali/quran","creator_name":"Nazim Ali","creator_url":"https://huggingface.co/nazimali","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","translation","feature-extraction","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_122","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Aniruddh79012/x_dataset_122.","url":"https://huggingface.co/datasets/Aniruddh79012/x_dataset_122","creator_name":"Singh","creator_url":"https://huggingface.co/Aniruddh79012","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_58","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_58.","url":"https://huggingface.co/datasets/James096/x_dataset_58","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_133639","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_133639.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_133639","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"tatoeba_kbd","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTatoeba Translations Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains parallel sentence translations from Tatoeba to Kabardian language (kbd), filtered by similarity score. The source languages are:\n\nGerman (deu)\nEnglish (eng)\nFrench (fra)\nPortuguese (por)\nRussian (rus)\nSpanish (spa)\nTurkish (tur)\n\nAll translations in this dataset are paired with Kabardian (kbd) as the target language.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe dataset consists of high-quality parallel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/panagoa/tatoeba_kbd.","url":"https://huggingface.co/datasets/panagoa/tatoeba_kbd","creator_name":"adam panagov","creator_url":"https://huggingface.co/panagoa","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","Kabardian","German","English","French"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_218","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_218.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_218","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_237","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tensorshield/reddit_dataset_237.","url":"https://huggingface.co/datasets/tensorshield/reddit_dataset_237","creator_name":"Tensor Shield","creator_url":"https://huggingface.co/tensorshield","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"codesearchnet-codegen","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for CodeSearchNet for CodeGen\n\t\n\n\n\nThis is a processed version of the CodeSearchNet dataset. Namely, I separated the doc (documentation/docstring), sign (function signature), and output (function body) into separate fields; doc and sign are concatenated (according to the correct order of the programming language) into the problem field, making it suitable for the code generation task.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pengyunie/codesearchnet-codegen.","url":"https://huggingface.co/datasets/pengyunie/codesearchnet-codegen","creator_name":"Pengyu Nie","creator_url":"https://huggingface.co/pengyunie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","code-search-net/code_search_net","code","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0711214","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_0711214.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_0711214","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"aya_dataset","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Aya Dataset is a multilingual instruction fine-tuning dataset curated by an open-science community via Aya Annotation Platform from Cohere Labs. The dataset contains a total of 204k human-annotated prompt-completion pairs along with the demographics data of the annotators.\nThis dataset can be used to train, finetune, and evaluate multilingual LLMs.\n\nCurated by: Contributors of Aya Open Science Intiative.\n\nLanguage(s): 65 languages (71 including dialects &‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/aya_dataset.","url":"https://huggingface.co/datasets/CohereLabs/aya_dataset","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","crowdsourced","expert-generated","crowdsourced","expert-generated"],"keywords_longer_than_N":true},
	{"name":"fleurs_clean","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tFLEURS\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark. \nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\nused and ‚Äùunit error rate‚Äù (characters, signs) of all languages is averaged. Languages and results are also grouped into seven‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Helloworld668/fleurs_clean.","url":"https://huggingface.co/datasets/Helloworld668/fleurs_clean","creator_name":"zhide Tang","creator_url":"https://huggingface.co/Helloworld668","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"GeoFact-X","keyword":"multilingual","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for GeoFact-X\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGeoFact-X is a benchmark of geography-aware multilingual reasoning, proposed in the paper, Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning.\nTL;DR: We introduce M2A and GeoFact-X to evaluate and improve multilingual reasoning in LLMs by aligning internal reasoning with the input language using language-consistency rewards.\n\nProject page: https://jd730.github.io/projects/M2A_GeoFact-X\nCode:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/geofact-x/GeoFact-X.","url":"https://huggingface.co/datasets/geofact-x/GeoFact-X","creator_name":"geofact-x","creator_url":"https://huggingface.co/geofact-x","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","Hindi","Japanese"],"keywords_longer_than_N":true},
	{"name":"multiblimp","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultiBLiMP\n\t\n\nMultiBLiMP is a massively Multilingual Benchmark for Linguistic Minimal Pairs. The dataset is composed of synthetic pairs generated using Universal Dependencies and UniMorph.\nThe paper can be found here.\nWe split the data set by language: each language consists of a single .tsv file. The rows contain many attributes for a particular pair, most important are the sen and wrong_sen fields, which we use for evaluating the language models.\n\n\t\n\t\t\n\t\n\t\n\t\tUsing MultiBLiMP\n\t\n\nTo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jumelet/multiblimp.","url":"https://huggingface.co/datasets/jumelet/multiblimp","creator_name":"Jaap Jumelet","creator_url":"https://huggingface.co/jumelet","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Buriat","Spanish","Sanskrit","Romanian"],"keywords_longer_than_N":true},
	{"name":"x_dataset_42","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_42.","url":"https://huggingface.co/datasets/James096/x_dataset_42","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"crypto-tweets","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RentonWEB3/crypto-tweets.","url":"https://huggingface.co/datasets/RentonWEB3/crypto-tweets","creator_name":"Renton Mark","creator_url":"https://huggingface.co/RentonWEB3","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Axel232/reddit_dataset_44.","url":"https://huggingface.co/datasets/Axel232/reddit_dataset_44","creator_name":"Pits","creator_url":"https://huggingface.co/Axel232","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_44.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_44","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"pawsx_mt_triplet","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tPAWS-X Multilingual Triplet Dataset\n\t\n\nThis dataset contains PAWS-X (Paraphrase Adversaries from Word Scrambling) data organized by translation models for paraphrase detection and text similarity tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach sample contains the following fields:\n\nid: Unique identifier for the text pair\ntext1: First sentence (originally sentence1)\ntext2: Second sentence (originally sentence2)  \nlabel: Binary label (1 for paraphrase, 0 for non-paraphrase)\nmodel: Translation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/patrickamadeus/pawsx_mt_triplet.","url":"https://huggingface.co/datasets/patrickamadeus/pawsx_mt_triplet","creator_name":"Patrick Amadeus Irawan","creator_url":"https://huggingface.co/patrickamadeus","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","translation","German","Spanish","French"],"keywords_longer_than_N":true},
	{"name":"x_dataset_250","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ashikshaffi08/x_dataset_250.","url":"https://huggingface.co/datasets/ashikshaffi08/x_dataset_250","creator_name":"Ashik","creator_url":"https://huggingface.co/ashikshaffi08","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Finance","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\nThe dataset includes content tailored for financial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mcube030/Finance.","url":"https://huggingface.co/datasets/Mcube030/Finance","creator_name":"Mcube","creator_url":"https://huggingface.co/Mcube030","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","üá∫üá∏ Region: US","finance","fine-tuning","conversational-ai"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_212","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wenknow/reddit_dataset_212.","url":"https://huggingface.co/datasets/wenknow/reddit_dataset_212","creator_name":"Dt","creator_url":"https://huggingface.co/wenknow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_5HEMpH3WtP6NubmWRNSCJ8nAZ7kEixQ84VeRj4Aq8ozLXXyb","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chenxinpingcxp/reddit_dataset_5HEMpH3WtP6NubmWRNSCJ8nAZ7kEixQ84VeRj4Aq8ozLXXyb.","url":"https://huggingface.co/datasets/chenxinpingcxp/reddit_dataset_5HEMpH3WtP6NubmWRNSCJ8nAZ7kEixQ84VeRj4Aq8ozLXXyb","creator_name":"tian chen","creator_url":"https://huggingface.co/chenxinpingcxp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_140","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/taowtje/x_dataset_140.","url":"https://huggingface.co/datasets/taowtje/x_dataset_140","creator_name":"TAO tje","creator_url":"https://huggingface.co/taowtje","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_9","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vdixnck/x_dataset_9.","url":"https://huggingface.co/datasets/vdixnck/x_dataset_9","creator_name":"vdixncksjfogjx63737","creator_url":"https://huggingface.co/vdixnck","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_241","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thevitruvianguy/reddit_dataset_241.","url":"https://huggingface.co/datasets/thevitruvianguy/reddit_dataset_241","creator_name":"Lagbaja Tabedi","creator_url":"https://huggingface.co/thevitruvianguy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_94","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/coldmind/reddit_dataset_94.","url":"https://huggingface.co/datasets/coldmind/reddit_dataset_94","creator_name":"Toc","creator_url":"https://huggingface.co/coldmind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"CA-EN_Parallel_Corpus","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for CA-EN Parallel Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe CA-EN Parallel Corpus is a Catalan-English dataset of parallel sentences created to \nsupport Catalan in NLP tasks, specifically Machine Translation.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset can be used to train Bilingual Machine Translation models between English and Catalan in any direction, \nas well as Multilingual Machine Translation models.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/CA-EN_Parallel_Corpus.","url":"https://huggingface.co/datasets/projecte-aina/CA-EN_Parallel_Corpus","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","Catalan","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"masakhapos","keyword":"multilingual","description":"MasakhaPOS is the largest publicly available high-quality dataset for part-of-speech (POS) tagging in 20 African languages. The languages covered are: \n- Bambara (bam)\n- Ghomala (bbj)\n- Ewe (ewe)\n- Fon (fon)\n- Hausa (hau)\n- Igbo (ibo)\n- Kinyarwanda (kin)\n- Luganda (lug)\n- Dholuo (luo) \n- Mossi (mos)\n- Chichewa (nya)\n- Nigerian Pidgin\n- chShona (sna)\n- Kiswahili (swƒÖ)\n- Setswana (tsn)\n- Twi (twi)\n- Wolof (wol)\n- isiXhosa (xho)\n- Yor√πb√° (yor)\n- isiZulu (zul)\n\nThe train/validation/test sets are available for all the ten languages.\n\nFor more details see https://aclanthology.org/2023.acl-long.609/","url":"https://huggingface.co/datasets/masakhane/masakhapos","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":null,"first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","expert-generated","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"laion-translated-to-en-korean-subset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tlaion-translated-to-en-korean-subset\n\t\n\n\n\t\n\t\t\n\t\tAbout dataset\n\t\n\na subset data of laion/laion2B-multi-joined-translated-to-en and laion/laion1B-nolang-joined-translated-to-en, including only korean\n\n\t\n\t\t\n\t\tLisence\n\t\n\nCC-BY-4.0\n\n\t\n\t\t\n\t\tData Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instance\n\t\n\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"Bingsu/laion-translated-to-en-korean-subset\")\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['hash', 'URL', 'TEXT', 'ENG TEXT'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Bingsu/laion-translated-to-en-korean-subset.","url":"https://huggingface.co/datasets/Bingsu/laion-translated-to-en-korean-subset","creator_name":"Dowon Hwang","creator_url":"https://huggingface.co/Bingsu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","crowdsourced","crowdsourced","multilingual","Korean"],"keywords_longer_than_N":true},
	{"name":"lr-sum","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for LR-Sum\n\t\n\nLR-Sum is a automatic summarization dataset of newswire text with a focus on less resourced languages with a cc-by 4.0 license.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nLR-Sum is a permissively-licensed dataset created with the goal of enabling further research in automatic summarization for less-resourced languages.\nLR-Sum contains human-written summaries for 39 languages, many of which are less-resourced. \nThe data is based on the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bltlab/lr-sum.","url":"https://huggingface.co/datasets/bltlab/lr-sum","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["summarization","text-generation","found","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"oscar-mini","keyword":"multilingual","description":"The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\\","url":"https://huggingface.co/datasets/nthngdy/oscar-mini","creator_name":"Nathan Godey","creator_url":"https://huggingface.co/nthngdy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"Bhasha-Abhijnaanam","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Aksharantar\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBhasha-Abhijnaanam is a language identification test set for native-script as well as Romanized text which spans 22 Indic languages.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\n\t\n\t\t\n\n\n\n\n\n\n\n\n\t\t\nAssamese (asm)\nHindi (hin)\nMaithili (mai)\nNepali (nep)\nSanskrit (san)\nTamil (tam)\n\n\nBengali (ben)\nKannada (kan)\nMalayalam (mal)\nOriya (ori)\nSantali (sat)\nTelugu (tel)\n\n\nBodo(brx)\nKashmiri‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/Bhasha-Abhijnaanam.","url":"https://huggingface.co/datasets/ai4bharat/Bhasha-Abhijnaanam","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","crowdsourced","expert-generated","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"blbooks-parquet-embedded","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for \"blbooks-parquet-embedded\"\n\t\n\nMore Information needed\n","url":"https://huggingface.co/datasets/davanstrien/blbooks-parquet-embedded","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","other","language-modeling","masked-language-modeling"],"keywords_longer_than_N":true},
	{"name":"voxpopuli","keyword":"multilingual","description":"A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation.","url":"https://huggingface.co/datasets/facebook/voxpopuli","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","multilingual","English","German","French"],"keywords_longer_than_N":true},
	{"name":"bnl_newspapers","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for BnL Historical Newspapers\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe BnL has digitised over 800.000 pages of Luxembourg newspapers. This dataset currently has one configuration covering a subset of these newspapers, which sit under the \"Processed Datasets\" collection. The BNL:\n\nprocessed all newspapers and monographs that are in the public domain and extracted the full text and associated meta data of every single article, section, advertisement‚Ä¶ The result is a large number of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bnl-data/bnl_newspapers.","url":"https://huggingface.co/datasets/bnl-data/bnl_newspapers","creator_name":"BnL Open Data","creator_url":"https://huggingface.co/bnl-data","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"FreshStackRetrieval","keyword":"multilingual","description":"\n  FreshStackRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA code retrieval task based on FreshStack dataset containing programming problems across multiple languages. Each query is a natural language description of a programming task (e.g., 'Write a function to reverse a string using recursion'), and the corpus contains code implementations in Python, JavaScript, and Go. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FreshStackRetrieval.","url":"https://huggingface.co/datasets/mteb/FreshStackRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","embedding-benchmark/FreshStack_mteb","code"],"keywords_longer_than_N":true},
	{"name":"named_matching","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKYC Name Matching Dataset for Cross-Encoder Training\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 38,383 person-to-person name matching pairs specifically designed for training cross-encoder models for KYC (Know Your Customer) name screening and entity resolution tasks. Each example consists of two person names with match type categorization and detailed reasoning.\nThe dataset includes:\n\n8,903 labeled examples (23.2%) with binary labels for supervised training\n29,480 unlabeled‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DiligentAI/named_matching.","url":"https://huggingface.co/datasets/DiligentAI/named_matching","creator_name":"Diligent AI","creator_url":"https://huggingface.co/DiligentAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentence-similarity","multilingual","English","multilingual"],"keywords_longer_than_N":true},
	{"name":"named_matching","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKYC Name Matching Dataset for Cross-Encoder Training\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 38,383 person-to-person name matching pairs specifically designed for training cross-encoder models for KYC (Know Your Customer) name screening and entity resolution tasks. Each example consists of two person names with match type categorization and detailed reasoning.\nThe dataset includes:\n\n8,903 labeled examples (23.2%) with binary labels for supervised training\n29,480 unlabeled‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DiligentAI/named_matching.","url":"https://huggingface.co/datasets/DiligentAI/named_matching","creator_name":"Diligent AI","creator_url":"https://huggingface.co/DiligentAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentence-similarity","multilingual","English","multilingual"],"keywords_longer_than_N":true},
	{"name":"fleurs","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tFLEURS\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark. \nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\nused and ‚Äùunit error rate‚Äù (characters, signs) of all languages is averaged. Languages and results are also grouped into seven‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cahya/fleurs.","url":"https://huggingface.co/datasets/cahya/fleurs","creator_name":"Cahya Wirawan","creator_url":"https://huggingface.co/cahya","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"all-scam-spam","keyword":"multilingual","description":"This is a large corpus of 42,619 preprocessed text messages and emails sent by humans in 43 languages. is_spam=1 means spam and is_spam=0 means ham.\n1040 rows of balanced data, consisting of casual conversations and scam emails in ‚âà10 languages, were manually collected and annotated by me, with some help from ChatGPT.\n\n\n\n\t\n\t\t\n\t\tSome preprcoessing algorithms\n\t\n\n\nspam_assassin.js, followed by spam_assassin.py\nenron_spam.py\n\n\n\n\n\t\n\t\t\n\t\tData composition\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nTo make the text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/all-scam-spam.","url":"https://huggingface.co/datasets/FredZhang7/all-scam-spam","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","Norwegian","Spanish","Somali"],"keywords_longer_than_N":true},
	{"name":"dstc11.t4","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDSTC11: Dialogue System Technology Challenge 11Track 4: Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems\n\t\n\n\n\t\n\t\t\n\t\tDirectory Structure Scheme\n\t\n\nRepresentation of the directory tree structure:\n.\n‚îî‚îÄ‚îÄ DSTC_11_Track_4             # DSTC11 data\n    ‚îú‚îÄ‚îÄ task1                   # Multilingual metrics data\n    ‚îÇ       ‚îú‚îÄ‚îÄ train           # Train data (CHANEL/CDIAL datasets)\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ en_es       # English/Spanish data\n    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ en_zh‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mario-rc/dstc11.t4.","url":"https://huggingface.co/datasets/mario-rc/dstc11.t4","creator_name":"Mario Rodr√≠guez-Cantelar","creator_url":"https://huggingface.co/mario-rc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","Spanish","apache-2.0","arxiv:2112.07194"],"keywords_longer_than_N":true},
	{"name":"MaWPS-ar-addCN","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for MAWPS_ar\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMAWPS: A Math Word Problem Repository\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nMath Word Problem Solving\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nSupports Arabic and English\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\ntext_en: a string feature.\ntext_ar: a string feature.\neqn: a string feature.\n\n\n\t\n\t\t\n\t\tData Splits\n\t\n\n\n\t\n\t\t\ntrain\nvalidation\ntest\n\n\n\t\t\n3636\n1040\n520\n\n\n\t\n\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tCuration Rationale‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aelneima/MaWPS-ar-addCN.","url":"https://huggingface.co/datasets/aelneima/MaWPS-ar-addCN","creator_name":"ashraf hatim","creator_url":"https://huggingface.co/aelneima","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["explanation-generation","crowdsourced","found","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"DS1000Retrieval","keyword":"multilingual","description":"\n  DS1000Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA code retrieval task based on 1,000 data science programming problems from DS-1000. Each query is a natural language description of a data science task (e.g., 'Create a scatter plot of column A vs column B with matplotlib'), and the corpus contains Python code implementations using libraries like pandas, numpy, matplotlib, scikit-learn, and scipy. The task is to retrieve the correct code snippet that solves the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DS1000Retrieval.","url":"https://huggingface.co/datasets/mteb/DS1000Retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","embedding-benchmark/DS1000","code"],"keywords_longer_than_N":true},
	{"name":"Vidore2EconomicsReportsRetrieval","keyword":"multilingual","description":"\n  Vidore2EconomicsReportsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/economics_reports_v2\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"Vidore2EconomicsReportsRetrieval\")\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Vidore2EconomicsReportsRetrieval.","url":"https://huggingface.co/datasets/mteb/Vidore2EconomicsReportsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"mittens","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMiTTenS: A Dataset for Evaluating Misgendering in Translation\n\t\n\nMisgendering is the act of referring to someone in a way that does not reflect their gender identity.  Translation systems, including foundation models capable of translation, can produce errors that result in misgendering harms. To measure the extent of such potential harms when translating into and out of English, we introduce a dataset, MiTTenS, covering 26 languages from a variety of language families and scripts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/mittens.","url":"https://huggingface.co/datasets/google/mittens","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","Arabic","Finnish","Oromo","Ganda"],"keywords_longer_than_N":true},
	{"name":"audio-keyword-spotting","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Audio Keyword Spotting\n\t\n\n \n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe initial version of this dataset is a subset of MLCommons/ml_spoken_words, which is derived from Common Voice, designed for easier loading. Specifically, the subset consists of ml_spoken_words files filtered by the names and placenames transliterated in Bible translations, as found in trabina. For our initial experiment, we have focused only on English, Spanish, and Indonesian, three languages whose name‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sil-ai/audio-keyword-spotting.","url":"https://huggingface.co/datasets/sil-ai/audio-keyword-spotting","creator_name":"SIL Global - AI","creator_url":"https://huggingface.co/sil-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","machine-generated","other","multilingual","extended|common_voice"],"keywords_longer_than_N":true},
	{"name":"professor_heideltime_en","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tProfessor HeidelTime\n\t\n\n\n\nProfessor HeidelTime is a project to create a multilingual corpus weakly labeled with HeidelTime, a temporal tagger.\n\n\t\n\t\t\n\t\n\t\n\t\tCorpus Details\n\t\n\nThe weak labeling was performed in six languages. Here are the specifics of the corpus for each language:\n\n\t\n\t\t\nDataset\nLanguage\nDocuments\nFrom\nTo\nTokens\nTimexs\n\n\n\t\t\nAll the News 2.0\nEN\n24,642\n2016-01-01\n2020-04-0218,755,616\n254,803\n\n\nItalian Crime News\nIT\n9,619\n2011-01-01\n2021-12-31\n3,296,898\n58,823\n\n\nGerman News‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hugosousa/professor_heideltime_en.","url":"https://huggingface.co/datasets/hugosousa/professor_heideltime_en","creator_name":"Hugo Sousa","creator_url":"https://huggingface.co/hugosousa","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","parsing","part-of-speech","named-entity-recognition","machine-generated"],"keywords_longer_than_N":true},
	{"name":"malicious-website-features-2.4M","keyword":"multilingual","description":"Important Notice:\n\nA subset of the URL dataset is from Kaggle, and the Kaggle datasets contained 10%-15% mislabelled data. See this dicussion I opened for some false positives. I have contacted Kaggle regarding their erroneous \"Usability\" score calculation for these unreliable datasets.\nThe feature extraction methods shown here are not robust at all in 2023, and there're even silly mistakes in 3 functions: not_indexed_by_google, domain_registration_length, and age_of_domain.\n\n\n\nThe features‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/malicious-website-features-2.4M.","url":"https://huggingface.co/datasets/FredZhang7/malicious-website-features-2.4M","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","tabular-classification","Norwegian","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"code_clippy_github","keyword":"multilingual","description":"The Code Clippy dataset consists of various public codebases from GitHub in 22 programming languages with 23 extensions     totalling about 16 TB of data when uncompressed. The dataset was created from the public GitHub dataset on Google BiqQuery.","url":"https://huggingface.co/datasets/CodedotAI/code_clippy_github","creator_name":"Code.AI","creator_url":"https://huggingface.co/CodedotAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["language-modeling","crowdsourced","expert-generated","multilingual","code"],"keywords_longer_than_N":true},
	{"name":"ro_sts_parallel","keyword":"multilingual","description":"The RO-STS-Parallel (a Parallel Romanian English dataset - translation of the Semantic Textual Similarity) contains 17256 sentences in Romanian and English. It is a high-quality translation of the English STS benchmark dataset into Romanian.","url":"https://huggingface.co/datasets/dumitrescustefan/ro_sts_parallel","creator_name":"Dumitrescu Stefan","creator_url":"https://huggingface.co/dumitrescustefan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","crowdsourced","crowdsourced","multilingual","extended|other-sts-b"],"keywords_longer_than_N":true},
	{"name":"tatoeba","keyword":"multilingual","description":"This is a collection of translated sentences from Tatoeba\n359 languages, 3,403 bitexts\ntotal number of files: 750\ntotal number of tokens: 65.54M\ntotal number of sentence fragments: 8.96M","url":"https://huggingface.co/datasets/Helsinki-NLP/tatoeba","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":null,"first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"vox_celeb","keyword":"multilingual","description":"VoxCeleb is an audio-visual dataset consisting of short clips of human speech, extracted from interview videos uploaded to YouTube","url":"https://huggingface.co/datasets/101arrowz/vox_celeb","creator_name":"Arjun Barrett","creator_url":"https://huggingface.co/101arrowz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","image-classification","speaker-identification","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"nchlt","keyword":"multilingual","description":"The development of linguistic resources for use in natural language processingis of utmost importance for the continued growth of research anddevelopment in the field, especially for resource-scarce languages. In this paper we describe the process and challenges of simultaneouslydevelopingmultiple linguistic resources for ten of the official languages of South Africa. The project focussed on establishing a set of foundational resources that can foster further development of both resources and technologies for the NLP industry in South Africa. The development efforts during the project included creating monolingual unannotated corpora, of which a subset of the corpora for each language was annotated on token, orthographic, morphological and morphosyntactic layers. The annotated subsetsincludes both development and test setsand were used in the creation of five core-technologies, viz. atokeniser, sentenciser,lemmatiser, part of speech tagger and morphological decomposer for each language. We report on the quality of these tools for each language and provide some more context of the importance of the resources within the South African context.","url":"https://huggingface.co/datasets/nwu-ctext/nchlt","creator_name":"Centre for Text Technology - Humanities - NWU","creator_url":"https://huggingface.co/nwu-ctext","license_name":"Creative Commons Attribution 2.5","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.5.html","language":null,"first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","expert-generated","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"MultiLegalPile_Wikipedia_Filtered","keyword":"multilingual","description":"A filtered version of the MultiLegalPile dataset, together with wikipedia articles.","url":"https://huggingface.co/datasets/joelniklaus/MultiLegalPile_Wikipedia_Filtered","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","other","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"xcsr","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for X-CSR\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTo evaluate multi-lingual language models (ML-LMs) for commonsense reasoning in a cross-lingual zero-shot transfer setting (X-CSR), i.e., training in English and test in other languages, we create two benchmark datasets, namely X-CSQA and X-CODAH. Specifically, we automatically translate the original CSQA and CODAH datasets, which only have English versions, to 15 other languages, forming development and test sets for studying X-CSR.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/INK-USC/xcsr.","url":"https://huggingface.co/datasets/INK-USC/xcsr","creator_name":"INK Lab @ USC","creator_url":"https://huggingface.co/INK-USC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","crowdsourced","crowdsourced","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ms_terms","keyword":"multilingual","description":"The Microsoft Terminology Collection can be used to develop localized versions of applications that integrate with Microsoft products.\nIt can also be used to integrate Microsoft terminology into other terminology collections or serve as a base IT glossary\nfor language development in the nearly 100 languages available. Terminology is provided in .tbx format, an industry standard for terminology exchange.","url":"https://huggingface.co/datasets/microsoft/ms_terms","creator_name":"Microsoft","creator_url":"https://huggingface.co/microsoft","license_name":"Microsoft Public License","license_url":"https://choosealicense.com/licenses/ms-pl/","language":null,"first_N":5,"first_N_keywords":["translation","expert-generated","expert-generated","multilingual","translation"],"keywords_longer_than_N":true},
	{"name":"kan_hope","keyword":"multilingual","description":"Numerous methods have been developed to monitor the spread of negativity in modern years by\neliminating vulgar, offensive, and fierce comments from social media platforms. However, there are relatively\nlesser amounts of study that converges on embracing positivity, reinforcing supportive and reassuring content in online forums.\nConsequently, we propose creating an English Kannada Hope speech dataset, KanHope and comparing several experiments to benchmark the dataset.\nThe dataset consists of 6,176 user generated comments in code mixed Kannada scraped from YouTube and manually annotated as bearing hope\nspeech or Not-hope speech.\nThis dataset was prepared for hope-speech text classification benchmark on code-mixed Kannada, an under-resourced language.","url":"https://huggingface.co/datasets/AdWeeb/kan_hope","creator_name":"Adeep Hande","creator_url":"https://huggingface.co/AdWeeb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","multi-label-classification","expert-generated","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"TurkmenTrilingualSemi-SyntheticDictionaryDF","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüèúÔ∏è Turkmen Trilingual Semi-Synthetic ‚Äî Dialogue Format\n\t\n\nLanguage: Turkmen üáπüá≤ | English üá¨üáß | Russian üá∑üá∫Type: Instruction-style / Dialogue datasetRecords: 61 970 base recordsDialog turns (flattened): 378 941Splits: train=363 783, val=7 578, test=7 580\n\n\n\t\n\t\t\n\t\tüìò Overview\n\t\n\nThis dataset is a dialogue-style extension of the original mamed0v/TurkmenTrilingualSemi-SyntheticDictionary.It was reformatted into conversational pairs to better suit instruction-tuning, chatbot‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mamed0v/TurkmenTrilingualSemi-SyntheticDictionaryDF.","url":"https://huggingface.co/datasets/mamed0v/TurkmenTrilingualSemi-SyntheticDictionaryDF","creator_name":"Bahtiyar Mamedov","creator_url":"https://huggingface.co/mamed0v","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","Turkmen","English","Russian"],"keywords_longer_than_N":true},
	{"name":"surname-nationality","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tPopular Surname Nationality Mapping\n\t\n\nSample of popular surnames for 30+ countries labeled with nationality (language)\n","url":"https://huggingface.co/datasets/Hobson/surname-nationality","creator_name":"Hobson Lane","creator_url":"https://huggingface.co/Hobson","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","text-classification","named-entity-recognition","List[str]","mit"],"keywords_longer_than_N":true},
	{"name":"tydi-as2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTyDi-AS2\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi-AS2 and Xtr-TyDi-AS2 are multilingual Answer Sentence Selection (AS2) datasets comprising 8 diverse languages, proposed in our paper accepted at ACL 2023 (Findings): Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages.\nBoth the datasets were created from TyDi-QA, a multilingual question-answering dataset. TyDi-AS2 was created by converting the QA instances in TyDi-QA to AS2 instances (see Dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AmazonScience/tydi-as2.","url":"https://huggingface.co/datasets/AmazonScience/tydi-as2","creator_name":"Amazon Science","creator_url":"https://huggingface.co/AmazonScience","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["question-answering","text-retrieval","open-domain-qa","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"answerable_tydiqa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for \"answerable-tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages. \nAnswerable TyDi QA is an extension of the GoldP subtask of the original TyDi QA dataset to also include unanswertable questions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains a train and a validation set, with 116067 and 13325 examples, respectively. Access them with\nfrom datasets import load_dataset\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/answerable_tydiqa.","url":"https://huggingface.co/datasets/copenlu/answerable_tydiqa","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"MultiLegalPileWikipediaFiltered","keyword":"multilingual","description":"A filtered version of the MultiLegalPile dataset, together with wikipedia articles.","url":"https://huggingface.co/datasets/joelniklaus/MultiLegalPileWikipediaFiltered","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","other","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"miracl-fi-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (fi) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-fi-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-fi-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-fi-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-fi-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Finnish"],"keywords_longer_than_N":true},
	{"name":"miracl-hi-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (hi) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-hi-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-hi-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-hi-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-hi-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Hindi"],"keywords_longer_than_N":true},
	{"name":"kinyarwanda-english-machine-translation-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKinyarwanda English Parallel Datasets for Machine translation\n\t\n\nA 48,000 Kinyarwanda English Parallel datasets for machine translation, made by curating and translating normal Kinyarwanda sentences into English\n","url":"https://huggingface.co/datasets/DigitalUmuganda/kinyarwanda-english-machine-translation-dataset","creator_name":"Digital Umuganda","creator_url":"https://huggingface.co/DigitalUmuganda","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["expert-generated","Digital Umuganda","multilingual","English","Kinyarwanda"],"keywords_longer_than_N":true},
	{"name":"norwegian-paws-x","keyword":"multilingual","description":"Norwegian PAWS-X, Bokmaal and Nynorsk machine-translated versions of PAWS-X.\n\nPAWS-X, a multilingual version of PAWS (Paraphrase Adversaries from Word Scrambling) for six languages.\n\nThis dataset contains 23,659 human translated PAWS evaluation pairs and 296,406 machine\ntranslated training pairs in six typologically distinct languages: French, Spanish, German,\nChinese, Japanese, and Korean. English language is available by default. All translated\npairs are sourced from examples in PAWS-Wiki.\n\nFor further details, see the accompanying paper: PAWS-X: A Cross-lingual Adversarial Dataset\nfor Paraphrase Identification (https://arxiv.org/abs/1908.11828)\n\nNOTE: There might be some missing or wrong labels in the dataset and we have replaced them with -1.","url":"https://huggingface.co/datasets/NbAiLab/norwegian-paws-x","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","semantic-similarity-scoring","text-scoring","multi-input-text-classification"],"keywords_longer_than_N":true},
	{"name":"MultiCoNER","keyword":"multilingual","description":"We present MultiCoNER, a large multilingual dataset for Named Entity Recognition that covers 3 domains (Wiki sentences, questions, and search queries) across 11 languages, as well as multilingual and code-mixing subsets. This dataset is designed to represent contemporary challenges in NER, including low-context scenarios (short and uncased text), syntactically complex entities like movie titles, and long-tail entity distributions. The 26M token dataset is compiled from public resources using techniques such as heuristic-based sentence sampling, template extraction and slotting, and machine translation. We applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a state-of-the-art GEMNET model that leverages gazetteers. The baseline achieves moderate performance (macro-F1=54%), highlighting the difficulty of our data. GEMNET, which uses gazetteers, improvement significantly (average improvement of macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained language models, and we believe that it can help further research in building robust NER systems. MultiCoNER is publicly available at https://registry.opendata.aws/multiconer/ and we hope that this resource will help advance research in various aspects of NER.","url":"https://huggingface.co/datasets/tomaarsen/MultiCoNER","creator_name":"Tom Aarsen","creator_url":"https://huggingface.co/tomaarsen","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","Bengali","German","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"MultiCoNER","keyword":"multilingual","description":"We present MultiCoNER, a large multilingual dataset for Named Entity Recognition that covers 3 domains (Wiki sentences, questions, and search queries) across 11 languages, as well as multilingual and code-mixing subsets. This dataset is designed to represent contemporary challenges in NER, including low-context scenarios (short and uncased text), syntactically complex entities like movie titles, and long-tail entity distributions. The 26M token dataset is compiled from public resources using techniques such as heuristic-based sentence sampling, template extraction and slotting, and machine translation. We applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a state-of-the-art GEMNET model that leverages gazetteers. The baseline achieves moderate performance (macro-F1=54%), highlighting the difficulty of our data. GEMNET, which uses gazetteers, improvement significantly (average improvement of macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained language models, and we believe that it can help further research in building robust NER systems. MultiCoNER is publicly available at https://registry.opendata.aws/multiconer/ and we hope that this resource will help advance research in various aspects of NER.","url":"https://huggingface.co/datasets/tomaarsen/MultiCoNER","creator_name":"Tom Aarsen","creator_url":"https://huggingface.co/tomaarsen","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","Bengali","German","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"cm.trial","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Common Voice Corpus 11.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Common Voice dataset consists of a unique MP3 and corresponding text file. \nMany of the 24210 recorded hours in the dataset also include demographic metadata like age, sex, and accent \nthat can help improve the accuracy of speech recognition engines.\nThe dataset currently consists of 16413 validated hours in 100 languages, but more voices and languages are always added. \nTake a look at the Languages page to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/taqwa92/cm.trial.","url":"https://huggingface.co/datasets/taqwa92/cm.trial","creator_name":"taqwa mohamed","creator_url":"https://huggingface.co/taqwa92","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","crowdsourced","crowdsourced","multilingual","extended|common_voice"],"keywords_longer_than_N":true},
	{"name":"bnl_newspapers1841-1879","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for BnL Newspapers 1841-1881\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n592.192 articles from historical newspapers (1841-1881) along with metadata and the full text.\n21 newspaper titles\n24.415 newspaper issues\n99.957 scanned pages\nTranscribed using a variety of OCR engines and corrected using https://github.com/natliblux/nautilusocr (95% threshold)\nPublic Domain, CC0 (See copyright notice)\nThe newspapers used are:\n\nDer Arbeiter (1878-1881)\nL'Arlequin (1848-1848)\nL'Avenir (1868-1871)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biglam/bnl_newspapers1841-1879.","url":"https://huggingface.co/datasets/biglam/bnl_newspapers1841-1879","creator_name":"BigLAM: BigScience Libraries, Archives and Museums","creator_url":"https://huggingface.co/biglam","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"Korean_Travel_Boards_Image_Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKorean Travel Boards Image Dataset\n\t\n\nThis dataset contains high-resolution images of Korean travel and tourism boards, including city maps, attraction guides, directional signs, transportation boards, and public tourism information displays. It supports OCR, multilingual translation, and computer vision research in travel-tech applications.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Korean_Travel_Boards_Image_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Korean_Travel_Boards_Image_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","Korean","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Korean-Signs-Image-Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKorean Signs Image Dataset\n\t\n\nThis dataset contains a diverse collection of high-resolution images of Korean street signs, shop boards, and public information signs. The dataset has been curated and anonymized to support research in multilingual OCR, text detection, and cultural-linguistic visual understanding.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask Categories:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Korean-Signs-Image-Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Korean-Signs-Image-Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","Korean","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MammAI_Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMammAI Dataset\n\t\n\nMammAI Dataset is an open, multilingual dataset (French üá´üá∑ & English üá¨üáß) designed to train and evaluate AI assistants for breast cancer education, awareness, and accessibility.\nThis dataset supports the development of language models capable of providing trustworthy, sourced, and multilingual information about breast cancer ‚Äî bridging the gap between healthcare knowledge and the public.\n\n\t\n\t\t\n\t\n\t\n\t\tüß† Dataset Overview\n\t\n\nEach entry follows this JSONL format:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TouradAi/MammAI_Dataset.","url":"https://huggingface.co/datasets/TouradAi/MammAI_Dataset","creator_name":"Tourad","creator_url":"https://huggingface.co/TouradAi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","French","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"tulu-3-sft-olmo-2-mixture","keyword":"multilingual","description":"Note that this collection is licensed under ODC-BY-1.0 license; different licenses apply to subsets of the data. Some portions of the dataset are non-commercial. We present the mixture as a research artifact.\nThe OLMo v2 SFT mixture was used to train the OLMo models.\nIt contains 939,344 samples from the following sets:\n\nCoCoNot (ODC-BY-1.0), 10,983 prompts (Brahman et al., 2024)\nFLAN v2 via ai2-adapt-dev/flan_v2_converted, 89,982 prompts (Longpre et al., 2023)\nNo Robots (CC-BY-NC-4.0), 9,500‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/tulu-3-sft-olmo-2-mixture.","url":"https://huggingface.co/datasets/allenai/tulu-3-sft-olmo-2-mixture","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["other","crowdsourced","expert-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"wildjailbreak-africa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWildJailbreak Africa\n\t\n\nThis dataset contains translations of 50,000 samples from the ai2-adapt-dev/tulu_v3.9_wildjailbreak_decontaminated_50k dataset into 5 African languages. The dataset is designed for instruction tuning and safety training of language models in low-resource African languages.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe original WildJailbreak dataset is a synthetic safety-training dataset containing both vanilla (direct harmful requests) and adversarial (complex adversarial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CraneAILabs/wildjailbreak-africa.","url":"https://huggingface.co/datasets/CraneAILabs/wildjailbreak-africa","creator_name":"Crane AI Labs","creator_url":"https://huggingface.co/CraneAILabs","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","allenai/wildjailbreak","English","Acoli"],"keywords_longer_than_N":true},
	{"name":"xP3","keyword":"multilingual","description":"xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks. It is used for the training of BLOOMZ and mT0, multilingual language models capable of following human instructions in dozens of languages zero-shot.","url":"https://huggingface.co/datasets/bigscience/xP3","creator_name":"BigScience Workshop","creator_url":"https://huggingface.co/bigscience","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Akan"],"keywords_longer_than_N":true},
	{"name":"una-fraza-al-diya","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tUna fraza al diya\n\t\n\nLadino language learning sentences prepared by Karen Sarhon of Sephardic Center of Istanbul. Each sentence has translations in Turkish, English, Spanish. Includes audio and image. 307 sentences in total.\nSource: https://sefarad.com.tr/judeo-espanyolladino/frazadeldia/\nImages and audio: http://collectivat.cat/share/judeoespanyol_audio_image.zip \nOffical link on Ladino Data Hub\nPaper on ArXiv\nCitation:\nPreparing an endangered language for the digital age: The Case of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/collectivat/una-fraza-al-diya.","url":"https://huggingface.co/datasets/collectivat/una-fraza-al-diya","creator_name":"Col¬∑lectivaT","creator_url":"https://huggingface.co/collectivat","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","translation","language-modeling","found","found"],"keywords_longer_than_N":true},
	{"name":"asleep_keyboard","keyword":"multilingual","description":"The Asleep at the Keyboard dataset contains 89 code generation scenarios that are designed to test the ability of code generation models to generate code secure code. The dataset is split into three evaluation axes: diversity of weaknesses (DoW), diversity of prompts (DoP), and diversity of domains (DoD).\n\nTo perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk cybersecurity weaknesses, e.g. those from MITRE‚Äôs ‚ÄúTop 25‚Äù Common Weakness Enumeration (CWE) list. We explore Copilot‚Äôs performance on three distinct code generation axes‚Äîexamining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios","url":"https://huggingface.co/datasets/moyix/asleep_keyboard","creator_name":"Brendan Dolan-Gavitt","creator_url":"https://huggingface.co/moyix","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["expert-generated","expert-generated","multilingual","original","English"],"keywords_longer_than_N":true},
	{"name":"tydiqa-primary","keyword":"multilingual","description":"TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\ninformation-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\ndon‚Äôt know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\nthe use of translation (unlike MLQA and XQuAD).","url":"https://huggingface.co/datasets/khalidalt/tydiqa-primary","creator_name":"Khalid Almubarak","creator_url":"https://huggingface.co/khalidalt","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"miracl-te-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (te) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-te-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-te-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-te-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-te-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Telugu"],"keywords_longer_than_N":true},
	{"name":"miracl-fi-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (fi) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-fi-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-fi-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-fi-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-fi-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Finnish"],"keywords_longer_than_N":true},
	{"name":"Korean_Receipts_Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKorean Receipts Dataset\n\t\n\nThis dataset contains high-resolution images of Korean retail receipts from supermarkets, restaurants, and stores. The dataset has been anonymized to remove personal information and is intended for AI research in OCR, document understanding, and financial analytics.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask Categories:  \n\nImage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Korean_Receipts_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Korean_Receipts_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","Korean","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"wiki_lingua","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for \"wiki_lingua\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWe introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of cross-lingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/esdurmus/wiki_lingua.","url":"https://huggingface.co/datasets/esdurmus/wiki_lingua","creator_name":"Esin Durmus","creator_url":"https://huggingface.co/esdurmus","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["summarization","crowdsourced","crowdsourced","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"ntcir_13_medweb","keyword":"multilingual","description":"NTCIR-13 MedWeb (Medical Natural Language Processing for Web Document) task requires\nto perform a multi-label classification that labels for eight diseases/symptoms must\nbe assigned to each tweet. Given pseudo-tweets, the output are Positive:p or Negative:n\nlabels for eight diseases/symptoms. The achievements of this task can almost be\ndirectly applied to a fundamental engine for actual applications.\n\nThis task provides pseudo-Twitter messages in a cross-language and multi-label corpus,\ncovering three languages (Japanese, English, and Chinese), and annotated with eight\nlabels such as influenza, diarrhea/stomachache, hay fever, cough/sore throat, headache,\nfever, runny nose, and cold.\n\nFor more information, see:\nhttp://research.nii.ac.jp/ntcir/permission/ntcir-13/perm-en-MedWeb.html\n\nAs this dataset also provides a parallel corpus of pseudo-tweets for english,\njapanese and chinese it can also be used to train translation models between\nthese three languages.","url":"https://huggingface.co/datasets/bigbio/ntcir_13_medweb","creator_name":"BigScience Biomedical Datasets","creator_url":"https://huggingface.co/bigbio","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multilingual","English","Chinese","Japanese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"dolphin-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tDolphin-ru üê¨\n\t\n\nThis is translated version of ehartford/dolphin into Russian.\n","url":"https://huggingface.co/datasets/d0rj/dolphin-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"common_language","keyword":"multilingual","description":"This dataset is composed of speech recordings from languages that were carefully selected from the CommonVoice database.\nThe total duration of audio recordings is 45.1 hours (i.e., 1 hour of material for each language).\nThe dataset has been extracted from CommonVoice to train language-id systems.","url":"https://huggingface.co/datasets/speechbrain/common_language","creator_name":"SpeechBrain","creator_url":"https://huggingface.co/speechbrain","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["audio-classification","speaker-identification","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"xor_tydi_qa","keyword":"multilingual","description":"    XOR-TyDi QA brings together for the first time information-seeking questions,\n    open-retrieval QA, and multilingual QA to create a multilingual open-retrieval\n    QA dataset that enables cross-lingual answer retrieval. It consists of questions\n    written by information-seeking native speakers in 7 typologically diverse languages\n    and answer annotations that are retrieved from multilingual document collections.\n    There are three sub-tasks: XOR-Retrieve, XOR-EnglishSpan, and XOR-Full.","url":"https://huggingface.co/datasets/akariasai/xor_tydi_qa","creator_name":"Akari Asai","creator_url":"https://huggingface.co/akariasai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["question-answering","open-domain-qa","crowdsourced","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"ComPile","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for ComPile: A Large IR Dataset from Production Sources\n\t\n\n\n\t\n\t\t\n\t\tChangelog\n\t\n\n\n\t\n\t\t\nRelease\nProgramming Languages\nDescription\n\n\n\t\t\nv1.0\nC/C++, Rust, Swift, Julia\nFine Tuning-scale dataset of 602GB of deduplicated LLVM (bitcode) IR\n\n\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nComPile contains over 2.7TB of permissively-licensed source code compiled to (textual) LLVM\nintermediate representation (IR) covering C/C++, Rust, Swift, and Julia.\nThe dataset was created by hooking into LLVM‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llvm-ml/ComPile.","url":"https://huggingface.co/datasets/llvm-ml/ComPile","creator_name":"Machine Learning on LLVM","creator_url":"https://huggingface.co/llvm-ml","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","code","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"miracl-en-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (en) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-en-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-en-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-en-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-en-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"kbd_lat-ru","keyword":"multilingual","description":"anzorq/kbd_lat-ru dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/anzorq/kbd_lat-ru","creator_name":"AQ","creator_url":"https://huggingface.co/anzorq","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","text2text-generation","multilingual","original","Kabardian"],"keywords_longer_than_N":true},
	{"name":"miracl-en-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (en) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-en-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-en-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-en-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-en-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"muhaz","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Muhaz.org\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 501,323 pages of educational content primarily in Turkish and Azerbaijani languages with some Russian content extracted from muhaz.org website. The content includes academic and educational materials, with a focus on technical and scientific topics.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Turkish (tr) and Azerbaijani (az) with some Russian (ru) content.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/muhaz.","url":"https://huggingface.co/datasets/nyuuzyou/muhaz","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","topic-classification","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-ko-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (ko) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (ko) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-ko-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-ko-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","multilingual","Korean","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"minds14-mirror","keyword":"multilingual","description":"MINDS-14 is training and evaluation resource for intent\ndetection task with spoken data. It covers 14\nintents extracted from a commercial system\nin the e-banking domain, associated with spoken examples in 14 diverse language varieties.","url":"https://huggingface.co/datasets/a6kme/minds14-mirror","creator_name":"Abhishek Kumar","creator_url":"https://huggingface.co/a6kme","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","keyword-spotting","expert-generated","crowdsourced","machine-generated"],"keywords_longer_than_N":true},
	{"name":"miracl-yo-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (yo) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-yo-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-yo-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-yo-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-yo-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Yoruba"],"keywords_longer_than_N":true},
	{"name":"fantasy_creature_bestiary","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüêâ Fantasy Creature Bestiary Dataset\n\t\n\n\nüá∑üá∫ –†—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è / Russian version...\n\n–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –ë–µ—Å—Ç–∏–∞—Ä–∏–π —Ñ–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å—É—â–µ—Å—Ç–≤ ‚Äî –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–∞–≥–∏—á–µ—Å–∫–∏—Ö —Å—É—â–µ—Å—Ç–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º. –≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏–≥—Ä, –ø–∏—Å–∞—Ç–µ–ª–µ–π, —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —ç–Ω—Ç—É–∑–∏–∞—Å—Ç–æ–≤ NLP, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –≤ –∂–∞–Ω—Ä–µ —Ñ—ç–Ω—Ç–µ–∑–∏.\n–ö–∞–∂–¥–æ–µ —Å—É—â–µ—Å—Ç–≤–æ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ–ø–∏—Å–∞–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö, —Å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/limloop/fantasy_creature_bestiary.","url":"https://huggingface.co/datasets/limloop/fantasy_creature_bestiary","creator_name":"Arsen Arutunan","creator_url":"https://huggingface.co/limloop","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Russian","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"gov-za-monolingual","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tThe South African Gov-ZA multilingual corpus\n\t\n\n\n\t\n\t\t\n\t\tAbout Dataset\n\t\n\nThe data set contains cabinet statements from the South African government, maintained by the Government Communication and Information System (GCIS). Data was scraped from the governments website:\nhttps://www.gov.za/cabinet-statements\nThe datasets contain government cabinet statements in 11 languages, namely:\n\n\t\n\t\t\nLanguage\nCode\nLanguage\nCode\n\n\n\t\t\nAfrikaans\n(af)\nSetswana\n(tn)\n\n\nEnglish\n(en)\nSepedi\n(nso)\n\n\nSesotho‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dsfsi/gov-za-monolingual.","url":"https://huggingface.co/datasets/dsfsi/gov-za-monolingual","creator_name":"Data Science for Social Impact","creator_url":"https://huggingface.co/dsfsi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","English","Afrikaans","South Ndebele","Xhosa"],"keywords_longer_than_N":true},
	{"name":"SWEPolyBenchRR","keyword":"multilingual","description":"\n  SWEPolyBenchRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual Software Issue Localization.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://amazon-science.github.io/SWE-PolyBench/\n\n\n\t\n\nSource datasets:\n\ntarsur909/mteb-swe-bench-poly-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"SWEPolyBenchRR\")\nevaluator =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SWEPolyBenchRR.","url":"https://huggingface.co/datasets/mteb/SWEPolyBenchRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-swe-bench-poly-reranking","code"],"keywords_longer_than_N":true},
	{"name":"mr-tydi-corpus","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMr. TyDi is a multi-lingual benchmark dataset built on TyDi, covering eleven typologically diverse languages. It is designed for monolingual retrieval, specifically to evaluate ranking with learned dense representations.\nThis dataset stores documents of Mr. TyDi. To access the queries and judgments, please refer to castorini/mr-tydi.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nThe only configuration here is the language. As all three folds (train, dev and test) share the same‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/castorini/mr-tydi-corpus.","url":"https://huggingface.co/datasets/castorini/mr-tydi-corpus","creator_name":"Castorini","creator_url":"https://huggingface.co/castorini","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multilingual","Arabic","Bengali","English"],"keywords_longer_than_N":true},
	{"name":"ragtime1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tRAGTIME1 Collection\n\t\n\nThis dataset contains the documents for TREC RAGTIME Track. \nPlease refer to the website for the details of the task. \nRAGTIME is a multilingual RAG task, which expects the participating system to retrieve relevant documents from all four languages and synthesize a response with citation to the report request. \nFor convenience, we separate the documents by their languages into four .jsonl files. However, they are intended to be used as a whole set. \nThe documents‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trec-ragtime/ragtime1.","url":"https://huggingface.co/datasets/trec-ragtime/ragtime1","creator_name":"TREC RAGTIME Track","creator_url":"https://huggingface.co/trec-ragtime","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","no-annotation","multilingual","extended|c4"],"keywords_longer_than_N":true},
	{"name":"ragtime1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tRAGTIME1 Collection\n\t\n\nThis dataset contains the documents for TREC RAGTIME Track. \nPlease refer to the website for the details of the task. \nRAGTIME is a multilingual RAG task, which expects the participating system to retrieve relevant documents from all four languages and synthesize a response with citation to the report request. \nFor convenience, we separate the documents by their languages into four .jsonl files. However, they are intended to be used as a whole set. \nThe documents‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trec-ragtime/ragtime1.","url":"https://huggingface.co/datasets/trec-ragtime/ragtime1","creator_name":"TREC RAGTIME Track","creator_url":"https://huggingface.co/trec-ragtime","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","no-annotation","multilingual","extended|c4"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Science","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Science is a dataset of BenchMAX, sourcing from GPQA, which evaluates the natural science reasoning capability in multilingual scenarios.\nWe extend the original English dataset to 16 non-English languages.\nThe data is first translated by Google‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Science.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Science","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_193266","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_193266.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_193266","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_21318","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_21318.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_21318","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_17879","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_17879.","url":"https://huggingface.co/datasets/icedwind/x_dataset_17879","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_46","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mgrtsv/reddit_dataset_46.","url":"https://huggingface.co/datasets/mgrtsv/reddit_dataset_46","creator_name":"Anton","creator_url":"https://huggingface.co/mgrtsv","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"clear-ru-translated","keyword":"translated","description":"This is the translated from English to Russian CLEAR Corpus. Link to original corpus: https://github.com/scrosseye/CLEAR-Corpus .For translation we used Qwen2.5-72B-Instruct-GPTQ-Int4 to translate the corpus.Our prompt was: \n  {\"role\": \"system\", \"content\": \"You are a helpful assistant. You are a good translator from English to Russian.\"},\n  {\"role\": \"user\", \"content\": content}\n\n","url":"https://huggingface.co/datasets/v-urushkin/clear-ru-translated","creator_name":"Victor Urushkin","creator_url":"https://huggingface.co/v-urushkin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","Russian","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"HPLT2.0_cleaned","keyword":"multilingual","description":"This is a large-scale collection of web-crawled documents in 191 world languages, produced by the HPLT project. \nThe source of the data is mostly Internet Archive with some additions from Common Crawl.\nFor a detailed description of the dataset, please refer to our website and our pre-print.\n\n\t\n\t\t\n\t\n\t\n\t\tThe Cleaned variant of HPLT Datasets v2.0\n\t\n\nThis is the cleaned variant of the HPLT Datasets v2.0 converted to the Parquet format semi-automatically when being uploaded here. \nThe original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPLT/HPLT2.0_cleaned.","url":"https://huggingface.co/datasets/HPLT/HPLT2.0_cleaned","creator_name":"HPLT","creator_url":"https://huggingface.co/HPLT","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["fill-mask","text-generation","language-modeling","multilingual","Achinese"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0311184","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_0311184.","url":"https://huggingface.co/datasets/james-1111/x_dataset_0311184","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_197","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/immortalizzy/reddit_dataset_197.","url":"https://huggingface.co/datasets/immortalizzy/reddit_dataset_197","creator_name":"Immortal Izzy","creator_url":"https://huggingface.co/immortalizzy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_041134","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_041134.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_041134","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\nThe dataset includes content tailored for financial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/oieieio/Finance-Instruct-500k.","url":"https://huggingface.co/datasets/oieieio/Finance-Instruct-500k","creator_name":"Jorge Alonso","creator_url":"https://huggingface.co/oieieio","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","üá∫üá∏ Region: US","finance","fine-tuning","conversational-ai"],"keywords_longer_than_N":true},
	{"name":"EmoTalk-7","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tEmoTalk-7\n\t\n\nEmoTalk-7 is a large-scale, multilingual, synthetic multimodal emotion recognition dataset generated using the Mistral API. It covers 7 major European languages and contains realistic social media scenarios with comprehensive emotion analysis, visual descriptions, and cultural context annotations.\n\n\t\n\t\t\n\t\tüìù Dataset Summary\n\t\n\nEmoTalk-7 contains 1400+ multimodal emotion records with high-quality annotations. It is designed to simulate authentic social media content across‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/EmoTalk-7.","url":"https://huggingface.co/datasets/NoeFlandre/EmoTalk-7","creator_name":"No√© Flandre","creator_url":"https://huggingface.co/NoeFlandre","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","French","Spanish","German"],"keywords_longer_than_N":true},
	{"name":"ner-cat","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tNERCat Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe NERCat dataset is a manually annotated collection of Catalan-language television transcriptions, designed to improve Named Entity Recognition (NER) performance for the Catalan language. The dataset covers diverse domains such as politics, sports, and culture, and includes 9,242 sentences with 13,732 named entities annotated across eight categories: Person, Facility, Organization, Location, Product, Event, Date, and Law. The dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ugiat/ner-cat.","url":"https://huggingface.co/datasets/Ugiat/ner-cat","creator_name":"Ugiat Technologies","creator_url":"https://huggingface.co/Ugiat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","multi-label-classification","named-entity-recognition","expert-generated"],"keywords_longer_than_N":true},
	{"name":"x_dataset_1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_1.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_1","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_111","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicchio816/x_dataset_111.","url":"https://huggingface.co/datasets/nicchio816/x_dataset_111","creator_name":"Alex Avery","creator_url":"https://huggingface.co/nicchio816","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_6","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_6.","url":"https://huggingface.co/datasets/suul999922/x_dataset_6","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_252","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bill00000/reddit_dataset_252.","url":"https://huggingface.co/datasets/bill00000/reddit_dataset_252","creator_name":"123","creator_url":"https://huggingface.co/bill00000","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"bigbench_jsonl","keyword":"multilingual","description":"BIG-Bench but it doesn't require the hellish dependencies (tensorflow, pypi-bigbench, protobuf) of the official version.\ndataset = load_dataset(\"tasksource/bigbench\",'movie_recommendation')\n\nCode to reproduce:\nhttps://colab.research.google.com/drive/1MKdLdF7oqrSQCeavAcsEnPdI85kD0LzU?usp=sharing\nDatasets are capped to 50k examples to keep things light.\nI also removed the default split when train was available also to save space, as default=train+val.\n@article{srivastava2022beyond‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NJUDeepEngine/bigbench_jsonl.","url":"https://huggingface.co/datasets/NJUDeepEngine/bigbench_jsonl","creator_name":"NJUDeepEngine","creator_url":"https://huggingface.co/NJUDeepEngine","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","text-generation","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"x_dataset_63","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Spark0801/x_dataset_63.","url":"https://huggingface.co/datasets/Spark0801/x_dataset_63","creator_name":"SparkLiu","creator_url":"https://huggingface.co/Spark0801","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_021112","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_021112.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_021112","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"MultiLingualSentiment","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nMultilingualSentiment is a sentiment classification dataset that encompasses three sentiment labels: Positive, Neutral, Negative\nThe dataset spans multiple languages and covers a wide range of domains, making it ideal for multilingual sentiment analysis tasks.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\nThe dataset was meticulously collected and aggregated from various sources, including Hugging Face and Kaggle. These sources provide diverse languages and domains to ensure a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clapAI/MultiLingualSentiment.","url":"https://huggingface.co/datasets/clapAI/MultiLingualSentiment","creator_name":"clapAI","creator_url":"https://huggingface.co/clapAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","Arabic","German","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"x_dataset_20503","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_20503.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_20503","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"SciFact-NL","keyword":"translated","description":"\n  SciFact-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSciFactNL verifies scientific claims in Dutch using evidence from the research literature containing scientific paper abstracts.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Medical, Written\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-scifact\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SciFact-NL.","url":"https://huggingface.co/datasets/mteb/SciFact-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/scifact","Dutch"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44829","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_44829.","url":"https://huggingface.co/datasets/momo1942/x_dataset_44829","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_24747","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_24747.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_24747","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"MintakaRetrieval","keyword":"translated","description":"\n  MintakaRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nWe introduce Mintaka, a complex, natural, and multilingual dataset designed for experimenting with end-to-end question-answering models. Mintaka is composed of 20,000 question-answer pairs collected in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka includes 8 types of complex questions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MintakaRetrieval.","url":"https://huggingface.co/datasets/mteb/MintakaRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","translated","jinaai/mintakaqa"],"keywords_longer_than_N":true},
	{"name":"x_dataset_14","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_14.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_14","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_206","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/intensity809/reddit_dataset_206.","url":"https://huggingface.co/datasets/intensity809/reddit_dataset_206","creator_name":"intensity heat","creator_url":"https://huggingface.co/intensity809","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"RonSTS","keyword":"translated","description":"\n  RonSTS\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nHigh-quality Romanian translation of STSBenchmark.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Social, Web, Written\n\n\nReference\nhttps://openreview.net/forum?id=JH61CD7afTv\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"RonSTS\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nTo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RonSTS.","url":"https://huggingface.co/datasets/mteb/RonSTS","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","semantic-similarity-scoring","human-annotated","translated","dumitrescustefan/ro_sts"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_159877","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_159877.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_159877","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gachenkenh14/reddit_dataset_44.","url":"https://huggingface.co/datasets/gachenkenh14/reddit_dataset_44","creator_name":"Viet Nam","creator_url":"https://huggingface.co/gachenkenh14","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"basqueparl","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBasqueParl: A Bilingual Corpus of Basque Parliamentary Transcriptions\n\t\n\nThis repository contains BasqueParl, a bilingual corpus for political discourse analysis. It covers transcriptions from the Parliament of \nthe Basque Autonomous Community for eight years and two legislative terms (2012-2020), and its main characteristic is the presence of Basque-Spanish \ncode-switching speeches.\nüìñ Paper: BasqueParl A Bilingual Corpus of Basque Parliamentary Transcriptions In LREC 2022.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/basqueparl.","url":"https://huggingface.co/datasets/HiTZ/basqueparl","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","summarization","translation","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"x_dataset_71","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_71.","url":"https://huggingface.co/datasets/suul999922/x_dataset_71","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_46165","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_46165.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_46165","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"animal-alignment-feedback","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOpen Paws Animal Alignment Feedback\n\t\n\nüêæ Human feedback and preference data for aligning AI with animal advocacy values\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Feedback Data\nFormat: CSV (Comma-separated values)\nLanguages: Multilingual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/animal-alignment-feedback.","url":"https://huggingface.co/datasets/open-paws/animal-alignment-feedback","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"x_dataset_30","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_30.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_30","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mala-monolingual-filter","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMaLA Corpus: Massive Language Adaptation Corpus\n\t\n\nThis is a cleaned version with some necessary data cleaning. \n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MaLA Corpus (Massive Language Adaptation) is a comprehensive, multilingual dataset designed to support the continual pre-training of large language models. It covers 939 languages and consists of over 74 billion tokens, making it one of the largest datasets of its kind. With a focus on improving the representation of low-resource languages, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MaLA-LM/mala-monolingual-filter.","url":"https://huggingface.co/datasets/MaLA-LM/mala-monolingual-filter","creator_name":"MaLA-LM","creator_url":"https://huggingface.co/MaLA-LM","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","odc-by","1B - 10B","arrow","Text"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_239","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smartnuel87/reddit_dataset_239.","url":"https://huggingface.co/datasets/smartnuel87/reddit_dataset_239","creator_name":"smartnuel","creator_url":"https://huggingface.co/smartnuel87","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"emirsaba","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Emirsaba.org\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 2,189,980 pages of educational content primarily in Kazakh language with some Russian content extracted from emirsaba.org website. The content includes academic and educational materials, with a focus on technical and scientific topics.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Kazakh (kk) with some Russian (ru) content.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/emirsaba.","url":"https://huggingface.co/datasets/nyuuzyou/emirsaba","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","topic-classification","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"znanio-images","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 19,060 educational images from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-images.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-images","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"znanio-images","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 19,060 educational images from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-images.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-images","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-NL","keyword":"translated","description":"\n  NFCorpus-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval. NFCorpus-NL is a Dutch translation.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical, Academic, Written\n\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-nfcorpus\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NFCorpus-NL\"])‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NFCorpus-NL.","url":"https://huggingface.co/datasets/mteb/NFCorpus-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/nfcorpus","Dutch"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_660618","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_660618.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_660618","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0204173","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_0204173.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_0204173","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"AmericanExpress_vision_dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tAMERICAN-EXPRESS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical and financial queries generated from American Express annual reports. It is designed to train and evaluate information retrieval models and improve AI understanding of financial documentation, with a specific focus on the credit card industry, payment processing, and banking services.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/AmericanExpress_vision_dataset.","url":"https://huggingface.co/datasets/Davidsv/AmericanExpress_vision_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"x_dataset_3891","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/x_dataset_3891.","url":"https://huggingface.co/datasets/StormKing99/x_dataset_3891","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_247","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zevebe/x_dataset_247.","url":"https://huggingface.co/datasets/zevebe/x_dataset_247","creator_name":"Andrea","creator_url":"https://huggingface.co/zevebe","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"dark_thoughts_casestudies_en_cn","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDark Thoughts Case Studies Dataset (English-Chinese)\n\t\n\nThis dataset contains a bilingual collection of case studies with detailed stakeholder analyses in English and Chinese. Each case study includes structured information about stakeholders and their motivations, along with comprehensive case analysis and solutions.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe dataset consists of 344,580 paired case studies in English and Chinese, with detailed stakeholder analyses and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DataTonic/dark_thoughts_casestudies_en_cn.","url":"https://huggingface.co/datasets/DataTonic/dark_thoughts_casestudies_en_cn","creator_name":"Data Tonic (Alignment Lab)","creator_url":"https://huggingface.co/DataTonic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"conversational-finetuning-llama-format","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOpen Paws Conversational Finetuning Llama Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Training Data\nFormat: CSV (Comma-separated values)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Open Paws‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/conversational-finetuning-llama-format.","url":"https://huggingface.co/datasets/open-paws/conversational-finetuning-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Yue-Benchmark","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tHow Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models\n\t\n\n\nHomepage: https://github.com/jiangjyjy/Yue-Benchmark\nRepository: https://huggingface.co/datasets/BillBao/Yue-Benchmark\nPaper: How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models.\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThe rapid evolution of large language models (LLMs), such as GPT-X and Llama-X, has driven significant advancements in NLP, yet much of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BillBao/Yue-Benchmark.","url":"https://huggingface.co/datasets/BillBao/Yue-Benchmark","creator_name":"Bao","creator_url":"https://huggingface.co/BillBao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","translation","Yue Chinese","multilingual"],"keywords_longer_than_N":true},
	{"name":"SOC-2508-MULTI","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Multilingual Synthetic Online Conversations\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains multilingual translations of the Synthetic Online Conversations (SOC-2508) dataset. Each conversation from the original dataset has been translated into French, Italian, German, Spanish, providing over 1,180 synthetically generated, multi-turn online conversations in multiple languages.\nThe translations were generated using google/gemma-3n-E4B-it with vLLM as the inference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/SOC-2508-MULTI.","url":"https://huggingface.co/datasets/marcodsn/SOC-2508-MULTI","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","French","Italian","German","Spanish"],"keywords_longer_than_N":true},
	{"name":"x_dataset_194","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bit0/x_dataset_194.","url":"https://huggingface.co/datasets/bit0/x_dataset_194","creator_name":"sn13","creator_url":"https://huggingface.co/bit0","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_205","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_205.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_205","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_231","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jasonmoore92/x_dataset_231.","url":"https://huggingface.co/datasets/jasonmoore92/x_dataset_231","creator_name":"Jason Moore","creator_url":"https://huggingface.co/jasonmoore92","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"ru-instruct","keyword":"translated","description":"\n\t\n\t\t\n\t\t–ö–∞—Ä—Ç–æ—á–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n\t\n\n–°–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞ (—Å–ø–∞—Å–∏–±–æ –º–æ–¥–µ–ª–∏ Den4ikAI/nonsense_gibberish_detector). –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω SimHash'–æ–º.\n–û–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –Ω—ë–º –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞ –Ω–µ –∑–∞–≤—ë–∑, in progress.\n\n\t\n\t\t\n\t\t–°–æ—Å—Ç–∞–≤\n\t\n\n–°–æ–±—Ä–∞–ª –∏–∑ —ç—Ç–∏—Ö –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö:\n\nd0rj/OpenOrca-ru (–æ—Ç Open-Orca/OpenOrca)\nd0rj/OpenHermes-2.5-ru (–æ—Ç teknium/OpenHermes-2.5)\nd0rj/dolphin-ru (–æ—Ç ehartford/dolphin)\nd0rj/alpaca-cleaned-ru (–æ—Ç‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct.","url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","machine-generated","found","translated"],"keywords_longer_than_N":true},
	{"name":"TurtleBench1.5k","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nTurtleBench is a novel evaluation benchmark designed to assess the reasoning capabilities of large language models (LLMs) using yes/no puzzles (commonly known as \"Turtle Soup puzzles\"). This dataset is constructed based on user guesses collected from our online Turtle Soup Puzzle platform, providing a dynamic and interactive means of evaluation. Unlike traditional static evaluation benchmarks, TurtleBench focuses on testing models in interactive settings to better capture‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Duguce/TurtleBench1.5k.","url":"https://huggingface.co/datasets/Duguce/TurtleBench1.5k","creator_name":"Qingchen Yu","creator_url":"https://huggingface.co/Duguce","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","language-modeling","expert-generated","expert-generated"],"keywords_longer_than_N":true},
	{"name":"x_dataset_25","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_25.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_25","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"multilingual-speech-commands-15lang-zip","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Speech Commands Dataset (15 Languages, Augmented)\n\t\n\nThis dataset contains augmented speech command samples in 15 languages, derived from multiple public datasets. Only commands that overlap with the Google Speech Commands (GSC) vocabulary are included, making the dataset suitable for multilingual keyword spotting tasks aligned with GSC-style classification.\nAudio samples have been augmented using standard audio techniques to improve model robustness (e.g., time-shifting‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/artur-muratov/multilingual-speech-commands-15lang-zip.","url":"https://huggingface.co/datasets/artur-muratov/multilingual-speech-commands-15lang-zip","creator_name":"Artur Muratov","creator_url":"https://huggingface.co/artur-muratov","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","Russian","Kazakh","Tatar","Arabic"],"keywords_longer_than_N":true},
	{"name":"Flores-Indic-Doc-Level","keyword":"multilingual","description":"This dataset was constructed by merging individual sentences from the Flores dataset based on matching domain, topic, and URL attributes. The result is a long-context, document-level parallel benchmark. For more details on the domains and dataset statistics, please refer to the original paper and the dataset.\n","url":"https://huggingface.co/datasets/VarunGumma/Flores-Indic-Doc-Level","creator_name":"Varun Gumma","creator_url":"https://huggingface.co/VarunGumma","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-generated","multilingual","translation","Assamese"],"keywords_longer_than_N":true},
	{"name":"x_dataset_27136","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_27136.","url":"https://huggingface.co/datasets/icedwind/x_dataset_27136","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_23","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_23.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_23","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_190","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CelestialWandererOfTheVoid/x_dataset_190.","url":"https://huggingface.co/datasets/CelestialWandererOfTheVoid/x_dataset_190","creator_name":"Kenneth Wayne Long","creator_url":"https://huggingface.co/CelestialWandererOfTheVoid","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_55757","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rainbowbridge/x_dataset_55757.","url":"https://huggingface.co/datasets/rainbowbridge/x_dataset_55757","creator_name":"Rain","creator_url":"https://huggingface.co/rainbowbridge","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"znanio-presentations-part2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Presentations\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 144,280 educational presentations from the znanio.ru platform, a comprehensive resource for teachers, educators, students, and parents that has been pioneering educational technologies and distance learning in the Russian-speaking internet since 2009. The dataset is split into two parts, each containing ~72,140 presentations organized across 25 archives. All files have been‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-presentations-part2.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-presentations-part2","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","document-question-answering","text-retrieval","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"znanio-presentations-part2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Presentations\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 144,280 educational presentations from the znanio.ru platform, a comprehensive resource for teachers, educators, students, and parents that has been pioneering educational technologies and distance learning in the Russian-speaking internet since 2009. The dataset is split into two parts, each containing ~72,140 presentations organized across 25 archives. All files have been‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-presentations-part2.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-presentations-part2","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","document-question-answering","text-retrieval","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_90","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_90.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_90","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bersov75/x_dataset_44.","url":"https://huggingface.co/datasets/bersov75/x_dataset_44","creator_name":"Bersov Bersov","creator_url":"https://huggingface.co/bersov75","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_1234","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_1234.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_1234","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p1-fineweb2-langs","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P1\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P1 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","feature-extraction","multilingual","mit","arxiv:2509.06888"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p1-fineweb2-langs","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P1\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P1 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","feature-extraction","multilingual","mit","arxiv:2509.06888"],"keywords_longer_than_N":true},
	{"name":"x_dataset_26384","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_26384.","url":"https://huggingface.co/datasets/momo1942/x_dataset_26384","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_684447","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_684447.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_684447","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_11","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jacksss123/x_dataset_11.","url":"https://huggingface.co/datasets/Jacksss123/x_dataset_11","creator_name":"Tony","creator_url":"https://huggingface.co/Jacksss123","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_20722","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rainbowbridge/x_dataset_20722.","url":"https://huggingface.co/datasets/rainbowbridge/x_dataset_20722","creator_name":"Rain","creator_url":"https://huggingface.co/rainbowbridge","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_231","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jasonmoore92/reddit_dataset_231.","url":"https://huggingface.co/datasets/jasonmoore92/reddit_dataset_231","creator_name":"Jason Moore","creator_url":"https://huggingface.co/jasonmoore92","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"synthetic_multilingual_llm_prompts","keyword":"multilingual","description":"\n  \n  Image generated by DALL-E. See prompt for more details\n\n\n\n\t\n\t\t\n\t\tüìùüåê Synthetic Multilingual LLM Prompts\n\t\n\nWelcome to the \"Synthetic Multilingual LLM Prompts\" dataset! This comprehensive collection features 1,250 synthetic LLM prompts generated using Gretel Navigator, available in seven different languages. To ensure accuracy and diversity in prompts, and translation quality and consistency across the different languages, we employed Gretel Navigator both as a generation tool and as an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gretelai/synthetic_multilingual_llm_prompts.","url":"https://huggingface.co/datasets/gretelai/synthetic_multilingual_llm_prompts","creator_name":"Gretel.ai","creator_url":"https://huggingface.co/gretelai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","translation","question-answering","English","Dutch"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p2-fineweb2-remaining","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P2\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P2 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p2-fineweb2-remaining.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p2-fineweb2-remaining","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","English","mit","arxiv:2509.06888","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"x_dataset_39","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eggmoo/x_dataset_39.","url":"https://huggingface.co/datasets/eggmoo/x_dataset_39","creator_name":"Vedant Behari","creator_url":"https://huggingface.co/eggmoo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_248","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/veyhoranohy/reddit_dataset_248.","url":"https://huggingface.co/datasets/veyhoranohy/reddit_dataset_248","creator_name":"Steve Karadimas","creator_url":"https://huggingface.co/veyhoranohy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"ViStoryBench","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tModel Card: ViStoryBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nViStoryBench is a comprehensive benchmark dataset for story visualization. It aims to thoroughly evaluate and advance the performance of story visualization models by providing diverse story types, artistic styles, and detailed annotations. The goal of story visualization is to generate a sequence of visually coherent and content-accurate images based on a given narrative text and character reference images.\nKey features of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ViStoryBench/ViStoryBench.","url":"https://huggingface.co/datasets/ViStoryBench/ViStoryBench","creator_name":"ViStoryBench","creator_url":"https://huggingface.co/ViStoryBench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","human-annotated","machine-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_41","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_41.","url":"https://huggingface.co/datasets/James096/reddit_dataset_41","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_178","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qr12138/x_dataset_178.","url":"https://huggingface.co/datasets/qr12138/x_dataset_178","creator_name":"wu","creator_url":"https://huggingface.co/qr12138","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"world-languages-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüåç World Languages Dataset\n\t\n\nThis dataset contains a list of official and unofficial languages categorized by language families...\n","url":"https://huggingface.co/datasets/SivaMallikarjun/world-languages-dataset","creator_name":"Parvatham Siva Mallikarjun","creator_url":"https://huggingface.co/SivaMallikarjun","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","language-identification","expert-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_73","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wenknow/reddit_dataset_73.","url":"https://huggingface.co/datasets/wenknow/reddit_dataset_73","creator_name":"Dt","creator_url":"https://huggingface.co/wenknow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"devto","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Dev.to Blogging Platform Posts\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is an unfinished dataset of blog posts from dev.to, a developer community.\nCurrently containing about 700,000 unfiltered posts. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in English, but also contains content in various other languages.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the following fields:\n\nid: Unique identifier for the article (integer)\ntitle: Title of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/teleren/devto.","url":"https://huggingface.co/datasets/teleren/devto","creator_name":"fg","creator_url":"https://huggingface.co/teleren","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"devto","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Dev.to Blogging Platform Posts\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is an unfinished dataset of blog posts from dev.to, a developer community.\nCurrently containing about 700,000 unfiltered posts. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in English, but also contains content in various other languages.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the following fields:\n\nid: Unique identifier for the article (integer)\ntitle: Title of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/teleren/devto.","url":"https://huggingface.co/datasets/teleren/devto","creator_name":"fg","creator_url":"https://huggingface.co/teleren","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"znanio-documents","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Documents\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 588,545 educational document files from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009. The dataset includes a small portion of English language content, primarily for language learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-documents.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-documents","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","document-question-answering","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"znanio-documents","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Documents\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 588,545 educational document files from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009. The dataset includes a small portion of English language content, primarily for language learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-documents.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-documents","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","document-question-answering","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Hampsty","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Hampsty","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"x_dataset_128","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malicious546/x_dataset_128.","url":"https://huggingface.co/datasets/malicious546/x_dataset_128","creator_name":"string malicious","creator_url":"https://huggingface.co/malicious546","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Emirates_dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tEMIRATES-AIRWAYS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical, financial, and sustainability queries generated from Emirates Airways annual and sustainability reports. It is designed to train and evaluate information retrieval models and improve AI understanding of aviation industry documentation, with a specific focus on airline operations, sustainability initiatives, and international business strategies.\n\n\t\n\t\t\n\t\n\t\n\t\tAbout Me\n\t\n\nI'm David‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/Emirates_dataset.","url":"https://huggingface.co/datasets/Davidsv/Emirates_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"x_dataset_050576","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_050576.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_050576","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_8191","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/reddit_dataset_8191.","url":"https://huggingface.co/datasets/StormKing99/reddit_dataset_8191","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zengsdfew/x_dataset_44.","url":"https://huggingface.co/datasets/zengsdfew/x_dataset_44","creator_name":"miner3","creator_url":"https://huggingface.co/zengsdfew","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_6071","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_6071.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_6071","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_16657","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_16657.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_16657","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"vqa-online-en-mk","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tVQA Online English-Macedonian Translation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 6,176 visual question-answering (VQA) records from Stack Exchange sites, translated from English to Macedonian. Each record includes questions, contextual information, and answers in both languages, along with associated images and metadata.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record contains the following fields:\n\nimage: Image filename (PNG format)\nquestion: Original English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ilijalichkovski/vqa-online-en-mk.","url":"https://huggingface.co/datasets/ilijalichkovski/vqa-online-en-mk","creator_name":"Ilija Lichkovski","creator_url":"https://huggingface.co/ilijalichkovski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","translation","bilingual","English"],"keywords_longer_than_N":true},
	{"name":"x_dataset_51","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/romban38/x_dataset_51.","url":"https://huggingface.co/datasets/romban38/x_dataset_51","creator_name":"Romban","creator_url":"https://huggingface.co/romban38","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_26","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_26.","url":"https://huggingface.co/datasets/suul999922/x_dataset_26","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_50132","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_50132.","url":"https://huggingface.co/datasets/icedwind/x_dataset_50132","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jacksss123/reddit_dataset_44.","url":"https://huggingface.co/datasets/Jacksss123/reddit_dataset_44","creator_name":"Tony","creator_url":"https://huggingface.co/Jacksss123","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"humanevalpack","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tDataset Card for HumanEvalPack\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nHumanEvalPack is an extension of OpenAI's HumanEval to cover 6 total languages across 3 tasks. The Python split is exactly the same as OpenAI's Python HumanEval. The other splits are translated by humans (similar to HumanEval-X but with additional cleaning, see here). Refer to the OctoPack paper for more details.\n\n\nLanguages: Python, JavaScript, Java, Go, C++, Rust\nOctoPacküêôüéí:\n\n\n\nData \nCommitPack\n4TB of GitHub commits‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bigcode/humanevalpack.","url":"https://huggingface.co/datasets/bigcode/humanevalpack","creator_name":"BigCode","creator_url":"https://huggingface.co/bigcode","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["expert-generated","multilingual","code","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"code-switching-tokenizer-robustness","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCode-Switching Dataset for Tokenizer Robustness Analysis\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is designed for tokenizer robustness testing in multilingual and code-switching contexts. It contains identical content expressed across 16 different language variants, including pure English and 15 English-X code-switching pairs, allowing researchers to isolate tokenization effects from semantic differences when evaluating language models.\n\n\t\n\t\t\n\t\tPurpose\n\t\n\n\nTokenizer Comparison:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Malikeh1375/code-switching-tokenizer-robustness.","url":"https://huggingface.co/datasets/Malikeh1375/code-switching-tokenizer-robustness","creator_name":"Malikeh Ehghaghi","creator_url":"https://huggingface.co/Malikeh1375","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","multilingual","English","Russian"],"keywords_longer_than_N":true},
	{"name":"code-switching-tokenizer-robustness","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCode-Switching Dataset for Tokenizer Robustness Analysis\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is designed for tokenizer robustness testing in multilingual and code-switching contexts. It contains identical content expressed across 16 different language variants, including pure English and 15 English-X code-switching pairs, allowing researchers to isolate tokenization effects from semantic differences when evaluating language models.\n\n\t\n\t\t\n\t\tPurpose\n\t\n\n\nTokenizer Comparison:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Malikeh1375/code-switching-tokenizer-robustness.","url":"https://huggingface.co/datasets/Malikeh1375/code-switching-tokenizer-robustness","creator_name":"Malikeh Ehghaghi","creator_url":"https://huggingface.co/Malikeh1375","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","multilingual","English","Russian"],"keywords_longer_than_N":true},
	{"name":"airline-vision-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tAirline Industry VQA Dataset\n\t\n\n‚ö†Ô∏è Note: This dataset currently contains only the text data (questions). The images are being processed and will be added in a future update.\nThis dataset contains a comprehensive collection of visual question-answering (VQA) pairs generated from official documentation of 18 major airline companies.\n\n\t\n\t\t\n\t\tAbout the Creator\n\t\n\nI'm David Soeiro-Vuong, an engineering student specializing in Computer Science, Big Data, and AI, currently working as an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/airline-vision-dataset.","url":"https://huggingface.co/datasets/Davidsv/airline-vision-dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_202507","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/goldentraversy07/reddit_dataset_202507.","url":"https://huggingface.co/datasets/goldentraversy07/reddit_dataset_202507","creator_name":"Steven K","creator_url":"https://huggingface.co/goldentraversy07","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reasoning-llama-format","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOpen Paws Reasoning Llama Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Reasoning Data\nFormat: JSONL (JSON Lines)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Open Paws\nLicense: Apache 2.0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/reasoning-llama-format.","url":"https://huggingface.co/datasets/open-paws/reasoning-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OGC_Renewable_Regulation","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOGC_Renewable_Regulation - Overview\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nOGC_Renewable_Regulation is a curated multimodal dataset focused on renewable energy technical documents, regulations, and legal frameworks. It combines text and image data extracted from real scientific and regulatory PDFs to support tasks such as RAG DSE, question answering, document search, and vision-language model training.\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\nThis dataset was created using our open-source tool‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/racineai/OGC_Renewable_Regulation.","url":"https://huggingface.co/datasets/racineai/OGC_Renewable_Regulation","creator_name":"racine.ai","creator_url":"https://huggingface.co/racineai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","text-retrieval","English","French"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ashikshaffi08/reddit_dataset_44.","url":"https://huggingface.co/datasets/ashikshaffi08/reddit_dataset_44","creator_name":"Ashik","creator_url":"https://huggingface.co/ashikshaffi08","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_24","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_24.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_24","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mmevol-zh-hant","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMMEvol - Translated Chinese Traditional\n\t\n\nA subset of Tongyi-ConvAI/MMEvol translated using yentinglin/Llama-3-Taiwan-70B-Instruct from english to traditional chinese.\nRead the Note below before use.\nImage source distribution:\n\n\t\n\t\t\nDataset\nCount\nPercentage\n\n\n\t\t\ncoco\n6598\n29.8%\n\n\nQ-Instruct-DB\n5856\n26.4%\n\n\nclevr\n2383\n10.8%\n\n\nchartqa\n1733\n7.8%\n\n\nhfdata\n1296\n5.9%\n\n\ngeo170k\n706\n3.2%\n\n\ndata_engine\n6983.2%\n\n\nmathvision\n644\n2.9%\n\n\ndocvqa\n600\n2.7%\n\n\nalfworld\n401\n1.8%\n\n\narxivqa\n337\n1.5%‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/syntaxsynth/mmevol-zh-hant.","url":"https://huggingface.co/datasets/syntaxsynth/mmevol-zh-hant","creator_name":"SyntaxSynth","creator_url":"https://huggingface.co/syntaxsynth","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"arabic_myanmar_quran_voices","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüìñ Arabic-Myanmar Quran Voice Dataset\n\t\n\n\n\t\n\t\t\n\t\tüïå Overview\n\t\n\nThis dataset contains high-quality MP3 audio recordings of the entire Holy Qur‚Äôan with:\n\nArabic recitation of each verse\nFollowed immediately by its Myanmar (Burmese) translation\n\nIt is the first complete Arabic-Myanmar Quran audio interpretation of its kind publicly released in Myanmar. The goal is to make the Qur‚Äôan more accessible to:\n\nElderly persons\nBlind or visually impaired people\nMyanmar speakers who wish to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/freococo/arabic_myanmar_quran_voices.","url":"https://huggingface.co/datasets/freococo/arabic_myanmar_quran_voices","creator_name":"Wynn","creator_url":"https://huggingface.co/freococo","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["audio-classification","audio-to-audio","automatic-speech-recognition","text-to-speech","human-annotated"],"keywords_longer_than_N":true},
	{"name":"arabic_myanmar_quran_voices","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüìñ Arabic-Myanmar Quran Voice Dataset\n\t\n\n\n\t\n\t\t\n\t\tüïå Overview\n\t\n\nThis dataset contains high-quality MP3 audio recordings of the entire Holy Qur‚Äôan with:\n\nArabic recitation of each verse\nFollowed immediately by its Myanmar (Burmese) translation\n\nIt is the first complete Arabic-Myanmar Quran audio interpretation of its kind publicly released in Myanmar. The goal is to make the Qur‚Äôan more accessible to:\n\nElderly persons\nBlind or visually impaired people\nMyanmar speakers who wish to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/freococo/arabic_myanmar_quran_voices.","url":"https://huggingface.co/datasets/freococo/arabic_myanmar_quran_voices","creator_name":"Wynn","creator_url":"https://huggingface.co/freococo","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["audio-classification","audio-to-audio","automatic-speech-recognition","text-to-speech","human-annotated"],"keywords_longer_than_N":true},
	{"name":"x_dataset_15","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_15.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_15","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bit0/x_dataset_12.","url":"https://huggingface.co/datasets/bit0/x_dataset_12","creator_name":"sn13","creator_url":"https://huggingface.co/bit0","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"test-datasets","keyword":"machine translation","description":"chanchungkit/test-datasets dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/chanchungkit/test-datasets","creator_name":"Chan Chung Kit","creator_url":"https://huggingface.co/chanchungkit","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","text-to-audio","text-to-speech","translation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_39","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eggmoo/reddit_dataset_39.","url":"https://huggingface.co/datasets/eggmoo/reddit_dataset_39","creator_name":"Vedant Behari","creator_url":"https://huggingface.co/eggmoo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_151","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jacksss123/reddit_dataset_151.","url":"https://huggingface.co/datasets/Jacksss123/reddit_dataset_151","creator_name":"Tony","creator_url":"https://huggingface.co/Jacksss123","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_144","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ashikshaffi08/x_dataset_144.","url":"https://huggingface.co/datasets/ashikshaffi08/x_dataset_144","creator_name":"Ashik","creator_url":"https://huggingface.co/ashikshaffi08","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_156","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/markrogolino/reddit_dataset_156.","url":"https://huggingface.co/datasets/markrogolino/reddit_dataset_156","creator_name":"Mark Rogolino","creator_url":"https://huggingface.co/markrogolino","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_15","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_15.","url":"https://huggingface.co/datasets/suul999922/x_dataset_15","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"LivingNER","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tLivingNER: Named entity recognition, normalization & classification of species, pathogens and food\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe LivingNER Gold Standard corpus is a collection of 2000 clinical case reports covering a broad range of medical specialities, i.e. infectious diseases (including Covid-19 cases), cardiology, neurology, oncology, dentistry, pediatrics, endocrinology, primary care, allergology, radiology, psychiatry, ophthalmology, urology, internal medicine, emergency and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Praise2112/LivingNER.","url":"https://huggingface.co/datasets/Praise2112/LivingNER","creator_name":"Praise","creator_url":"https://huggingface.co/Praise2112","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","multilingual","English","French","Galolen"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_3","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_3.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_3","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Any2GPT-Provider","keyword":"multilingual","description":"{\"provider\":\"google\", \"model\":\"gemini\"} // Model provider and provided by services\n\n","url":"https://huggingface.co/datasets/kulia-moon/Any2GPT-Provider","creator_name":"Kulia","creator_url":"https://huggingface.co/kulia-moon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multilingual","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"x_dataset_30","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_30.","url":"https://huggingface.co/datasets/suul999922/x_dataset_30","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_10","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_10.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_10","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"takaraspider","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTakaraSpider Japanese Web Crawl Dataset\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTakaraSpider is a large-scale web crawl dataset specifically designed to capture Japanese web content alongside international sources. The dataset contains 257,900 web pages collected through systematic crawling, with a primary focus on Japanese language content (78.5%) while maintaining substantial international representation (21.5%). This makes it ideal for Japanese-English comparative studies, cross-cultural web‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/takarajordan/takaraspider.","url":"https://huggingface.co/datasets/takarajordan/takaraspider","creator_name":"Jordan Legg","creator_url":"https://huggingface.co/takarajordan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","text-classification","feature-extraction","Japanese","English"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Axel232/x_dataset_44.","url":"https://huggingface.co/datasets/Axel232/x_dataset_44","creator_name":"Pits","creator_url":"https://huggingface.co/Axel232","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_17","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlemdatameow/reddit_dataset_17.","url":"https://huggingface.co/datasets/mlemdatameow/reddit_dataset_17","creator_name":"Mlem Meow","creator_url":"https://huggingface.co/mlemdatameow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_11","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smmrokn/reddit_dataset_11.","url":"https://huggingface.co/datasets/smmrokn/reddit_dataset_11","creator_name":"Mohammad Mahdi","creator_url":"https://huggingface.co/smmrokn","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_31933","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_31933.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_31933","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_5","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_5.","url":"https://huggingface.co/datasets/suul999922/x_dataset_5","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"squad_v2_french_translated_fr_prompt_context_generation_with_question","keyword":"squad_v2_french_translated","description":"\n\t\n\t\t\n\t\tsquad_v2_french_translated_fr_prompt_context_generation_with_question\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nsquad_v2_french_translated_fr_prompt_context_generation_with_question is a subset of the Dataset of French Prompts (DFP).It contains 3,795,312 rows that can be used for a context-generation (with question) task.The original data (without prompts) comes from the dataset pragnakalp/squad_v2_french_translated and was augmented by questions in SQUAD 2.0 format in the FrenchQA dataset.\nA list of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_context_generation_with_question.","url":"https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_context_generation_with_question","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","squad_v2_french_translated"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_108","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wavecreator22/reddit_dataset_108.","url":"https://huggingface.co/datasets/wavecreator22/reddit_dataset_108","creator_name":"Krovanov","creator_url":"https://huggingface.co/wavecreator22","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_49","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/reddit_dataset_49.","url":"https://huggingface.co/datasets/kimbuja/reddit_dataset_49","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_28","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_28.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_28","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"biblenlp-corpus-mmteb","keyword":"multilingual","description":"This dataset pre-computes all English-centric directions from bible-nlp/biblenlp-corpus, and as a result loading is significantly faster.\nLoading example:\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"davidstap/biblenlp-corpus-mmteb\", \"eng-arb\", trust_remote_code=True)\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 28723\n    })\n    validation: Dataset({\n        features: ['eng', 'arb'],\n        num_rows: 1578\n    })‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb.","url":"https://huggingface.co/datasets/davidstap/biblenlp-corpus-mmteb","creator_name":"David Stap","creator_url":"https://huggingface.co/davidstap","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["no-annotation","expert-generated","translation","multilingual","Arifama-Miniafia"],"keywords_longer_than_N":true},
	{"name":"x_dataset_55395","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_55395.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_55395","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"toxi-text-3M","keyword":"multilingual","description":"This is a large multilingual toxicity dataset with 3M rows of text data from 55 natural languages, all of which are written/sent by humans, not machine translation models.\nThe preprocessed training data alone consists of 2,880,667 rows of comments, tweets, and messages. Among these rows, 416,529 are classified as toxic, while the remaining 2,463,773 are considered neutral. Below is a table to illustrate the data composition:\n\n\t\n\t\t\n\nToxic\nNeutral\nTotal\n\n\n\t\t\nmultilingual-train-deduplicated.csv‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/toxi-text-3M.","url":"https://huggingface.co/datasets/FredZhang7/toxi-text-3M","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","Arabic","Spanish","Panjabi"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs","keyword":"multilingual","description":"\n    \n\n\nmLLM IMPLEMENTATION OF Intel/orca_dpo_pairs.\nLANGUAGES:\nARABIC\nCHINESE\nFRENCH\nGERMAN\nRUSSIAN\nSPANISH\nTURKISH\n(WIP)\n","url":"https://huggingface.co/datasets/multilingual/orca_dpo_pairs","creator_name":"mLLM multilingual","creator_url":"https://huggingface.co/multilingual","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","German","French"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_178","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Aniruddh79012/reddit_dataset_178.","url":"https://huggingface.co/datasets/Aniruddh79012/reddit_dataset_178","creator_name":"Singh","creator_url":"https://huggingface.co/Aniruddh79012","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_3","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_3.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_3","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_29","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_29.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_29","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_104","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_104.","url":"https://huggingface.co/datasets/gk4u/x_dataset_104","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_19","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_19.","url":"https://huggingface.co/datasets/suul999922/x_dataset_19","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_22","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_22.","url":"https://huggingface.co/datasets/James096/reddit_dataset_22","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_252","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bill199284/reddit_dataset_252.","url":"https://huggingface.co/datasets/bill199284/reddit_dataset_252","creator_name":"thomas","creator_url":"https://huggingface.co/bill199284","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"kz-rus-articles-comprehensive","keyword":"multilingual","description":"\n\n\n\t\n\t\t\n\t\tüá∞üáøüá∑üá∫ Kazakh-Russian Articles Comprehensive Dataset\n\t\n\nA high-quality bilingual corpus for cross-lingual NLP research\n\n\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tüìã Dataset Overview\n\t\n\nThe Kazakh-Russian Articles Comprehensive Dataset is a meticulously curated bilingual corpus designed to advance natural language processing research for Kazakh and Russian languages. This dataset addresses the critical need for high-quality parallel and comparable text resources in Central Asian language pairs, particularly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Adilbai/kz-rus-articles-comprehensive.","url":"https://huggingface.co/datasets/Adilbai/kz-rus-articles-comprehensive","creator_name":"Baidalin Adilzhan","creator_url":"https://huggingface.co/Adilbai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-classification","text-generation","question-answering","machine-generated"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0506234","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_0506234.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_0506234","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"clams","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCLAMS - Cross-Linguistic Assessment of Models on Syntax\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCLAMS (Cross-Linguistic Assessment of Models on Syntax) is a dataset of syntactic minimal pairs for English, French, German, Hebrew, and Russian. The dataset contains grammaticality judgment pairs (good/bad sentence pairs) for various syntactic phenomena, designed to evaluate language models' syntactic knowledge across different languages.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains minimal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/juletxara/clams.","url":"https://huggingface.co/datasets/juletxara/clams","creator_name":"Julen Etxaniz","creator_url":"https://huggingface.co/juletxara","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","expert-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"x_dataset_17","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_17.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_17","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_4","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andreans27/x_dataset_4.","url":"https://huggingface.co/datasets/andreans27/x_dataset_4","creator_name":"Andrean","creator_url":"https://huggingface.co/andreans27","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_223","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mattnguyendev/reddit_dataset_223.","url":"https://huggingface.co/datasets/Mattnguyendev/reddit_dataset_223","creator_name":"Head","creator_url":"https://huggingface.co/Mattnguyendev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"tydi_xor_rc","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydi_xor_rc\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages. \nXORQA is an extension of the original TyDi QA dataset to also include unanswerable questions, where context documents are only in English but questions are in 7 languages.\nXOR-AttriQA contains annotated attribution data for a sample of XORQA.\nThis dataset is a combined and simplified version of the Reading Comprehension data from XORQA and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/coastalcph/tydi_xor_rc.","url":"https://huggingface.co/datasets/coastalcph/tydi_xor_rc","creator_name":"CoAStaL NLP Group","creator_url":"https://huggingface.co/coastalcph","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"PolyFiQA-Expert","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for PolyFiQA-Expert\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPolyFiQA-Expert is a multilingual financial question-answering dataset designed to evaluate expert-level financial reasoning in low-resource and multilingual settings. Each instance consists of a task identifier, a query prompt, an associated financial question, and the correct answer.The Expert split emphasizes complex, high-level financial understanding, requiring deeper domain knowledge and nuanced reasoning.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TheFinAI/PolyFiQA-Expert.","url":"https://huggingface.co/datasets/TheFinAI/PolyFiQA-Expert","creator_name":"The Fin AI","creator_url":"https://huggingface.co/TheFinAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","Chinese","jp","Spanish"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_174","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Alen77/reddit_dataset_174.","url":"https://huggingface.co/datasets/Alen77/reddit_dataset_174","creator_name":"Moro","creator_url":"https://huggingface.co/Alen77","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_44.","url":"https://huggingface.co/datasets/gk4u/x_dataset_44","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_22","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_22.","url":"https://huggingface.co/datasets/suul999922/x_dataset_22","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Function_Completion","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Function_Completion is a dataset of BenchMAX, sourcing from humanevalplus, which evaluates the code generation capability in multilingual scenarios.\nWe extend the original English dataset to 16 non-English languages.\nThe data is first translated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Function_Completion.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Function_Completion","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Function_Completion","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Function_Completion is a dataset of BenchMAX, sourcing from humanevalplus, which evaluates the code generation capability in multilingual scenarios.\nWe extend the original English dataset to 16 non-English languages.\nThe data is first translated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Function_Completion.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Function_Completion","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_101","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hfgfchris/reddit_dataset_101.","url":"https://huggingface.co/datasets/hfgfchris/reddit_dataset_101","creator_name":"Christian Behrens","creator_url":"https://huggingface.co/hfgfchris","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_57303","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_57303.","url":"https://huggingface.co/datasets/icedwind/x_dataset_57303","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_197","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chaiamy/reddit_dataset_197.","url":"https://huggingface.co/datasets/chaiamy/reddit_dataset_197","creator_name":"Amy","creator_url":"https://huggingface.co/chaiamy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"PolyGuardMix","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tPolyGuard: A Multilingual Safety Moderation Tool for 17 Languages\n\t\n\nAbstract: Truly multilingual safety moderation efforts for Large Language Models (LLMs) have been hindered by a narrow focus on a small set of languages (e.g., English, Chinese) as well as a limited scope of safety definition, resulting in significant gaps in moderation capabilities. To bridge these gaps, we release PolyGuard, a new state-of-the-art multilingual safety model for safeguarding LLM generations, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ToxicityPrompts/PolyGuardMix.","url":"https://huggingface.co/datasets/ToxicityPrompts/PolyGuardMix","creator_name":"ToxicityPrompts","creator_url":"https://huggingface.co/ToxicityPrompts","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","Chinese","Czech","Dutch","English"],"keywords_longer_than_N":true},
	{"name":"Collective-Corpus","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüß† Collective Corpus ‚Äî Universal Pretraining + Finetuning Dataset (500B+ Tokens)\n\t\n\n\n\n\nCollective-Corpus is a massive-scale, multi-domain dataset designed to train Transformer-based language models from scratch and finetune them across a wide variety of domains ‚Äî all in one place.\n\n\t\n\t\n\t\n\t\tüìö Dataset Scope\n\t\n\nThis dataset aims to cover the full LLM lifecycle, from raw pretraining to domain-specialized finetuning.\n\t\n\t\t\n\t\t1. Pretraining Corpus\n\t\n\n\nLarge-scale, diverse multilingual text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dignity045/Collective-Corpus.","url":"https://huggingface.co/datasets/dignity045/Collective-Corpus","creator_name":"Dhiraj","creator_url":"https://huggingface.co/dignity045","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","text-classification","summarization","question-answering"],"keywords_longer_than_N":true},
	{"name":"x_dataset_031079","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_031079.","url":"https://huggingface.co/datasets/james-1111/x_dataset_031079","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_11","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_11.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_11","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_19","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_19.","url":"https://huggingface.co/datasets/James096/x_dataset_19","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"pdf-rag-embed-bench","keyword":"multilingual","description":"This is a benchmark dataset for PDF RAG embedding systems.\nSee gpahal/pdf-rag-embed-bench for more details.\n","url":"https://huggingface.co/datasets/gpahal/pdf-rag-embed-bench","creator_name":"Garvit Pahal","creator_url":"https://huggingface.co/gpahal","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","mit","< 1K","Document","Image"],"keywords_longer_than_N":true},
	{"name":"continued-pretraining-llama-format","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOpen Paws Continued Pretraining Llama Format\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Specialized Data\nFormat: CSV (Comma-separated values)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/continued-pretraining-llama-format.","url":"https://huggingface.co/datasets/open-paws/continued-pretraining-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"muInstruct-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tmuInstruct-ru\n\t\n\nTranslated instructions from EleutherAI/muInstruct into Russian using gemini-flash-1.5-8b.\n","url":"https://huggingface.co/datasets/d0rj/muInstruct-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","EleutherAI/muInstruct","Russian"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0508228","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_0508228.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_0508228","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_10830","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_10830.","url":"https://huggingface.co/datasets/momo1942/x_dataset_10830","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"pedsite","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Pedsite.ru Pedagogical Website\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 9,536 educational materials from the pedsite.ru platform, a website for teachers to share experiences and showcase the best creative findings in the field of teaching and learning. The dataset includes information such as material titles, URLs, download URLs, author information, and extracted text content where available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/pedsite.","url":"https://huggingface.co/datasets/nyuuzyou/pedsite","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"pedsite","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Pedsite.ru Pedagogical Website\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 9,536 educational materials from the pedsite.ru platform, a website for teachers to share experiences and showcase the best creative findings in the field of teaching and learning. The dataset includes information such as material titles, URLs, download URLs, author information, and extracted text content where available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/pedsite.","url":"https://huggingface.co/datasets/nyuuzyou/pedsite","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"doc4web","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Doc4web.ru Documents\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and content for 223,739 documents from the doc4web.ru platform, a document hosting service for students and teachers. The dataset includes information such as document titles, URLs, download links, and file paths. The documents cover various educational topics and are primarily in Russian.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual, with Russian being the primary language. Other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/doc4web.","url":"https://huggingface.co/datasets/nyuuzyou/doc4web","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"doc4web","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Doc4web.ru Documents\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and content for 223,739 documents from the doc4web.ru platform, a document hosting service for students and teachers. The dataset includes information such as document titles, URLs, download links, and file paths. The documents cover various educational topics and are primarily in Russian.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual, with Russian being the primary language. Other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/doc4web.","url":"https://huggingface.co/datasets/nyuuzyou/doc4web","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"znanio-others","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Other Educational Materials\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 3,092 educational files from the platform [znanio.ru] (https://znanio.ru) that were not categorized in the main groups. Znanio.ru is a resource for teachers, educators, students, and parents that provides a variety of educational content.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The majority of the content‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-others.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-others","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","document-question-answering","image-classification","other","found"],"keywords_longer_than_N":true},
	{"name":"znanio-others","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Other Educational Materials\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 3,092 educational files from the platform [znanio.ru] (https://znanio.ru) that were not categorized in the main groups. Znanio.ru is a resource for teachers, educators, students, and parents that provides a variety of educational content.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The majority of the content‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-others.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-others","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","document-question-answering","image-classification","other","found"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_139","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_139.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_139","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_461985","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_461985.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_461985","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vouu/x_dataset_44.","url":"https://huggingface.co/datasets/vouu/x_dataset_44","creator_name":"Pham Manh Truong","creator_url":"https://huggingface.co/vouu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"ESCIReranking","keyword":"multilingual","description":"\n  ESCIReranking\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\n\n\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://github.com/amazon-science/esci-data/\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"ESCIReranking\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nTo learn more about how to run models on mteb task check out‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ESCIReranking.","url":"https://huggingface.co/datasets/mteb/ESCIReranking","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"squad_v2_french_translated_fr_prompt_context_generation_with_answer_and_question","keyword":"squad_v2_french_translated","description":"\n\t\n\t\t\n\t\tsquad_v2_french_translated_fr_prompt_context_generation_with_answer_and_question\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nsquad_v2_french_translated_fr_prompt_context_generation_with_answer_and_question is a subset of the Dataset of French Prompts (DFP).It contains 1,271,928 rows that can be used for a context-generation (with answer and question) task.The original data (without prompts) comes from the dataset pragnakalp/squad_v2_french_translated and was augmented by questions in SQUAD 2.0 format in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_context_generation_with_answer_and_question.","url":"https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_context_generation_with_answer_and_question","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","squad_v2_french_translated"],"keywords_longer_than_N":true},
	{"name":"instructional-dialogues-multilingual","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Instructional Dialogues (10-Language Dataset)\n\t\n\nMultilingual Instructional Dialogues is a high-quality dataset of 100 structured, goal-oriented dialogues in 10 major world languages, created for training and fine-tuning AI assistants, chatbots, and instruction-tuned large language models.\nEach dialogue simulates a clear, polite interaction where a user asks for guidance on how to perform a task, and the assistant responds with easy-to-follow steps. This dataset has been‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Raftico/instructional-dialogues-multilingual.","url":"https://huggingface.co/datasets/Raftico/instructional-dialogues-multilingual","creator_name":"Raftico Oy","creator_url":"https://huggingface.co/Raftico","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","1K - 10K","csv","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"English-Hinglish-TOP","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tEnglish Hinglish (TOP Dataset)\n\t\n\nThis dataset is generated from Hinglish-TOP Dataset.\nData distribution:\n\nTrain a. Human Generated - 6513 b. Synthetically generated - 170083  \nValidation a. Human Generated - 1390 b. Synthetically generated - 0  \nTest a. Human Generated - 6513 b. Synthetically generated - 0\n\n","url":"https://huggingface.co/datasets/rvv-karma/English-Hinglish-TOP","creator_name":"Rahul Vishwakarma","creator_url":"https://huggingface.co/rvv-karma","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","multilingual","translation","English"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_217","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tensorshield/reddit_dataset_217.","url":"https://huggingface.co/datasets/tensorshield/reddit_dataset_217","creator_name":"Tensor Shield","creator_url":"https://huggingface.co/tensorshield","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_12.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_12","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"HINMIX_hi-en","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Hindi English Codemix Dataset - HINMIX\n\t\n\nHINMIX is a massive parallel codemixed dataset for Hindi-English code switching.\nSee the üìö paper on arxiv to dive deep into this synthetic codemix data generation pipeline. \nDataset contains 4.2M fully parallel sentences in 6 Hindi-English forms.\nFurther, we release gold standard codemix dev and test set manually translated by proficient bilingual annotators.\n\nDev Set consists of 280 examples\nTest set consists of 2507 examples‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kartikagg98/HINMIX_hi-en.","url":"https://huggingface.co/datasets/kartikagg98/HINMIX_hi-en","creator_name":"kartik","creator_url":"https://huggingface.co/kartikagg98","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Hindi","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"x_dataset_63648","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_63648.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_63648","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_184","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mo0han3d/reddit_dataset_184.","url":"https://huggingface.co/datasets/Mo0han3d/reddit_dataset_184","creator_name":"AbdElMonsef","creator_url":"https://huggingface.co/Mo0han3d","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_218","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SAVE0x0/reddit_dataset_218.","url":"https://huggingface.co/datasets/SAVE0x0/reddit_dataset_218","creator_name":"x","creator_url":"https://huggingface.co/SAVE0x0","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_250","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aaron927dev/reddit_dataset_250.","url":"https://huggingface.co/datasets/aaron927dev/reddit_dataset_250","creator_name":"William Hudson","creator_url":"https://huggingface.co/aaron927dev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_39","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_39.","url":"https://huggingface.co/datasets/James096/x_dataset_39","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_11","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jacksss123/reddit_dataset_11.","url":"https://huggingface.co/datasets/Jacksss123/reddit_dataset_11","creator_name":"Tony","creator_url":"https://huggingface.co/Jacksss123","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/reddit_dataset_44.","url":"https://huggingface.co/datasets/arrmlet/reddit_dataset_44","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_8191","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/x_dataset_8191.","url":"https://huggingface.co/datasets/StormKing99/x_dataset_8191","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_216","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Pavlo0912/reddit_dataset_216.","url":"https://huggingface.co/datasets/Pavlo0912/reddit_dataset_216","creator_name":"lim","creator_url":"https://huggingface.co/Pavlo0912","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_193","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sesen01/reddit_dataset_193.","url":"https://huggingface.co/datasets/sesen01/reddit_dataset_193","creator_name":"Selim Esen","creator_url":"https://huggingface.co/sesen01","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0212148","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_0212148.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_0212148","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"xP3x-Kongo","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for xP3x Kikongo Focus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nxP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts & datasets across 277 languages & 16 NLP tasks. It contains all of xP3 + much more! It is used for training future contenders of mT0 & BLOOMZ at project Aya @C4AI üß°\n\n\nCreation: The dataset can be recreated using instructions available here together with the file in this repository named xp3x_create.py. We provide this version to save‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Svngoku/xP3x-Kongo.","url":"https://huggingface.co/datasets/Svngoku/xP3x-Kongo","creator_name":"NIONGOLO Chrys F√©-Marty","creator_url":"https://huggingface.co/Svngoku","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","translation","expert-generated","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_221","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bit0/reddit_dataset_221.","url":"https://huggingface.co/datasets/bit0/reddit_dataset_221","creator_name":"sn13","creator_url":"https://huggingface.co/bit0","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_7","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_7.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_7","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_136","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/universe-riez/reddit_dataset_136.","url":"https://huggingface.co/datasets/universe-riez/reddit_dataset_136","creator_name":"universe","creator_url":"https://huggingface.co/universe-riez","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"ukr-lit","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Ukr-lit.com.ua Presentations\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 18,001 presentations from the ukr-lit.com.ua platform, a presentation storage and viewing service primarily focused on Ukrainian literature and related subjects. The dataset includes information such as presentation titles, URLs, download URLs, and filepaths for the original presentation files.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual, with Ukrainian‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/ukr-lit.","url":"https://huggingface.co/datasets/nyuuzyou/ukr-lit","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"ukr-lit","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Ukr-lit.com.ua Presentations\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 18,001 presentations from the ukr-lit.com.ua platform, a presentation storage and viewing service primarily focused on Ukrainian literature and related subjects. The dataset includes information such as presentation titles, URLs, download URLs, and filepaths for the original presentation files.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual, with Ukrainian‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/ukr-lit.","url":"https://huggingface.co/datasets/nyuuzyou/ukr-lit","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/quanglt/reddit_dataset_44.","url":"https://huggingface.co/datasets/quanglt/reddit_dataset_44","creator_name":"Quang Le","creator_url":"https://huggingface.co/quanglt","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_120","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Spark0801/reddit_dataset_120.","url":"https://huggingface.co/datasets/Spark0801/reddit_dataset_120","creator_name":"SparkLiu","creator_url":"https://huggingface.co/Spark0801","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"vtuber-youtube-list-dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tVTuber YouTube Channel List Dataset\n\t\n\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅVTuber „ÉÅ„É£„É≥„Éç„É´„Å® VTuber „Åß„Å™„ÅÑÔºà‰æãÔºöÊñôÁêÜ„ÉÅ„É£„É≥„Éç„É´„Å™„Å©Ôºâ„ÅÆ YouTube „ÉÅ„É£„É≥„Éç„É´„ÅÆ„É°„Çø„Éá„Éº„Çø„Çí JSONL ÂΩ¢Âºè„Åß„Åæ„Å®„ÇÅ„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØ‰ª•‰∏ã„ÅÆ„Éï„Ç£„Éº„É´„Éâ„ÇíÂê´„Çì„Åß„ÅÑ„Åæ„ÅôÔºö\n\nchannel_id: YouTube „ÉÅ„É£„É≥„Éç„É´„ÅÆÂõ∫Êúâ ID\ntitle: „ÉÅ„É£„É≥„Éç„É´„ÅÆ„Çø„Ç§„Éà„É´\ndescription: „ÉÅ„É£„É≥„Éç„É´„ÅÆË™¨ÊòéÊñá\ntext: „Çø„Ç§„Éà„É´„Å®Ë™¨ÊòéÊñá„ÇíÈÄ£Áµê„Åó„Åü„ÉÜ„Ç≠„Çπ„ÉàÔºà„É¢„Éá„É´„ÅÆÂÖ•ÂäõÁî®„Å´Âà©Áî®„Åß„Åç„Åæ„ÅôÔºâ\nlabel: „Éê„Ç§„Éä„É™„É©„Éô„É´ÔºàVTuber „ÅÆÂ†¥Âêà„ÅØ 1„ÄÅÈùûVTuber „ÅÆÂ†¥Âêà„ÅØ 0Ôºâ\n\n\n\t\n\t\t\n\t\n\t\n\t\t„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊ¶ÇË¶Å\n\t\n\n\nÁõÆÁöÑ: „Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅ„ÉÜ„Ç≠„Çπ„ÉàÂàÜÈ°û„Çø„Çπ„ÇØ„Å´„Åä„ÅÑ„Å¶ VTuber „ÉÅ„É£„É≥„Éç„É´„Åã„Å©„ÅÜ„Åã„ÇíÂà§ÂÆö„Åô„Çã„É¢„Éá„É´„ÅÆÂ≠¶Áøí„Åä„Çà„Å≥Ë©ï‰æ°„Å´Âà©Áî®„Åß„Åç„Åæ„Åô„ÄÇ\nË®ÄË™û: ‰∏ª„Å´Êó•Êú¨Ë™û„Åß„Åô„Åå„ÄÅ‰∏ÄÈÉ®Ëã±Ë™û„ÇÑ„Åù„ÅÆ‰ªñ„ÅÆË®ÄË™û„ÅÆË®òËø∞„ÇÇÂê´„Åæ„Çå„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n„É©„Ç§„Çª„É≥„Çπ: MIT License\n\n\n\t\n\t\n\t\n\t\t‰ΩøÁî®‰∏ä„ÅÆÊ≥®ÊÑèÁÇπ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset.","url":"https://huggingface.co/datasets/ayousanz/vtuber-youtube-list-dataset","creator_name":"yousan","creator_url":"https://huggingface.co/ayousanz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","no-annotation","multilingual","original","Japanese"],"keywords_longer_than_N":true},
	{"name":"M3GIA","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tM3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability\n\t\n\n[üåê Homepage] | ü§ó Dataset | ü§ó Paper | üìñ arXiv | üíª GitHub\nThe evaluation code can be found in üíª GitHub.\n[Abstract]\nAs recent multi-modality large language models (MLLMs) have shown formidable proficiency on various complex tasks, there has been increasing attention on debating whether these models could eventually mirror human intelligence. However, existing benchmarks mainly focus on evaluating‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Songweii/M3GIA.","url":"https://huggingface.co/datasets/Songweii/M3GIA","creator_name":"Wei Song","creator_url":"https://huggingface.co/Songweii","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","Spanish","French","Portuguese"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_16","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vuhongtien/reddit_dataset_16.","url":"https://huggingface.co/datasets/vuhongtien/reddit_dataset_16","creator_name":"Vu Hong Tien","creator_url":"https://huggingface.co/vuhongtien","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"fante_multispeaker_audio_transcribed","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tFante Multispeaker Audio Transcribed Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Fante Multispeaker Audio Transcribed dataset is a collection of speech recordings and their transcriptions in Fante, a widely spoken dialect of the Akan language in Ghana. The dataset is designed for training and evaluating automatic speech recognition (ASR) models and other natural language processing (NLP) applications.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: The dataset is derived from the Financial Inclusion Speech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/fante_multispeaker_audio_transcribed.","url":"https://huggingface.co/datasets/michsethowusu/fante_multispeaker_audio_transcribed","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","Fanti","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_223","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_223.","url":"https://huggingface.co/datasets/James096/reddit_dataset_223","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_12970","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_12970.","url":"https://huggingface.co/datasets/icedwind/x_dataset_12970","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"NaijaSenti","keyword":"multilingual","description":"\n  NaijaSenti\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNaijaSenti is the first large-scale human-annotated Twitter sentiment dataset for the four most widely spoken languages in Nigeria ‚Äî Hausa, Igbo, Nigerian-Pidgin, and Yor√πb√° ‚Äî consisting of around 30,000 annotated tweets per language, including a significant fraction of code-mixed tweets.\n\n\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, Written\n\n\nReference\nhttps://github.com/hausanlp/NaijaSenti\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NaijaSenti.","url":"https://huggingface.co/datasets/mteb/NaijaSenti","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"x_dataset_223","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mattnguyendev/x_dataset_223.","url":"https://huggingface.co/datasets/Mattnguyendev/x_dataset_223","creator_name":"Head","creator_url":"https://huggingface.co/Mattnguyendev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_206","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chidinna/reddit_dataset_206.","url":"https://huggingface.co/datasets/chidinna/reddit_dataset_206","creator_name":"chidinn","creator_url":"https://huggingface.co/chidinna","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_52","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/1980QVQ/reddit_dataset_52.","url":"https://huggingface.co/datasets/1980QVQ/reddit_dataset_52","creator_name":"QVQ","creator_url":"https://huggingface.co/1980QVQ","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_62103","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_62103.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_62103","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-898550","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tFiQA2018-256-24-gpt-4o-2024-05-13-898550 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial sentiment and QA analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-898550 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-898550.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-898550","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"truthful_judge","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for HiTZ/truthful_judge (Truthfulness Data)\n\t\n\nThis dataset provides training data for fine-tuning LLM-as-a-Judge models to evaluate the truthfulness of text generated by other language models. It is a core component of the \"Truth Knows No Language: Evaluating Truthfulness Beyond English\" project, extending such evaluations to English, Basque, Catalan, Galician, and Spanish.\nThe dataset is provided in two configurations:\n\nen: Training data for judging truthfulness in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/truthful_judge.","url":"https://huggingface.co/datasets/HiTZ/truthful_judge","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Catalan","Galician","Basque"],"keywords_longer_than_N":true},
	{"name":"x_dataset_28","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_28.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_28","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_63354","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/x_dataset_63354.","url":"https://huggingface.co/datasets/StormKing99/x_dataset_63354","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_060640","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_060640.","url":"https://huggingface.co/datasets/john-1111/x_dataset_060640","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_test","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_test.","url":"https://huggingface.co/datasets/suul999922/x_dataset_test","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/veyhoranohy/x_dataset_44.","url":"https://huggingface.co/datasets/veyhoranohy/x_dataset_44","creator_name":"Steve Karadimas","creator_url":"https://huggingface.co/veyhoranohy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0301244","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_0301244.","url":"https://huggingface.co/datasets/james-1111/x_dataset_0301244","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"twi_multispeaker_audio_transcribed","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTwi Multispeaker Audio Transcribed Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Twi Multispeaker Audio Transcribed dataset is a collection of speech recordings and their transcriptions in Asante Twi, a widely spoken dialect of the Akan language in Ghana. The dataset is designed for training and evaluating automatic speech recognition (ASR) models and other natural language processing (NLP) applications.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: The dataset is derived from the Financial Inclusion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi_multispeaker_audio_transcribed.","url":"https://huggingface.co/datasets/michsethowusu/twi_multispeaker_audio_transcribed","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","Twi","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"afrimgsm","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for afrimgsm\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAFRIMGSM is an evaluation dataset comprising translations of a subset of the GSM8k dataset into 16 African languages. \nIt includes test sets across all 18 languages, maintaining an English and French subsets from the original GSM8k dataset. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThere are 18 languages available :\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe examples look like this for English:\nfrom datasets import load_dataset\ndata =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yuntian-deng/afrimgsm.","url":"https://huggingface.co/datasets/yuntian-deng/afrimgsm","creator_name":"Yuntian Deng","creator_url":"https://huggingface.co/yuntian-deng","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","natural-language-inference","multilingual","gsm8k","Amharic"],"keywords_longer_than_N":true},
	{"name":"IN22GenBitextMining","keyword":"multilingual","description":"\n  IN22GenBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Gen is a n-way parallel general-purpose multi-domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Legal, Government, News, Religious, Non-fiction, Written\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Gen\n\n\n\t\n\nSource datasets:\n\nmteb/IN22-Gen\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22GenBitextMining.","url":"https://huggingface.co/datasets/mteb/IN22GenBitextMining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","multilingual","mteb/IN22-Gen","Assamese"],"keywords_longer_than_N":true},
	{"name":"mmlu-winogrande-afr","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBridging the Gap: Enhancing LLM Performance for Low-Resource African Languages with New Benchmarks, Fine-Tuning, and Cultural Adjustments\n\t\n\nAuthors:\nTuka Alhanai tuka@ghamut.com, Adam Kasumovic adam.kasumovic@ghamut.com, Mohammad Ghassemi ghassemi@ghamut.com, Aven Zitzelberger aven.zitzelberger@ghamut.com, Jessica Lundin jessica.lundin@gatesfoundation.org, Guillaume Chabot-Couture Guillaume.Chabot-Couture@gatesfoundation.org\nThis HuggingFace Dataset contains the human-translated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Institute-Disease-Modeling/mmlu-winogrande-afr.","url":"https://huggingface.co/datasets/Institute-Disease-Modeling/mmlu-winogrande-afr","creator_name":"Institute-Disease-Modeling","creator_url":"https://huggingface.co/Institute-Disease-Modeling","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","expert-generated","multilingual","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_99","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jasonmoore92/reddit_dataset_99.","url":"https://huggingface.co/datasets/jasonmoore92/reddit_dataset_99","creator_name":"Jason Moore","creator_url":"https://huggingface.co/jasonmoore92","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_139","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_139.","url":"https://huggingface.co/datasets/gk4u/x_dataset_139","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_133","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dineshreddy/reddit_dataset_133.","url":"https://huggingface.co/datasets/dineshreddy/reddit_dataset_133","creator_name":"dinesh reddy","creator_url":"https://huggingface.co/dineshreddy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_19124","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_19124.","url":"https://huggingface.co/datasets/momo1942/x_dataset_19124","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"wikidata","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikidata Entities Connected to Wikipedia\n\t\n\nThis dataset is a multilingual, JSON-formatted version of the Wikidata dump from September 18, 2024. It only includes Wikidata entities that are connected to a Wikipedia page in any language.\nA total of 112,467,802 entities are included in the original data dump, of which 30,072,707 are linked to a Wikipedia page (26.73% of all entities have at least one Wikipedia sitelink).\n\nCurated by: Jonathan Fraine & Philippe Saad√©, Wikimedia Deutschland‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/philippesaade/wikidata.","url":"https://huggingface.co/datasets/philippesaade/wikidata","creator_name":"Philippe Saad√©","creator_url":"https://huggingface.co/philippesaade","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","cc0-1.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_63","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Spark0801/reddit_dataset_63.","url":"https://huggingface.co/datasets/Spark0801/reddit_dataset_63","creator_name":"SparkLiu","creator_url":"https://huggingface.co/Spark0801","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"squad_v2_french_translated_fr_prompt_question_generation_with_context","keyword":"squad_v2_french_translated","description":"\n\t\n\t\t\n\t\tsquad_v2_french_translated_fr_prompt_question_generation_with_context\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nsquad_v2_french_translated_fr_prompt_question_generation_with_context is a subset of the Dataset of French Prompts (DFP).It contains 3,795,312 rows that can be used for a question-generation (with context) task.The original data (without prompts) comes from the dataset pragnakalp/squad_v2_french_translated and was augmented by questions in SQUAD 2.0 format in the FrenchQA dataset.\nA list of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_question_generation_with_context.","url":"https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_question_generation_with_context","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","squad_v2_french_translated"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vouu/reddit_dataset_44.","url":"https://huggingface.co/datasets/vouu/reddit_dataset_44","creator_name":"Pham Manh Truong","creator_url":"https://huggingface.co/vouu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_245","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/reddit_dataset_245.","url":"https://huggingface.co/datasets/arrmlet/reddit_dataset_245","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_31731","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hshwk1983/x_dataset_31731.","url":"https://huggingface.co/datasets/hshwk1983/x_dataset_31731","creator_name":"hshwh","creator_url":"https://huggingface.co/hshwk1983","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_36658","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rainbowbridge/x_dataset_36658.","url":"https://huggingface.co/datasets/rainbowbridge/x_dataset_36658","creator_name":"Rain","creator_url":"https://huggingface.co/rainbowbridge","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_4561","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_4561.","url":"https://huggingface.co/datasets/icedwind/x_dataset_4561","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_021084","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_021084.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_021084","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_39","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_39.","url":"https://huggingface.co/datasets/James096/reddit_dataset_39","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"multilingual-speech-commands-15lang","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Speech Commands Dataset (15 Languages, Augmented)\n\t\n\nThis dataset contains augmented speech command samples in 15 languages, derived from multiple public datasets. Only commands that overlap with the Google Speech Commands (GSC) vocabulary are included, making the dataset suitable for multilingual keyword spotting tasks aligned with GSC-style classification.\nAudio samples have been augmented using standard audio techniques to improve model robustness (e.g., time-shifting‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/artur-muratov/multilingual-speech-commands-15lang.","url":"https://huggingface.co/datasets/artur-muratov/multilingual-speech-commands-15lang","creator_name":"Artur Muratov","creator_url":"https://huggingface.co/artur-muratov","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","Russian","Kazakh","Tatar","Arabic"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0110104","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_0110104.","url":"https://huggingface.co/datasets/william-1111/x_dataset_0110104","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_031267","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_031267.","url":"https://huggingface.co/datasets/james-1111/x_dataset_031267","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"NusaParagraphTopicClassification","keyword":"multilingual","description":"\n  NusaParagraphTopicClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNusaParagraphTopicClassification is a multi-class topic classification on 10 Indonesian languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNon-fiction, Fiction, Written\nReference\nhttps://github.com/IndoNLP/nusa-writes\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NusaParagraphTopicClassification.","url":"https://huggingface.co/datasets/mteb/NusaParagraphTopicClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","human-annotated","multilingual","Batak Toba"],"keywords_longer_than_N":true},
	{"name":"kz-gov-complaints-data-kz-ru","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKazakhstan Government Complaints Dataset (Kazakh/Russian)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains synthetically generated citizen complaints directed to the Kazakhstan government along with estimated government responses. The data is generated using Gemini 2.5 Pro and includes complaints written in both Kazakh (kz) and Russian (ru) languages, reflecting the linguistic diversity of Kazakhstan's population.\nThe dataset aims to facilitate research in natural language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Adilbai/kz-gov-complaints-data-kz-ru.","url":"https://huggingface.co/datasets/Adilbai/kz-gov-complaints-data-kz-ru","creator_name":"Baidalin Adilzhan","creator_url":"https://huggingface.co/Adilbai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","sentiment-analysis","multi-class-classification","machine-generated"],"keywords_longer_than_N":true},
	{"name":"kz-gov-complaints-data-kz-ru","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKazakhstan Government Complaints Dataset (Kazakh/Russian)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains synthetically generated citizen complaints directed to the Kazakhstan government along with estimated government responses. The data is generated using Gemini 2.5 Pro and includes complaints written in both Kazakh (kz) and Russian (ru) languages, reflecting the linguistic diversity of Kazakhstan's population.\nThe dataset aims to facilitate research in natural language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Adilbai/kz-gov-complaints-data-kz-ru.","url":"https://huggingface.co/datasets/Adilbai/kz-gov-complaints-data-kz-ru","creator_name":"Baidalin Adilzhan","creator_url":"https://huggingface.co/Adilbai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","sentiment-analysis","multi-class-classification","machine-generated"],"keywords_longer_than_N":true},
	{"name":"x_dataset_76","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_76.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_76","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0307178","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_0307178.","url":"https://huggingface.co/datasets/james-1111/x_dataset_0307178","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_0109104","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/reddit_dataset_0109104.","url":"https://huggingface.co/datasets/william-1111/reddit_dataset_0109104","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_118","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sm4rtdev/reddit_dataset_118.","url":"https://huggingface.co/datasets/sm4rtdev/reddit_dataset_118","creator_name":"ElonScott","creator_url":"https://huggingface.co/sm4rtdev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretraining-data-chunk2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk2.","url":"https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk2","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"actuarial-global-glossary-multilingual","keyword":"multilingual","description":"\n  \n\n\n\n  \n\n\n\t\n\t\t\n\t\tü§ù Connect with me on LinkedIn!\n\t\n\n  \n  Join the mission to make actuarial knowledge accessible worldwide\n  Let's discuss how AI can transform professional education and break language barriers in finance!\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüåç Global Actuarial Glossary - Breaking Language Barriers in Finance\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\tüöÄ The World's Most Comprehensive Multilingual Actuarial Dataset\n\t\n\nImagine: A brilliant actuarial student in Tokyo, a risk analyst in S√£o Paulo, and an insurance executive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/manuelcaccone/actuarial-global-glossary-multilingual.","url":"https://huggingface.co/datasets/manuelcaccone/actuarial-global-glossary-multilingual","creator_name":"Manuel Caccone","creator_url":"https://huggingface.co/manuelcaccone","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","translation","text-generation","question-answering","multi-class-classification"],"keywords_longer_than_N":true},
	{"name":"actuarial-global-glossary-multilingual","keyword":"multilingual","description":"\n  \n\n\n\n  \n\n\n\t\n\t\t\n\t\tü§ù Connect with me on LinkedIn!\n\t\n\n  \n  Join the mission to make actuarial knowledge accessible worldwide\n  Let's discuss how AI can transform professional education and break language barriers in finance!\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüåç Global Actuarial Glossary - Breaking Language Barriers in Finance\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\tüöÄ The World's Most Comprehensive Multilingual Actuarial Dataset\n\t\n\nImagine: A brilliant actuarial student in Tokyo, a risk analyst in S√£o Paulo, and an insurance executive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/manuelcaccone/actuarial-global-glossary-multilingual.","url":"https://huggingface.co/datasets/manuelcaccone/actuarial-global-glossary-multilingual","creator_name":"Manuel Caccone","creator_url":"https://huggingface.co/manuelcaccone","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","translation","text-generation","question-answering","multi-class-classification"],"keywords_longer_than_N":true},
	{"name":"x_dataset_51244","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_51244.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_51244","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"bam-asr-early","keyword":"multilingual","description":"The **Bambara-ASR-Early Audio Dataset** is a multilingual dataset containing audio samples in Bambara, accompanied by semi-expert transcriptions and French translations. \nThe dataset includes various subsets: `jeli-asr`, `oza-mali-pense`, and `rt-data-collection`. Each audio file is aligned with Bambara transcriptions or French translations, making it suitable for tasks such as automatic speech recognition (ASR) and translation. \nData sources include all publicly available collections of audio with Bambara transcriptions as of December 2024, organized for accessibility and usability.\n","url":"https://huggingface.co/datasets/RobotsMali/bam-asr-early","creator_name":"RobotsMali AI4D LAB","creator_url":"https://huggingface.co/RobotsMali","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","audio-language-identification","keyword-spotting"],"keywords_longer_than_N":true},
	{"name":"NeuCLIR2023RetrievalHardNegatives","keyword":"multilingual","description":"\n  NeuCLIR2023RetrievalHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\nSource datasets:\n\nmteb/neuclir-2022\nmteb/neuclir-2023-hard-negatives‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NeuCLIR2023RetrievalHardNegatives.","url":"https://huggingface.co/datasets/mteb/NeuCLIR2023RetrievalHardNegatives","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2022","mteb/neuclir-2023-hard-negatives"],"keywords_longer_than_N":true},
	{"name":"koel-benchmark","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKoel Benchmark Suite\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Koel Benchmark Suite is a comprehensive set of evaluation datasets designed to rigorously test the real-world performance of Text-to-Speech (TTS) models for major Indian languages. The suite focuses on challenges unique to the Indian context, such as code-switching, domain-specific terminology, proper nouns, and complex numeric formats.\nThis dataset was created to help developers and researchers build more natural, accurate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NisargBhavsar25/koel-benchmark.","url":"https://huggingface.co/datasets/NisargBhavsar25/koel-benchmark","creator_name":"Nisarg Bhavsar","creator_url":"https://huggingface.co/NisargBhavsar25","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Hindi","Tamil","Telugu","Kannada"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_145","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trimness8/reddit_dataset_145.","url":"https://huggingface.co/datasets/Trimness8/reddit_dataset_145","creator_name":"Trimness8","creator_url":"https://huggingface.co/Trimness8","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"hailuoai","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for HailuoAI Video Metadata\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 544,646 entries of video metadata collected from HailuoAI, a platform that offers AI-powered image-to-video generation services. Each entry includes detailed information about AI-generated videos, such as video URLs, dimensions, creation parameters, model IDs, and associated tags. This collection represents a diverse range of AI-generated videos that can be used for multimodal analysis, video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/hailuoai.","url":"https://huggingface.co/datasets/nyuuzyou/hailuoai","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-to-video","image-to-video","found"],"keywords_longer_than_N":true},
	{"name":"hailuoai","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for HailuoAI Video Metadata\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 544,646 entries of video metadata collected from HailuoAI, a platform that offers AI-powered image-to-video generation services. Each entry includes detailed information about AI-generated videos, such as video URLs, dimensions, creation parameters, model IDs, and associated tags. This collection represents a diverse range of AI-generated videos that can be used for multimodal analysis, video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/hailuoai.","url":"https://huggingface.co/datasets/nyuuzyou/hailuoai","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-to-video","image-to-video","found"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_178","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qr12138/reddit_dataset_178.","url":"https://huggingface.co/datasets/qr12138/reddit_dataset_178","creator_name":"wu","creator_url":"https://huggingface.co/qr12138","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"informative_judge","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for HiTZ/truthful_judge (Informativeness Data)\n\t\n\nThis dataset provides training data for fine-tuning LLM-as-a-Judge models to evaluate the informativeness of text generated by other language models. It is a core component of the \"Truth Knows No Language: Evaluating Truthfulness Beyond English\" project, extending such evaluations to English, Basque, Catalan, Galician, and Spanish.\nThe dataset is provided in two configurations:\n\nen: Training data for judging informativeness‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/informative_judge.","url":"https://huggingface.co/datasets/HiTZ/informative_judge","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Catalan","Galician","Basque"],"keywords_longer_than_N":true},
	{"name":"neuclir-2023-hard-negatives","keyword":"multilingual","description":"\n  NeuCLIR2023RetrievalHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/neuclir-2023-hard-negatives.","url":"https://huggingface.co/datasets/mteb/neuclir-2023-hard-negatives","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2022","Persian"],"keywords_longer_than_N":true},
	{"name":"news-topics","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tNeuCLIR News Topics\n\t\n\n","url":"https://huggingface.co/datasets/neuclir/news-topics","creator_name":"NeuCLIR TREC Track","creator_url":"https://huggingface.co/neuclir","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","text-ranking","document-retrieval","NIST","multilingual"],"keywords_longer_than_N":true},
	{"name":"FineTome-single-turn-dedup-amharic","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for FineTome Single Turn Conversations - Amharic\n\t\n\nThis dataset contains 83,290 conversational examples translated from English to Amharic, providing high-quality instruction-following conversations for training language models in Amharic.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a translation of the FineTome-single-turn-dedup dataset into Amharic, creating one of the largest publicly available collection of instruction-following‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/addisai/FineTome-single-turn-dedup-amharic.","url":"https://huggingface.co/datasets/addisai/FineTome-single-turn-dedup-amharic","creator_name":"Addis AI","creator_url":"https://huggingface.co/addisai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Amharic","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Multiple_Functions","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Multiple_Functions is a dataset of BenchMAX, sourcing from Nexus.\nThis dataset evaluates the tool use capability in multilingual senarios, which requires a model to call the correct function given the user query and multiple functions.\nWe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Multiple_Functions.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Multiple_Functions","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"reflection-v1-ru_subset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\td0rj/reflection-v1-ru_subset\n\t\n\nTranslated glaiveai/reflection-v1 dataset into Russian language using GPT-4o.\n\nAlmost all the rows of the dataset have been translated. I have removed those translations that do not match the original by the presence of the tags \"thinking\", \"reflection\" and \"output\". Mapping to the original dataset rows can be taken from the \"index\" column.\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata = datasets.load_dataset(\"d0rj/reflection-v1-ru_subset\")\nprint(data)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset.","url":"https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","multilingual","glaiveai/reflection-v1"],"keywords_longer_than_N":true},
	{"name":"reflection-v1-ru_subset","keyword":"translated","description":"\n\t\n\t\t\n\t\td0rj/reflection-v1-ru_subset\n\t\n\nTranslated glaiveai/reflection-v1 dataset into Russian language using GPT-4o.\n\nAlmost all the rows of the dataset have been translated. I have removed those translations that do not match the original by the presence of the tags \"thinking\", \"reflection\" and \"output\". Mapping to the original dataset rows can be taken from the \"index\" column.\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata = datasets.load_dataset(\"d0rj/reflection-v1-ru_subset\")\nprint(data)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset.","url":"https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","multilingual","glaiveai/reflection-v1"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/veyhoranohy/reddit_dataset_44.","url":"https://huggingface.co/datasets/veyhoranohy/reddit_dataset_44","creator_name":"Steve Karadimas","creator_url":"https://huggingface.co/veyhoranohy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"AfriHist-CoT","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tAfriHist-CoT\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nAfriHist-CoT is a dataset of question-answer pairs derived from African history books, created using a Chain-of-Thought (CoT) reasoning approach with the Gemini language model via OpenRouter. The dataset supports training and evaluating question-answering models, with a focus on African history and CoT reasoning. It is available in English and French, catering to both monolingual and multilingual applications.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Svngoku/AfriHist-CoT.","url":"https://huggingface.co/datasets/Svngoku/AfriHist-CoT","creator_name":"NIONGOLO Chrys F√©-Marty","creator_url":"https://huggingface.co/Svngoku","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","ai-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_123","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Axioris/reddit_dataset_123.","url":"https://huggingface.co/datasets/Axioris/reddit_dataset_123","creator_name":"DaneFoster","creator_url":"https://huggingface.co/Axioris","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"PhincBitextMining","keyword":"multilingual","description":"\n  PhincBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nPhinc is a parallel corpus for machine translation pairing code-mixed Hinglish (a fusion of Hindi and English commonly used in modern India) with human-generated English translations.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\nDomains\nSocial, Written\n\n\nReference\nhttps://huggingface.co/datasets/veezbo/phinc\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/PhincBitextMining.","url":"https://huggingface.co/datasets/mteb/PhincBitextMining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","human-annotated","multilingual","English","Hindi"],"keywords_longer_than_N":true},
	{"name":"x_dataset_25","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_25.","url":"https://huggingface.co/datasets/suul999922/x_dataset_25","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"squad_v2_french_translated_fr_prompt_qa","keyword":"squad_v2_french_translated","description":"\n\t\n\t\t\n\t\tsquad_v2_french_translated_fr_prompt_qa\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nsquad_v2_french_translated_fr_prompt_qa is a subset of the Dataset of French Prompts (DFP).It contains 3,320,898 rows that can be used for a question-answering task.The original data (without prompts) comes from the dataset pragnakalp/squad_v2_french_translated and was augmented by questions in SQUAD 2.0 format in the FrenchQA dataset.\nA list of prompts (see below) was then applied in order to build the input and target‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_qa.","url":"https://huggingface.co/datasets/CATIE-AQ/squad_v2_french_translated_fr_prompt_qa","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","found","found","monolingual","squad_v2_french_translated"],"keywords_longer_than_N":true},
	{"name":"LIMIT-GRAPH-v1.1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüåê LIMIT-GRAPH v1.1\n\t\n\nLIMIT-GRAPH is a multilingual benchmark for semantic graph alignment and agentic reasoning. It includes annotated corpora in English, Indonesian, Arabic, and Spanish.\n\n\t\n\t\t\n\t\tüìÅ Contents\n\t\n\n\ncorpora/: Annotated reasoning chains\ngraph_vocab/: Semantic vocabularies per language\nedges/: Graph edges for alignment\nevaluation/: Harness and metrics\n\n\n\t\n\t\t\n\t\tüß† Use Cases\n\t\n\n\nTrain and evaluate multilingual agents\nAlign reasoning chains to semantic graphs\nSubmit agents to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIResAgTeam/LIMIT-GRAPH-v1.1.","url":"https://huggingface.co/datasets/AIResAgTeam/LIMIT-GRAPH-v1.1","creator_name":"AI Research Agent Team","creator_url":"https://huggingface.co/AIResAgTeam","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["English","Indonesian","Arabic","Spanish","mit"],"keywords_longer_than_N":true},
	{"name":"audiocaps-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\taudiocaps-ru\n\t\n\nTranslated version of d0rj/audiocaps into Russian.\n","url":"https://huggingface.co/datasets/d0rj/audiocaps-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-speech","translated","monolingual","d0rj/audiocaps","Russian"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wenknow/reddit_dataset_44.","url":"https://huggingface.co/datasets/wenknow/reddit_dataset_44","creator_name":"Dt","creator_url":"https://huggingface.co/wenknow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mnli-norwegian","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMNLI Norwegian\n\t\n\nThe Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that it covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalisation evaluation. There is also a HuggingFace version of the dataset available. \nThis dataset is machine translated using Google Translate. From‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NbAiLab/mnli-norwegian.","url":"https://huggingface.co/datasets/NbAiLab/mnli-norwegian","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","text-classification","natural-language-inference","semantic-similarity-classification","expert-generated"],"keywords_longer_than_N":true},
	{"name":"xtreme","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for \"xtreme\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Cross-lingual Natural Language Inference (XNLI) corpus is a crowd-sourced collection of 5,000 test and\n2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into\n14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese,\nHindi, Swahili and Urdu. This results in 112.5k annotated pairs. Each premise can be associated with the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/xtreme.","url":"https://huggingface.co/datasets/google/xtreme","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","token-classification","text-classification","text-retrieval"],"keywords_longer_than_N":true},
	{"name":"psychoanalysis-dataset-v2-100k","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tPsychoanalysis v2 ‚Äî 100k (Hinglish + English)\n\t\n\nThis dataset contains 100k psychoanalytic-style conversational samples in Hinglish and English.\nIt includes messages (system/user/assistant), a supervised output, safety/evaluation metadata,\nand a pair field for preference learning (DPO/ORPO).\n\n\t\n\t\t\n\t\tLoading\n\t\n\nfrom datasets import load_dataset\nds = load_dataset(\"SomyaSaraswati/psychoanalysis-dataset-v2-100k\")\nprint(ds)\nprint(ds[\"train\"][0].keys())\n\n","url":"https://huggingface.co/datasets/SomyaSaraswati/psychoanalysis-dataset-v2-100k","creator_name":"Somya Saraswati","creator_url":"https://huggingface.co/SomyaSaraswati","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","Hindi","multilingual","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"paramed","keyword":"multilingual","description":"NEJM is a Chinese-English parallel corpus crawled from the New England Journal of Medicine website. \nEnglish articles are distributed through https://www.nejm.org/ and Chinese articles are distributed through \nhttp://nejmqianyan.cn/. The corpus contains all article pairs (around 2000 pairs) since 2011.","url":"https://huggingface.co/datasets/bigbio/paramed","creator_name":"BigScience Biomedical Datasets","creator_url":"https://huggingface.co/bigbio","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","English","Chinese","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\talpaca-cleaned-ru\n\t\n\nTranslated version of yahma/alpaca-cleaned into Russian.\n","url":"https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","translated","monolingual","yahma/alpaca-cleaned","Russian"],"keywords_longer_than_N":true},
	{"name":"opus_infopankki","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for infopankki\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA parallel corpus of 12 languages, 66 bitexts.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe underlying task is machine translation.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Splits\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tCuration Rationale\n\t\n\n[More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Helsinki-NLP/opus_infopankki.","url":"https://huggingface.co/datasets/Helsinki-NLP/opus_infopankki","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"Code-170k-acholi","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-acholi is a groundbreaking dataset containing 33,148 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Acholi, making coding education accessible to Acholi speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n33,148 high-quality conversations about programming and coding\nPure Acholi language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-acholi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-acholi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Acoli","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-afrikaans","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-afrikaans is a groundbreaking dataset containing 151,533 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Afrikaans, making coding education accessible to Afrikaans speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n151,533 high-quality conversations about programming and coding\nPure Afrikaans language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-afrikaans.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-afrikaans","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Afrikaans","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"humaneval-x","keyword":"multilingual","description":"HumanEval-X is a benchmark for the evaluation of the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks.","url":"https://huggingface.co/datasets/zai-org/humaneval-x","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","language-modeling","crowdsourced","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"rosie_mind_topics","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tROSIE-MIND-Topics\n\t\n\nROSIE-MIND-Topics is a bilingual (English‚ÄìSpanish) dataset of 875,230 passages containing topic modeling information derived from training a PLTM model on this data with 30 topics. It serves as the input dataset for the MIND pipeline, which detects multilingual and cultural discrepancies in question‚Äìanswer pairs.\nEach record includes the passage and corresponding full document, preprocessing outputs (lemmas, translations), and topic model features (topic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lcalvobartolome/rosie_mind_topics.","url":"https://huggingface.co/datasets/lcalvobartolome/rosie_mind_topics","creator_name":"Lorena Calvo Bartolom√©","creator_url":"https://huggingface.co/lcalvobartolome","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","mit","100K<n<1M","Text"],"keywords_longer_than_N":true},
	{"name":"xed_en_fi","keyword":"multilingual","description":"A multilingual fine-grained emotion dataset. The dataset consists of human annotated Finnish (25k) and English sentences (30k). Plutchik‚Äôs\ncore emotions are used to annotate the dataset with the addition of neutral to create a multilabel multiclass\ndataset. The dataset is carefully evaluated using language-specific BERT models and SVMs to\nshow that XED performs on par with other similar datasets and is therefore a useful tool for\nsentiment analysis and emotion detection.","url":"https://huggingface.co/datasets/Helsinki-NLP/xed_en_fi","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","intent-classification","multi-class-classification","multi-label-classification","sentiment-classification"],"keywords_longer_than_N":true},
	{"name":"tweetyface","keyword":"multilingual","description":"Dataset containing Tweets from prominent Twitter Users in various languages. The dataset has been created utilizing a crawler for the Twitter API.\\n \\","url":"https://huggingface.co/datasets/ML-Projects-Kiel/tweetyface","creator_name":"Machine Learning Projects FH Kiel","creator_url":"https://huggingface.co/ML-Projects-Kiel","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","machine-generated","crowdsourced","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"offenseval_dravidian","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Offenseval Dravidian\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nOffensive language identification is classification task in natural language processing (NLP) where the aim is to moderate and minimise offensive content in social media. It has been an active area of research in both academia and industry for the past two decades. There is an increasing demand for offensive language identification on social media texts which are largely code-mixed. Code-mixing is a prevalent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/offenseval_dravidian.","url":"https://huggingface.co/datasets/community-datasets/offenseval_dravidian","creator_name":"Community Datasets","creator_url":"https://huggingface.co/community-datasets","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-generated","crowdsourced","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"PsiloQA","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nPsiloQA is the largest dataset for training and evaluating systems on multilingual span-level hallucination detection with retrieved context. It offers:\n\nAn automated and scalable pipeline for generating, annotating and filtering data for hallucination detection task\nA large multilingual dataset for 14 languages with high-quality and fine-grained span-level hallucination annotations for numerous open-source LLMs\nA comprehensive empirical evaluations of various‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/s-nlp/PsiloQA.","url":"https://huggingface.co/datasets/s-nlp/PsiloQA","creator_name":"s-nlp","creator_url":"https://huggingface.co/s-nlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","text-classification","text-generation","zero-shot-classification","question-answering"],"keywords_longer_than_N":true},
	{"name":"Code-170k-dombe","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-dombe is a groundbreaking dataset containing 176,898 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Dombe, making coding education accessible to Dombe speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n176,898 high-quality conversations about programming and coding\nPure Dombe language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-dombe.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-dombe","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Dombe","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-susu","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-susu is a groundbreaking dataset containing 29,858 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Susu, making coding education accessible to Susu speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n29,858 high-quality conversations about programming and coding\nPure Susu language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-susu.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-susu","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Susu","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-twi","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-twi is a groundbreaking dataset containing over 136,000 programming conversations translated into Twi (Akan), a major language spoken in Ghana. This dataset aims to democratize access to programming education and AI-assisted coding for Twi speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n136,944+ high-quality conversations about programming and coding\nPure Twi language - making coding education accessible to Twi speakers\nMulti-turn dialogues covering various‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-twi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-twi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Twi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"vukuzenzele-monolingual","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tThe Vuk'uzenzele South African Multilingual Corpus\n\t\n\nGive Feedback üìë: DSFSI Resource Feedback Form\n\n\t\n\t\t\n\t\tAbout Dataset\n\t\n\nThe dataset was obtained from the South African government magazine Vuk'uzenzele, created by the Government Communication and Information System (GCIS). \nThe original raw PDFs were obtatined from the Vuk'uzenzele website.\nThe datasets contain government magazine editions in 11 languages, namely:\n\n\t\n\t\t\nLanguage\nCode\nLanguage\nCode\n\n\n\t\t\nEnglish\n(eng)\nSepedi\n(nso)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dsfsi/vukuzenzele-monolingual.","url":"https://huggingface.co/datasets/dsfsi/vukuzenzele-monolingual","creator_name":"Data Science for Social Impact","creator_url":"https://huggingface.co/dsfsi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","English","Afrikaans","South Ndebele","Xhosa"],"keywords_longer_than_N":true},
	{"name":"tydiqa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google-research-datasets/tydiqa.","url":"https://huggingface.co/datasets/google-research-datasets/tydiqa","creator_name":"Google Research Datasets","creator_url":"https://huggingface.co/google-research-datasets","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"rosie_mind","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tROSIE-MIND\n\t\n\nROSIE-MIND is a bilingual (English‚ÄìSpanish) question‚Äìanswering dataset annotated for discrepancy detection between pairs of answers. Each example compares two answers ‚Äî an anchor and a comparison ‚Äî derived from different passages responding to the same health-related question.  \nDiscrepancy labels:\n\nCONTRADICTION (C) ‚Äì The answers provide directly opposing factual information, meaning one explicitly denies what the other asserts. A contradiction occurs only if both‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lcalvobartolome/rosie_mind.","url":"https://huggingface.co/datasets/lcalvobartolome/rosie_mind","creator_name":"Lorena Calvo Bartolom√©","creator_url":"https://huggingface.co/lcalvobartolome","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"multi3-nlu","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Multi3NLU++\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPlease access the dataset using \ngit clone https://huggingface.co/datasets/uoe-nlp/multi3-nlu/\n\nMulti3NLU++ consists of 3080 utterances per language representing challenges in building multilingual multi-intent multi-domain task-oriented dialogue systems. The domains include banking and hotels. There are 62 unique intents. \n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\nmulti-label intent detection\nslot filling\ncross-lingual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uoe-nlp/multi3-nlu.","url":"https://huggingface.co/datasets/uoe-nlp/multi3-nlu","creator_name":"NLP @ University of Edinburgh","creator_url":"https://huggingface.co/uoe-nlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multilingual","nluplusplus","multilingual","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"multi3-nlu","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Multi3NLU++\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPlease access the dataset using \ngit clone https://huggingface.co/datasets/uoe-nlp/multi3-nlu/\n\nMulti3NLU++ consists of 3080 utterances per language representing challenges in building multilingual multi-intent multi-domain task-oriented dialogue systems. The domains include banking and hotels. There are 62 unique intents. \n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\nmulti-label intent detection\nslot filling\ncross-lingual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uoe-nlp/multi3-nlu.","url":"https://huggingface.co/datasets/uoe-nlp/multi3-nlu","creator_name":"NLP @ University of Edinburgh","creator_url":"https://huggingface.co/uoe-nlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multilingual","nluplusplus","multilingual","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Langbridge_wazobia_data","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tLangBridge Multilingual Translation Dataset üá≥üá¨\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis dataset contains parallel translation pairs between Yoruba (yor_Latn), Hausa (hau_Latn), Igbo (ibo_Latn), and English (eng_Latn).\nIt was designed for fine-tuning multilingual translation models such as facebook/nllb-200-distilled-600M, in order for it to achieve state of the art performance at translation tasks.\n\t\n\t\n\t\t\n\t\tüì¶ Dataset Summary\n\t\n\nThe LangBridge dataset provides approximately 3.5 million translation pairs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/coded-by-49/Langbridge_wazobia_data.","url":"https://huggingface.co/datasets/coded-by-49/Langbridge_wazobia_data","creator_name":"Henry Aloh Fabian","creator_url":"https://huggingface.co/coded-by-49","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["original","Yoruba","Hausa","Igbo","English"],"keywords_longer_than_N":true},
	{"name":"xtreme_s","keyword":"multilingual","description":"XTREME-S covers four task families: speech recognition, classification, speech-to-text translation and retrieval. Covering 102\nlanguages from 10+ language families, 3 different domains and 4\ntask families, XTREME-S aims to simplify multilingual speech\nrepresentation evaluation, as well as catalyze research in ‚Äúuniversal‚Äù speech representation learning.","url":"https://huggingface.co/datasets/google/xtreme_s","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"MBPPRetrieval","keyword":"multilingual","description":"\n  MBPPRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA code retrieval task based on 378 Python programming problems from MBPP (Mostly Basic Python Programming). Each query is a natural language description of a programming task (e.g., 'Write a function to find the shared elements from the given two lists'), and the corpus contains Python code implementations. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem descriptions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MBPPRetrieval.","url":"https://huggingface.co/datasets/mteb/MBPPRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","embedding-benchmark/MBPP","code"],"keywords_longer_than_N":true},
	{"name":"RuSciBenchCiteRetrieval","keyword":"multilingual","description":"\n  RuSciBenchCiteRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is focused on Direct Citation Prediction for scientific papers from eLibrary,\n        Russia's largest electronic library of scientific publications. Given a query paper (title and abstract),\n        the goal is to retrieve papers that are directly cited by it from a larger corpus of papers.\n        The dataset for this task consists of 3,000 query papers, 15,000 relevant (cited) papers,\n        and 75‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RuSciBenchCiteRetrieval.","url":"https://huggingface.co/datasets/mteb/RuSciBenchCiteRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","multilingual","mlsa-iai-msu-lab/ru_sci_bench_cite_retrieval"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p3-others","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P3\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P3 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p3-others.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p3-others","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","English","mit","arxiv:2509.06888","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-it-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (it) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (it) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-it-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-it-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Italian"],"keywords_longer_than_N":true},
	{"name":"text_coordinates_seasons","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Geo-Tagged Social Media Posts with Timestamps\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe \"Seasons\" dataset is a collection of over 600,000 social media posts spanning 12 months and encompassing 15 distinct time zones. It focuses on six countries: Cuba, Iran, Russia, North Korea, Syria, and Venezuela, with each post containing textual content, timestamps, and geographical coordinates. The dataset's primary objective is to investigate the correlation between the timing of posts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yachay/text_coordinates_seasons.","url":"https://huggingface.co/datasets/yachay/text_coordinates_seasons","creator_name":"Yachay AI","creator_url":"https://huggingface.co/yachay","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","token-classification","text-classification","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"super_eurlex","keyword":"multilingual","description":"Super-EURLEX dataset containing legal documents from multiple languages.\n                The datasets are build/scrapped from the EURLEX Website [https://eur-lex.europa.eu/homepage.html]\n                With one split per language and sector, because the available features (metadata) differs for each \n                sector. Therefore, each sample contains the content of a full legal document in up to 3 different \n                formats. Those are raw HTML and cleaned HTML (if the HTML format was available on the EURLEX website \n                during the scrapping process) and cleaned text.\n                The cleaned text should be available for each sample and was extracted from HTML or PDF.\n                'Cleaned' HTML stands here for minor cleaning that was done to preserve to a large extent the necessary \n                HTML information like table structures while removing unnecessary complexity which was introduced to the \n                original documents due to actions like writing each sentence into a new object. \n                Additionally, each sample contains metadata which was scrapped on the fly, this implies the following \n                2 things. First, not every sector contains the same metadata. Second, most metadata might be \n                irrelevant for most use cases. \n                In our minds the most interesting metadata is the celex-id which is used to identify the legal \n                document at hand, but also contains a lot of information about the document \n                see [https://eur-lex.europa.eu/content/tools/eur-lex-celex-infographic-A3.pdf] as well as eurovoc-\n                concepts, which are labels that define the content of the documents. \n                Eurovoc-Concepts are, for example, only available for the sectors 1, 2, 3, 4, 5, 6, 9, C, and E.\n                The Naming of most metadata is kept like it was on the eurlex website, except for converting \n                it to lower case and replacing whitespaces with '_'.","url":"https://huggingface.co/datasets/ddrg/super_eurlex","creator_name":"Dresden Database Research Group","creator_url":"https://huggingface.co/ddrg","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","fill-mask","multi-class-classification","multi-label-classification","found"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-fr-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (fr) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (fr) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-fr-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-fr-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","French"],"keywords_longer_than_N":true},
	{"name":"oscar-small","keyword":"multilingual","description":"The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\\","url":"https://huggingface.co/datasets/djstrong/oscar-small","creator_name":"Krzysztof Wr√≥bel","creator_url":"https://huggingface.co/djstrong","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"oscar-small","keyword":"multilingual","description":"The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.\\","url":"https://huggingface.co/datasets/nthngdy/oscar-small","creator_name":"Nathan Godey","creator_url":"https://huggingface.co/nthngdy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"miracl","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for MIRACL (Topics and Qrels)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nHomepage | \nRepository: | \nPaper | \nArXiv\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages, which collectively encompass over three billion native speakers around the world.\nThis dataset contains the collection data of the 16 \"known languages\". The remaining 2 \"surprise languages\" will not‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/miracl/miracl.","url":"https://huggingface.co/datasets/miracl/miracl","creator_name":"MIRACL","creator_url":"https://huggingface.co/miracl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tDataset Card for \"hh-rlhf-ru\"\n\t\n\nThis is translated version of Anthropic/hh-rlhf dataset into Russian.\n","url":"https://huggingface.co/datasets/d0rj/hh-rlhf-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translated","monolingual","Anthropic/hh-rlhf","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"pwesuite-eval","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tPWESuite-Eval\n\t\n\nDataset composed of multiple smaller datasets used for the evaluation of phonetic word embeddings.\nSee code for evaluation here.\nIf you use this dataset/evaluation, please cite the paper at LREC-COLING 2024:\n@inproceedings{zouhar-etal-2024-pwesuite,\n    title = \"{PWES}uite: Phonetic Word Embeddings and Tasks They Facilitate\",\n    author = \"Zouhar, Vil{\\'e}m  and\n      Chang, Kalvin  and\n      Cui, Chenxuan  and\n      Carlson, Nate B.  and\n      Robinson, Nathaniel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zouharvi/pwesuite-eval.","url":"https://huggingface.co/datasets/zouharvi/pwesuite-eval","creator_name":"Vil√©m Zouhar","creator_url":"https://huggingface.co/zouharvi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","English","Amharic","Bengali","Swahili"],"keywords_longer_than_N":true},
	{"name":"mapa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Multilingual European Datasets for Sensitive Entity Detection in the Legal Domain\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset consists of 12 documents (9 for Spanish due to parsing errors) taken from EUR-Lex, a multilingual corpus of court\ndecisions and legal dispositions in the 24 official languages of the European Union. The documents have been annotated\nfor named entities following the guidelines of the MAPA project which foresees two\nannotation level, a general and a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/joelniklaus/mapa.","url":"https://huggingface.co/datasets/joelniklaus/mapa","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","other","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"mapa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Multilingual European Datasets for Sensitive Entity Detection in the Legal Domain\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset consists of 12 documents (9 for Spanish due to parsing errors) taken from EUR-Lex, a multilingual corpus of court\ndecisions and legal dispositions in the 24 official languages of the European Union. The documents have been annotated\nfor named entities following the guidelines of the MAPA project which foresees two\nannotation level, a general and a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/joelniklaus/mapa.","url":"https://huggingface.co/datasets/joelniklaus/mapa","creator_name":"Joel Niklaus","creator_url":"https://huggingface.co/joelniklaus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","other","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"miracl-de-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (de) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-de-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-de-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-de-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-de-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","German"],"keywords_longer_than_N":true},
	{"name":"Korean_User_Manuals_Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKorean User Manuals Dataset\n\t\n\nThis dataset contains high-resolution images and PDFs of Korean user manuals and instruction guides for electronics, appliances, and consumer products. It is curated and anonymized to support AI research in OCR, document understanding, and multilingual text extraction.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask Categories:  \n\nDocument‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Korean_User_Manuals_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Korean_User_Manuals_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Korean","cc-by-4.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Korean_Menus_Image_Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKorean Menus Image Dataset\n\t\n\nThis dataset contains high-resolution images of Korean restaurant menus, including traditional, fast-food, and caf√© menus. It has been curated and anonymized to support AI research in OCR, menu understanding, multilingual translation, and food recommendation systems.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask Categories:  \n\nImage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Korean_Menus_Image_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Korean_Menus_Image_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","Korean","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\talpaca-cleaned-ru\n\t\n\nconverter for autotrain from d0rj/alpaca-cleaned-ru\nTranslated version of yahma/alpaca-cleaned into Russian.\n","url":"https://huggingface.co/datasets/ASIDS/alpaca-cleaned-ru","creator_name":"Greed","creator_url":"https://huggingface.co/ASIDS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","translated","monolingual","yahma/alpaca-cleaned","Russian"],"keywords_longer_than_N":true},
	{"name":"Korean_Price_Tags_Image_Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKorean Price Tags Image Dataset\n\t\n\nThis dataset contains high-resolution images of Korean price tags found in retail stores, supermarkets, and markets. It has been curated and anonymized to support AI research in OCR, pricing recognition, product detection, and retail analytics.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask Categories:  \n\nImage Classification  \nText‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Korean_Price_Tags_Image_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Korean_Price_Tags_Image_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","Korean","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"miracl-es-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (es) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-es-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-es-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-es-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-es-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Spanish"],"keywords_longer_than_N":true},
	{"name":"miracl-th-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (th) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-th-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-th-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-th-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-th-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Thai"],"keywords_longer_than_N":true},
	{"name":"multidomain-kazakh-dataset","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPoint of Contact: Sanzhar Murzakhmetov, Besultan Sagyndyk\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMDBKD | Multi-Domain Bilingual Kazakh Dataset is a Kazakh-language dataset containing just over 24 883 808 unique texts from multiple domains.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\n'MLM/CLM': can be used to train a model for casual and masked languange modeling\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe kk code for Kazakh as generally spoken in the Kazakhstan \n\n\t\n\t\t\n\t\tData Instances\n\t\n\nFor each instance‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kz-transformers/multidomain-kazakh-dataset.","url":"https://huggingface.co/datasets/kz-transformers/multidomain-kazakh-dataset","creator_name":"Kaz-Transformers","creator_url":"https://huggingface.co/kz-transformers","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"miracl-es-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (es) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-es-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-es-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-es-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-es-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Spanish"],"keywords_longer_than_N":true},
	{"name":"miracl-ru-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (ru) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-ru-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-ru-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-ru-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-ru-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Russian"],"keywords_longer_than_N":true},
	{"name":"miracl-ar-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (ar) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-ar-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-ar-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-ar-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-ar-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"miracl-ar-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (ar) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-ar-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-ar-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-ar-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-ar-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"tydiqa_copenlu","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/tydiqa_copenlu.","url":"https://huggingface.co/datasets/copenlu/tydiqa_copenlu","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"nomiracl","keyword":"multilingual","description":"Data Loader for the NoMIRACL dataset.","url":"https://huggingface.co/datasets/miracl/nomiracl","creator_name":"MIRACL","creator_url":"https://huggingface.co/miracl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","expert-generated","multilingual","miracl/miracl","Arabic"],"keywords_longer_than_N":true},
	{"name":"rlhf-reward-datasets-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tDataset Card for \"rlhf-reward-datasets-ru\"\n\t\n\nThis is translated version of yitingxie/rlhf-reward-datasets dataset into Russian.\n","url":"https://huggingface.co/datasets/d0rj/rlhf-reward-datasets-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translated","monolingual","yitingxie/rlhf-reward-datasets","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"TyDiP","keyword":"multilingual","description":"The TyDiP dataset is a dataset of requests in conversations between wikipedia editors\nthat have been annotated for politeness. The splits available below consists of only\nrequests from the top 25 percentile (polite) and bottom 25 percentile (impolite) of\npoliteness scores. The English train set and English test set that are\nadapted from the Stanford Politeness Corpus, and test data in 9 more languages\n(Hindi, Korean, Spanish, Tamil, French, Vietnamese, Russian, Afrikaans, Hungarian) \nwas annotated by us.","url":"https://huggingface.co/datasets/Genius1237/TyDiP","creator_name":"Genius1237","creator_url":"https://huggingface.co/Genius1237","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","crowdsourced","found","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"TyDiP","keyword":"multilingual","description":"The TyDiP dataset is a dataset of requests in conversations between wikipedia editors\nthat have been annotated for politeness. The splits available below consists of only\nrequests from the top 25 percentile (polite) and bottom 25 percentile (impolite) of\npoliteness scores. The English train set and English test set that are\nadapted from the Stanford Politeness Corpus, and test data in 9 more languages\n(Hindi, Korean, Spanish, Tamil, French, Vietnamese, Russian, Afrikaans, Hungarian) \nwas annotated by us.","url":"https://huggingface.co/datasets/Genius1237/TyDiP","creator_name":"Genius1237","creator_url":"https://huggingface.co/Genius1237","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","crowdsourced","found","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"xcopa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for \"xcopa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n  XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning\nThe Cross-lingual Choice of Plausible Alternatives dataset is a benchmark to evaluate the ability of machine learning models to transfer commonsense reasoning across\nlanguages. The dataset is the translation and reannotation of the English COPA (Roemmele et al. 2011) and covers 11 languages from 11 families and several areas around\nthe globe. The dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cambridgeltl/xcopa.","url":"https://huggingface.co/datasets/cambridgeltl/xcopa","creator_name":"Language Technology Lab @University of Cambridge","creator_url":"https://huggingface.co/cambridgeltl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","expert-generated","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"mls_eng_10k","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a 10K hours subset of English version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls_eng_10k.","url":"https://huggingface.co/datasets/parler-tts/mls_eng_10k","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"ntcir_13_medweb","keyword":"multilingual","description":"NTCIR-13 MedWeb (Medical Natural Language Processing for Web Document) task requires\nto perform a multi-label classification that labels for eight diseases/symptoms must\nbe assigned to each tweet. Given pseudo-tweets, the output are Positive:p or Negative:n\nlabels for eight diseases/symptoms. The achievements of this task can almost be\ndirectly applied to a fundamental engine for actual applications.\n\nThis task provides pseudo-Twitter messages in a cross-language and multi-label corpus,\ncovering three languages (Japanese, English, and Chinese), and annotated with eight\nlabels such as influenza, diarrhea/stomachache, hay fever, cough/sore throat, headache,\nfever, runny nose, and cold.\n\nFor more information, see:\nhttp://research.nii.ac.jp/ntcir/permission/ntcir-13/perm-en-MedWeb.html\n\nAs this dataset also provides a parallel corpus of pseudo-tweets for english,\njapanese and chinese it can also be used to train translation models between\nthese three languages.","url":"https://huggingface.co/datasets/asus-aics/ntcir_13_medweb","creator_name":"ASUS Intelligent Cloud Services","creator_url":"https://huggingface.co/asus-aics","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multilingual","English","Chinese","Japanese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"mls-eng-10k-tags_tagged_10k_generated","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Annotations of 10K hours of English MLS\n\t\n\nThis dataset consists in annotations of a 10K hours subset of English version of the Multilingual LibriSpeech (MLS) dataset. \nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls-eng-10k-tags_tagged_10k_generated.","url":"https://huggingface.co/datasets/parler-tts/mls-eng-10k-tags_tagged_10k_generated","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"Brazilian_Item_Price_and_Description_Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBrazilian Item Price and Description Dataset\n\t\n\nThis dataset contains high-resolution images and structured text data of product price tags and item descriptions collected from Brazilian retail stores and e-commerce platforms. It enables AI research in OCR, product recognition, and retail analytics for the Portuguese-speaking market.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupported‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Brazilian_Item_Price_and_Description_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Brazilian_Item_Price_and_Description_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","Portuguese","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"alt","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Asian Language Treebank (ALT)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe ALT project aims to advance the state-of-the-art Asian natural language processing (NLP) techniques through the open collaboration for developing and using ALT. It was first conducted by NICT and UCSY as described in Ye Kyaw Thu, Win Pa Pa, Masao Utiyama, Andrew Finch and Eiichiro Sumita (2016). Then, it was developed under ASEAN IVO as described in this Web page. \nThe process of building ALT began with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mutiyama/alt.","url":"https://huggingface.co/datasets/mutiyama/alt","creator_name":"Masao Utiyama","creator_url":"https://huggingface.co/mutiyama","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","token-classification","parsing","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tamazight-tifinagh","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tamazight-tifinagh is a groundbreaking dataset containing 121,845 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tamazight (Tifinagh), making coding education accessible to Tamazight (Tifinagh) speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n121,845 high-quality conversations about programming and coding\nPure Tamazight (Tifinagh) language - democratizing coding education\nMulti-turn dialogues covering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tamazight-tifinagh.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tamazight-tifinagh","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","ber","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-venda","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-venda is a groundbreaking dataset containing 118,838 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Venda, making coding education accessible to Venda speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n118,838 high-quality conversations about programming and coding\nPure Venda language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-venda.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-venda","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Venda","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-yoruba","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-yoruba is a groundbreaking dataset containing 12,287 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Yoruba, making coding education accessible to Yoruba speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n12,287 high-quality conversations about programming and coding\nPure Yoruba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-yoruba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-yoruba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Yoruba","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-luo","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-luo is a groundbreaking dataset containing 140,631 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Luo, making coding education accessible to Luo speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n140,631 high-quality conversations about programming and coding\nPure Luo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-luo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-luo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Luo (Kenya and Tanzania)","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-xhosa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-xhosa is a groundbreaking dataset containing 12,296 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Xhosa, making coding education accessible to Xhosa speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n12,296 high-quality conversations about programming and coding\nPure Xhosa language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-xhosa.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-xhosa","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Xhosa","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-sango","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-sango is a groundbreaking dataset containing 103,766 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Sango, making coding education accessible to Sango speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n103,766 high-quality conversations about programming and coding\nPure Sango language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-sango.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-sango","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Sango","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-hausa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-hausa is a groundbreaking dataset containing 14,095 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Hausa, making coding education accessible to Hausa speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n14,095 high-quality conversations about programming and coding\nPure Hausa language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-hausa.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-hausa","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Hausa","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-luganda","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-luganda is a groundbreaking dataset containing 136,290 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Luganda, making coding education accessible to Luganda speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n136,290 high-quality conversations about programming and coding\nPure Luganda language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-luganda.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-luganda","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Ganda","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-bemba","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-bemba is a groundbreaking dataset containing 54,131 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Bemba, making coding education accessible to Bemba speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n54,131 high-quality conversations about programming and coding\nPure Bemba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-bemba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-bemba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Bemba (Zambia)","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-wolof","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-wolof is a groundbreaking dataset containing 101,894 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Wolof, making coding education accessible to Wolof speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n101,894 high-quality conversations about programming and coding\nPure Wolof language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-wolof.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-wolof","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Wolof","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-shona","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-shona is a groundbreaking dataset containing 12,269 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Shona, making coding education accessible to Shona speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n12,269 high-quality conversations about programming and coding\nPure Shona language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-shona.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-shona","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Shona","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-nuer","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-nuer is a groundbreaking dataset containing 128,677 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Nuer, making coding education accessible to Nuer speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n128,677 high-quality conversations about programming and coding\nPure Nuer language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-nuer.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-nuer","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Nuer","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-zulu","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-zulu is a groundbreaking dataset containing 13,591 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Zulu, making coding education accessible to Zulu speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n13,591 high-quality conversations about programming and coding\nPure Zulu language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-zulu.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-zulu","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Zulu","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-dyula","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-dyula is a groundbreaking dataset containing 99,057 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Dyula, making coding education accessible to Dyula speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n99,057 high-quality conversations about programming and coding\nPure Dyula language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-dyula.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-dyula","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Dyula","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-sesotho","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-sesotho is a groundbreaking dataset containing 12,287 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Sesotho, making coding education accessible to Sesotho speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n12,287 high-quality conversations about programming and coding\nPure Sesotho language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-sesotho.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-sesotho","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Southern Sotho","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-lingala","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-lingala is a groundbreaking dataset containing 74,431 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Lingala, making coding education accessible to Lingala speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n74,431 high-quality conversations about programming and coding\nPure Lingala language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-lingala.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-lingala","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Lingala","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tswana","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tswana is a groundbreaking dataset containing 115,572 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tswana, making coding education accessible to Tswana speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n115,572 high-quality conversations about programming and coding\nPure Tswana language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tswana.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tswana","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tswana","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-chichewa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-chichewa is a groundbreaking dataset containing 12,321 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Chichewa, making coding education accessible to Chichewa speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n12,321 high-quality conversations about programming and coding\nPure Chichewa language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-chichewa.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-chichewa","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Chichewa","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-dinka","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-dinka is a groundbreaking dataset containing 30,404 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Dinka, making coding education accessible to Dinka speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n30,404 high-quality conversations about programming and coding\nPure Dinka language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-dinka.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-dinka","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Dinka","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tigrinya","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tigrinya is a groundbreaking dataset containing 121,080 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tigrinya, making coding education accessible to Tigrinya speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n121,080 high-quality conversations about programming and coding\nPure Tigrinya language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tigrinya.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tigrinya","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tigrinya","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-amharic","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-amharic is a groundbreaking dataset containing 12,769 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Amharic, making coding education accessible to Amharic speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n12,769 high-quality conversations about programming and coding\nPure Amharic language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-amharic.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-amharic","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Amharic","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-krio","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-krio is a groundbreaking dataset containing 93,627 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Krio, making coding education accessible to Krio speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n93,627 high-quality conversations about programming and coding\nPure Krio language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-krio.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-krio","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Krio","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tiv","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tiv is a groundbreaking dataset containing 93,821 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tiv, making coding education accessible to Tiv speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n93,821 high-quality conversations about programming and coding\nPure Tiv language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data structures‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tiv.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tiv","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tiv","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-somali","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-somali is a groundbreaking dataset containing 12,244 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Somali, making coding education accessible to Somali speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n12,244 high-quality conversations about programming and coding\nPure Somali language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-somali.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-somali","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Somali","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-fulani","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-fulani is a groundbreaking dataset containing 110,292 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Fulani, making coding education accessible to Fulani speakers.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n110,292 high-quality conversations about programming and coding\nPure Fulani language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-fulani.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-fulani","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Fula","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"synthetic-multilingual-speaker-diarization","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Speaker Diarization Dataset\n\t\n\nThis dataset contains synthetic multilingual speaker diarization data with Hindi, English, and Punjabi audio samples.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n‚îú‚îÄ‚îÄ audio/           # WAV audio files (16kHz) - 627 files\n‚îú‚îÄ‚îÄ csv/            # Individual CSV annotations - 627 files\n‚îú‚îÄ‚îÄ rttm/           # RTTM format files for diarization - 627 files\n‚îú‚îÄ‚îÄ all_samples_combined.csv  # Complete dataset annotations\n‚îî‚îÄ‚îÄ all_samples_combined.rttm # Complete RTTM‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/noty7gian/synthetic-multilingual-speaker-diarization.","url":"https://huggingface.co/datasets/noty7gian/synthetic-multilingual-speaker-diarization","creator_name":"Saksham Bansal","creator_url":"https://huggingface.co/noty7gian","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","Hindi","English","Panjabi"],"keywords_longer_than_N":true},
	{"name":"multiconer_v2","keyword":"multilingual","description":"Complex named entities (NE), like the titles of creative works, are not simple nouns and pose challenges for NER systems (Ashwini and Choi, 2014). They can take the form of any linguistic constituent, like an imperative clause (‚ÄúDial M for Murder‚Äù), and do not look like traditional NEs (Persons, Locations, etc.). This syntactic ambiguity makes it challenging to recognize them based on context. We organized the MultiCoNER task (Malmasi et al., 2022) at SemEval-2022 to address these challenges in 11 languages, receiving a very positive community response with 34 system papers. Results confirmed the challenges of processing complex and long-tail NEs: even the largest pre-trained Transformers did not achieve top performance without external knowledge. The top systems infused transformers with knowledge bases and gazetteers. However, such solutions are brittle against out of knowledge-base entities and noisy scenarios like the presence of spelling mistakes and typos. We propose MultiCoNER II which represents novel challenges through new tasks that emphasize the shortcomings of the current top models.\n\nMultiCoNER II features complex NER in these languages:\n\n1. English\n2. Spanish\n3. Hindi\n4. Bangla\n5. Chinese\n6. Swedish\n7. Farsi\n8. French\n9. Italian\n10. Portugese\n11. Ukranian\n12. German\n\nFor more details see https://multiconer.github.io/\n\n## References\n* Sandeep Ashwini and Jinho D. Choi. 2014. Targetable named entity recognition in social media. CoRR, abs/1408.0782.\n* Shervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta Kar, Oleg Rokhlenko. 2022. SemEval-2022 Task 11: Multilingual Complex Named Entity Recognition (MultiCoNER).","url":"https://huggingface.co/datasets/MultiCoNER/multiconer_v2","creator_name":"MultiCoNER","creator_url":"https://huggingface.co/MultiCoNER","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","Bengali","Chinese","German","English"],"keywords_longer_than_N":true},
	{"name":"MaWPS-ar","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for MAWPS_ar\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMAWPS: A Math Word Problem Repository\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nMath Word Problem Solving\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nSupports Arabic and English\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\ntext_en: a string feature.\ntext_ar: a string feature.\neqn: a string feature.\n\n\n\t\n\t\t\n\t\tData Splits\n\t\n\n\n\t\n\t\t\ntrain\nvalidation\ntest\n\n\n\t\t\n3636\n1040\n520\n\n\n\t\n\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tCuration Rationale‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omarxadel/MaWPS-ar.","url":"https://huggingface.co/datasets/omarxadel/MaWPS-ar","creator_name":"Omar Adel","creator_url":"https://huggingface.co/omarxadel","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["explanation-generation","crowdsourced","found","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"openminuscule","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOpen Minuscule\n\t\n\nA little small wee corpus to train little small wee models.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a raw text corpus, mainly intended for testing purposes.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nFrench\nEnglish\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Splits\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tSource Data\n\t\n\nIt is a mashup‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lgrobol/openminuscule.","url":"https://huggingface.co/datasets/lgrobol/openminuscule","creator_name":"Lo√Øc Grobol","creator_url":"https://huggingface.co/lgrobol","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","crowdsourced","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"mc4-sampling","keyword":"multilingual","description":"A sampling-enabled version of mC4, the colossal, cleaned version of Common Crawl's web crawl corpus.\n\nBased on Common Crawl dataset: \"https://commoncrawl.org\".\n\nThis is a version of the processed version of Google's mC4 dataset by AllenAI, in which sampling methods are implemented to perform on the fly.","url":"https://huggingface.co/datasets/bertin-project/mc4-sampling","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"miracl-fa-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (fa) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-fa-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-fa-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-fa-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-fa-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Persian"],"keywords_longer_than_N":true},
	{"name":"sv-ident","keyword":"multilingual","description":"The SV-Ident corpus (version 0.3) is a collection of 4,248 expert-annotated English\nand German sentences from social science publications, supporting the task of\nmulti-label text classification.","url":"https://huggingface.co/datasets/vadis/sv-ident","creator_name":"VAriable Detection, Interlinking and Summarization project","creator_url":"https://huggingface.co/vadis","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","multi-label-classification","semantic-similarity-classification","expert-generated","expert-generated"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-ar-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (ar) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (ar) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-ar-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-ar-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"miracl-zh-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (zh) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-zh-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-zh-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-zh-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-zh-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Chinese"],"keywords_longer_than_N":true},
	{"name":"wikipedia-22-12-ja-embeddings","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikipedia (ja) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded Wikipedia (ja) using the cohere.ai multilingual-22-12 embedding model.\nTo get an overview how this dataset was created and pre-processed, have a look at Cohere/wikipedia-22-12.\n\n\t\n\t\t\n\t\n\t\n\t\tEmbeddings\n\t\n\nWe compute for title+\" \"+text the embeddings using our multilingual-22-12 embedding model, a state-of-the-art model that works for semantic search in 100 languages.  If you want to learn more about this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/wikipedia-22-12-ja-embeddings.","url":"https://huggingface.co/datasets/Cohere/wikipedia-22-12-ja-embeddings","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","multilingual","Japanese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"tydiqa-goldp","keyword":"multilingual","description":"TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\ninformation-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\ndon‚Äôt know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\nthe use of translation (unlike MLQA and XQuAD).","url":"https://huggingface.co/datasets/khalidalt/tydiqa-goldp","creator_name":"Khalid Almubarak","creator_url":"https://huggingface.co/khalidalt","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"xlel_wd","keyword":"multilingual","description":"XLEL-WD is a multilingual event linking dataset. This dataset contains mention references from multilingual Wikipedia/Wikinews articles to event items in Wikidata. The text descriptions for Wikidata events are compiled from Wikipedia articles.","url":"https://huggingface.co/datasets/adithya7/xlel_wd","creator_name":"Adithya Pratapa","creator_url":"https://huggingface.co/adithya7","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["found","found","multilingual","original","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"olaph-data","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOLaPh Phonemization Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset was created with the OLaPh framework and used to train the OLaPh grapheme-to-phoneme model. It contains multilingual text from the FineWeb datasets, automatically phonemized into text‚Äìphoneme pairs for English, German, French, and Spanish.\n\n\t\n\t\t\n\t\tSource Data\n\t\n\nText was taken from:\n\nHuggingFaceFW/fineweb\nHuggingFaceFW/fineweb-2\n\n\n\t\n\t\t\n\t\tPhonemization\n\t\n\nAll text was processed and phonemized automatically using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iisys-hof/olaph-data.","url":"https://huggingface.co/datasets/iisys-hof/olaph-data","creator_name":"Institute for Information Systems of Hof University","creator_url":"https://huggingface.co/iisys-hof","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["English","German","French","Spanish","odc-by"],"keywords_longer_than_N":true},
	{"name":"miracl-id-corpus-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (id) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-id-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-id-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-id-corpus-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-id-corpus-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Indonesian"],"keywords_longer_than_N":true},
	{"name":"radon-test-multilingual","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tradon-test-multilingual\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nMultilingual test dataset for RADON model evaluation with Russian and English prompts\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tLoad Dataset\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MagistrTheOne/radon-test-multilingual\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tUse with RADON Model\n\t\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load RADON model\nmodel = AutoModelForCausalLM.from_pretrained(\"MagistrTheOne/RadonSAI\")\ntokenizer =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MagistrTheOne/radon-test-multilingual.","url":"https://huggingface.co/datasets/MagistrTheOne/radon-test-multilingual","creator_name":"Maga","creator_url":"https://huggingface.co/MagistrTheOne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","original","Russian","English"],"keywords_longer_than_N":true},
	{"name":"miracl-sw-queries-22-12","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMIRACL (sw) embedded with cohere.ai multilingual-22-12 encoder\n\t\n\nWe encoded the MIRACL dataset using the cohere.ai multilingual-22-12 embedding model.\nThe query embeddings can be found in Cohere/miracl-sw-queries-22-12 and the corpus embeddings can be found in Cohere/miracl-sw-corpus-22-12.\nFor the orginal datasets, see miracl/miracl and miracl/miracl-corpus.\nDataset info:\n\nMIRACL üåçüôåüåè (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cohere/miracl-sw-queries-22-12.","url":"https://huggingface.co/datasets/Cohere/miracl-sw-queries-22-12","creator_name":"Cohere","creator_url":"https://huggingface.co/Cohere","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","multilingual","Swahili"],"keywords_longer_than_N":true},
	{"name":"OpenOrca-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\tOpenOrca-ru\n\t\n\nThis is translated version of Open-Orca/OpenOrca into Russian.\n","url":"https://huggingface.co/datasets/d0rj/OpenOrca-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"test-esp","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMulti-Domain Spanish Speech Dataset\n\t\n\nThis dataset contains 1 audio recordings with corresponding text transcriptions across multiple languages and domains.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive collection of audio files paired with text transcriptions, featuring both synthetic and natural speech across various domains. Suitable for automatic speech recognition (ASR), text-to-speech (TTS), and domain-specific speech processing tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jsbeaudry/test-esp.","url":"https://huggingface.co/datasets/jsbeaudry/test-esp","creator_name":"Jean Sauvenel Beaudry","creator_url":"https://huggingface.co/jsbeaudry","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Spanish","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Korean_Product_Labels_Image_Dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKorean Product Labels Image Dataset\n\t\n\nThis dataset contains a diverse collection of high-resolution images of Korean product labels and packaging. The dataset spans categories such as food, cosmetics, beverages, and household items, designed to support OCR, product classification, and visual-language AI research.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Korean_Product_Labels_Image_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Korean_Product_Labels_Image_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","Korean","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"termith-eval","keyword":"multilingual","description":"TermITH-Eval benchmark dataset for keyphrase extraction an generation.","url":"https://huggingface.co/datasets/taln-ls2n/termith-eval","creator_name":"TALN research group at LS2N lab","creator_url":"https://huggingface.co/taln-ls2n","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","unknown","unknown","multilingual","French"],"keywords_longer_than_N":true},
	{"name":"humaneval-x","keyword":"multilingual","description":"HumanEval-X is a benchmark for the evaluation of the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks.","url":"https://huggingface.co/datasets/zai-org/humaneval-x","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","language-modeling","crowdsourced","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"every_prompt","keyword":"multilingual","description":"Every prompt dataset.\nEvery Prompt is a data-driven approach to mining instructions from the web.\nIt contains over a million FAQs and HowTos from around the world in a structured format.\nIt also has basic pre-processing to calculate the length of the useful text and identify the language of that text with the help of GCLD3","url":"https://huggingface.co/datasets/lang-uk/every_prompt","creator_name":"Lang UK","creator_url":"https://huggingface.co/lang-uk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multilingual","mit","1M - 10M","Tabular"],"keywords_longer_than_N":true},
	{"name":"ShareGPT52K","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for ShareGPT52K90K\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\nThis repository now contains the new 90K conversations version. The previous 52K may\nbe found in the old/ directory.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\ntext-generation\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThis dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RyokoAI/ShareGPT52K.","url":"https://huggingface.co/datasets/RyokoAI/ShareGPT52K","creator_name":"Ryoko AI","creator_url":"https://huggingface.co/RyokoAI","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Spanish","German","multilingual"],"keywords_longer_than_N":true},
	{"name":"blbooksgenre","keyword":"multilingual","description":"This dataset contains metadata for resources belonging to the British Library‚Äôs digitised printed books (18th-19th century) collection (bl.uk/collection-guides/digitised-printed-books).\nThis metadata has been extracted from British Library catalogue records.\nThe metadata held within our main catalogue is updated regularly.\nThis metadata dataset should be considered a snapshot of this metadata.","url":"https://huggingface.co/datasets/TheBritishLibrary/blbooksgenre","creator_name":"British Library","creator_url":"https://huggingface.co/TheBritishLibrary","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","fill-mask","topic-classification","multi-label-classification"],"keywords_longer_than_N":true},
	{"name":"opus_paracrawl","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for OpusParaCrawl\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nParallel corpora from Web Crawls collected in the ParaCrawl project.\nTha dataset contains:\n\n42 languages, 43 bitexts\ntotal number of files: 59,996\ntotal number of tokens: 56.11G\ntotal number of sentence fragments: 3.13G\n\nTo load a language pair which isn't part of the config, all you need to do is specify the language code as pairs,\ne.g.\ndataset = load_dataset(\"opus_paracrawl\", lang1=\"en\", lang2=\"so\")\n\nYou can find the valid‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Helsinki-NLP/opus_paracrawl.","url":"https://huggingface.co/datasets/Helsinki-NLP/opus_paracrawl","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"Cylonix_ASR_dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKoddaDuck/Cylonix_ASR_dataset\n\t\n\n","url":"https://huggingface.co/datasets/KoddaDuck/Cylonix_ASR_dataset","creator_name":"Haodong Huang","creator_url":"https://huggingface.co/KoddaDuck","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","crowdsourced","crowdsourced","multilingual","extended|common_voice"],"keywords_longer_than_N":true},
	{"name":"IE_SemParse","keyword":"multilingual","description":"    IE-SemParse is an Inter-bilingual Seq2seq Semantic parsing dataset for 11 distinct Indian languages","url":"https://huggingface.co/datasets/Divyanshu/IE_SemParse","creator_name":"Divyanshu Aggarwal","creator_url":"https://huggingface.co/Divyanshu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["parsing","machine-generated","machine-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"parallel_ab-ru","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Abkhaz Russian parallel corpus dataset is a collection of 205,665 sentences/words extracted from different sources; e-books, web scrapping.\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tSource Data\n\t\n\nHere is a link to the source on github\n\n\t\n\t\t\n\t\tConsiderations for Using the Data\n\t\n\n\n\t\n\t\t\n\t\tOther Known Limitations\n\t\n\nThe accuracy of the dataset is around 95% (gramatical, arthographical errors)\n","url":"https://huggingface.co/datasets/Nart/parallel_ab-ru","creator_name":"Danial Zakaria","creator_url":"https://huggingface.co/Nart","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","translation","expert-generated","translation","multilingual"],"keywords_longer_than_N":true},
	{"name":"mmBERT-decay-data","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMMBERT Decay Phase Data\n\t\n\n\n\n\n\n\nPhase 3 of 3: Annealed language learning decay phase (100B tokens) with massive multilingual expansion to 1833 languages.\n\n\n\t\n\t\n\t\n\t\tüìä Data Composition\n\t\n\nNOTE: there are multiple decay data mixtures: this mixture described below is the Decay-Cont mixture. However, the data in this repository is the Decay-Eng. If you are interested in the others, please let me know so I can prioritize it.\n\t\n\t\t\nData Source\nTokens (B)\nPercentage\nDescription\n\n\n\t\t\nFineWeb2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-decay-data.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-decay-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"Code-170k-ga","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-ga is a groundbreaking dataset containing over 136,000 programming conversations, orginally sourced from glaiveai/glaive-code-assistant-v2 and translated into Ga , a major language spoken in Ghana. This dataset aims to democratize access to programming education and AI-assisted coding for Ga speakers.\n\n\t\n\t\t\n\t\n\t\n\t\tüåü Key Features\n\t\n\n\n136,944+ high-quality conversations about programming and coding\nPure Ga language - making coding education accessible to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-ga.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-ga","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Ga","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"xP3all","keyword":"multilingual","description":"xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks. It is used for the training of BLOOMZ and mT0, multilingual language models capable of following human instructions in dozens of languages zero-shot.","url":"https://huggingface.co/datasets/bigscience/xP3all","creator_name":"BigScience Workshop","creator_url":"https://huggingface.co/bigscience","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","expert-generated","crowdsourced","multilingual","Akan"],"keywords_longer_than_N":true},
	{"name":"SqCLIRIL","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüó£Ô∏è SqCLIRIL: Spoken Query Benchmark for Cross-Lingual IR in Indian Languages\n\t\n\nSqCLIRIL is a Spoken Query Benchmark designed to evaluate cross-lingual information retrieval (CLIR) systems using both spoken and text queries.It covers five Indian languages ‚Äî Hindi, Gujarati, Bengali, Kannada, and English ‚Äî with diverse speech samples from male and female speakers to capture natural variability in pronunciation and acoustic conditions.\n\n\n\t\n\t\t\n\t\n\t\n\t\tüìò Dataset Summary\n\t\n\n\n\t\n\t\t\nFeature‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/irlab-daiict/SqCLIRIL.","url":"https://huggingface.co/datasets/irlab-daiict/SqCLIRIL","creator_name":"IRLAB","creator_url":"https://huggingface.co/irlab-daiict","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-retrieval","Hindi","Gujarati","Bengali"],"keywords_longer_than_N":true},
	{"name":"TurkmenTrilingualSemi-SyntheticDictionary","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüìö Turkmen Trilingual Semi-Synthetic Dictionary\n\t\n\n\n\t\n\t\t\n\t\tüåç –û–±–∑–æ—Ä\n\t\n\n–≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 61 970 —Ç—Ä–∏—è–∑—ã—á–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (—Ç—É—Ä–∫–º–µ–Ω—Å–∫–∏–π‚Äì–∞–Ω–≥–ª–∏–π—Å–∫–∏–π‚Äì—Ä—É—Å—Å–∫–∏–π), –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –ó–∞–≥–æ–ª–æ–≤–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –∏—Ö –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã –±—ã–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç—É—Ä–∫–º–µ–Ω—Å–∫–∏—Ö PDF-—Å–ª–æ–≤–∞—Ä–µ–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç ¬´–ø–æ–ª—É—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º¬ª.\n\n–Ø–∑—ã–∫–∏: —Ç—É—Ä–∫–º–µ–Ω—Å–∫–∏–π (tk), –∞–Ω–≥–ª–∏–π—Å–∫–∏–π (en), —Ä—É—Å—Å–∫–∏–π (ru)  \n–§–æ—Ä–º–∞—Ç: JSONL\n–†–∞–∑–º–µ—Ä: 61 970 –∑–∞–ø–∏—Å–µ–π  \n–ò—Å—Ç–æ—á–Ω–∏–∫: 18‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mamed0v/TurkmenTrilingualSemi-SyntheticDictionary.","url":"https://huggingface.co/datasets/mamed0v/TurkmenTrilingualSemi-SyntheticDictionary","creator_name":"Bahtiyar Mamedov","creator_url":"https://huggingface.co/mamed0v","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","Turkmen","English","Russian"],"keywords_longer_than_N":true},
	{"name":"cv_rocket_demo","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüì¶ My Image Dataset\n\t\n\n\n\t\n\t\t\n\t\t–û–ø–∏—Å–∞–Ω–∏–µ\n\t\n\nMy Image Dataset ‚Äî —ç—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–∞ Hugging Face Hub.–í –¥–∞—Ç–∞—Å–µ—Ç–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –Ω–µ–±–æ–ª—å—à–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç—Ä—ë—Ö –∫–ª–∞—Å—Å–æ–≤:\n\nüê± –ö–æ—à–∫–∞ (cat)\nüê∂ –°–æ–±–∞–∫–∞ (dog)\nüê¶ –ü—Ç–∏—Ü–∞ (bird)\n\n–ö–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç—Å—è –º–µ—Ç–∫–æ–π (label).\n\n\n\t\n\t\t\n\t\t–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n\t\n\n\n\t\n\t\t\n–ü–æ–ª–µ\n–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö\n–û–ø–∏—Å–∞–Ω–∏–µ\n\n\n\t\t\nid\nint32\n–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–∏–º–µ—Ä–∞\n\n\nimage\nImage\n–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n\n\nlabel\nstring\n–ú–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (cat, dog, bird)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/voronik1801/cv_rocket_demo.","url":"https://huggingface.co/datasets/voronik1801/cv_rocket_demo","creator_name":"Voronik_test","creator_url":"https://huggingface.co/voronik1801","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","multi-class-image-classification","machine-generated","not_multilingual","original"],"keywords_longer_than_N":true},
	{"name":"JinaVDRAirbnbSyntheticRetrieval","keyword":"multilingual","description":"\n  JinaVDRAirbnbSyntheticRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve rendered tables from Airbnb listings based on templated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/airbnb-synthetic-retrieval_beir\n\n\n\t\n\nSource datasets:\n\njinaai/airbnb-synthetic-retrieval_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRAirbnbSyntheticRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRAirbnbSyntheticRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"hermes-3-dataset-ru-translated-prompts","keyword":"translated","description":"\n\t\n\t\t\n\t\t–ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–º—Ç—ã –∏–∑ hermes-3-dataset\n\t\n\n\n–ú–æ–¥–µ–ª—å-–ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ Gemma-3-27b-it.\n–ü–µ—Ä–µ–≤–µ–¥–µ–Ω—ã –≤—Å–µ –ø—Ä–æ–º—Ç—ã.\nMulti-turn –ø—Ä–æ–º—Ç—ã –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.\n–ë—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä—É–ø–Ω—ã—Ö —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏–ª–∏ Online RL.\n\n\n\t\n\t\t\n\t\n\t\n\t\tTranslated prompts from hermes-3-dataset\n\t\n\n\nTranslator model: Gemma-3-27b-it.\nAll prompts have been translated.\nMulti-turn prompts were translated considering the context of the English response.\nThis will be useful‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kristaller486/hermes-3-dataset-ru-translated-prompts.","url":"https://huggingface.co/datasets/kristaller486/hermes-3-dataset-ru-translated-prompts","creator_name":"Kristaller486","creator_url":"https://huggingface.co/kristaller486","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"prezentacii","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Prezentacii.org Educational Materials\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 100,381 educational materials from the prezentacii.org platform, a resource for teachers and students providing multimedia presentations and other educational content on various topics. The dataset includes information such as material titles, URLs, download URLs, and extracted text content where available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/prezentacii.","url":"https://huggingface.co/datasets/nyuuzyou/prezentacii","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"prezentacii","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Prezentacii.org Educational Materials\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 100,381 educational materials from the prezentacii.org platform, a resource for teachers and students providing multimedia presentations and other educational content on various topics. The dataset includes information such as material titles, URLs, download URLs, and extracted text content where available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/prezentacii.","url":"https://huggingface.co/datasets/nyuuzyou/prezentacii","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"voxpopolo_2","keyword":"multilingual","description":"A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation.","url":"https://huggingface.co/datasets/mcapozi/voxpopolo_2","creator_name":"Matteo Capozi","creator_url":"https://huggingface.co/mcapozi","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","multilingual","English","German","French"],"keywords_longer_than_N":true},
	{"name":"x_dataset_4","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_4.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_4","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mmBERT-data-midtraining","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-data-midtraining.","url":"https://huggingface.co/datasets/orionweller/mmBERT-data-midtraining","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"x_dataset_15","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_15.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_15","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_2025","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/goldentraversy07/reddit_dataset_2025.","url":"https://huggingface.co/datasets/goldentraversy07/reddit_dataset_2025","creator_name":"Steven K","creator_url":"https://huggingface.co/goldentraversy07","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"neuclir-2023","keyword":"multilingual","description":"\n  NeuCLIR2023Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NeuCLIR2023Retrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/neuclir-2023.","url":"https://huggingface.co/datasets/mteb/neuclir-2023","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","Persian","Russian"],"keywords_longer_than_N":true},
	{"name":"fungi_trait_circus_database","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tfungi_trait_circus_database\n\t\n\n\n\t\n\t\t\n\t\tÂ§ßËèåËº™„ÄåTrait Circus„Äç„Éá„Éº„Çø„Çª„ÉÉ„ÉàÔºàÁµ±Âà∂ÂΩ¢Ë≥™Ôºâ\nÊúÄÁµÇÊõ¥Êñ∞Êó•Ôºö2025/09/28\nÈáçË¶ÅÔºö„Éá„Éº„ÇøÂΩ¢Âºè„ÇíÂ§ßÂπÖ„Å´Êõ¥Êñ∞„Åó„Åæ„Åó„ÅüÔºàv2.0Ôºâ\n\t\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nJapanese and English\nPlease do not use this dataset for academic purposes for the time being. (casual use only)\nÈùûÂ∞ÇÈñÄÂÆ∂„Åå‰ΩúÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇÂ≠¶Ë°ìÁõÆÁöÑ„Åß„ÅÆ‰ΩøÁî®„ÅØ„ÅîÈÅ†ÊÖÆ„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n\t\n\t\t\n\t\tÊõ¥Êñ∞Â±•Ê≠¥\n\t\n\n\n2025/09/28 (v2.0) - „Éá„Éº„ÇøÊßãÈÄ†„ÇíÂÖ®Èù¢ÊîπË®Ç„ÄÅParquetÂΩ¢Âºè„Å´ÁßªË°å„ÄÅ„Éá„Éº„ÇøÈáè„ÇíÁ¥Ñ2ÂÄç„Å´Êã°ÂÖÖÔºàÁ¥Ñ400‰∏á‰ª∂Ôºâ\n2025/08/12 (v1.0) - ÂàùÂõûÂÖ¨ÈñãÁâàÔºàÁ¥Ñ180‰∏á‰ª∂Ôºâ\n\n\n\t\n\t\t\n\t\tÊ¶ÇË¶Å\n\t\n\nAtsushi NakajimaÔºà‰∏≠Â≥∂Ê∑≥ÂøóÔºâ„ÅåÂÄã‰∫∫„ÅßÈÅãÂñ∂„Åó„Å¶„ÅÑ„ÇãWeb„Çµ„Ç§„ÉàÂ§ßËèåËº™‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database.","url":"https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database","creator_name":"Atsushi Nakajima","creator_url":"https://huggingface.co/Atsushi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["other","other","multilingual","original","English"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_4","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andreans27/reddit_dataset_4.","url":"https://huggingface.co/datasets/andreans27/reddit_dataset_4","creator_name":"Andrean","creator_url":"https://huggingface.co/andreans27","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Document-Translation-en-es","keyword":"machine translation","description":"This dataset contains 10533 news articles from ELiRF/dacsa translated from Spanish to English using GPT-3.5-turbo. The dataset is intended to be used for training a model to translate text from English to Spanish and vicerversa. The dataset is also usefull to evaluate document level machine translation models.\nWe use the following prompt\n\ndef get_conversation(text: str, id: int) -> str:\n    messages = {\n        \"custom_id\": str(id),\n        \"method\": \"POST\",\n        \"url\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Iker/Document-Translation-en-es.","url":"https://huggingface.co/datasets/Iker/Document-Translation-en-es","creator_name":"Iker Garc√≠a-Ferrero","creator_url":"https://huggingface.co/Iker","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","English","Spanish","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Indic-Rag-Suite","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüåè Multilingual Indic RAG Suite\n\t\n\nA comprehensive multilingual question-answering dataset covering 18 Indian languages with 21,439,886 total samples, designed for RAG (Retrieval-Augmented Generation) applications and multilingual NLP research.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load specific language (recommended)\ndataset = load_dataset(\"ai4bharat/Indic-Rag-Suite\", \"as\")\ntrain_data = dataset['train']\n\nprint(f\"Loaded {len(train_data)} samples\")\n\n# Access‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite.","url":"https://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multilingual","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"Indic-Rag-Suite","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tüåè Multilingual Indic RAG Suite\n\t\n\nA comprehensive multilingual question-answering dataset covering 18 Indian languages with 21,439,886 total samples, designed for RAG (Retrieval-Augmented Generation) applications and multilingual NLP research.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load specific language (recommended)\ndataset = load_dataset(\"ai4bharat/Indic-Rag-Suite\", \"as\")\ntrain_data = dataset['train']\n\nprint(f\"Loaded {len(train_data)} samples\")\n\n# Access‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite.","url":"https://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multilingual","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"x_dataset_011210","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_011210.","url":"https://huggingface.co/datasets/william-1111/x_dataset_011210","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_96","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/reddit_dataset_96.","url":"https://huggingface.co/datasets/arrmlet/reddit_dataset_96","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44657","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rainbowbridge/x_dataset_44657.","url":"https://huggingface.co/datasets/rainbowbridge/x_dataset_44657","creator_name":"Rain","creator_url":"https://huggingface.co/rainbowbridge","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Synthdog-Multilingual-100","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tSynthdog Multilingual\n\t\n\n\n\nThe Synthdog dataset created for training in Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model.\nUsing the official Synthdog code, we created >1 million training samples for improving OCR capabilities in Large Vision-Language Models.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nWe provide the images for download in two .tar.gz files. Download and extract them in folders of the same name (so cat images.tar.gz.* | tar xvzf -C images; tar xvzf‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/Synthdog-Multilingual-100.","url":"https://huggingface.co/datasets/WueNLP/Synthdog-Multilingual-100","creator_name":"W√ºNLP","creator_url":"https://huggingface.co/WueNLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","multilingual","Afrikaans","Amharic","Arabic"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_8","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_8.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_8","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_107","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wolfghost/x_dataset_107.","url":"https://huggingface.co/datasets/wolfghost/x_dataset_107","creator_name":"ghost","creator_url":"https://huggingface.co/wolfghost","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_19","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_19.","url":"https://huggingface.co/datasets/James096/reddit_dataset_19","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0504178","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_0504178.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_0504178","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0305158","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/james-1111/x_dataset_0305158.","url":"https://huggingface.co/datasets/james-1111/x_dataset_0305158","creator_name":"james","creator_url":"https://huggingface.co/james-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_188","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hadesgod517/reddit_dataset_188.","url":"https://huggingface.co/datasets/hadesgod517/reddit_dataset_188","creator_name":"Hades","creator_url":"https://huggingface.co/hadesgod517","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"neuclir-2022-hard-negatives","keyword":"multilingual","description":"\n  NeuCLIR2022RetrievalHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/neuclir-2022-hard-negatives.","url":"https://huggingface.co/datasets/mteb/neuclir-2022-hard-negatives","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2022","Persian"],"keywords_longer_than_N":true},
	{"name":"x_dataset_040484","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_040484.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_040484","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_128","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malicious546/reddit_dataset_128.","url":"https://huggingface.co/datasets/malicious546/reddit_dataset_128","creator_name":"string malicious","creator_url":"https://huggingface.co/malicious546","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"RAGTruth-TR","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tRAGTruth-TR\n\t\n\nnewmindai/RAGTruth-TR is a Turkish-translated version of the wandb/RAGTruth-processed dataset.\nIt is designed for evaluating Retrieval-Augmented Generation (RAG) systems in Turkish, enabling research in hallucination detection, fact-checking, and response quality assessment.\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSource Dataset: wandb/RAGTruth-processed\nTarget Language: Turkish\nPurpose: Hallucination detection and RAG evaluation in Turkish NLP systems\nLicense: MIT (inherits from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/newmindai/RAGTruth-TR.","url":"https://huggingface.co/datasets/newmindai/RAGTruth-TR","creator_name":"NewMind AI","creator_url":"https://huggingface.co/newmindai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","summarization","text-classification","text-retrieval"],"keywords_longer_than_N":true},
	{"name":"x_dataset_22","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_22.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_22","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"twi-english-parallel-synthetic-50m","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTwi-English Parallel Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains a large-scale parallel corpus of Twi-English sentence pairs, featuring synthetically generated Twi sentences with corresponding English paraphrases. The dataset is designed to support machine translation, cross-lingual understanding, and other NLP tasks involving the Twi language (a dialect of Akan spoken in Ghana).\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Size: 47,924,398 parallel sentence pairs\nLanguage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi-english-parallel-synthetic-50m.","url":"https://huggingface.co/datasets/michsethowusu/twi-english-parallel-synthetic-50m","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","original","Twi","English"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_64","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lesnikutsa/reddit_dataset_64.","url":"https://huggingface.co/datasets/lesnikutsa/reddit_dataset_64","creator_name":"Igor Ponomarev","creator_url":"https://huggingface.co/lesnikutsa","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_170","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qr12138/reddit_dataset_170.","url":"https://huggingface.co/datasets/qr12138/reddit_dataset_170","creator_name":"wu","creator_url":"https://huggingface.co/qr12138","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_178","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Aniruddh79012/x_dataset_178.","url":"https://huggingface.co/datasets/Aniruddh79012/x_dataset_178","creator_name":"Singh","creator_url":"https://huggingface.co/Aniruddh79012","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_5","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_5.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_5","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_209","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wenknow/reddit_dataset_209.","url":"https://huggingface.co/datasets/wenknow/reddit_dataset_209","creator_name":"Dt","creator_url":"https://huggingface.co/wenknow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_94","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/coldmind/x_dataset_94.","url":"https://huggingface.co/datasets/coldmind/x_dataset_94","creator_name":"Toc","creator_url":"https://huggingface.co/coldmind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_90","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_90.","url":"https://huggingface.co/datasets/gk4u/x_dataset_90","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"BrightLongRetrieval","keyword":"multilingual","description":"\n  BrightLongRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nBright retrieval dataset with long documents.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNon-fiction, Written\n\n\nReference\nhttps://huggingface.co/datasets/xlangai/BRIGHT\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"BrightLongRetrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BrightLongRetrieval.","url":"https://huggingface.co/datasets/mteb/BrightLongRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","multilingual","xlangai/BRIGHT"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0502178","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_0502178.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_0502178","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0512140","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_0512140.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_0512140","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_1.","url":"https://huggingface.co/datasets/suul999922/x_dataset_1","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_48244","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/x_dataset_48244.","url":"https://huggingface.co/datasets/StormKing99/x_dataset_48244","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chaiamy/x_dataset_44.","url":"https://huggingface.co/datasets/chaiamy/x_dataset_44","creator_name":"Amy","creator_url":"https://huggingface.co/chaiamy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_660618","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chenxinpingcxp/reddit_dataset_660618.","url":"https://huggingface.co/datasets/chenxinpingcxp/reddit_dataset_660618","creator_name":"tian chen","creator_url":"https://huggingface.co/chenxinpingcxp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_060792","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_060792.","url":"https://huggingface.co/datasets/john-1111/x_dataset_060792","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_99","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jasonmoore92/x_dataset_99.","url":"https://huggingface.co/datasets/jasonmoore92/x_dataset_99","creator_name":"Jason Moore","creator_url":"https://huggingface.co/jasonmoore92","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"xtreme-up-semantic-parsing","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for afrixnli\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSee XTREME-UP GitHub\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThere are 20 languages available :\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe examples look like this for English:\nfrom datasets import load_dataset\ndata = load_dataset('Davlan/xtreme-up-semantic-parsing', 'yor') \n# Please, specify the language code\n# A data point example is below:\n{\n\"id\": \"3231323330393336\",\n\"split\": \"test\",\n\"intent\": \"IN:GET_REMINDER\",\n\"locale\": \"en\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davlan/xtreme-up-semantic-parsing.","url":"https://huggingface.co/datasets/Davlan/xtreme-up-semantic-parsing","creator_name":"David Adelani","creator_url":"https://huggingface.co/Davlan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multilingual","Amharic","Belarusian","Bengali"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_239","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lookpraise/reddit_dataset_239.","url":"https://huggingface.co/datasets/lookpraise/reddit_dataset_239","creator_name":"priase","creator_url":"https://huggingface.co/lookpraise","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_9","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_9.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_9","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"IN22-Gen","keyword":"multilingual","description":"\n  IN22GenBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Gen is a n-way parallel general-purpose multi-domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Legal, Government, News, Religious, Non-fiction, Written\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Gen\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22-Gen.","url":"https://huggingface.co/datasets/mteb/IN22-Gen","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","expert-generated","multilingual","Assamese"],"keywords_longer_than_N":true},
	{"name":"uhura-truthfulqa","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Uhura-TruthfulQA\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTruthfulQA is a widely recognized safety benchmark designed to measure the truthfulness of language model outputs across 38 categories, including health, law, finance, and politics. The English version of the benchmark originates from TruthfulQA: Measuring How Models Mimic Human Falsehoods (Lin et al., 2022) and consists of 817 questions in both multiple-choice and generation formats, targeting common misconceptions and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/uhura-truthfulqa.","url":"https://huggingface.co/datasets/masakhane/uhura-truthfulqa","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-generation","multiple-choice-qa","multilingual"],"keywords_longer_than_N":true},
	{"name":"Quora-NL","keyword":"translated","description":"\n  Quora-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nQuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. QuoraRetrieval-NL is a Dutch translation.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\nDomains\nWritten\n\n\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-quora\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Quora-NL.","url":"https://huggingface.co/datasets/mteb/Quora-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/quora","Dutch"],"keywords_longer_than_N":true},
	{"name":"2025-24679-text-dataset","keyword":"multilingual","description":"Anyuhhh/2025-24679-text-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Anyuhhh/2025-24679-text-dataset","creator_name":"Anyuhuang","creator_url":"https://huggingface.co/Anyuhhh","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","Chinese","Japanese","mit"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_73","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OGNOOB/reddit_dataset_73.","url":"https://huggingface.co/datasets/OGNOOB/reddit_dataset_73","creator_name":"a","creator_url":"https://huggingface.co/OGNOOB","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_11","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smmrokn/x_dataset_11.","url":"https://huggingface.co/datasets/smmrokn/x_dataset_11","creator_name":"Mohammad Mahdi","creator_url":"https://huggingface.co/smmrokn","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_15977","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rainbowbridge/x_dataset_15977.","url":"https://huggingface.co/datasets/rainbowbridge/x_dataset_15977","creator_name":"Rain","creator_url":"https://huggingface.co/rainbowbridge","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_149184","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_149184.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_149184","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"tatoeba_kbd_filtered","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTatoeba Kabardian Filtered Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a filtered version of the Multilingual-to-Kabardian Tatoeba Translations Dataset, containing higher-quality parallel sentence translations to Kabardian language (kbd). The dataset has been further filtered to ensure greater translation accuracy and consistency.\nThe source languages are:\n\n\t\n\t\t\nLanguage Code\nLanguage Name\nNumber of Examples\n\n\n\t\t\neng_Latn\nEnglish\n468,894\n\n\nrus_Cyrl\nRussian\n284,256‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/panagoa/tatoeba_kbd_filtered.","url":"https://huggingface.co/datasets/panagoa/tatoeba_kbd_filtered","creator_name":"adam panagov","creator_url":"https://huggingface.co/panagoa","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","Kabardian","German","English","French"],"keywords_longer_than_N":true},
	{"name":"mls-eng-128kb","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for English MLS\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a streamable version of the English version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ntt123/mls-eng-128kb.","url":"https://huggingface.co/datasets/ntt123/mls-eng-128kb","creator_name":"Th√¥ng Nguy·ªÖn","creator_url":"https://huggingface.co/ntt123","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"TradeNewsSum","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTradeNewsSum: Multilingual Summarization Dataset for Trade News\n\t\n\nTradeNewsSum is a multilingual dataset for abstractive summarization of trade-related news.It includes over 59,000 manually aligned article-summary pairs in Russian and English, focused on topics such as international trade, sanctions, investment, and oil markets.\nThe dataset is intended for training and evaluating summarization systems in both monolingual and cross-lingual settings.\n\n\t\n\t\t\n\t\n\t\n\t\tAnnotation Guidelines‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lyutovad/TradeNewsSum.","url":"https://huggingface.co/datasets/lyutovad/TradeNewsSum","creator_name":"Daria Lyutova","creator_url":"https://huggingface.co/lyutovad","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["summarization","news-articles-summarization","expert-generated","found","expert-generated"],"keywords_longer_than_N":true},
	{"name":"TradeNewsSum","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTradeNewsSum: Multilingual Summarization Dataset for Trade News\n\t\n\nTradeNewsSum is a multilingual dataset for abstractive summarization of trade-related news.It includes over 59,000 manually aligned article-summary pairs in Russian and English, focused on topics such as international trade, sanctions, investment, and oil markets.\nThe dataset is intended for training and evaluating summarization systems in both monolingual and cross-lingual settings.\n\n\t\n\t\t\n\t\n\t\n\t\tAnnotation Guidelines‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lyutovad/TradeNewsSum.","url":"https://huggingface.co/datasets/lyutovad/TradeNewsSum","creator_name":"Daria Lyutova","creator_url":"https://huggingface.co/lyutovad","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["summarization","news-articles-summarization","expert-generated","found","expert-generated"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_218","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/reddit_dataset_218.","url":"https://huggingface.co/datasets/arrmlet/reddit_dataset_218","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"English-Tamazight-Dictionnary-2007","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTamazight-English Dictionary 2007\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains cleaned and structured data extracted from the Tamazight-English Dictionary 2007 book, converted into machine-readable TSV files for linguistic research, language learning, and NLP applications.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset provides comprehensive bilingual resources for Tamazight (Central Atlas Tamazight/Berber) and English, including:\n\nBidirectional translation pairs (English‚ÜîTamazight)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Abdeljalil-Ounaceur/English-Tamazight-Dictionnary-2007.","url":"https://huggingface.co/datasets/Abdeljalil-Ounaceur/English-Tamazight-Dictionnary-2007","creator_name":"Abdeljalil  Ounaceur","creator_url":"https://huggingface.co/Abdeljalil-Ounaceur","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-classification","multilingual","original","English"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chaiamy/reddit_dataset_44.","url":"https://huggingface.co/datasets/chaiamy/reddit_dataset_44","creator_name":"Amy","creator_url":"https://huggingface.co/chaiamy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_44311","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_44311.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_44311","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_16","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_16.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_16","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretraining-data-chunk1","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk1.","url":"https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk1","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_211","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chain03/reddit_dataset_211.","url":"https://huggingface.co/datasets/chain03/reddit_dataset_211","creator_name":"chain","creator_url":"https://huggingface.co/chain03","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Dataset_test","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/harrylee1900/Dataset_test.","url":"https://huggingface.co/datasets/harrylee1900/Dataset_test","creator_name":"harrylee","creator_url":"https://huggingface.co/harrylee1900","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","multilingual"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0406135","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_0406135.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_0406135","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"IndicXnliPairClassification","keyword":"translated","description":"\n  IndicXnliPairClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nINDICXNLI is similar to existing XNLI dataset in shape/form, but\n        focusses on Indic language family.\n        The train (392,702), validation (2,490), and evaluation sets (5,010) of English\n        XNLI were translated from English into each of the eleven Indic languages. IndicTrans\n        is a large Transformer-based sequence to sequence model. It is trained on Samanantar\n        dataset (Ramesh et‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicXnliPairClassification.","url":"https://huggingface.co/datasets/mteb/IndicXnliPairClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","derived","translated","Divyanshu/indicxnli"],"keywords_longer_than_N":true},
	{"name":"x_dataset_21716","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_21716.","url":"https://huggingface.co/datasets/icedwind/x_dataset_21716","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_3753","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_3753.","url":"https://huggingface.co/datasets/icedwind/x_dataset_3753","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_42905","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_42905.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_42905","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mala-opus-dedup-2410","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMaLA Corpus: Massive Language Adaptation Corpus\n\t\n\nThis mala-opus-dedup-2410 is the bilingual part of the MaLA Corpus. It is a cleaned and deduplicated version of OPUS corpus, collected from OPUS with a cutoff of October 2024 (2410). Particularly, it contains bilingual translation data (aka, parallel data or bitexts) in 29,202 language pairs. \nThe MaLA Corpus (Massive Language Adaptation) is a series of comprehensive, multilingual datasets designed to support the continual pre-training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MaLA-LM/mala-opus-dedup-2410.","url":"https://huggingface.co/datasets/MaLA-LM/mala-opus-dedup-2410","creator_name":"MaLA-LM","creator_url":"https://huggingface.co/MaLA-LM","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","odc-by","10B - 100B","json"],"keywords_longer_than_N":true},
	{"name":"nopaste-paefchen-archive","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Nopaste Paefchen Archive\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is an archive of posts from nopaste.paefchen.net, a now-defunct pastebin-like service. It includes approximately 1.7 million unique posts, identified by sequential IDs starting from 1. The content spans various types of text data, including plain text, formatted text, URLs, and potentially code snippets or other formats in multiple languages.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/nopaste-paefchen-archive.","url":"https://huggingface.co/datasets/nyuuzyou/nopaste-paefchen-archive","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"nopaste-paefchen-archive","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Nopaste Paefchen Archive\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is an archive of posts from nopaste.paefchen.net, a now-defunct pastebin-like service. It includes approximately 1.7 million unique posts, identified by sequential IDs starting from 1. The content spans various types of text data, including plain text, formatted text, URLs, and potentially code snippets or other formats in multiple languages.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/nopaste-paefchen-archive.","url":"https://huggingface.co/datasets/nyuuzyou/nopaste-paefchen-archive","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","no-annotation","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"common_voice_21_0","keyword":"multilingual","description":"Due to storage limits some files had to be split into multiple parts. They can be merged like this: cat file.* > file.\n","url":"https://huggingface.co/datasets/2Jyq/common_voice_21_0","creator_name":"2Jyq","creator_url":"https://huggingface.co/2Jyq","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["crowdsourced","crowdsourced","multilingual","extended|common_voice","Abkhaz"],"keywords_longer_than_N":true},
	{"name":"gest","keyword":"machine translation","description":"\n\t\n\t\t\n\t\tGEST Dataset\n\t\n\nThis is a repository for the GEST dataset used to measure gender-stereotypical reasoning in language models and machine translation systems.\n\nPaper: Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling\nCode and additional data (annotation details, translations) are available in our repository\n\n\n\t\n\t\t\n\t\n\t\n\t\tChangelog\n\t\n\n\nDecember 6th 2024 - gest_1.1.csv was added. This is a new version that has 244 typos and other errors‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kinit/gest.","url":"https://huggingface.co/datasets/kinit/gest","creator_name":"Kempelen Institute of Intelligent Technologies","creator_url":"https://huggingface.co/kinit","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Slovenian","Slovak","Czech","Polish"],"keywords_longer_than_N":true},
	{"name":"translated_text2cypher24_testset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tTranslated Text2Cypher'24 Test Set\n\t\n\nThis dataset provides Spanish (es) and Turkish (tr) translations of the test split of the Neo4jText2Cypher'24 dataset.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nOnly the question field (user's natural language input) is translated.\nOriginal questions were in English (en), and translated versions are available in Spanish(es) and Turkish (tr).\nAll questions across languages are paired with the same Cypher query for consistent evaluation.\n\n\n\t\n\t\t\n\t\tUsage Example\n\t\n\nfrom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mgoNeo4j/translated_text2cypher24_testset.","url":"https://huggingface.co/datasets/mgoNeo4j/translated_text2cypher24_testset","creator_name":"MGO","creator_url":"https://huggingface.co/mgoNeo4j","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["apache-2.0","üá∫üá∏ Region: US","multilingual","text2cypher","question-to-query"],"keywords_longer_than_N":true},
	{"name":"PolyGuardPrompts","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tPolyGuard: A Multilingual Safety Moderation Tool for 17 Languages\n\t\n\nAbstract: Truly multilingual safety moderation efforts for Large Language Models (LLMs) have been hindered by a narrow focus on a small set of languages (e.g., English, Chinese) as well as a limited scope of safety definition, resulting in significant gaps in moderation capabilities. To bridge these gaps, we release PolyGuard, a new state-of-the-art multilingual safety model for safeguarding LLM generations, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ToxicityPrompts/PolyGuardPrompts.","url":"https://huggingface.co/datasets/ToxicityPrompts/PolyGuardPrompts","creator_name":"ToxicityPrompts","creator_url":"https://huggingface.co/ToxicityPrompts","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","Chinese","Czech","Dutch","English"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_122","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Aniruddh79012/reddit_dataset_122.","url":"https://huggingface.co/datasets/Aniruddh79012/reddit_dataset_122","creator_name":"Singh","creator_url":"https://huggingface.co/Aniruddh79012","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretraining-data-chunk3","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk3.","url":"https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk3","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Domain_Translation","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Domain_Translation is a dataset of BenchMAX, which evaluates the translation capability on specific domains.\nWe collect the domain multi-way parallel data from other tasks in BenchMAX, such as math data, code data, etc.\nEach sample contains one‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Domain_Translation.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Domain_Translation","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"x_dataset_84","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_84.","url":"https://huggingface.co/datasets/gk4u/x_dataset_84","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_8140","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_8140.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_8140","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_30","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_30.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_30","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_47","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tarzan19990815/x_dataset_47.","url":"https://huggingface.co/datasets/tarzan19990815/x_dataset_47","creator_name":"matthew allen","creator_url":"https://huggingface.co/tarzan19990815","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_17682","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_17682.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_17682","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/roknedin/reddit_dataset_44.","url":"https://huggingface.co/datasets/roknedin/reddit_dataset_44","creator_name":"Mohammad Roknedin","creator_url":"https://huggingface.co/roknedin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"language-metric-data","keyword":"multilingual","description":"# This dataset contains the entire content of three files loaded as a single example:\n# - `languages_list.pkl`: A pickled list of language strings.\n# - `average_distances_matrix.npy`: A NumPy matrix converted to a list of lists of floats.\n# - `distances_matrices.pkl`: A pickled dict of dicts of NumPy matrices.  \n#    It is converted into a list of records where each record corresponds to a dataset with a nested list of models and their associated distance matrices.\n#","url":"https://huggingface.co/datasets/mshamrai/language-metric-data","creator_name":"Maksym Shamrai","creator_url":"https://huggingface.co/mshamrai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["feature-extraction","mit","arxiv:2508.11676","üá∫üá∏ Region: US","multilingual"],"keywords_longer_than_N":true},
	{"name":"x_dataset_07096","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_07096.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_07096","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"QatarAirways_dataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tAMERICAN-EXPRESS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical and financial queries generated from American Express annual reports. It is designed to train and evaluate information retrieval models and improve AI understanding of financial documentation, with a specific focus on the credit card industry, payment processing, and banking services.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/QatarAirways_dataset.","url":"https://huggingface.co/datasets/Davidsv/QatarAirways_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_286316","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_286316.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_286316","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_19217","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_19217.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_19217","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_225","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tarzan19990815/x_dataset_225.","url":"https://huggingface.co/datasets/tarzan19990815/x_dataset_225","creator_name":"matthew allen","creator_url":"https://huggingface.co/tarzan19990815","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_18251","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/x_dataset_18251.","url":"https://huggingface.co/datasets/StormKing99/x_dataset_18251","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_120","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Spark0801/x_dataset_120.","url":"https://huggingface.co/datasets/Spark0801/x_dataset_120","creator_name":"SparkLiu","creator_url":"https://huggingface.co/Spark0801","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_181","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vuhongtien/x_dataset_181.","url":"https://huggingface.co/datasets/vuhongtien/x_dataset_181","creator_name":"Vu Hong Tien","creator_url":"https://huggingface.co/vuhongtien","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"ai-culture-multilingual-json-dolma","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tAI-Culture Multilingual JSON + DOLMA Corpus\n\t\n\n\n16M words ¬∑ 12 languages ¬∑ CC-BY-4.0\n\nThe AI-Culture corpus contains 5K articles providing comprehensive philosophical and cultural content, exploring the intersection of technology, artificial intelligence, and human culture, perfectly aligned across 12 languages. All content maintains identical parallel structure across translations with zero duplication and editor-curated quality.\nThis project is maintained by a non-profit digital‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-Culture-Commons/ai-culture-multilingual-json-dolma.","url":"https://huggingface.co/datasets/AI-Culture-Commons/ai-culture-multilingual-json-dolma","creator_name":"AI‚ÄëCulture‚ÄëCommons","creator_url":"https://huggingface.co/AI-Culture-Commons","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text-classification","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"x_dataset_21","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_21.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_21","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"tatoeba-bitext-mining","keyword":"multilingual","description":"\n  Tatoeba\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\n1,000 English-aligned sentence pairs for each language based on the Tatoeba corpus\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://github.com/facebookresearch/LASER/tree/main/data/tatoeba/v1\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"Tatoeba\"])\nevaluator = mteb.MTEB(task)\n\nmodel =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/tatoeba-bitext-mining.","url":"https://huggingface.co/datasets/mteb/tatoeba-bitext-mining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","human-annotated","multilingual","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"tulu3-100samples","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset\n\t\n\nThis dataset contains 100 samples from the Tulu 3 SFT Mixture.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example in the dataset contains the standard instruction-tuning data points as follow:\n\nid (str): a unique identifier\nmessages (list): message format used for supervised fine-tuning (this contains user prompt and assistant responses)\nsource (str): the source dataset for the given sample\n\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is licensed under ODC-BY-1.0. It is intended for research and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anayak/tulu3-100samples.","url":"https://huggingface.co/datasets/anayak/tulu3-100samples","creator_name":"Akash Nayak","creator_url":"https://huggingface.co/anayak","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["other","crowdsourced","expert-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"dark_thoughts_case_study_merged","keyword":"multilingual","description":"\n\n\t\n\t\t\n\t\tDark Thoughts Ê°à‰æãÁ†îÁ©∂Êé®ÁêÜÊï∞ÊçÆÈõÜ\n\t\n\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜÊèèËø∞\n\t\n\n\n\t\n\t\t\n\t\tÊ¶ÇËø∞\n\t\n\nDark Thoughts Ê°à‰æãÁ†îÁ©∂Êé®ÁêÜÊï∞ÊçÆÈõÜÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ§öËØ≠Ë®ÄÂïÜ‰∏öÊ°à‰æãÁ†îÁ©∂ÂèäÁõ∏ÂÖ≥Êé®ÁêÜÂìçÂ∫îÈõÜÂêà„ÄÇÂÆÉÈÄöËøáÂÖàËøõÁöÑËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜ Cablegate ÁîµÊä•ÔºåÁîüÊàê‰∏≠Ëã±ÊñáÂïÜ‰∏öÊ°à‰æãÁ†îÁ©∂ÔºåÂπ∂Ëøõ‰∏ÄÊ≠•‰∏∞ÂØå‰∫ÜÂà©ÁõäÁõ∏ÂÖ≥ËÄÖÁâπÂÆöÁöÑÊé®ÁêÜËßÜËßí„ÄÇÂØπ‰∫éÂØπÂïÜ‰∏öÂàÜÊûê„ÄÅÂ§öËØ≠Ë®ÄÂÜÖÂÆπÁîüÊàêÂíåÊé®ÁêÜËÉΩÂäõÊÑüÂÖ¥Ë∂£ÁöÑÁ†îÁ©∂‰∫∫ÂëòÂíå‰ªé‰∏ö‰∫∫ÂëòÊù•ËØ¥ÔºåËØ•Êï∞ÊçÆÈõÜÊòØÂÆùË¥µÁöÑËµÑÊ∫ê„ÄÇ\n\n\t\n\t\t\n\t\tÊîØÊåÅÁöÑ‰ªªÂä°\n\t\n\nËØ•Êï∞ÊçÆÈõÜÊîØÊåÅ‰ª•‰∏ã‰ªªÂä°Ôºö\n\nÊñáÊú¨ÁîüÊàê\nÊé®ÁêÜ‰∏éÂàÜÊûê\nÂèåËØ≠Ê°à‰æãÁ†îÁ©∂ÁîüÊàê\nË∑®ËØ≠Ë®ÄÂÜÖÂÆπÂàÜÊûê\nÂïÜ‰∏öÊàòÁï•Âà∂ÂÆö\nÂà©ÁõäÁõ∏ÂÖ≥ËÄÖËßÜËßíÂª∫Ê®°\n\n\n\t\n\t\t\n\t\tËØ≠Ë®Ä\n\t\n\nËØ•Êï∞ÊçÆÈõÜ‰∏∫ÂèåËØ≠Êï∞ÊçÆÈõÜÔºö\n\nËã±ËØ≠ (en)\n‰∏≠Êñá (zh)\n\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜÁªìÊûÑ\n\t\n\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÂ≠óÊÆµ\n\t\n\n{\n'id': 'int32', # Êù°ÁõÆÁöÑÂîØ‰∏ÄÊ†áËØÜÁ¨¶\n'response': 'string', # ÁîüÊàêÁöÑÊé®ÁêÜÂìçÂ∫î\n'query': 'string', # ÂéüÂßãÊü•ËØ¢ÊàñÊ°à‰æãÁ†îÁ©∂ÂÜÖÂÆπ\n'source_data': 'string', #‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DataTonic/dark_thoughts_case_study_merged.","url":"https://huggingface.co/datasets/DataTonic/dark_thoughts_case_study_merged","creator_name":"Data Tonic (Alignment Lab)","creator_url":"https://huggingface.co/DataTonic","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","DataTonic","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"Text_Guided_Image_Editing-ru","keyword":"translated","description":"Translated instructions from ImagenHub/Text_Guided_Image_Editing into Russian using gemini-flash-1.5-8b.\n","url":"https://huggingface.co/datasets/d0rj/Text_Guided_Image_Editing-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-image","translated","ImagenHub/Text_Guided_Image_Editing","Russian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"dataset_218","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/dataset_218.","url":"https://huggingface.co/datasets/arrmlet/dataset_218","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_16","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vuhongtien/x_dataset_16.","url":"https://huggingface.co/datasets/vuhongtien/x_dataset_16","creator_name":"Vu Hong Tien","creator_url":"https://huggingface.co/vuhongtien","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"HALvest-Geometric","keyword":"multilingual","description":"\n     HALvest-Geometric \n     Citation Network of Open Scientific Papers Harvested from HAL \n\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\n\t\n\t\t\n\t\toverview:\n\t\n\nFrench and English fulltexts from open papers found on Hyper Articles en Ligne (HAL) and its citation network.\nYou can download the dataset using Hugging Face datasets:\nfrom datasets import load_dataset\n\nds = load_dataset(\"Madjakul/HALvest-Geometric\", \"en\")\n\n\n\t\n\t\t\n\t\n\t\n\t\tDetails\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tNodes\n\t\n\n\nPapers: 18,662,037\nAuthors: 238,397\nAffiliations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/almanach/HALvest-Geometric.","url":"https://huggingface.co/datasets/almanach/HALvest-Geometric","creator_name":"ALMAnaCH (Inria)","creator_url":"https://huggingface.co/almanach","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_48558","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/littleGuagua/x_dataset_48558.","url":"https://huggingface.co/datasets/littleGuagua/x_dataset_48558","creator_name":"Felix","creator_url":"https://huggingface.co/littleGuagua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-ru","keyword":"translated","description":"\n\t\n\t\t\n\t\td0rj/OpenHermes-2.5-ru\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is translated version of teknium/OpenHermes-2.5 into Russian using Google Translate.\n","url":"https://huggingface.co/datasets/d0rj/OpenHermes-2.5-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"high-quality-multilingual-sentences","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tHigh Quality Multilingual Sentences\n\t\n\n\nThis dataset contains multilingual sentences derived from the agentlans/LinguaNova dataset.\nIt includes 1.58 million rows across 51 different languages, each in its own configuration.\n\nExample row (from the all config):\n{\n    \"text\": \"ÿßŸÖÿßŸÖ ÿ¨ŸÖÿπŸá ÿßÿµŸÅŸáÿßŸÜ ⁄ØŸÅÿ™: ŸÖ€åÿ≤ÿßŸÜ ŸÜ€åÿßÿ≤ ÿ¢ÿ® ÿ¥ÿ±ÿ® ÿßÿµŸÅŸáÿßŸÜ €±€±.€µ ŸÖÿ™ÿ± ŸÖ⁄©ÿπÿ® ÿßÿ≥ÿ™ ⁄©Ÿá ÿ™ŸÖÿßŸÖ ÿßÿ≥ÿ™ÿßŸÜ ÿßÿµŸÅŸáÿßŸÜ ÿ±ÿß ŸæŸàÿ¥ÿ¥ ŸÖ€åÿØŸáÿØ Ÿà ŸÜÿ≥ÿ®ÿ™ ÿ®Ÿá ŸÇÿ®ŸÑ ÿßÿ≤ ÿßŸÜŸÇŸÑÿßÿ® €å⁄©€å ÿßÿ≤ Ÿæ€åÿ¥ÿ±ŸÅÿ™Ÿáÿß ÿØÿ± ÿ≠Ÿàÿ≤Ÿá ÿ¢ÿ® ÿ®ŸàÿØŸá ÿßÿ≥ÿ™.\",\n    \"fasttext\": \"fa\",\n    \"gcld3\": \"fa\"\n}\n\nFields:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/high-quality-multilingual-sentences.","url":"https://huggingface.co/datasets/agentlans/high-quality-multilingual-sentences","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","text-retrieval","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_142","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RentonWEB3/reddit_dataset_142.","url":"https://huggingface.co/datasets/RentonWEB3/reddit_dataset_142","creator_name":"Renton Mark","creator_url":"https://huggingface.co/RentonWEB3","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_2.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_2","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_193","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sesen01/x_dataset_193.","url":"https://huggingface.co/datasets/sesen01/x_dataset_193","creator_name":"Selim Esen","creator_url":"https://huggingface.co/sesen01","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Tatoeba_br_fr","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nPaires breton/fran√ßais issues du site Tatoeba (export√©es du site le 11/05/2025)\n\n\t\n\t\t\n\t\tCitation\n\t\n\n\n\t\n\t\t\n\t\tOPUS\n\t\n\n@inbook{4992de1b5fb34f3e9691772606b36edf,\ntitle = \"News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces\",\nauthor = \"J{\\\"o}rg Tiedemann\",\nyear = \"2009\",\nlanguage = \"odefinierat/ok{\\\"a}nt\",\nvolume = \"V\",\npages = \"237--248\",\neditor = \"N. Nicolov and K. Bontcheva and G. Angelova and R. Mitkov\",\nbooktitle = \"Recent Advances in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Bretagne/Tatoeba_br_fr.","url":"https://huggingface.co/datasets/Bretagne/Tatoeba_br_fr","creator_name":"Bretagne","creator_url":"https://huggingface.co/Bretagne","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","Breton","French","cc-by-2.0"],"keywords_longer_than_N":true},
	{"name":"x_dataset_63681","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_63681.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_63681","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"DatasetResearch","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Research\n\t\n\n\n\n\nThis dataset is part of the DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery research project, presented in the paper DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery.\nAbstract:\nThe rapid advancement of large language models has fundamentally shifted the bottleneck in AI development from computational power to data availability-with countless valuable datasets remaining hidden across specialized‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GAIR/DatasetResearch.","url":"https://huggingface.co/datasets/GAIR/DatasetResearch","creator_name":"SII - GAIR","creator_url":"https://huggingface.co/GAIR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","English","Chinese","multilingual","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"x_dataset_7114","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_7114.","url":"https://huggingface.co/datasets/icedwind/x_dataset_7114","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_18","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_18.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_18","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_11","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_11.","url":"https://huggingface.co/datasets/suul999922/x_dataset_11","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_41414","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_41414.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_41414","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"bahadoransports","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMENA & CIS Business Index Dataset\n\t\n\n\n\t\n\t\t\n\t\tüåç Overview\n\t\n\nThis dataset contains structured, multilingual data about real-world businesses in the MENA (Middle East and North Africa) and CIS (Commonwealth of Independent States) regions, optimized for indexing in AI and LLM models. It aims to enhance business recognition in AI-based search, chatbots, and voice assistants.\n\n\t\n\t\t\n\t\tüì¶ Data Structure\n\t\n\nEach record includes:\n\nBusiness name (in English, Arabic, and Persian)\nLocation and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zahrasaghafi/bahadoransports.","url":"https://huggingface.co/datasets/Zahrasaghafi/bahadoransports","creator_name":"Zahra Saghafi","creator_url":"https://huggingface.co/Zahrasaghafi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","English","Persian","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_58","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_58.","url":"https://huggingface.co/datasets/James096/reddit_dataset_58","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_7834","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_7834.","url":"https://huggingface.co/datasets/momo1942/x_dataset_7834","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Korpus-divyezhek-brezhoneg-galleg","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tKorpus-divyezhek-brezhoneg-galleg\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nLe corpus bilingue breton- fraan√ßais de l'Office public de la langue bretonne est un corpus de textes traduits par des traducteurs humains.\nIl s'agit principalement de documents administratifs, d'articles ou d'expositions.\nLe jeu de donn√©es original provient de ce r√©pertoire GitHub.\nNous l'avons nettoy√© (suppression des lignes dupliqu√©es), passant alors de 62 861 lignes indiqu√©es par les auteurs √† 61 503 dans la version‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Bretagne/Korpus-divyezhek-brezhoneg-galleg.","url":"https://huggingface.co/datasets/Bretagne/Korpus-divyezhek-brezhoneg-galleg","creator_name":"Bretagne","creator_url":"https://huggingface.co/Bretagne","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","French","Breton","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MULTICOM","keyword":"multilingual","description":"This repository hosts the MULTICOM dataset, a novel benchmark for evaluating the multilingual commonsense generation abilities of Large Language Models (LLMs), as presented in the paper Do LLMs exhibit the same commonsense capabilities across languages?.\nThe dataset extends the COCOTEROS dataset to four languages: English, Spanish, Dutch, and Valencian. The task involves generating a commonsensical sentence that includes a given triplet of words.\n","url":"https://huggingface.co/datasets/gplsi/MULTICOM","creator_name":"GPLSI UA","creator_url":"https://huggingface.co/gplsi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Spanish","Dutch","Catalan"],"keywords_longer_than_N":true},
	{"name":"indic_sentiment_analyzer","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\n\t\n\t\t\n\t\tMultilingual Sentiment Analysis Dataset for Indian Languages\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis repository contains a comprehensive sentiment analysis dataset covering 11 Indian languages and English. The dataset is designed to support sentiment analysis tasks across multiple domains and languages, making it valuable for developing multilingual sentiment analysis models and applications.\n\n\t\n\t\t\n\t\tLanguages Covered\n\t\n\n\nEnglish (en) - Original\nHindi (hi)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dhruv0808/indic_sentiment_analyzer.","url":"https://huggingface.co/datasets/dhruv0808/indic_sentiment_analyzer","creator_name":"Dhruv Bhatnagar","creator_url":"https://huggingface.co/dhruv0808","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","Hindi","Telugu","Tamil","Kannada"],"keywords_longer_than_N":true},
	{"name":"x_dataset_62648","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rainbowbridge/x_dataset_62648.","url":"https://huggingface.co/datasets/rainbowbridge/x_dataset_62648","creator_name":"Rain","creator_url":"https://huggingface.co/rainbowbridge","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-Polylingo-50k","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tGammaCorpus Polylingo 50k\n\t\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nThe GammaCorpus Polylingo 50k dataset consists of 50,000 structured, unfiltered, single-turn conversations, where each interaction includes:\n\nInput: A user prompt or question.\nOutput: A response generated by an AI assistant.\nLanguage: The language used in the interaction.\n\nThis dataset is designed to help in the training and evaluation of conversational AI models for linguistic purposes. This dataset can be especially helpful if you‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k.","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Russian","Vietnamese","German"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_154","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PlanAPlanB/reddit_dataset_154.","url":"https://huggingface.co/datasets/PlanAPlanB/reddit_dataset_154","creator_name":"Andrei","creator_url":"https://huggingface.co/PlanAPlanB","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_01085","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/william-1111/x_dataset_01085.","url":"https://huggingface.co/datasets/william-1111/x_dataset_01085","creator_name":"william","creator_url":"https://huggingface.co/william-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_232","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Amylyx/x_dataset_232.","url":"https://huggingface.co/datasets/Amylyx/x_dataset_232","creator_name":"jianghonglin30@gmail.com","creator_url":"https://huggingface.co/Amylyx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_4","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_4.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_4","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"test_4","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tFLEURS\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark. \nWe use 2009 n-way parallel sentences from the FLoRes dev and devtest publicly available sets, in 102 languages. \nTraining sets have around 10 hours of supervision. Speakers of the train sets are different than speakers from the dev/test sets. Multilingual fine-tuning is\nused and ‚Äùunit error rate‚Äù (characters, signs) of all languages is averaged. Languages and results are also grouped into seven‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tunngo/test_4.","url":"https://huggingface.co/datasets/tunngo/test_4","creator_name":"nguyen tuan","creator_url":"https://huggingface.co/tunngo","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"x_dataset_51674","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_51674.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_51674","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_20","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_20.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_20","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Neo-GATE","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset card for Neo-GATE\n\t\n\nHomepage: https://mt.fbk.eu/neo-gate/\n\n\t\n\t\t\n\t\tDataset summary\n\t\n\nNeo-GATE is a bilingual corpus designed to benchmark the ability of machine translation (MT) systems to translate from English into Italian using gender-inclusive neomorphemes.\nIt is built upon GATE (Rarrick et al., 2023), a benchmark for the evaluation of gender rewriters and gender bias in MT.\nNeo-GATE includes 841 test entries (Neo-GATE.tsv) and 100 dev entries (Neo-GATE-dev.tsv).\nEach‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FBK-MT/Neo-GATE.","url":"https://huggingface.co/datasets/FBK-MT/Neo-GATE","creator_name":"FBK-MT","creator_url":"https://huggingface.co/FBK-MT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","multilingual","translation","English"],"keywords_longer_than_N":true},
	{"name":"GlobalNLI","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for global_nli\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGlobal NLI is a new text-based benchmark based on the aggregation of existing NLI datasets that are publicly available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThere are 59 languages available :\n{\n    'amh': 'Amharic',\n    'ara': 'Arabic',\n    'asm': 'Assamese',\n    'aym': 'Aymara',\n    'ben': 'Bengali',\n    'bul': 'Bulgarian',\n    'bzd': 'Bribri',\n    'cat': 'Catalan',\n    'cni': 'Ash√°ninka',\n    'deu': 'German',\n    'ell': 'Greek',\n    'eng':‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/McGill-NLP/GlobalNLI.","url":"https://huggingface.co/datasets/McGill-NLP/GlobalNLI","creator_name":"McGill NLP Group","creator_url":"https://huggingface.co/McGill-NLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","multilingual","XNLI, AfriXNLI, IndicXNLI, AmericasNLI [30], XNLI-ca, myXNLI, IndoNLI, JNLI , InferBR, sick_pl, JamPatoisNLI, KLUE, RoNLI.","Amharic"],"keywords_longer_than_N":true},
	{"name":"x_dataset_060232","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_060232.","url":"https://huggingface.co/datasets/john-1111/x_dataset_060232","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_127","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/x_dataset_127.","url":"https://huggingface.co/datasets/James096/x_dataset_127","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_204","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/goldentraversy07/reddit_dataset_204.","url":"https://huggingface.co/datasets/goldentraversy07/reddit_dataset_204","creator_name":"Steven K","creator_url":"https://huggingface.co/goldentraversy07","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"mala-monolingual-split","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMaLA Corpus: Massive Language Adaptation Corpus\n\t\n\nThis version contains train and validation splits.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MaLA Corpus (Massive Language Adaptation) is a comprehensive, multilingual dataset designed to support the continual pre-training of large language models. It covers 939 languages and consists of over 74 billion tokens, making it one of the largest datasets of its kind. With a focus on improving the representation of low-resource languages, the MaLA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MaLA-LM/mala-monolingual-split.","url":"https://huggingface.co/datasets/MaLA-LM/mala-monolingual-split","creator_name":"MaLA-LM","creator_url":"https://huggingface.co/MaLA-LM","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","odc-by","100M - 1B","json","Text"],"keywords_longer_than_N":true},
	{"name":"SwissJudgementClassification","keyword":"multilingual","description":"\n  SwissJudgementClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual, diachronic dataset of Swiss Federal Supreme Court cases annotated with the respective binarized judgment outcome (approval/dismissal)\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomainsLegal, Written\n\n\nReference\nhttps://aclanthology.org/2021.nllp-1.3/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SwissJudgementClassification.","url":"https://huggingface.co/datasets/mteb/SwissJudgementClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","multilingual","rcds/swiss_judgment_prediction","German"],"keywords_longer_than_N":true},
	{"name":"wikidata-label-maps-2025-all-languages","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikidata Multilingual Label Maps 2025\n\t\n\nComprehensive multilingual label and description maps extracted from the 2025 Wikidata dump.This dataset contains labels and descriptions for Wikidata entities (Q-items and P-properties) across 613 languages.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nüìä Total Records: 725,274,530 label/description pairs\nüÜî Unique Entities: 117,229,348 (Q-items and P-properties)\nüåç Languages: 613 unique language codes\nüìù With Descriptions: 339,691,043 pairs (46.8% coverage)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-2025-all-languages.","url":"https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-2025-all-languages","creator_name":"Yash Kumar Atri","creator_url":"https://huggingface.co/yashkumaratri","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["other","wikidata","multilingual","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"wikidata-label-maps-2025-all-languages","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tWikidata Multilingual Label Maps 2025\n\t\n\nComprehensive multilingual label and description maps extracted from the 2025 Wikidata dump.This dataset contains labels and descriptions for Wikidata entities (Q-items and P-properties) across 613 languages.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nüìä Total Records: 725,274,530 label/description pairs\nüÜî Unique Entities: 117,229,348 (Q-items and P-properties)\nüåç Languages: 613 unique language codes\nüìù With Descriptions: 339,691,043 pairs (46.8% coverage)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-2025-all-languages.","url":"https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-2025-all-languages","creator_name":"Yash Kumar Atri","creator_url":"https://huggingface.co/yashkumaratri","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["other","wikidata","multilingual","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_117","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_117.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_117","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_240","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sm4rtdev/reddit_dataset_240.","url":"https://huggingface.co/datasets/sm4rtdev/reddit_dataset_240","creator_name":"ElonScott","creator_url":"https://huggingface.co/sm4rtdev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zengsdfew/reddit_dataset_44.","url":"https://huggingface.co/datasets/zengsdfew/reddit_dataset_44","creator_name":"miner3","creator_url":"https://huggingface.co/zengsdfew","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_44","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trungnam299/reddit_dataset_44.","url":"https://huggingface.co/datasets/trungnam299/reddit_dataset_44","creator_name":"Trung Nam","creator_url":"https://huggingface.co/trungnam299","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0603159","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/john-1111/x_dataset_0603159.","url":"https://huggingface.co/datasets/john-1111/x_dataset_0603159","creator_name":"john","creator_url":"https://huggingface.co/john-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_26","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andreans27/reddit_dataset_26.","url":"https://huggingface.co/datasets/andreans27/reddit_dataset_26","creator_name":"Andrean","creator_url":"https://huggingface.co/andreans27","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_245","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/williamlewis0620/x_dataset_245.","url":"https://huggingface.co/datasets/williamlewis0620/x_dataset_245","creator_name":"William Lewis","creator_url":"https://huggingface.co/williamlewis0620","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"EESE","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tThe Ever-Evolving Science Exam\n\t\n\n\n  \n  AIBENCH\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nAs foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad Range, wide Reach, and high Rigor, yet they often face two major challenges: data leakage risks that compromise benchmarking validity, and evaluation inefficiency due to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIBench/EESE.","url":"https://huggingface.co/datasets/AIBench/EESE","creator_name":"AIBench","creator_url":"https://huggingface.co/AIBench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","aiben.ch","expert-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"mls-eng-speaker-descriptions","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Annotations of English MLS\n\t\n\nThis dataset consists in annotations of the English subset of the Multilingual LibriSpeech (MLS) dataset. \nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours for other languages.\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls-eng-speaker-descriptions.","url":"https://huggingface.co/datasets/parler-tts/mls-eng-speaker-descriptions","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"x_dataset_3","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/x_dataset_3.","url":"https://huggingface.co/datasets/gk4u/x_dataset_3","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_ds_214449","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zkpbeats/reddit_ds_214449.","url":"https://huggingface.co/datasets/zkpbeats/reddit_ds_214449","creator_name":"YDS","creator_url":"https://huggingface.co/zkpbeats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_32","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Axioris/reddit_dataset_32.","url":"https://huggingface.co/datasets/Axioris/reddit_dataset_32","creator_name":"DaneFoster","creator_url":"https://huggingface.co/Axioris","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_69","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James096/reddit_dataset_69.","url":"https://huggingface.co/datasets/James096/reddit_dataset_69","creator_name":"Brown","creator_url":"https://huggingface.co/James096","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_192","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Crystal1101/x_dataset_192.","url":"https://huggingface.co/datasets/Crystal1101/x_dataset_192","creator_name":"Butterfly","creator_url":"https://huggingface.co/Crystal1101","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_17","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlemdatameow/x_dataset_17.","url":"https://huggingface.co/datasets/mlemdatameow/x_dataset_17","creator_name":"Mlem Meow","creator_url":"https://huggingface.co/mlemdatameow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"GlobalNLI","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for global_nli\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGlobal NLI is a new text-based benchmark based on the aggregation of existing NLI datasets that are publicly available.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThere are 59 languages available :\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe examples look like this for English:\nfrom datasets import load_dataset\ndata = load_dataset('vivekvermaiit/globalnli', 'eng') \n# Please, specify the language code\n# A data point example is below:\n{‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vivekvermaiit/GlobalNLI.","url":"https://huggingface.co/datasets/vivekvermaiit/GlobalNLI","creator_name":"Vivek Verma","creator_url":"https://huggingface.co/vivekvermaiit","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","multilingual","XNLI, AfriXNLI, IndicXNLI, AmericasNLI [30], XNLI-ca, myXNLI, IndoNLI, JNLI , InferBR, sick_pl, JamPatoisNLI, KLUE, RoNLI.","Amharic"],"keywords_longer_than_N":true},
	{"name":"x_dataset_118","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sm4rtdev/x_dataset_118.","url":"https://huggingface.co/datasets/sm4rtdev/x_dataset_118","creator_name":"ElonScott","creator_url":"https://huggingface.co/sm4rtdev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0507238","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marry-1111/x_dataset_0507238.","url":"https://huggingface.co/datasets/marry-1111/x_dataset_0507238","creator_name":"marry","creator_url":"https://huggingface.co/marry-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"MKQARetrieval","keyword":"multilingual","description":"\n  MKQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual Knowledge Questions & Answers (MKQA)contains 10,000 queries sampled from the Google Natural Questions dataset.\n        For each query we collect new passage-independent answers. These queries and answers are then human translated into 25 Non-English languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://github.com/apple/ml-mkqa\n\n\n\t\n\nSource datasets:\n\napple/mkqa\n\n\n\t\n\t\t\n\t\tHow to evaluate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MKQARetrieval.","url":"https://huggingface.co/datasets/mteb/MKQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","human-annotated","multilingual","apple/mkqa"],"keywords_longer_than_N":true},
	{"name":"x_dataset_240","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sm4rtdev/x_dataset_240.","url":"https://huggingface.co/datasets/sm4rtdev/x_dataset_240","creator_name":"ElonScott","creator_url":"https://huggingface.co/sm4rtdev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_152","keyword":"multilingual","description":"\n\t\n\t\t\n\t\n\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/reddit_dataset_152.","url":"https://huggingface.co/datasets/suul999922/reddit_dataset_152","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"Maux-Persian-SFT-30k","keyword":"translated","description":"\n\t\n\t\t\n\t\tMaux-Persian-SFT-30k\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 30,000 high-quality Persian (Farsi) conversations for supervised fine-tuning (SFT) of conversational AI models. The dataset combines multiple sources to provide diverse, natural Persian conversations covering various topics and interaction patterns.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nmessages: List of conversation messages with role (user/assistant/system) and content\nsource: Source dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Maux-Persian-SFT-30k.","url":"https://huggingface.co/datasets/xmanii/Maux-Persian-SFT-30k","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Persian","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"afrixnli","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for afrixnli\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAFRIXNLI is an evaluation dataset comprising translations of a subset of the XNLI dataset into 16 African languages. \nIt includes both validation and test sets across all 18 languages, maintaining the English and French subsets from the original XNLI dataset. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThere are 18 languages available :\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe examples look like this for English:\nfrom datasets import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/afrixnli.","url":"https://huggingface.co/datasets/masakhane/afrixnli","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","multilingual","xnli","English"],"keywords_longer_than_N":true},
	{"name":"x_dataset_41362","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/icedwind/x_dataset_41362.","url":"https://huggingface.co/datasets/icedwind/x_dataset_41362","creator_name":"Felix","creator_url":"https://huggingface.co/icedwind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_55847","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_55847.","url":"https://huggingface.co/datasets/momo1942/x_dataset_55847","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0403203","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_0403203.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_0403203","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_47139","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_47139.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_47139","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-46082","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tFiQA2018-256-24-gpt-4o-2024-05-13-46082 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial sentiment and QA analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-46082 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-46082.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-46082","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"zenamt-document-level","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tZenaMT corpus (document-level)\n\t\n\nThis is an Italian ‚Äì Ligurian (Genoese) parallel corpus covering a number of domains of cultural relevance to Ligurian speakers. Parts of the corpus also contain aligned English translations, available in the column eng. Whenever an English translation is not available, the corresponding column is set to null.\nThis is the document-level version of the corpus. Source elements which were available in document form were retained as full documents, rather‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ConseggioLigure/zenamt-document-level.","url":"https://huggingface.co/datasets/ConseggioLigure/zenamt-document-level","creator_name":"Council for Ligurian Linguistic Heritage","creator_url":"https://huggingface.co/ConseggioLigure","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","original","Ligurian","Italian"],"keywords_longer_than_N":true},
	{"name":"MultiLongDocRetrieval","keyword":"multilingual","description":"\n  MultiLongDocRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMulti Long Doc Retrieval (MLDR) 'is curated by the multilingual articles from Wikipedia, Wudao and mC4 (see Table 7), and NarrativeQA (KocÀáisky ÃÅ et al., 2018; Gu Ãànther et al., 2023), which is only for English.' (Chen et al., 2024).\n        It is constructed by sampling lengthy articles from Wikipedia, Wudao and mC4 datasets and randomly choose paragraphs from them. Then we use GPT-3.5 to generate questions based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MultiLongDocRetrieval.","url":"https://huggingface.co/datasets/mteb/MultiLongDocRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","LM-generated","multilingual","Shitao/MLDR","Arabic"],"keywords_longer_than_N":true},
	{"name":"x_dataset_2244","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_2244.","url":"https://huggingface.co/datasets/momo1942/x_dataset_2244","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"vdr-multilingual-train","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tMultilingual Visual Document Retrieval Dataset\n\t\n\n\n\nThis dataset consists of 500k multilingual query image samples, collected and generated from scratch using public internet pdfs. The queries are synthetic and generated using VLMs (gemini-1.5-pro and Qwen2-VL-72B).\n\nIt was used to train the vdr-2b-multi-v1 retrieval multimodal, multilingual embedding model.\n\n\t\n\t\t\n\t\n\t\n\t\tHow it was created\n\t\n\nThis is the entire data pipeline used to create the Italian subset of this dataset. Each step‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llamaindex/vdr-multilingual-train.","url":"https://huggingface.co/datasets/llamaindex/vdr-multilingual-train","creator_name":"LlamaIndex","creator_url":"https://huggingface.co/llamaindex","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","German","Italian","French","Spanish"],"keywords_longer_than_N":true},
	{"name":"x_dataset_202507","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/goldentraversy07/x_dataset_202507.","url":"https://huggingface.co/datasets/goldentraversy07/x_dataset_202507","creator_name":"Steven K","creator_url":"https://huggingface.co/goldentraversy07","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_246","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stard8447/reddit_dataset_246.","url":"https://huggingface.co/datasets/stard8447/reddit_dataset_246","creator_name":"omarwalter","creator_url":"https://huggingface.co/stard8447","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0707238","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_0707238.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_0707238","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_155","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_155.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_155","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_107","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this dataset allows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wolfghost/reddit_dataset_107.","url":"https://huggingface.co/datasets/wolfghost/reddit_dataset_107","creator_name":"ghost","creator_url":"https://huggingface.co/wolfghost","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"multi-hatecheck","keyword":"multilingual","description":"\n  MultiHateClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nHate speech detection dataset with binary\n                       (hateful vs non-hateful) labels. Includes 25+ distinct types of hate\n                       and challenging non-hate, and 11 languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nConstructed, Written\n\n\nReference\nhttps://aclanthology.org/2022.woah-1.15/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/multi-hatecheck.","url":"https://huggingface.co/datasets/mteb/multi-hatecheck","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"x_dataset_20","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_20.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_20","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0710195","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_0710195.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_0710195","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_47268","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/x_dataset_47268.","url":"https://huggingface.co/datasets/StormKing99/x_dataset_47268","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"BigAudioDataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tAstraMindAI/BigAudioDataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAstraMindAI/BigAudioDataset is a large-scale, multilingual dataset designed for a wide range of audio and speech processing tasks. It comprises a diverse collection of audio clips, including both spoken voice and music, making it a valuable resource for training and evaluating models for automatic speech recognition (ASR), text-to-speech (TTS), audio classification, and more.\nThe voice data is aggregated from well-known‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AstraMindAI/BigAudioDataset.","url":"https://huggingface.co/datasets/AstraMindAI/BigAudioDataset","creator_name":"AstraMindAI","creator_url":"https://huggingface.co/AstraMindAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","English","Italian","French","German"],"keywords_longer_than_N":true},
	{"name":"BigAudioDataset","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tAstraMindAI/BigAudioDataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAstraMindAI/BigAudioDataset is a large-scale, multilingual dataset designed for a wide range of audio and speech processing tasks. It comprises a diverse collection of audio clips, including both spoken voice and music, making it a valuable resource for training and evaluating models for automatic speech recognition (ASR), text-to-speech (TTS), audio classification, and more.\nThe voice data is aggregated from well-known‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AstraMindAI/BigAudioDataset.","url":"https://huggingface.co/datasets/AstraMindAI/BigAudioDataset","creator_name":"AstraMindAI","creator_url":"https://huggingface.co/AstraMindAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","English","Italian","French","German"],"keywords_longer_than_N":true},
	{"name":"mmBERT-data-decay-eng","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-data-decay-eng.","url":"https://huggingface.co/datasets/orionweller/mmBERT-data-decay-eng","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","üá∫üá∏ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"x_dataset_245","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arrmlet/x_dataset_245.","url":"https://huggingface.co/datasets/arrmlet/x_dataset_245","creator_name":"Volodymyr Truba","creator_url":"https://huggingface.co/arrmlet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"CC-OCR","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tCC-OCR\n\t\n\nThis is the Repository for CC-OCR Benchmark.\nDataset and evaluation code for the Paper \"CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy\".\n\nüöÄ GitHub¬†¬† | ¬†¬†ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬† üìë Paper ¬†¬† | ¬†¬†üìó Blog\n\n\n\n\nHere is hosting the tsv version of CC-OCR data, which is used for evaluation in VLMEvalKit. Please refer to our GitHub for more information.\n\n\n\t\n\t\t\n\t\tBenchmark Leaderboard\n\t\n\n\n\n\t\n\t\t\nModel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wulipc/CC-OCR.","url":"https://huggingface.co/datasets/wulipc/CC-OCR","creator_name":"wulipc","creator_url":"https://huggingface.co/wulipc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","csv","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"znanio-archives","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Archives\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 14,273 educational archive files from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009. The dataset includes archives that may contain various file formats, making it potentially suitable for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-archives.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-archives","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["document-question-answering","image-classification","text-classification","visual-question-answering","found"],"keywords_longer_than_N":true},
	{"name":"znanio-archives","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Archives\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 14,273 educational archive files from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009. The dataset includes archives that may contain various file formats, making it potentially suitable for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-archives.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-archives","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["document-question-answering","image-classification","text-classification","visual-question-answering","found"],"keywords_longer_than_N":true},
	{"name":"ppt4web","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for PPT4Web Educational Materials\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 182,405 educational presentations from the ppt4web.ru platform, which provides online viewing and downloading of PowerPoint presentations. The dataset includes information such as presentation titles, URLs, download URLs, and file paths.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with some presentations in other languages such as English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/ppt4web.","url":"https://huggingface.co/datasets/nyuuzyou/ppt4web","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"ppt4web","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for PPT4Web Educational Materials\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata and original files for 182,405 educational presentations from the ppt4web.ru platform, which provides online viewing and downloading of PowerPoint presentations. The dataset includes information such as presentation titles, URLs, download URLs, and file paths.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with some presentations in other languages such as English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/ppt4web.","url":"https://huggingface.co/datasets/nyuuzyou/ppt4web","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"subdomains","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Card for Subdomain Statistics from scanner.ducks.party\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains monthly archives of subdomain statistics for websites seen by the scanner.ducks.party web bot. The data is provided in CSV format, with each archive containing two columns: Subdomain and Count.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in English, with subdomains potentially in multiple languages.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nSubdomain: The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/subdomains.","url":"https://huggingface.co/datasets/nyuuzyou/subdomains","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["other","machine-generated","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"reddit_dataset_172","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 Reddit Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed Reddit data. The data is continuously updated by network miners, providing a real-time stream of Reddit content for various analytical and machine learning tasks.\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gk4u/reddit_dataset_172.","url":"https://huggingface.co/datasets/gk4u/reddit_dataset_172","creator_name":"John Snow","creator_url":"https://huggingface.co/gk4u","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_16","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimbuja/x_dataset_16.","url":"https://huggingface.co/datasets/kimbuja/x_dataset_16","creator_name":"buja","creator_url":"https://huggingface.co/kimbuja","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0712117","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zephyr-1111/x_dataset_0712117.","url":"https://huggingface.co/datasets/zephyr-1111/x_dataset_0712117","creator_name":"zephyr","creator_url":"https://huggingface.co/zephyr-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_104","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smmrokn/x_dataset_104.","url":"https://huggingface.co/datasets/smmrokn/x_dataset_104","creator_name":"Mohammad Mahdi","creator_url":"https://huggingface.co/smmrokn","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_39","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_39.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_39","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_19","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_19.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_19","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_2","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_2.","url":"https://huggingface.co/datasets/suul999922/x_dataset_2","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_55139","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/StormKing99/x_dataset_55139.","url":"https://huggingface.co/datasets/StormKing99/x_dataset_55139","creator_name":"Storm King","creator_url":"https://huggingface.co/StormKing99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_91","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/coldmind/x_dataset_91.","url":"https://huggingface.co/datasets/coldmind/x_dataset_91","creator_name":"Toc","creator_url":"https://huggingface.co/coldmind","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"allenai_tulu-3-sft-mixture-DolphinLabeled","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tallenai tulu-3-sft-mixture DolphinLabeled\n\t\n\n\n\t\n\t\t\n\t\tPart of the DolphinLabeled series of datasets\n\t\n\n\n\t\n\t\t\n\t\tPresented by Eric Hartford and Cognitive Computations\n\t\n\nThe purpose of this dataset is to enable filtering of allenai/tulu-3-sft-mixture dataset.\nThe original dataset is allenai/tulu-3-sft-mixture\nI have modified the dataset using two scripts.\n\ndedupe.py - removes rows with identical final message content\nlabel.py - adds a \"flags\" column containing the following boolean‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cognitivecomputations/allenai_tulu-3-sft-mixture-DolphinLabeled.","url":"https://huggingface.co/datasets/cognitivecomputations/allenai_tulu-3-sft-mixture-DolphinLabeled","creator_name":"Cognitive Computations","creator_url":"https://huggingface.co/cognitivecomputations","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["other","crowdsourced","expert-generated","machine-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"Tiranige_lexicon","keyword":"multilingual","description":"\n[!NOTE]\nDataset origin: https://zenodo.org/records/3829167\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nLexicon of Tiranige, a western Dogon language of east central Mali, in csv spreadsheet form. Columns from left to right (omitting blanks) are:\n\ntirranige (transcription)\nfinnder (English finder list, i.e. usually one-word glosses)\nrecherche (finder list in French)\nengglish (full English gloss)\nfraan√ßais (full French gloss)\ncode (for certain lexical categories: fauna, flora, body, kinship)\norider/family (for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FrancophonIA/Tiranige_lexicon.","url":"https://huggingface.co/datasets/FrancophonIA/Tiranige_lexicon","creator_name":"FrancophonIA","creator_url":"https://huggingface.co/FrancophonIA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","multilingual","English","French","Tiranige Diga Dogon"],"keywords_longer_than_N":true},
	{"name":"x_dataset_28","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suul999922/x_dataset_28.","url":"https://huggingface.co/datasets/suul999922/x_dataset_28","creator_name":"mong","creator_url":"https://huggingface.co/suul999922","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_22","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futuremoon/x_dataset_22.","url":"https://huggingface.co/datasets/futuremoon/x_dataset_22","creator_name":"futuremoon","creator_url":"https://huggingface.co/futuremoon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_020216","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michael-1111/x_dataset_020216.","url":"https://huggingface.co/datasets/michael-1111/x_dataset_020216","creator_name":"michael","creator_url":"https://huggingface.co/michael-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_130","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Spark0801/x_dataset_130.","url":"https://huggingface.co/datasets/Spark0801/x_dataset_130","creator_name":"SparkLiu","creator_url":"https://huggingface.co/Spark0801","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_36129","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LadyMia/x_dataset_36129.","url":"https://huggingface.co/datasets/LadyMia/x_dataset_36129","creator_name":"Mia Cynthia","creator_url":"https://huggingface.co/LadyMia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"fusion-synth-data-s1kx","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tOffline Synthetic Data (s1K-X) for: Making, not taking, the Best-of-N\n\t\n\n\n\t\n\t\t\n\t\tContent\n\t\n\nThis data contains completions for the  s1K-X training split prompts from 5 different teacher models and 2 aggregations:\nTeachers: We sample one completion from each of the following models at temperature T=0.3. For kimik2, qwen3, and deepseek-v3 we use TogetherAI, for gemma3-27b and command-a we use locally hosted images.\n\ngemma3-27b: GEMMA3-27B-IT\nkimik2: KIMI-K2-INSTRUCT\nqwen3: QWEN3-235B‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/fusion-synth-data-s1kx.","url":"https://huggingface.co/datasets/CohereLabs/fusion-synth-data-s1kx","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","German","Spanish"],"keywords_longer_than_N":true},
	{"name":"x_dataset_248","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/veyhoranohy/x_dataset_248.","url":"https://huggingface.co/datasets/veyhoranohy/x_dataset_248","creator_name":"Steve Karadimas","creator_url":"https://huggingface.co/veyhoranohy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"x_dataset_10290","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning tasks.\nFor more information about the dataset, please visit the official repository.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe versatility of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/momo1942/x_dataset_10290.","url":"https://huggingface.co/datasets/momo1942/x_dataset_10290","creator_name":"MoMo.eth","creator_url":"https://huggingface.co/momo1942","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"xCodeEval","keyword":"multilingual","description":"The ability to solve problems is a hallmark of intelligence and has been an enduring goal in AI. AI systems that can create programs as solutions to problems or assist developers in writing programs can increase productivity and make programming more accessible. Recently, pre-trained large language models have shown impressive abilities in generating new codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap rather than actual execution whereas semantic similarity (or equivalence) of two code segments depends only on their ``execution similarity'', i.e., being able to get the same output for a given input.","url":"https://huggingface.co/datasets/NTU-NLP-sg/xCodeEval","creator_name":"NLP Group of Nanyang Technological University","creator_url":"https://huggingface.co/NTU-NLP-sg","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","token-classification","text-retrieval","text-generation","text-classification"],"keywords_longer_than_N":true},
	{"name":"x_dataset_0410139","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tBittensor Subnet 13 X (Twitter) Dataset\n\t\n\n\n    \n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMiner Data Compliance Agreement\n\t\n\nIn uploading this dataset, I am agreeing to the Macrocosmos Miner Data Compliance Policy. \n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is part of the Bittensor Subnet 13 decentralized network, containing preprocessed data from X (formerly Twitter). The data is continuously updated by network miners, providing a real-time stream of tweets for various analytical and machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robert-1111/x_dataset_0410139.","url":"https://huggingface.co/datasets/robert-1111/x_dataset_0410139","creator_name":"robert","creator_url":"https://huggingface.co/robert-1111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","summarization","text-generation"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Question_Answering","keyword":"multilingual","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Question_Answering is a dataset of BenchMAX for evaluating the long-context capability of LLMs in multilingual scenarios.\nThe subtasks are similar to the subtasks in RULER.\nThe data is sourcing from UN Parallel Corpus and xquad.\nThe haystacks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Question_Answering.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Question_Answering","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true}
]
;
