const data_for_modality_benchmark = 
[
	{"name":"SMMILE","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMs haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE.","url":"https://huggingface.co/datasets/smmile/SMMILE","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"CUADCovenantNotToSueLegalBenchClassification","keyword":"mteb","description":"\n  CUADCovenantNotToSueLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies that a party is restricted from contesting the validity of the counterparty's ownership of intellectual property or otherwise bringing a claim against the counterparty for matters unrelated to the contract.\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADCovenantNotToSueLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADCovenantNotToSueLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"cmedqav2-c-256-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tcmedqav2-c-256-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information and advice search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the cmedqav2-c-256-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/cmedqav2-c-256-24.","url":"https://huggingface.co/datasets/fine-tuned/cmedqav2-c-256-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"mimic_lerobot","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 1,\n    \"total_frames\": 927,\n    \"total_tasks\": 1,\n    \"total_videos\": 3,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chengkunli/mimic_lerobot.","url":"https://huggingface.co/datasets/chengkunli/mimic_lerobot","creator_name":"Chengkun Li","creator_url":"https://huggingface.co/chengkunli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"protein_chain_conformational_states","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSchema description:\n\t\n\nThe manually curated dataset of open-closed monomers is included here as benchmarking_monomeric_open_closed_conformers.csv.  \nColumn descriptions:\n\n\t\n\t\t\n\t\tSchema description:\n\t\n\nThe manually curated dataset of open-closed monomers is included here as benchmarking_monomeric_open_closed_conformers.csv.  \nColumn descriptions:\n\nUNP_ACC | UniProt accession code\nUNP_START | Start of UniProt sequence for given PDBe entries\nUNP_END | End of UniProt sequence for givenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PDBEurope/protein_chain_conformational_states.","url":"https://huggingface.co/datasets/PDBEurope/protein_chain_conformational_states","creator_name":"Protein Data Bank in Europe","creator_url":"https://huggingface.co/PDBEurope","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","cc-by-4.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"ClimaQA","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tClimaQA: An Automated Evaluation Framework for Climate Question Answering Models (ICLR 2025)\n\t\n\nCheck the paper's webpage and GitHub for more info!\nThe ClimaQA benchmark is designed to evaluate Large Language Models (LLMs) on climate science question-answering tasks by ensuring scientific rigor and complexity. It is built from graduate-level climate science textbooks, which provide a reliable foundation for generating questions with precise terminology and complex scientific theories.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rose-STL-Lab/ClimaQA.","url":"https://huggingface.co/datasets/Rose-STL-Lab/ClimaQA","creator_name":"Rose Spatiotemporal Machine Learning Lab","creator_url":"https://huggingface.co/Rose-STL-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","apache-2.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"CodeFeedbackMT","keyword":"mteb","description":"\n  CodeFeedbackMT\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe dataset is a collection of user queries and assistant responses. The task is to retrieve the most relevant response for a given query.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\nReference\nhttps://arxiv.org/abs/2402.14658\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CodeFeedbackMT\"])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CodeFeedbackMT.","url":"https://huggingface.co/datasets/mteb/CodeFeedbackMT","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","English","mit"],"keywords_longer_than_N":true},
	{"name":"reward-aira-dataset","keyword":"alignment","description":"\n\t\n\t\t\n\t\tReward-Aira Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of prompt + completion examples of LLM following instructions in a conversational manner. All prompts come with two possible completions (one better than the other). The dataset is available in both Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized to train a reward/preference model or DPO fine-tuning.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nEnglish and Portuguese.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset.","url":"https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","Portuguese","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SWEbenchLiteRR","keyword":"mteb","description":"\n  SWEbenchLiteRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSoftware Issue Localization.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://www.swebench.com/Source datasets:\n\ntarsur909/mteb-swe-bench-lite-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"SWEbenchLiteRR\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SWEbenchLiteRR.","url":"https://huggingface.co/datasets/mteb/SWEbenchLiteRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-swe-bench-lite-reranking","code"],"keywords_longer_than_N":true},
	{"name":"llama-2-13b-hf-alpaca-eval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tllama-2-13b-hf-alpaca-eval\n\t\n\nAlpacaEval outputs for Llama 2 13B base model\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains model outputs generated using Llama 2 13B model on benchmark questions.\nModel: meta-llama/Llama-2-13b-hf or meta-llama/Llama-2-13b-chat-hf\nBenchmark: AlpacaEval\nGeneration Date: 2025-10-19\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\nalpaca_eval_llama2_base.json: Main output file with completions\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite the original Llama 2 paper andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jeqcho/llama-2-13b-hf-alpaca-eval.","url":"https://huggingface.co/datasets/jeqcho/llama-2-13b-hf-alpaca-eval","creator_name":"Chooi Je Qin","creator_url":"https://huggingface.co/jeqcho","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"llama-2-13b-chat-hf-alpaca-eval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tllama-2-13b-chat-hf-alpaca-eval\n\t\n\nAlpacaEval outputs for Llama 2 13B chat model\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains model outputs generated using Llama 2 13B model on benchmark questions.\nModel: meta-llama/Llama-2-13b-hf or meta-llama/Llama-2-13b-chat-hf\nBenchmark: AlpacaEval\nGeneration Date: 2025-10-19\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\nalpaca_eval_llama2_chat.json: Main output file with completions\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite the original Llama 2 paperâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jeqcho/llama-2-13b-chat-hf-alpaca-eval.","url":"https://huggingface.co/datasets/jeqcho/llama-2-13b-chat-hf-alpaca-eval","creator_name":"Chooi Je Qin","creator_url":"https://huggingface.co/jeqcho","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Feedback_Friction_Dataset","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFeedback Friction Dataset\n\t\n\nThis dataset contains the LLaMA-4 Maverick results from the iterative feedback experiments described in the paper: FEEDBACK FRICTION: LLMs Struggle to Fully Incorporate External Feedback.\nGithub Repository: https://github.com/JHU-CLSP/Feedback-Friction\nNote: While the paper evaluated multiple frontier models including LLaMA-3.3-70B-Instruct, LLaMA-4-Scout-17B-16E-Instruct, Claude 3.7 Sonnet, and Claude 3.7 Sonnet with Extended Thinking, this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dongwei/Feedback_Friction_Dataset.","url":"https://huggingface.co/datasets/Dongwei/Feedback_Friction_Dataset","creator_name":"Jiang","creator_url":"https://huggingface.co/Dongwei","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"probability_words_nli","keyword":"nli","description":"Probing neural language models for understanding of words of estimative probability","url":"https://huggingface.co/datasets/sileod/probability_words_nli","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","open-domain-qa","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"openai-tldr-summarisation-preferences","keyword":"alignment","description":"\n\t\n\t\t\n\t\tHuman feedback data\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nSee https://github.com/openai/summarize-from-feedback for original details of the dataset.\nHere the data is formatted to enable huggingface transformers sequence classification models to be trained as reward functions.\n","url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-summarisation-preferences","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","crowdsourced","crowdsourced","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"debug-upload-test-20251003163332","keyword":"test","description":"\n\t\n\t\t\n\t\tdebug-upload-test\n\t\n\nDebug test for file upload issues\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nTotal Files: 1\nTotal Size: 0.00 MB\nUpload Date: 2025-10-03 16:33:34\n\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\ndebug_test.txt (62 bytes)\n\n","url":"https://huggingface.co/datasets/vivekgr92/debug-upload-test-20251003163332","creator_name":"Vivek","creator_url":"https://huggingface.co/vivekgr92","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","ðŸ‡ºðŸ‡¸ Region: US","dataset","debug","test"],"keywords_longer_than_N":false},
	{"name":"TestEval-LR","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ“š TestEval-LR: Unit Test Generation Benchmark for Low-Resource Languages\n\t\n\nTestEval-LR is a validation benchmark designed to measure how well models can generate unit tests for Low-Resource Programming Languages (LRPLs) â€” specifically Rust, Go, and Julia.\n\n\t\n\t\t\n\t\tðŸ“Œ Purpose\n\t\n\nEvaluate how well a model can generate test code, given a focal function's source code and context.\n\n\t\n\t\t\n\t\tðŸ“‚ Dataset Structure\n\t\n\nEach example contains:\n\nfunction_name: Name of the focal function.\nfocal_code:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lqdunxgx2005/TestEval-LR.","url":"https://huggingface.co/datasets/lqdunxgx2005/TestEval-LR","creator_name":"Quang Dung Le","creator_url":"https://huggingface.co/lqdunxgx2005","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["code","mit","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US","rust"],"keywords_longer_than_N":true},
	{"name":"MathCanvas-Bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMathCanvas-Bench\n\t\n\n\n  \n    \n  \n  Â Â Â Â Â Â Â Â \n  \n    \n  \n  Â Â Â Â Â Â Â Â \n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“– Introduction\n\t\n\nMathCanvas-Bench is a challenging new benchmark designed to evaluate the intrinsic Visual Chain-of-Thought (VCoT) capabilities of Large Multimodal Models (LMMs). It serves as the primary evaluation testbed for the [MathCanvas] framework.\nWhile existing math benchmarks have advanced textual reasoning, they largely overlook a critical skill: the ability to generate and reason withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shiwk24/MathCanvas-Bench.","url":"https://huggingface.co/datasets/shiwk24/MathCanvas-Bench","creator_name":"Weikang Shi","creator_url":"https://huggingface.co/shiwk24","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"mteb-nl-sick-pcls-pr","keyword":"mteb","description":"\n  SICKNLPairClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSICK-NL is a Dutch translation of SICK \n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Written\n\n\nReference\nhttps://aclanthology.org/2021.eacl-main.126/\n\n\n\t\n\nSource datasets:\n\nclips/mteb-nl-sick\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"SICKNLPairClassification\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/clips/mteb-nl-sick-pcls-pr.","url":"https://huggingface.co/datasets/clips/mteb-nl-sick-pcls-pr","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","translated","clips/mteb-nl-sick","Dutch"],"keywords_longer_than_N":true},
	{"name":"MWS-Vision-Bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMWS-Vision-Bench\n\t\n\n\nðŸ‡·ðŸ‡º Ð ÑƒÑÑÐºÐ¾ÑÐ·Ñ‹Ñ‡Ð½Ð¾Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð½Ð¸Ð¶Ðµ / Russian summary below.\n\nMWS Vision Bench â€” the first Russian-language business-OCR benchmark designed for multimodal large language models (MLLMs).This is the validation split - publicly available for open evaluation and comparison.ðŸ§© Paper is coming soon.\nðŸ”— Official repository: github.com/mts-ai/MWS-Vision-BenchðŸ¢ Organization: MTSAIR on Hugging FaceðŸ“° Article on Habr (in Russian): â€œMWS Vision Bench â€” the first Russianâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MTSAIR/MWS-Vision-Bench.","url":"https://huggingface.co/datasets/MTSAIR/MWS-Vision-Bench","creator_name":"MTSAIR","creator_url":"https://huggingface.co/MTSAIR","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","document-question-answering","expert-generated","Russian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"mteb-nl-vabb-mlcls-pr","keyword":"mteb","description":"\n  VABBMultiLabelClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset contains the fourteenth edition of the Flemish Academic Bibliography for the Social Sciences and Humanities (VABB-SHW), a database of academic publications from the social sciences and humanities authored by researchers affiliated to Flemish universities (more information). Publications in the database are used as one of the parameters of the Flemish performance-based research funding systemâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/clips/mteb-nl-vabb-mlcls-pr.","url":"https://huggingface.co/datasets/clips/mteb-nl-vabb-mlcls-pr","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","monolingual","clips/mteb-nl-vabb","Dutch"],"keywords_longer_than_N":true},
	{"name":"mteb-nl-opentender-cls-pr","keyword":"mteb","description":"\n  OpenTenderClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset contains Belgian and Dutch tender calls from OpenTender in Dutch\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nGovernment, Written\n\n\nReference\nhttps://arxiv.org/abs/2509.12340\n\n\n\t\n\nSource datasets:\n\nclips/mteb-nl-opentender-cls\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/clips/mteb-nl-opentender-cls-pr.","url":"https://huggingface.co/datasets/clips/mteb-nl-opentender-cls-pr","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","monolingual","clips/mteb-nl-opentender-cls","Dutch"],"keywords_longer_than_N":true},
	{"name":"difraud","keyword":"benchmark","description":"DIFrauD -- (Domain Independent Fraud Detection) is a corpus of deceptive and truthful texts from 7 domains:\n\n\"fake_news\",\n\"job_scams\",\n\"phishing\",\n\"political_statements\",\n\"product_reviews\",\n\"sms\",\n\"twitter_rumours\"\n\nTo load a specific domain, pass it as the \"name\" parameter to load_dataset()","url":"https://huggingface.co/datasets/redasers/difraud","creator_name":"ReDAS Lab at the University of Houston","creator_url":"https://huggingface.co/redasers","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","monolingual","English","mit"],"keywords_longer_than_N":true},
	{"name":"LocBenchRR","keyword":"mteb","description":"\n  LocBenchRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSoftware Issue Localization.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://arxiv.org/abs/2503.09089Source datasets:\n\ntarsur909/mteb-loc-bench-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"LocBenchRR\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/LocBenchRR.","url":"https://huggingface.co/datasets/mteb/LocBenchRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-loc-bench-reranking","code"],"keywords_longer_than_N":true},
	{"name":"inferes","keyword":"nli","description":"\n\t\n\t\t\n\t\tDataset Card for InferES\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nNatural Language Inference dataset for European Spanish\nPaper accepted and (to be) presented at COLING 2022\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nNatural Language Inference\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nSpanish\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains two texts inputs (Premise and Hypothesis), Label for three-way classification, and annotation data.\n\n\t\n\t\t\n\t\tData Instances\n\t\n\ntrain size = 6444 \ntest size = 1612\n\n\t\n\t\t\n\t\tDataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/venelin/inferes.","url":"https://huggingface.co/datasets/venelin/inferes","creator_name":"Venelin Kovatchev","creator_url":"https://huggingface.co/venelin","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","expert-generated","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"CroQS-Benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tCroQS: Cross-modal Query Suggestion Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCroQS (Cross-modal Query Suggestion) is a benchmark dataset for evaluating cross-modal query suggestion systems in text-to-image retrieval scenarios. The dataset addresses the novel task of suggesting minimal textual modifications to help users explore visually consistent subsets of image collections, following the premise of \"Maybe you are looking for...\"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nCroQS comprises:\n\n50â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ruggero1912/CroQS-Benchmark.","url":"https://huggingface.co/datasets/Ruggero1912/CroQS-Benchmark","creator_name":"Giacomo Pacini","creator_url":"https://huggingface.co/Ruggero1912","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","visual-document-retrieval","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"SLF5K","keyword":"alignment","description":"The Summarization with Language Feedback (SLF5K) dataset is an English-language dataset containing 5K unique samples that can be used for the task of abstraction summarization. Each sample consists of a Reddit title and post, a model-generated (FeedME) summary, and human-written language feedback on that summary. Additionally, each sample has a high-quality, human-written (gold) summary that should be ideal for the Reddit post. Lastly, each sample has two additional model-generated summaries with binary human preference labels, on which summary is preferred by a human. The dataset can be used to train language models with language feedback on abstractive summarization. It can also be used to train a reward model on binary preferences.","url":"https://huggingface.co/datasets/JeremyAlain/SLF5K","creator_name":"JÃ©rÃ©my Scheurer","creator_url":"https://huggingface.co/JeremyAlain","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["summarization","expert-generated","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"inverse-scaling-ttc-main","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tInverse Scaling in Test-Time Compute\n\t\n\nPaper: Inverse Scaling in Test-Time Compute\nProject Page: https://safety-research.github.io/inverse-scaling-ttc/\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nWe construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. Our evaluation tasks span four categories: simple counting tasks with distractors, regression tasks withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/inverse-scaling-ttc/inverse-scaling-ttc-main.","url":"https://huggingface.co/datasets/inverse-scaling-ttc/inverse-scaling-ttc-main","creator_name":"Inverse Scaling","creator_url":"https://huggingface.co/inverse-scaling-ttc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"tape","keyword":"benchmark","description":"The Winograd schema challenge composes tasks with syntactic ambiguity,\nwhich can be resolved with logic and reasoning (Levesque et al., 2012).\n\nThe texts for the Winograd schema problem are obtained using a semi-automatic \npipeline. First, lists of 11 typical grammatical structures with syntactic \nhomonymy (mainly case) are compiled. For example, two noun phrases with a \ncomplex subordinate: 'A trinket from Pompeii that has survived the centuries'.\nRequests corresponding to these constructions are submitted in search of the \nRussian National Corpus, or rather its sub-corpus with removed homonymy. In the \nresulting 2+k examples, homonymy is removed automatically with manual validation\nafterward. Each original sentence is split into multiple examples in the binary \nclassification format, indicating whether the homonymy is resolved correctly or\nnot.","url":"https://huggingface.co/datasets/RussianNLP/tape","creator_name":"Natural Language Processing in Russian","creator_url":"https://huggingface.co/RussianNLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","question-answering","multiple-choice","Russian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ApplesM5-Dataset","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸŽ ApplesM5: Synthetic Apple Detection Benchmark\n\t\n\nThis repository hosts the data files (images and annotations) used in the Synetic AI research paper, \"Better Than Real: Synthetic Apple Detection for Orchards.\" This dataset was created through procedural content generation and physically-based rendering (PBR) to provide a clean, highly generalized training signal for robust agricultural AI.\nThe data demonstrates that training exclusively on this synthetic dataset yields superiorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SyneticAI/ApplesM5-Dataset.","url":"https://huggingface.co/datasets/SyneticAI/ApplesM5-Dataset","creator_name":"SyneticAI","creator_url":"https://huggingface.co/SyneticAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["object-detection","English","cc-by-4.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MultiSWEbenchRR","keyword":"mteb","description":"\n  MultiSWEbenchRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual Software Issue Localization.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://multi-swe-bench.github.io/#/\n\n\n\t\n\nSource datasets:\n\ntarsur909/mteb-swe-bench-multi-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"MultiSWEbenchRR\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MultiSWEbenchRR.","url":"https://huggingface.co/datasets/mteb/MultiSWEbenchRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-swe-bench-multi-reranking","code"],"keywords_longer_than_N":true},
	{"name":"LeCaRDv2","keyword":"mteb","description":"\n  LeCaRDv2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the case document that best matches or is most relevant to the scenario described in each of the provided queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://github.com/THUIR/LeCaRDv2\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/LeCaRDv2.","url":"https://huggingface.co/datasets/mteb/LeCaRDv2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","Chinese"],"keywords_longer_than_N":true},
	{"name":"time-series-dataset","keyword":"benchmark","description":"ðŸ“Š Data description table as follows:\n\n  \n    \n      Task\n      Dataset\n      Variants\n      Series Length\n      Dataset size\n      Time Span\n      Frequency\n      Forecastability*\n      Domain\n    \n  \n  \n    \n      Long-termForecasting\n      ETTm1\n      7\n      {96,192,336,720}\n      (34465, 11521, 11521)\n      20 months\n      15 minutes\n      0.46\n      Temperature\n    \n    \n      ETTm2\n      7\n      {96,192,336,720}\n      (34465, 11521, 11521)\n      20 months\n      15 minutes\n      0.55â€¦ See the full description on the dataset page: https://huggingface.co/datasets/dunzane/time-series-dataset.","url":"https://huggingface.co/datasets/dunzane/time-series-dataset","creator_name":"dunzane","creator_url":"https://huggingface.co/dunzane","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","English","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"test","description":"FamppyDopamine/test dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/FamppyDopamine/test","creator_name":"Umkilyong","creator_url":"https://huggingface.co/FamppyDopamine","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"fe-algorithm-validation","keyword":"benchmark","description":"\n\n# The FE Algorithm â€” Replication Library\n\n## Overview\nThe **FE Algorithm** is a paradoxâ€‘retention optimization method. Instead of discarding contradictory or â€œbadâ€ candidates, it preserves them as potential sources of breakthrough solutions. This approach has shown consistent improvements over Monte Carlo and other stochastic methods across multiple domains.\n\n## Key Results\n- **Protein Folding**: 2,000 trials, p < 0.001, 2.1Ã— faster than Monte Carlo, ~80% higher success rate  \n- **Travelingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Derek-Angell/fe-algorithm-validation.","url":"https://huggingface.co/datasets/Derek-Angell/fe-algorithm-validation","creator_name":"Derek Angell","creator_url":"https://huggingface.co/Derek-Angell","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","optimization,","metaheuristics","replication"],"keywords_longer_than_N":true},
	{"name":"GiftEvalPretrain","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tGIFT-Eval Pre-training Datasets\n\t\n\nPretraining dataset aligned with GIFT-Eval that has 71 univariate and 17 multivariate datasets, spanning seven domains and 13 frequencies, totaling 4.5 million time series and 230 billion data points. Notably this collection of data has no leakage issue with the train/test split and can be used to pretrain foundation models that can be fairly evaluated on GIFT-Eval.\nðŸ“„ Paper\nðŸ–¥ï¸ Code\nðŸ“” Blog Post\nðŸŽï¸ Leader Board\n\n\t\n\t\n\t\n\t\tEthical Considerationsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/GiftEvalPretrain.","url":"https://huggingface.co/datasets/Salesforce/GiftEvalPretrain","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","1M<n<10M","Time-series","arxiv:2410.10393"],"keywords_longer_than_N":true},
	{"name":"Long-Horizon-Execution","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLong Horizon Execution\n\t\n\nThis project contains the dataset accompanying the paper \"The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs\"\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nDoes continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponentialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arvindh75/Long-Horizon-Execution.","url":"https://huggingface.co/datasets/arvindh75/Long-Horizon-Execution","creator_name":"Arvindh Arun","creator_url":"https://huggingface.co/arvindh75","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-04062024-hsmq-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-04062024-hsmq-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-04062024-hsmq-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-04062024-hsmq-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-04062024-hsmq-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-eh35-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-eh35-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-eh35-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-eh35-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-eh35-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"benchmark","description":"\n\n\nðŸš€ Project Page | ðŸ“œ arXiv | ðŸ‘¨â€ðŸ’» Github | ðŸ† Leaderboard | ðŸ“ blog | ðŸ¤— HF Paper | ð• Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizationsâ€™ flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"DBPedia_PL_test_top_250_only_w_correct-v2","keyword":"mteb","description":"\n  DBPedia-PLHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\n\n\nReference\nhttps://github.com/iai-group/DBpedia-Entity/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DBPedia_PL_test_top_250_only_w_correct-v2.","url":"https://huggingface.co/datasets/mteb/DBPedia_PL_test_top_250_only_w_correct-v2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/dbpedia","Polish"],"keywords_longer_than_N":true},
	{"name":"CUADAffiliateLicenseLicenseeLegalBenchClassification","keyword":"mteb","description":"\n  CUADAffiliateLicenseLicenseeLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if a clause describes a license grant to a licensee (incl. sublicensor) and the affiliates of such licensee/sublicensor.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADAffiliateLicenseLicenseeLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADAffiliateLicenseLicenseeLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-2ayt-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-2ayt-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-2ayt-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-2ayt-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-2ayt-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"GujaratiNewsClassification","keyword":"mteb","description":"\n  GujaratiNewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Gujarati dataset for 3-class classification of Gujarati news articles\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://github.com/goru001/nlp-for-gujarati\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"GujaratiNewsClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/GujaratiNewsClassification.","url":"https://huggingface.co/datasets/mteb/GujaratiNewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","Gujarati"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-english","keyword":"mteb","description":"\n  CQADupstackEnglishRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackEnglishRetrieval\"])\nevaluator = mteb.MTEB(task)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-english.","url":"https://huggingface.co/datasets/mteb/cqadupstack-english","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"amazon_massive_intent","keyword":"mteb","description":"\n  MassiveIntentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSpoken\n\n\nReference\nhttps://arxiv.org/abs/2204.08582\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"MassiveIntentClassification\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/amazon_massive_intent.","url":"https://huggingface.co/datasets/mteb/amazon_massive_intent","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","translated","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"SpaCE-10","keyword":"benchmark","description":"This repository contains the dataset for the paper SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence.\n\n SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence\n\n\nGitHub Repository: https://github.com/Cuzyoung/SpaCE-10\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ§  What is SpaCE-10?\n\t\n\nSpaCE-10 is a compositional spatial intelligence benchmark for evaluating Multimodal Large Language Models (MLLMs) in indoorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cusyoung/SpaCE-10.","url":"https://huggingface.co/datasets/Cusyoung/SpaCE-10","creator_name":"ZiYang Gong","creator_url":"https://huggingface.co/Cusyoung","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-de-932024-59f9-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-de-932024-59f9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"FEVER-256-24-gpt-4o-2024-05-13-989429","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFEVER-256-24-gpt-4o-2024-05-13-989429 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"dataset search for fact verification\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FEVER-256-24-gpt-4o-2024-05-13-989429 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FEVER-256-24-gpt-4o-2024-05-13-989429.","url":"https://huggingface.co/datasets/fine-tuned/FEVER-256-24-gpt-4o-2024-05-13-989429","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so100_chess_test2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 30,\n    \"total_frames\": 16185,\n    \"total_tasks\": 1,\n    \"total_videos\": 60,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:30\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TzuShian/so100_chess_test2.","url":"https://huggingface.co/datasets/TzuShian/so100_chess_test2","creator_name":"Tzu-Shian Yang","creator_url":"https://huggingface.co/TzuShian","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"NanoNQRetrieval","keyword":"mteb","description":"\n  NanoNQRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoNQ is a smaller subset of a dataset which contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Web\n\n\nReference\nhttps://ai.google.com/research/NaturalQuestions\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoNQRetrieval.","url":"https://huggingface.co/datasets/mteb/NanoNQRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","human-annotated","monolingual","mteb/nq"],"keywords_longer_than_N":true},
	{"name":"AlloProfClusteringP2P","keyword":"mteb","description":"\n  AlloProfClusteringP2P\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of document titles and descriptions from Allo Prof dataset. Clustering of 10 sets on the document topic.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nEncyclopaedic, Written\nReference\nhttps://huggingface.co/datasets/lyon-nlp/alloprof\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AlloProfClusteringP2P.","url":"https://huggingface.co/datasets/mteb/AlloProfClusteringP2P","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","monolingual","French","mit"],"keywords_longer_than_N":true},
	{"name":"CUADSourceCodeEscrowLegalBenchClassification","keyword":"mteb","description":"\n  CUADSourceCodeEscrowLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause requires one party to deposit its source code into escrow with a third party, which can be released to the counterparty upon the occurrence of certain events (bankruptcy, insolvency, etc.).\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADSourceCodeEscrowLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADSourceCodeEscrowLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"AlloProfClusteringP2P.v2","keyword":"mteb","description":"\n  AlloProfClusteringP2P.v2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of document titles and descriptions from Allo Prof dataset. Clustering of 10 sets on the document topic.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nEncyclopaedic, Written\nReference\nhttps://huggingface.co/datasets/lyon-nlp/alloprof\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AlloProfClusteringP2P.v2.","url":"https://huggingface.co/datasets/mteb/AlloProfClusteringP2P.v2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","monolingual","lyon-nlp/alloprof","French"],"keywords_longer_than_N":true},
	{"name":"legalbench_corporate_lobbying","keyword":"mteb","description":"\n  LegalBenchCorporateLobbying\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe dataset includes bill titles and bill summaries related to corporate lobbying.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench/viewer/corporate_lobbying\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/legalbench_corporate_lobbying.","url":"https://huggingface.co/datasets/mteb/legalbench_corporate_lobbying","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"PRISM-DPO","keyword":"alignment","description":"\n\t\n\t\t\n\t\tPRISM: Principled Reasoning for Integrated Safety in Multimodality Datasets\n\t\n\nThis repository provides access to the datasets developed for PRISM (Principled Reasoning for Integrated Safety in Multimodality), a system2-like framework that aligns Vision-Language Models (VLMs) by embedding a structured, safety-aware reasoning process.\n\nPaper: PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality\nCode: https://github.com/SaFoLab-WISC/PRISMâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/andyc03/PRISM-DPO.","url":"https://huggingface.co/datasets/andyc03/PRISM-DPO","creator_name":"Nanxi Li","creator_url":"https://huggingface.co/andyc03","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","Image","arxiv:2508.18649"],"keywords_longer_than_N":true},
	{"name":"RomaniBibleClustering","keyword":"mteb","description":"\n  RomaniBibleClustering\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering verses from the Bible in Kalderash Romani by book.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReligious, Written\n\n\nReference\nhttps://romani.global.bible/info\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"RomaniBibleClustering\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RomaniBibleClustering.","url":"https://huggingface.co/datasets/mteb/RomaniBibleClustering","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","translated","kardosdrur/romani-bible","Romany"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-235808","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFiQA2018-256-24-gpt-4o-2024-05-13-235808 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial sentiment analysis and opinion-based QA\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-235808 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-235808.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-235808","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_1","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3697,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_1.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_1","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ChemBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tChemBench\n\t\n\n\n\n\n\n\n\n\n\n\nA manually curated benchmark for evaluating chemistry and materials capabilities of Large Language Models\n\n\n\n\n\t\n\t\t\n\t\tâš ï¸ IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tðŸš« THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY ðŸš«\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/ChemBench.","url":"https://huggingface.co/datasets/jablonkagroup/ChemBench","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","language-modeling","natural-language-inference","expert-generated"],"keywords_longer_than_N":true},
	{"name":"AEGIS","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tAEGIS: Automated Error Generation and Identification for Multi-Agent Systems\n\t\n\nPaper | Project Page | Code\n\n\nAEGIS Framework\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nAEGIS (Automated Error Generation and Identification for Multi-Agent Systems) is a large-scale dataset and benchmark for detecting errors in Multi-Agent Systems (MAS). It addresses the critical lack of large-scale, diverse datasets with precise, ground-truth error labels for MAS, which has hampered research in understanding MAS errorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Fancylalala/AEGIS.","url":"https://huggingface.co/datasets/Fancylalala/AEGIS","creator_name":"KongFanqi","creator_url":"https://huggingface.co/Fancylalala","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"SciArena","keyword":"benchmark","description":"\n  SciArena: A New Platform for Evaluating Foundation Models in Scientific Literature Tasks\n\n\n  ðŸ“ Blog\n  ðŸŒ SciArena Platform\n  ðŸ’» Code\n  ðŸ“° Paper\n\n\nWe present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. Byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/SciArena.","url":"https://huggingface.co/datasets/yale-nlp/SciArena","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-stats","keyword":"mteb","description":"\n  CQADupstackStatsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Academic, Non-fiction\n\n\nReferencehttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackStatsRetrieval\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-stats.","url":"https://huggingface.co/datasets/mteb/cqadupstack-stats","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-20062024-djhb-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-20062024-djhb-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-20062024-djhb-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-20062024-djhb-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-20062024-djhb-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"vocsim-applications-avian-perception","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for VocSim - Avian Perception Alignment\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is used in the VocSim benchmark paper, specifically designed to evaluate how well neural audio embeddings align with biological perceptual judgments of similarity. It utilizes data from Zandberg et al. (2024), which includes recordings of zebra finch (Taeniopygia guttata) song syllables and results from behavioral experiments (probe and triplet tasks) measuring the birds' perception ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anonymous-submission000/vocsim-applications-avian-perception.","url":"https://huggingface.co/datasets/anonymous-submission000/vocsim-applications-avian-perception","creator_name":"Anonymous","creator_url":"https://huggingface.co/anonymous-submission000","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","< 1K","parquet","Audio","Text"],"keywords_longer_than_N":true},
	{"name":"Qu-QA-v2","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tQu QA v2 Dataset\n\t\n\nQu QA v2 is a large-scale question-answering (QA) dataset designed for training and evaluating machine learning models. It consists of question-answer pairs in English, making it suitable for general-purpose QA tasks, as well as specialized domains like code-related question answering and GSM8k-style problems.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nFeatures:\n\ninput: A string representing the question (dtype: string).\noutput: A string representing the answer (dtype: string).â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ereeeeef3/Qu-QA-v2.","url":"https://huggingface.co/datasets/Ereeeeef3/Qu-QA-v2","creator_name":"ffhs9","creator_url":"https://huggingface.co/Ereeeeef3","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"CMedQAv2-reranking-improved","keyword":"mteb","description":"\n\t\n\t\t\n\t\tCMedQAv2-reranking-improved Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the CMedQAv2-reranking-improved model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/CMedQAv2-reranking-improved.","url":"https://huggingface.co/datasets/fine-tuned/CMedQAv2-reranking-improved","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"InstructIR-mteb","keyword":"mteb","description":"\n  InstructIR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA benchmark specifically designed to evaluate the instruction following ability in information retrieval models. Our approach focuses on user-aligned instructions tailored to each query instance, reflecting the diverse characteristics inherent in real-world search scenarios. NOTE: scores on this may differ unless you include instruction first, then \"[SEP]\" and then the query via redefining combine_query_and_instruction inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/InstructIR-mteb.","url":"https://huggingface.co/datasets/mteb/InstructIR-mteb","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","human-annotated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"UWB_IMU_GT_QDrone_Benchmark_Dataset","keyword":"benchmark","description":"For additional details, please visit our website: https://benchmark.qdrone.ausmlab.com.\n\n\t\n\t\t\n\t\n\t\n\t\tQ-Drone UWB Benchmark Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nWe present the Q-Drone UWB Benchmark, a unique dataset derived from experiments conducted using the Q-Drone systemâ€”a UAV equipped with a UWB network at York University. This dataset encompasses data from five different sites, including an indoor environment, an open sports field, an area near a glass building, a semi-open tunnel, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QDrone/UWB_IMU_GT_QDrone_Benchmark_Dataset.","url":"https://huggingface.co/datasets/QDrone/UWB_IMU_GT_QDrone_Benchmark_Dataset","creator_name":"Zara Arj","creator_url":"https://huggingface.co/QDrone","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","ðŸ‡ºðŸ‡¸ Region: US","UAV","UWB"],"keywords_longer_than_N":true},
	{"name":"llmfao","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLarge Language Model Feedback Analysis and Optimization (LLMFAO)\n\t\n\nThe original Crowdsourced LLM Benchmark dataset in files prompts.parqet and outputs.parquet was kindly provided by the team at llmonitor.com under a CCÂ BY 4.0 license. This dataset can be conveniently processed with Evalica (arXiv).\n","url":"https://huggingface.co/datasets/dustalov/llmfao","creator_name":"Dmitry Ustalov","creator_url":"https://huggingface.co/dustalov","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"JSICK","keyword":"mteb","description":"\n  JSICK\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nJSICK is the Japanese NLI and STS dataset by manually translating the English dataset SICK (Marelli et al., 2014) into Japanese.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Written\n\n\nReference\nhttps://github.com/sbintuitions/JMTEB\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"JSICK\"])\nevaluator = mteb.MTEB(task)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JSICK.","url":"https://huggingface.co/datasets/mteb/JSICK","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","semantic-similarity-scoring","human-annotated","monolingual","Japanese"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-code-11_05_2024-hbxc-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-code-11_05_2024-hbxc-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"AI framework for improving LLM responses\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-code-11_05_2024-hbxc-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-11_05_2024-hbxc-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-11_05_2024-hbxc-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"RAVine-nuggets","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tRAVine-nuggets\n\t\n\nThis dataset contains the queries and nuggets (gold information) required by the test set of the RAVine evaluation framework, a Reality-Aligned eValuation framework for agentic LLMs with search. It is part of the comprehensive evaluation system designed for agentic search.\nPaper: RAVine: Reality-Aligned Evaluation for Agentic Search\nCode: https://github.com/SwordFaith/RAVine\nWe collected the queries from trec-2024-rag.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nThe main fieldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sapphirex/RAVine-nuggets.","url":"https://huggingface.co/datasets/sapphirex/RAVine-nuggets","creator_name":"yilong xu","creator_url":"https://huggingface.co/sapphirex","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MMBench-DEV-RU","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMMBench-DEV-RU\n\t\n\nÐ­Ñ‚Ð¾ Ð¿ÐµÑ€ÐµÐ²ÐµÐ´ÐµÐ½Ð½Ñ‹Ð¹ Dev ÑÐ¿Ð»Ð¸Ñ‚ mmbench Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ñ€ÑƒÑÑÐºÐ¾ÑÐ·Ñ‹Ñ‡Ð½Ñ‹Ñ… Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… LLM.\nÐŸÐµÑ€ÐµÐ²Ð¾Ð´ Ð¿Ñ€Ð¾Ð¸ÑÑ…Ð¾Ð´Ð¸Ð» Ð¿Ñ€Ð¸ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð¸ gpt-4, Ñ‡Ð°ÑÑ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð±Ñ‹Ð»Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐµÐ½Ð° Ð°ÑÑÐµÑÐ¾Ñ€Ð°Ð¼Ð¸.\nÐ’ Ð´Ð°Ð½Ð½Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð¼Ð°Ð»Ð°Ñ Ñ‡Ð°ÑÑ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°. \nÐ¡ÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº: https://huggingface.co/spaces/opencompass/MMBench\n\n\t\n\t\t\n\t\n\t\n\t\tÐ¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°\n\t\n\nhttps://github.com/Natyren/mmbench-ru-eval\nÐ¤Ð°Ð¹Ð», ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð²Ñ‹ ÑÐ¾Ð±Ð¸Ñ€Ð°ÐµÑ‚ÐµÑÑŒ Ð¿Ñ€Ð¾Ð³Ð½Ð°Ñ‚ÑŒ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ‚ÑŒ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ gtâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Vikhrmodels/MMBench-DEV-RU.","url":"https://huggingface.co/datasets/Vikhrmodels/MMBench-DEV-RU","creator_name":"Vikhr models","creator_url":"https://huggingface.co/Vikhrmodels","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","monolingual","original","Russian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"browsecomp-plus","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tBrowseComp-Plus\n\t\n\nBrowseComp-Plus is a new benchmark for Deep-Research system, isolating the effect of the retriever and the LLM agent to enable fair, transparent comparisons of Deep-Research agents. The benchmark sources challenging, reasoning-intensive queries from OpenAI's BrowseComp. However, instead of searching the live web, BrowseComp-Plus evaluates against a fixed, curated corpus of ~100K web documents from the web. The corpus includes both human-verified evidence documentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Tevatron/browsecomp-plus.","url":"https://huggingface.co/datasets/Tevatron/browsecomp-plus","creator_name":"Tevatron","creator_url":"https://huggingface.co/Tevatron","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"arguana-c-128-24-gpt-4o-2024-05-13-68212","keyword":"mteb","description":"\n\t\n\t\t\n\t\targuana-c-128-24-gpt-4o-2024-05-13-68212 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the arguana-c-128-24-gpt-4o-2024-05-13-68212 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-128-24-gpt-4o-2024-05-13-68212.","url":"https://huggingface.co/datasets/fine-tuned/arguana-c-128-24-gpt-4o-2024-05-13-68212","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-12h5-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-12h5-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-12h5-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-12h5-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-12h5-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADRenewalTermLegalBenchClassification","keyword":"mteb","description":"\n  CUADRenewalTermLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies a renewal term.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADRenewalTermLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADRenewalTermLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-pos-eight","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11175,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-eight.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-eight","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"aznli-benchmark","keyword":"nli","description":"\n\t\n\t\t\n\t\tAzNLI-Benchmark: Azerbaijani Natural Language Inference\n\t\n\nThis repository contains the AzNLI benchmark dataset for Natural Language Inference (NLI) in the Azerbaijani language. NLI is a fundamental task in natural language understanding where the goal is to determine the logical relationship between two sentences. This dataset is part of the translated version of https://huggingface.co/datasets/sentence-transformers/all-nli\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\nThe AzNLI benchmark consistsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LocalDoc/aznli-benchmark.","url":"https://huggingface.co/datasets/LocalDoc/aznli-benchmark","creator_name":"LocalDoc","creator_url":"https://huggingface.co/LocalDoc","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","Azerbaijani","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Hidream_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Hidream I1 full Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 38k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Hidream I1 full across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Hidream_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Hidream_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Hidream_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Hidream I1 full Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 38k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Hidream I1 full across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Hidream_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Hidream_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"IndicSentiment","keyword":"mteb","description":"\n  IndicSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA new, multilingual, and n-way parallel dataset for sentiment analysis in 13 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReferencehttps://arxiv.org/abs/2212.05409\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"IndicSentimentClassification\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicSentiment.","url":"https://huggingface.co/datasets/mteb/IndicSentiment","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"IndicReviewsClusteringP2P","keyword":"mteb","description":"\n  IndicReviewsClusteringP2P\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of reviews from IndicSentiment dataset. Clustering of 14 sets on the generic categories label.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReference\nhttps://arxiv.org/abs/2212.05409\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"IndicReviewsClusteringP2P\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicReviewsClusteringP2P.","url":"https://huggingface.co/datasets/mteb/IndicReviewsClusteringP2P","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","translated","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"VieStudentFeedbackClassification","keyword":"mteb","description":"\n  VieStudentFeedbackClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Vietnamese dataset for classification of student feedback\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReference\nhttps://ieeexplore.ieee.org/document/8573337\n\n\n\t\n\nSource datasets:\n\nuitnlp/vietnamese_students_feedback\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VieStudentFeedbackClassification.","url":"https://huggingface.co/datasets/mteb/VieStudentFeedbackClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"PARADE_audio","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tAHELM: A Holistic Evaluation of Audio-Language Models\n\t\n\nThis repository contains datasets used in AHELM: A Holistic Evaluation of Audio-Language Models.\nPaper: AHELM: A Holistic Evaluation of Audio-Language Models\nProject Page: https://crfm.stanford.edu/helm/audio/v1.0.0/\nCode (HELM framework): https://github.com/stanford-crfm/helm\nAHELM is a benchmark designed to holistically measure the performance of Audio-Language Models (ALMs) across 10 key aspects: audio perception, knowledgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/PARADE_audio.","url":"https://huggingface.co/datasets/UCSC-VLAA/PARADE_audio","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["audio-text-to-text","mit","< 1K","soundfolder","Audio"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-m3-6122024-ibs3-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-m3-6122024-ibs3-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Pet care\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-m3-6122024-ibs3-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-6122024-ibs3-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-6122024-ibs3-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Korean","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"InternationalCitizenshipQuestionsLegalBenchClassification","keyword":"mteb","description":"\n  InternationalCitizenshipQuestionsLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nAnswer questions about citizenship law from across the world. Dataset was made using the GLOBALCIT citizenship law dataset, by constructing questions about citizenship law as Yes or No questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/InternationalCitizenshipQuestionsLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/InternationalCitizenshipQuestionsLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-m3-6232024-4vtf-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-m3-6232024-4vtf-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Heating Ventilation and Air Conditioning units\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-m3-6232024-4vtf-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-6232024-4vtf-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-6232024-4vtf-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NeuCLIR2023Retrieval","keyword":"mteb","description":"\n  NeuCLIR2023Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\nSource datasets:\n\nmteb/neuclir-2023\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"NeuCLIR2023Retrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NeuCLIR2023Retrieval.","url":"https://huggingface.co/datasets/mteb/NeuCLIR2023Retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2023","Persian"],"keywords_longer_than_N":true},
	{"name":"CodeCompass","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tCodeCompass: A Benchmark for Code Generation\n\t\n\nPaper: Rethinking Verification for LLM Code Generation: From Generation to Testing\n\n\t\n\t\t\n\t\tDescription\n\t\n\nCodeCompass is a rigorous benchmark designed to evaluate the code generation capabilities of Large Language Models (LLMs). It comprises a comprehensive collection of programming problems sourced from competitive platforms, offering a standardized framework for assessing algorithmic reasoning, problem-solving, and code synthesis in aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/opencompass/CodeCompass.","url":"https://huggingface.co/datasets/opencompass/CodeCompass","creator_name":"OpenCompass","creator_url":"https://huggingface.co/opencompass","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","monolingual","English","apache-2.0","arxiv:2507.06920"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-512-192-gpt-4o-2024-05-13-73934","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-512-192-gpt-4o-2024-05-13-73934 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-512-192-gpt-4o-2024-05-13-73934 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-512-192-gpt-4o-2024-05-13-73934.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-512-192-gpt-4o-2024-05-13-73934","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TRECCOVID-512-192-gpt-4o-2024-05-13-347397","keyword":"mteb","description":"\n\t\n\t\t\n\t\tTRECCOVID-512-192-gpt-4o-2024-05-13-347397 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"COVID-19\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the TRECCOVID-512-192-gpt-4o-2024-05-13-347397 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/TRECCOVID-512-192-gpt-4o-2024-05-13-347397.","url":"https://huggingface.co/datasets/fine-tuned/TRECCOVID-512-192-gpt-4o-2024-05-13-347397","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_11","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3923,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_11.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_11","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Tessera2025","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ“š Tessera: Exposing the Challenges of LLM-based Test Generation for Low-Resource Programming Languages\n\t\n\nTessera is a validation benchmark designed to measure how well models can generate unit tests for Low-Resource Programming Languages (LRPLs) â€” specifically Rust, Go, and Julia.\n\n\t\n\t\t\n\t\tðŸ“Œ Purpose\n\t\n\nEvaluate how well a model can generate test code, given a focal function's source code and additional context.\n\n\t\n\t\t\n\t\tðŸ“‚ Dataset Structure\n\t\n\nEach sample contains:\n\nfunction_name:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Tessera2025/Tessera2025.","url":"https://huggingface.co/datasets/Tessera2025/Tessera2025","creator_name":"__","creator_url":"https://huggingface.co/Tessera2025","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["code","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"HagridRetrieval","keyword":"mteb","description":"\n  HagridRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nHAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset)is a dataset for generative information-seeking scenarios. It consists of queriesalong with a set of manually labelled relevant passages\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nEncyclopaedic, Written\n\n\nReference\nhttps://github.com/project-miracl/hagrid\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embeddingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HagridRetrieval.","url":"https://huggingface.co/datasets/mteb/HagridRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","monolingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"scidocs-c","keyword":"mteb","description":"\n\t\n\t\t\n\t\tscidocs-c Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research papers search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the scidocs-c model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/scidocs-c.","url":"https://huggingface.co/datasets/fine-tuned/scidocs-c","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"OverrulingLegalBenchClassification","keyword":"mteb","description":"\n  OverrulingLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task consists of classifying whether or not a particular sentence of case law overturns the decision of a previous case.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/OverrulingLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/OverrulingLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"WebGen-Bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tWebGen-Bench\n\t\n\nWebGen-Bench is created to benchmark LLM-based agent's ability to generate websites from scratch. The dataset is introduced in WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch. It contains 101 instructions and 647 test cases. It also has a training set of 6667 instructions, named WebGen-Instruct.\nThe code for evaluation as well as the training code and data are released at WebGen-Bench (Github)\n\n\t\n\t\t\n\t\n\t\n\t\tCategoriesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/luzimu/WebGen-Bench.","url":"https://huggingface.co/datasets/luzimu/WebGen-Bench","creator_name":"Zimu Lu","creator_url":"https://huggingface.co/luzimu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"mid-space","keyword":"alignment","description":"\n\t\n\t\t\n\t\tMID-Space: Aligning Diverse Communitiesâ€™ Needs to Inclusive Public Spaces\n\t\n\n\n\t\n\t\t\n\t\tA new version of the dataset will be released soon, incorporating user identity markers and expanded annotations.\n\t\n\n\n\t\n\t\t\n\t\tLIVS PAPER \n\t\n\n\nClick below to see more:\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe MID-Space dataset is designed to align AI-generated visualizations of urban public spaces with the preferences of diverse and marginalized communities in Montreal. It includes textual prompts, Stable Diffusionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CUPUM/mid-space.","url":"https://huggingface.co/datasets/CUPUM/mid-space","creator_name":"CUPUM","creator_url":"https://huggingface.co/CUPUM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-5272024-2fs4-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-5272024-2fs4-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"sentiment analysis model fine-tuning\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-5272024-2fs4-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5272024-2fs4-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5272024-2fs4-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Seedream-3_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Seedream 3 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~30'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating OpenAI 4o (version from 26.3.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Seedream-3_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Seedream-3_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Seedream-3_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Seedream 3 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~30'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating OpenAI 4o (version from 26.3.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Seedream-3_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Seedream-3_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-32000-384-gpt-4o-2024-05-13-4321481","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFiQA2018-32000-384-gpt-4o-2024-05-13-4321481 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-32000-384-gpt-4o-2024-05-13-4321481 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-32000-384-gpt-4o-2024-05-13-4321481.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-32000-384-gpt-4o-2024-05-13-4321481","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_28_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1202,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_28_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_28_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"AlloProfClusteringS2S.v2","keyword":"mteb","description":"\n  AlloProfClusteringS2S.v2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of document titles from Allo Prof dataset. Clustering of 10 sets on the document topic.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nEncyclopaedic, Written\n\n\nReference\nhttps://huggingface.co/datasets/lyon-nlp/alloprof\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"AlloProfClusteringS2S.v2\"])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AlloProfClusteringS2S.v2.","url":"https://huggingface.co/datasets/mteb/AlloProfClusteringS2S.v2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","monolingual","lyon-nlp/alloprof","French"],"keywords_longer_than_N":true},
	{"name":"IndicLangClassification","keyword":"mteb","description":"\n  IndicLangClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA language identification test set for native-script as well as Romanized text which spans 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nWeb, Non-fiction, Written\nReference\nhttps://arxiv.org/abs/2305.15814\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"IndicLangClassification\"])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicLangClassification.","url":"https://huggingface.co/datasets/mteb/IndicLangClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","language-identification","expert-annotated","monolingual","Assamese"],"keywords_longer_than_N":true},
	{"name":"stackoverflow-c","keyword":"mteb","description":"\n\t\n\t\t\n\t\tstackoverflow-c Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"code snippet search for developers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the stackoverflow-c model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/stackoverflow-c.","url":"https://huggingface.co/datasets/fine-tuned/stackoverflow-c","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"banking77","keyword":"mteb","description":"\n  Banking77Classification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDataset composed of online banking queries annotated with their corresponding intents.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nWritten\n\n\nReference\nhttps://arxiv.org/abs/2003.04807\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"Banking77Classification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/banking77.","url":"https://huggingface.co/datasets/mteb/banking77","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","monolingual","English","mit"],"keywords_longer_than_N":true},
	{"name":"CUADNoSolicitOfEmployeesLegalBenchClassification","keyword":"mteb","description":"\n  CUADNoSolicitOfEmployeesLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause restricts a party's soliciting or hiring employees and/or contractors from the counterparty, whether during the contract or after the contract ends (or both).\n\n\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADNoSolicitOfEmployeesLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADNoSolicitOfEmployeesLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-base-en-1362024-n19c-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-base-en-1362024-n19c-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Semantic Relationships\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-base-en-1362024-n19c-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-base-en-1362024-n19c-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-base-en-1362024-n19c-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-v2-two","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11174,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-two.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-two","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"SNAP","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSNAP Benchmark\n\t\n\nCode and annotations: [https://github.com/ykotseruba/SNAP]\nSNAP (stands for Shutter speed, ISO seNsitivity, and APerture) is a new benchmark consisting of images of objects taken under controlled lighting conditions and with densely sampled camera settings.\nThis benchmark allows testing the effects of capture bias, which includes camera settings and illumination, on performance of vision algorithms. \nSNAP contains 37,558 images of 100 scenes (10 scenes per 10 objectâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ykotseruba/SNAP.","url":"https://huggingface.co/datasets/ykotseruba/SNAP","creator_name":"Iuliia Kotseruba","creator_url":"https://huggingface.co/ykotseruba","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","question-answering","object-detection","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"NuminaMath-100k","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tMerged Math Datasets (100k Subset)\n\t\n\nThis dataset combines multiple mathematical datasets for training and evaluation purposes. This version contains a shuffled 100k subset of the training data for faster experimentation.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive collection of mathematical problems and solutions from various sources, organized into training and multiple test subsets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tTraining Set\n\t\n\n\nSize: 100000 examples\nFields: sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weijiezz/NuminaMath-100k.","url":"https://huggingface.co/datasets/weijiezz/NuminaMath-100k","creator_name":"weijie","creator_url":"https://huggingface.co/weijiezz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-05062024-16gq-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-05062024-16gq-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-05062024-16gq-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05062024-16gq-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05062024-16gq-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"mozzarella","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMozzarella-0.3.1 \n\t\n\n\n\t\n\t\t\n\t\tMotivation\n\t\n\n\nMozzarella is a dataset matching issues (= problem statements) and corresponding pull requests (PRs = problem solutions) of a selection of well maintained Java GitHub repositories. The original purpose was to serve as training and evaluation data for ML models concerned with fault localization and automated program repair of complex code bases. However, there might be more use cases that could benefit from this data. \nInspired by SWEBenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/feedback-to-code/mozzarella.","url":"https://huggingface.co/datasets/feedback-to-code/mozzarella","creator_name":"Feedback-2-Code","creator_url":"https://huggingface.co/feedback-to-code","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"askubuntu-c-64-24-gpt-4o-2024-05-131171","keyword":"mteb","description":"\n\t\n\t\t\n\t\taskubuntu-c-64-24-gpt-4o-2024-05-131171 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Technology Stack Documentation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the askubuntu-c-64-24-gpt-4o-2024-05-131171 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/askubuntu-c-64-24-gpt-4o-2024-05-131171.","url":"https://huggingface.co/datasets/fine-tuned/askubuntu-c-64-24-gpt-4o-2024-05-131171","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so101_office_test_20250802","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 5,\n    \"total_frames\": 2240,\n    \"total_tasks\": 1,\n    \"total_videos\": 10,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:5\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ktkd/so101_office_test_20250802.","url":"https://huggingface.co/datasets/ktkd/so101_office_test_20250802","creator_name":"whhe","creator_url":"https://huggingface.co/ktkd","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"HumanSense_Benchmark","keyword":"benchmark","description":"\n\nHumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs\n\n\n    Zheng Qin1,\n    Ruobing Zheng*2,\n    Yabing Wang1,\n    Tianqi Li2,\n    Yi Yuan2,\n    Jingdong Chen2,\n    Le Wangâ€ 1 \n    \n    \n    *Co-first authors. Project Lead.\n    â€ Corresponding Author.\n    \n    1Xiâ€™an Jiaotong University. 2Ant Group.\n    \n    \n\n\n Hugging Face Paper\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n arXiv:2508.10576\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n Homepage\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n GitHub\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n\n  \n    \n    Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/antgroup/HumanSense_Benchmark.","url":"https://huggingface.co/datasets/antgroup/HumanSense_Benchmark","creator_name":"Ant Group","creator_url":"https://huggingface.co/antgroup","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","arxiv:2508.10576","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"or-bench","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOR-Bench: An Over-Refusal Benchmark for Large Language Models\n\t\n\nPlease see our demo at HuggingFace Spaces. \n\n\t\n\t\t\n\t\tOverall Plots of Model Performances\n\t\n\nBelow is the overall model performance. X axis shows the rejection rate on OR-Bench-Hard-1K and Y axis shows the rejection rate on OR-Bench-Toxic. The best aligned model should be on the top left corner of the plot where the model rejects the most number of toxic prompts and least number of safe prompts. We also plot a blue lineâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bench-llms/or-bench.","url":"https://huggingface.co/datasets/bench-llms/or-bench","creator_name":"bench-llm","creator_url":"https://huggingface.co/bench-llms","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-polyai-nlu","keyword":"aveni-bench","description":"\n\t\n\t\t\n\t\tAveniBench: PolyAI NLU++\n\t\n\nPolyAI NLU++ split used in the AveniBench.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is made available under the CC-BY-4.0 license.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nAveniBench\nTDB\n\nPolyAI NLU++\n@inproceedings{casanueva-etal-2022-nlu,\n    title = \"{NLU}++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue\",\n    author = \"Casanueva, Inigo  and\n      Vuli{\\'c}, Ivan  and\n      Spithourakis, Georgios  and\n      Budzianowskiâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-polyai-nlu.","url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-polyai-nlu","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"SpartQA","keyword":"mteb","description":"\n  SpartQA\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMeasuring the ability to retrieve the groundtruth answers to reasoning task queries on SpartQA.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nEncyclopaedic, Written\n\n\nReference\nhttps://github.com/HLR/SpartQA_generation\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"SpartQA\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SpartQA.","url":"https://huggingface.co/datasets/mteb/SpartQA","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","RAR-b/spartqa","English"],"keywords_longer_than_N":true},
	{"name":"ContractNLISharingWithThirdPartiesLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLISharingWithThirdPartiesLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may share some Confidential Information with some third-parties (including consultants, agents and professional advisors).\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLISharingWithThirdPartiesLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLISharingWithThirdPartiesLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"green_cup_pour_fixed3","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 52,\n    \"total_frames\": 34041,\n    \"total_tasks\": 1,\n    \"total_videos\": 156,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:52\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiamFy/green_cup_pour_fixed3.","url":"https://huggingface.co/datasets/LiamFy/green_cup_pour_fixed3","creator_name":"Liam Achenbach","creator_url":"https://huggingface.co/LiamFy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["robotics","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","LeRobot","test"],"keywords_longer_than_N":false},
	{"name":"MaCBench-Prompt-Ablations","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMaCBench-Prompt-Ablations\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\tâš ï¸ IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tðŸš« THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY ðŸš«\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench-Prompt-Ablations.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench-Prompt-Ablations","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"RN_TR_R2_Benchmark_Results","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tRefinedNeuro/RN_TR_R2 Turkish Culture & Reasoning Benchmark\n\t\n\nThis repository contains the results of a custom benchmark designed to evaluate the performance of open-source language models on Turkish culture questions and basic reasoning tasks.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nWe crafted a set of 25 questions covering:\n\nTurkish general knowledge (e.g., capital city, national holidays, geography)\nBasic arithmetic and logic puzzles\nSimple calculus and string-processing tasks\n\nEach question is pairedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RefinedNeuro/RN_TR_R2_Benchmark_Results.","url":"https://huggingface.co/datasets/RefinedNeuro/RN_TR_R2_Benchmark_Results","creator_name":"RefinedNeuro","creator_url":"https://huggingface.co/RefinedNeuro","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"OmniSpatial","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tOmniSpatial\n\t\n\nThis repository contains the data presented in OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models.\n\n\t\n\t\t\n\t\tTask Schema Documentation\n\t\n\nThis document provides a structured explanation of the task schema for the visual-spatial reasoning benchmark.\n\n\n\t\n\t\t\n\t\tSchema Structure\n\t\n\nThe schema is represented in JSON format, containing the following key components:\n\n\t\n\t\t\nKey\nDescription\n\n\n\t\t\nid\nIdentifier for the question, formatted asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/qizekun/OmniSpatial.","url":"https://huggingface.co/datasets/qizekun/OmniSpatial","creator_name":"Zekun Qi","creator_url":"https://huggingface.co/qizekun","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"SWE-Perf","keyword":"benchmark","description":"\n\n\n\n\n\t\n\t\t\n\t\tSWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?\n\t\n\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nOptimizing code performance is paramount in software engineering, yet it remains a largely unexplored frontier for Large Language Models (LLMs). While models excel at fixing bugs, their ability to make code faster at a repository-scale is not well understood.\nTo address this, we introduce SWE-Perf, the first benchmark meticulously designed toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SWE-Perf/SWE-Perf.","url":"https://huggingface.co/datasets/SWE-Perf/SWE-Perf","creator_name":"SWE-Perf","creator_url":"https://huggingface.co/SWE-Perf","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MixEval","keyword":"benchmark","description":"\n\n\nðŸ  Homepage | ðŸ‘¨â€ðŸ’» Github | ðŸ† Leaderboard | ðŸ“œ arXiv | ðŸ“ blog | ðŸ¤— HF Paper | ð• Twitter\n\n\n\n\n\n\n\nBenchmark correlations (%) with Chatbot Arena Elo, against the total costs of evaluating a single GPT-3.5-Turbo-0125 model. MixEval and MixEval-Hard show the highest correlations with Arena Elo and Arena Elo (En) among leading benchmarks. We reference the crowdsourcing price for Amazon Mechanical Turk ($0.05 per vote) when estimating the cost of evaluating a single model on Chatbot Arenaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval.","url":"https://huggingface.co/datasets/MixEval/MixEval","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","text-retrieval","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"RAVine-logs","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tRAVine-logs\n\t\n\nThis repository contains the running logs of the experiments conducted in the paper RAVine: Reality-Aligned Evaluation for Agentic Search. These logs can be used for result reproduction or detailed case analysis of agentic LLMs with search performance.\nRAVine is a comprehensive evaluation system for agentic search, encompassing the web environment, benchmark datasets, and a novel evaluation method, serving as a full-process, reproducible, and goal-aligned evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sapphirex/RAVine-logs.","url":"https://huggingface.co/datasets/sapphirex/RAVine-logs","creator_name":"yilong xu","creator_url":"https://huggingface.co/sapphirex","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","English","apache-2.0","arxiv:2507.16725","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"IN22ConvBitextMining","keyword":"mteb","description":"\n  IN22ConvBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Conv is a n-way parallel conversation domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nSocial, Spoken, Fiction, Spoken\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Conv\n\n\n\t\n\nSource datasets:\n\nmteb/IN22-Conv\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22ConvBitextMining.","url":"https://huggingface.co/datasets/mteb/IN22ConvBitextMining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","multilingual","mteb/IN22-Conv","Assamese"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-13052024-35bv-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-13052024-35bv-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"legal regulations search for life sciences industry\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-13052024-35bv-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-13052024-35bv-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-13052024-35bv-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MasakhaNEWSClusteringP2P","keyword":"mteb","description":"\n  MasakhaNEWSClusteringP2P\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of news article headlines and texts from MasakhaNEWS dataset. Clustering of 10 sets on the news article label.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written, Non-fiction\nReference\nhttps://huggingface.co/datasets/masakhane/masakhanews\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MasakhaNEWSClusteringP2P.","url":"https://huggingface.co/datasets/mteb/MasakhaNEWSClusteringP2P","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","multilingual","Amharic","English"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-18062024-56t5-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-18062024-56t5-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-18062024-56t5-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-18062024-56t5-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-18062024-56t5-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-m3-782024-wl54-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-m3-782024-wl54-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-m3-782024-wl54-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-782024-wl54-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-782024-wl54-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"mathdial","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tMathdial dataset\n\t\n\nhttps://arxiv.org/abs/2305.14536\nMathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems.\nMathDial is grounded in math word problems as well as student confusions which provide a challenging testbed for creating faithful and equitable dialogue tutoring models able to reason over complex information. Current models achieve high accuracy in solving such problems but they fail in the task of teaching.\n\n\t\n\t\t\n\t\n\t\n\t\tDataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/eth-nlped/mathdial.","url":"https://huggingface.co/datasets/eth-nlped/mathdial","creator_name":"Language, Reasoning and Education lab | ETH Zurich","creator_url":"https://huggingface.co/eth-nlped","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"test_one_cam_new","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101_follower\",\n    \"total_episodes\": 3,\n    \"total_frames\": 1263,\n    \"total_tasks\": 1,\n    \"total_videos\": 3,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:3\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/malin141/test_one_cam_new.","url":"https://huggingface.co/datasets/malin141/test_one_cam_new","creator_name":"Malin Braatz","creator_url":"https://huggingface.co/malin141","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_24_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1201,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_24_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_24_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"insert_lego_no_recovery","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"franka\",\n    \"total_episodes\": 103,\n    \"total_frames\": 32686,\n    \"total_tasks\": 1,\n    \"total_videos\": 309,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 15,\n    \"splits\": {\n        \"train\": \"0:103\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/JohannesSaut/insert_lego_no_recovery.","url":"https://huggingface.co/datasets/JohannesSaut/insert_lego_no_recovery","creator_name":"Sautier","creator_url":"https://huggingface.co/JohannesSaut","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"financial-reports-extractive-summarization_eval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFinancial Reports Extractive Summarization Evaluation Dataset\n\t\n\nValidation and test splits for evaluating models on Arabic financial reports extractive summarization.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nFormat: Simple prompt-answer pairs\nValidation: ~20 examples (10%)\nTest: ~20 examples (10%)\nLanguage: Arabic\nDomain: Financial reports and market news\n\n\n\t\n\t\t\n\t\tFields\n\t\n\n\nid: Unique identifier\nprompt: The summarization prompt\nfull_text: Complete financial report\nanswer: Ground truthâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SahmBenchmark/financial-reports-extractive-summarization_eval.","url":"https://huggingface.co/datasets/SahmBenchmark/financial-reports-extractive-summarization_eval","creator_name":"Sahm_Benchmark","creator_url":"https://huggingface.co/SahmBenchmark","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","text-generation","Arabic","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"eval_so101_test3_2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 6790,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cornito/eval_so101_test3_2.","url":"https://huggingface.co/datasets/Cornito/eval_so101_test3_2","creator_name":"Corneille Marechal","creator_url":"https://huggingface.co/Cornito","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-pos-four","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11162,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-four.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-four","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"extended-refusal","keyword":"alignment","description":"\n\t\n\t\t\n\t\tExtended Refusal Dataset\n\t\n\nPaper: An Embarrassingly Simple Defense Against LLM Abliteration Attacks\nCite:\n@misc{shairah2025embarrassinglysimpledefensellm,\n      title={An Embarrassingly Simple Defense Against LLM Abliteration Attacks}, \n      author={Harethah Abu Shairah and Hasan Abed Al Kader Hammoud and Bernard Ghanem and George Turkiyyah},\n      year={2025},\n      eprint={2505.19056},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/HarethahMo/extended-refusal.","url":"https://huggingface.co/datasets/HarethahMo/extended-refusal","creator_name":"Harethah Abu Shairah","creator_url":"https://huggingface.co/HarethahMo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-veo3","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Veo 3 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~46k human responses from ~20k human annotators were collected to evaluate Veo3 video generation model on our benchmark. This dataset was collected in roughly 35 minutes using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider likingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo3.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo3","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5192024-henp-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5192024-henp-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Accounting laws and guidelines search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5192024-henp-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-henp-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-henp-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"UrduRomanSentimentClassification","keyword":"mteb","description":"\n  UrduRomanSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe Roman Urdu dataset is a data corpus comprising of more than 20000 records tagged for sentiment (Positive, Negative, Neutral)\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, Written\nReference\nhttps://archive.ics.uci.edu/dataset/458/roman+urdu+data+set\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/UrduRomanSentimentClassification.","url":"https://huggingface.co/datasets/mteb/UrduRomanSentimentClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"DBPedia-256-24-gpt-4o-2024-05-13-190101","keyword":"mteb","description":"\n\t\n\t\t\n\t\tDBPedia-256-24-gpt-4o-2024-05-13-190101 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"dataset search for text classification\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the DBPedia-256-24-gpt-4o-2024-05-13-190101 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/DBPedia-256-24-gpt-4o-2024-05-13-190101.","url":"https://huggingface.co/datasets/fine-tuned/DBPedia-256-24-gpt-4o-2024-05-13-190101","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-02092024-jqg1-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-02092024-jqg1-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-02092024-jqg1-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-02092024-jqg1-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-02092024-jqg1-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"GreekLegalCodeClassification","keyword":"mteb","description":"\n  GreekLegalCodeClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGreek Legal Code Dataset for Classification. (subset = chapter)\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://arxiv.org/abs/2109.15298\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"GreekLegalCodeClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/GreekLegalCodeClassification.","url":"https://huggingface.co/datasets/mteb/GreekLegalCodeClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","human-annotated","monolingual","Modern Greek (1453-)"],"keywords_longer_than_N":true},
	{"name":"SCIDOCS-256-24-gpt-4o-2024-05-13-10630","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSCIDOCS-256-24-gpt-4o-2024-05-13-10630 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"service search for translation and editing\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SCIDOCS-256-24-gpt-4o-2024-05-13-10630 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-10630.","url":"https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-10630","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-8-8-gpt-4o-2024-05-13-322852","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-8-8-gpt-4o-2024-05-13-322852 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-8-8-gpt-4o-2024-05-13-322852 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-322852.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-322852","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MSMARCO-256-24-gpt-4o-2024-05-13-374380","keyword":"mteb","description":"\n\t\n\t\t\n\t\tMSMARCO-256-24-gpt-4o-2024-05-13-374380 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"research dataset search for AI and NLP tasks\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the MSMARCO-256-24-gpt-4o-2024-05-13-374380 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-374380.","url":"https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-374380","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-2024615-ioyu-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-2024615-ioyu-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Smart home technology brand\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-2024615-ioyu-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-2024615-ioyu-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-2024615-ioyu-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NewsClassification","keyword":"mteb","description":"\n  NewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nLarge News Classification Dataset\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://arxiv.org/abs/1509.01626\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NewsClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nTo learn more about howâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NewsClassification.","url":"https://huggingface.co/datasets/mteb/NewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","expert-annotated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"so101_test001","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 50,\n    \"total_frames\": 18930,\n    \"total_tasks\": 1,\n    \"total_videos\": 100,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:50\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cristiannnn/so101_test001.","url":"https://huggingface.co/datasets/cristiannnn/so101_test001","creator_name":"Cristian P","creator_url":"https://huggingface.co/cristiannnn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"LAE-1M","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLAE-1M: Locate Anything on Earth Dataset\n\t\n\n\n  \n\n\nLAE-1M (Locate Anything on Earth - 1 Million) is a large-scale open-vocabulary remote sensing object detection dataset introduced in the paper \"Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for Remote Sensing Community\" (AAAI 2025).  \nIt contains over 1M images with coarse-grained (LAE-COD) and fine-grained (LAE-FOD) annotations, unified in COCO format, enabling zero-shot and few-shot detection in remote sensing.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jaychempan/LAE-1M.","url":"https://huggingface.co/datasets/jaychempan/LAE-1M","creator_name":"Jiancheng Pan","creator_url":"https://huggingface.co/jaychempan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["object-detection","zero-shot-object-detection","DOTA","DIOR","FAIR1M"],"keywords_longer_than_N":true},
	{"name":"ArguAna-32000-384-gpt-4o-2024-05-13-3663751","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-32000-384-gpt-4o-2024-05-13-3663751 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-32000-384-gpt-4o-2024-05-13-3663751 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-32000-384-gpt-4o-2024-05-13-3663751.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-32000-384-gpt-4o-2024-05-13-3663751","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"askubuntu-c-128-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\taskubuntu-c-128-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Technical troubleshooting search engine for Ubuntu\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the askubuntu-c-128-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/askubuntu-c-128-24.","url":"https://huggingface.co/datasets/fine-tuned/askubuntu-c-128-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"blue_slim_groot","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 52,\n    \"total_frames\": 36715,\n    \"total_tasks\": 1,\n    \"total_videos\": 156,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:52\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiamFy/blue_slim_groot.","url":"https://huggingface.co/datasets/LiamFy/blue_slim_groot","creator_name":"Liam Achenbach","creator_url":"https://huggingface.co/LiamFy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_13","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3326,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_13.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_13","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"CowCleanRandom","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 20,\n    \"total_frames\": 10721,\n    \"total_tasks\": 1,\n    \"total_videos\": 40,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:20\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pierfabre/CowCleanRandom.","url":"https://huggingface.co/datasets/pierfabre/CowCleanRandom","creator_name":"Pierre FABRE","creator_url":"https://huggingface.co/pierfabre","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-08082024-dfhx-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-08082024-dfhx-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"holistic health and well-being services\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-08082024-dfhx-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-08082024-dfhx-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-08082024-dfhx-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"mini-imagenet-c","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMiniImageNet-C Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMiniImageNet-C is a compact version of the ImageNet-C robustness benchmark dataset. It contains corrupted images from ImageNet designed to test the robustness of computer vision models to various types of image corruptions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a subset of the original ImageNet-C dataset, containing:\n\n15 corruption types: gaussian_noise, shot_noise, impulse_noise, defocus_blur, glass_blur, motion_blurâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/niuniandaji/mini-imagenet-c.","url":"https://huggingface.co/datasets/niuniandaji/mini-imagenet-c","creator_name":"chenqiang","creator_url":"https://huggingface.co/niuniandaji","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-classification","multi-class-image-classification","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"MECAT-Caption","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks\n\t\n\nðŸ“– Paper | ðŸ› ï¸ GitHub |  ðŸ”Š MECAT-Caption Dataset |  ðŸ”Š MECAT-QA Dataset\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nMECAT (Multi-Expert Chain for Audio Tasks) is a comprehensive benchmark constructed on large-scale data to evaluate machine understanding of audio content through two core tasks:\n\nAudio Captioning: Generating textual descriptions for given audio\nAudio Question Answering: Answering questionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mispeech/MECAT-Caption.","url":"https://huggingface.co/datasets/mispeech/MECAT-Caption","creator_name":"Horizon Team, Xiaomi MiLM Plus","creator_url":"https://huggingface.co/mispeech","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["audio-classification","audio-text-to-text","summarization","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-21052024-5qm5-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-21052024-5qm5-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-21052024-5qm5-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-5qm5-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-5qm5-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PersonalJurisdictionLegalBenchClassification","keyword":"mteb","description":"\n  PersonalJurisdictionLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGiven a fact pattern describing the set of contacts between a plaintiff, defendant, and forum, determine if a court in that forum could excercise personal jurisdiction over the defendant.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/PersonalJurisdictionLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/PersonalJurisdictionLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"arxiv-clustering-s2s","keyword":"mteb","description":"\n  ArXivHierarchicalClusteringS2S\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of titles from arxiv. Clustering of 30 sets, either on the main or secondary category\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://www.kaggle.com/Cornell-University/arxiv\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"ArXivHierarchicalClusteringS2S\"])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/arxiv-clustering-s2s.","url":"https://huggingface.co/datasets/mteb/arxiv-clustering-s2s","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","monolingual","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-21052024-5qm5-webapp","keyword":"testing","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-21052024-5qm5-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-21052024-5qm5-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-5qm5-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-5qm5-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"COMET_score","keyword":"gsm8k","description":"Tadesse/COMET_score dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Tadesse/COMET_score","creator_name":"Tadesse Destaw Belay","creator_url":"https://huggingface.co/Tadesse","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","English","Amharic","Ewe","French"],"keywords_longer_than_N":true},
	{"name":"RuReviewsClassification","keyword":"mteb","description":"\n  RuReviewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nProduct review classification (3-point scale) based on RuRevies dataset\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReference\nhttps://github.com/sismetanin/rureviews\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"RuReviewsClassification\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RuReviewsClassification.","url":"https://huggingface.co/datasets/mteb/RuReviewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-de-922024-pwti-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-de-922024-pwti-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-de-922024-pwti-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-de-922024-pwti-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-de-922024-pwti-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-202457-oc31-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-202457-oc31-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"legal advice search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-202457-oc31-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-202457-oc31-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-202457-oc31-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NanoDBPediaRetrieval","keyword":"mteb","description":"\n  NanoDBPediaRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoDBPediaRetrieval is a small version of the standard test collection for entity search over the DBpedia knowledge base.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nEncyclopaedic\n\nReference\nhttps://huggingface.co/datasets/zeta-alpha-ai/NanoDBPedia\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoDBPediaRetrieval.","url":"https://huggingface.co/datasets/mteb/NanoDBPediaRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","topic-classification","expert-annotated","monolingual","mteb/dbpedia"],"keywords_longer_than_N":true},
	{"name":"R2MEDBioinformaticsRetrieval","keyword":"mteb","description":"\n  R2MEDBioinformaticsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nBioinformatics retrieval dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical\n\n\nReference\nhttps://huggingface.co/datasets/R2MED/Bioinformatics\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"R2MEDBioinformaticsRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/R2MEDBioinformaticsRetrieval.","url":"https://huggingface.co/datasets/mteb/R2MEDBioinformaticsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","LM-generated and reviewed","monolingual","R2MED/Bioinformatics"],"keywords_longer_than_N":true},
	{"name":"ditec-wdn","keyword":"benchmark","description":"--\n\n\t\n\t\t\n\t\tDataset Card for DiTEC-WDN\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nDiTEC-WDN Dataset consists of 36 Water Distribution Networks (WDNs). Each network has unique 1,000 scenarios with distinct characteristics.\nScenario represents a timeseries of directed shared-topology graphs, referred to as states or snapshots. In terms of graph-ml, it can be seen as a spatiotemporal graph where nodes and edges are multivariate time series. \nA node can represent a reservoir, junction, or tank, while an edgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rugds/ditec-wdn.","url":"https://huggingface.co/datasets/rugds/ditec-wdn","creator_name":"Distributed Systems @ University of Groningen","creator_url":"https://huggingface.co/rugds","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["graph-ml","time-series-forecasting","English","cc-by-4.0","1B - 10B"],"keywords_longer_than_N":true},
	{"name":"CQADupstackRetrieval-256-24-gpt-4o-2024-05-13-261378","keyword":"mteb","description":"\n\t\n\t\t\n\t\tCQADupstackRetrieval-256-24-gpt-4o-2024-05-13-261378 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research dataset search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the CQADupstackRetrieval-256-24-gpt-4o-2024-05-13-261378 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/CQADupstackRetrieval-256-24-gpt-4o-2024-05-13-261378.","url":"https://huggingface.co/datasets/fine-tuned/CQADupstackRetrieval-256-24-gpt-4o-2024-05-13-261378","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-05062024-445b-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-05062024-445b-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Automotive industry\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-05062024-445b-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05062024-445b-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05062024-445b-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NLI_Dataset","keyword":"nli","description":"Rupesh2/NLI_Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Rupesh2/NLI_Dataset","creator_name":"Rupesh","creator_url":"https://huggingface.co/Rupesh2","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"symbolic-regression-surfaces","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tComprehensive Symbolic Regression Surface Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains a comprehensive collection of symbolic regression problems focused on 3D surface modeling. The dataset includes 18 different categories of surface types, each with multiple instances, providing a diverse benchmark for symbolic regression algorithms.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized in HDF5 format with the following structure:\n/\nâ”œâ”€â”€ Category_1/\nâ”‚   â”œâ”€â”€ Instance_1/\nâ”‚   â”‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pandoradox/symbolic-regression-surfaces.","url":"https://huggingface.co/datasets/pandoradox/symbolic-regression-surfaces","creator_name":"sanchit ","creator_url":"https://huggingface.co/pandoradox","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["English","mit","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US","symbolic-regression"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-6272024-qn9b-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-6272024-qn9b-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-6272024-qn9b-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6272024-qn9b-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6272024-qn9b-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TRECCOVID-NL","keyword":"mteb","description":"\n  TRECCOVID-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nTRECCOVID is an ad-hoc search challenge based on the COVID-19 dataset containing scientific articles related to the COVID-19 pandemic. TRECCOVID-NL is a Dutch translation. \n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical, Academic, Written\n\n\nReference\nhttps://colab.research.google.com/drive/1R99rjeAGt8S9IfAIRR3wS052sNu3Bjo-#scrollTo=4HduGW6xHnrZ\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embeddingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TRECCOVID-NL.","url":"https://huggingface.co/datasets/mteb/TRECCOVID-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/trec-covid","Dutch"],"keywords_longer_than_N":true},
	{"name":"SciFact-256-24-gpt-4o-2024-05-13-994884","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSciFact-256-24-gpt-4o-2024-05-13-994884 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"scientific claim verification search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SciFact-256-24-gpt-4o-2024-05-13-994884 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-994884.","url":"https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-994884","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TIME-Lite","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tâŒ›ï¸TIME-Lite: High-Quality Human-Annotated Subset for Temporal Reasoning Evaluation\n\t\n\n\n\t\n\t\t\n\t\tðŸŒ Project Links\n\t\n\nGitHub Repository: https://github.com/sylvain-wei/TIME\nGitHub Project Page: https://omni-time.github.io\narXiv Paper: https://arxiv.org/pdf/2505.12891\nTIME@HuggingFace: https://huggingface.co/datasets/SylvainWei/TIME\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ‘‹ðŸ» Introduction\n\t\n\nâŒ›ï¸TIME-Lite is a carefully curated human-annotated subset from the large-scale TIME benchmark dataset, containing 943â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SylvainWei/TIME-Lite.","url":"https://huggingface.co/datasets/SylvainWei/TIME-Lite","creator_name":"Shaohang Wei","creator_url":"https://huggingface.co/SylvainWei","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"discoverybench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDiscoveryBench - Alias\n\t\n\nA reformatted version of the original DiscoveryBench dataset for easier usage.\n\nðŸ¤—  Original Dataset on HF  \nðŸ’» GitHub Repository  \nðŸ“„ Paper (arXiv)\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“ Dataset Structure\n\t\n\nThe dataset consists of real and synthetic subsets:\nReal Splits:\n\nreal_train\nreal_test\n\nSynthetic Splits:\n\nsynth_train\nsynth_dev\nsynth_test\n\nEach split contains a list of tasks with references to associated CSV datasets needed to answer the query. LLMs are expected to use theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nhop/discoverybench.","url":"https://huggingface.co/datasets/nhop/discoverybench","creator_name":"Niklas Hoepner","creator_url":"https://huggingface.co/nhop","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","English","odc-by","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-code-06052024-k5bq-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-code-06052024-k5bq-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"insurance search for car policies\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-code-06052024-k5bq-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-06052024-k5bq-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-06052024-k5bq-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"RARbMath","keyword":"mteb","description":"\n  RARbMath\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMeasuring the ability to retrieve the groundtruth answers to reasoning task queries on RAR-b math-pooled dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nEncyclopaedic, Written\n\n\nReference\nhttps://arxiv.org/abs/2404.06347\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"RARbMath\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RARbMath.","url":"https://huggingface.co/datasets/mteb/RARbMath","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","RAR-b/math-pooled","English"],"keywords_longer_than_N":true},
	{"name":"SongFormBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSongFormBench ðŸ†\n\t\n\n[English ï½œ ä¸­æ–‡]\nA High-Quality Benchmark for Music Structure Analysis\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tðŸŒŸ What is SongFormBench?\n\t\n\nSongFormBench is a carefully curated, expert-annotated benchmark designed to revolutionize music structure analysis (MSA) evaluation. Our dataset provides a unified standard for comparing MSA models.\n\t\n\t\t\n\t\tðŸ“Š Dataset Composition\n\t\n\n\nðŸŽ¸ SongFormBench-HarmonixSet (BHX): 200 songs from HarmonixSet\nðŸŽ¤ SongFormBench-CN (BC): 100 Chinese popular songs\n\nTotal:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ASLP-lab/SongFormBench.","url":"https://huggingface.co/datasets/ASLP-lab/SongFormBench","creator_name":"ASLP-lab","creator_url":"https://huggingface.co/ASLP-lab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","cc-by-4.0","< 1K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5192024-seuc-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5192024-seuc-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"book search for startup advice\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5192024-seuc-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-seuc-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-seuc-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"GreenNodeTableMarkdownRetrieval","keyword":"mteb","description":"\n  GreenNodeTableMarkdownRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGreenNodeTable documents\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nFinancial, Encyclopaedic, Non-fiction\n\n\nReference\nhttps://huggingface.co/GreenNode\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"GreenNodeTableMarkdownRetrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/GreenNodeTableMarkdownRetrieval.","url":"https://huggingface.co/datasets/mteb/GreenNodeTableMarkdownRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","human-annotated","monolingual","Vietnamese"],"keywords_longer_than_N":true},
	{"name":"Diversity6LegalBenchClassification","keyword":"mteb","description":"\n  Diversity6LegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGiven a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 6).\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Diversity6LegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/Diversity6LegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"EchoX-Dialogues-Plus","keyword":"alignment","description":"\n\n  EchoX-Dialogues-Plus: Training Data Plus for EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs\n\n\n\n\n  ðŸˆâ€â¬› GithubÂ ï½œÂ ðŸ“ƒ PaperÂ ï½œÂ ðŸš€ SpaceÂ \n\n\n  ðŸ§  EchoX-8BÂ ï½œÂ ðŸ§  EchoX-3BÂ ï½œÂ ðŸ“¦ EchoX-Dialogues (base)Â \n\n\n\n\t\n\t\n\t\n\t\tEchoX-Dialogues-Plus\n\t\n\nEchoX-Dialogues-Plus extends KurtDu/EchoX-Dialogues with large-scale Speech-to-Speech (S2S) and Speech-to-Text (S2T) dialogues.\nAll assistant/output speech is synthetic (single, consistent timbre for S2S). Texts are fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KurtDu/EchoX-Dialogues-Plus.","url":"https://huggingface.co/datasets/KurtDu/EchoX-Dialogues-Plus","creator_name":"Yuhao Du","creator_url":"https://huggingface.co/KurtDu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","question-answering","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-05072024-aj6g-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-05072024-aj6g-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-05072024-aj6g-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05072024-aj6g-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05072024-aj6g-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_14_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1201,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_14_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_14_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"IndicQARetrieval","keyword":"mteb","description":"\n  IndicQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIndicQA is a manually curated cloze-style reading comprehension dataset that can be used for evaluating question-answering models in 11 Indic languages. It is repurposed retrieving relevant context for each question.\n\n\t\n\t\t\n\n\n\n\n\t\tTask category\nt2t\n\n\nDomains\nWeb, Written\n\n\nReference\nhttps://arxiv.org/abs/2212.05409\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicQARetrieval.","url":"https://huggingface.co/datasets/mteb/IndicQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","human-annotated","translated","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"PROALegalBenchClassification","keyword":"mteb","description":"\n  PROALegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGiven a statute, determine if the text contains an explicit private right of action. Given a privacy policy clause and a description of the clause, determine if the description is correct. A private right of action (PROA) exists when a statute empowers an ordinary individual (i.e., a private person) to legally enforce their rights by bringing an action in court. In short, a PROA creates the ability for anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/PROALegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/PROALegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MixBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench.","url":"https://huggingface.co/datasets/mixed-modality-search/MixBench","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"gsm8k-base-vs-verl-comparison","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tGSM8K: BASE vs VERL-trained Model Comparison Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset provides a comprehensive comparison between a base language model and its VERL (Reinforcement Learning from Human Feedback) fine-tuned version on mathematical reasoning tasks from GSM8K.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Samples: 50 GSM8K test problems\nBASE Model: Qwen/Qwen2.5-0.5B-Instruct (22.0% accuracy)  \nVERL Model: karthik/verl-qwen2.5-0.5b-gsm8k-ppo-step360 (28.0% accuracy)\nImprovement: +6.0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/karthik/gsm8k-base-vs-verl-comparison.","url":"https://huggingface.co/datasets/karthik/gsm8k-base-vs-verl-comparison","creator_name":"karthik","creator_url":"https://huggingface.co/karthik","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-17052024-uhub-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-17052024-uhub-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"legal case document search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-17052024-uhub-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-17052024-uhub-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-17052024-uhub-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"rook_to_d4_v3_old","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1680,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/7jep7/rook_to_d4_v3_old.","url":"https://huggingface.co/datasets/7jep7/rook_to_d4_v3_old","creator_name":"Jonas Petersen","creator_url":"https://huggingface.co/7jep7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_29_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1200,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_29_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_29_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"phase4-quantum-benchmarks","keyword":"benchmarks","description":"\n\t\n\t\t\n\t\tPhase 4 Quantum-ML Benchmarks Dataset ðŸ“Šâš›ï¸\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tðŸ”— Related Resources\n\t\n\n\nðŸ“¦ Models: phase4-quantum-compression - Compressed PyTorch models\nðŸš€ Demo: Interactive Explorer - Visualize and explore data\nðŸ“ Documentation: Technical Details - Complete methodology\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nComprehensive benchmark dataset from Phase 4 experiment combining:\n\nQuantum computing: Real IBM hardware & simulator results\nModel compression: Actual file sizes and quality metrics\nEnergyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jmurray10/phase4-quantum-benchmarks.","url":"https://huggingface.co/datasets/jmurray10/phase4-quantum-benchmarks","creator_name":"Jeff Murray","creator_url":"https://huggingface.co/jmurray10","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["tabular-classification","time-series-forecasting","apache-2.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"CUADNoticePeriodToTerminateRenewalLegalBenchClassification","keyword":"mteb","description":"\n  CUADNoticePeriodToTerminateRenewalLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies a notice period required to terminate renewal.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADNoticePeriodToTerminateRenewalLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADNoticePeriodToTerminateRenewalLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"twinviews-13k","keyword":"alignment","description":"\n\t\n\t\t\n\t\tDataset Card for TwinViews-13k\n\t\n\nThis dataset contains 13,855 pairs of left-leaning and right-leaning political statements matched by topic. The dataset was generated using GPT-3.5 Turbo and has been audited to ensure quality and ideological balance. It is designed to facilitate the study of political bias in reward models and language models, with a focus on the relationship between truthfulness and political views.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Descriptionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wwbrannon/twinviews-13k.","url":"https://huggingface.co/datasets/wwbrannon/twinviews-13k","creator_name":"William Brannon","creator_url":"https://huggingface.co/wwbrannon","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","reinforcement-learning","machine-generated","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"HumanAgencyBench_Human_Annotations","keyword":"alignment","description":"\n\t\n\t\t\n\t\tHuman annotations and LLM judge comparative Dataset\n\t\n\nPaper: HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants\nCode: https://github.com/BenSturgeon/HumanAgencyBench/\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 60,000 evaluated AI assistant responses across 6 dimensions of behaviour relevant to human agency support, with both model-based and human annotations. Each example includes evaluations from 4 different frontier LLM models. We also provideâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Experimental-Orange/HumanAgencyBench_Human_Annotations.","url":"https://huggingface.co/datasets/Experimental-Orange/HumanAgencyBench_Human_Annotations","creator_name":"Benjamin Sturgeon","creator_url":"https://huggingface.co/Experimental-Orange","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-822545","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-822545 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"counter arguments in a debate\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-822545 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-822545.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-822545","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SCIDOCS-256-24-gpt-4o-2024-05-13-862053","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSCIDOCS-256-24-gpt-4o-2024-05-13-862053 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"service search for translation and editing\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SCIDOCS-256-24-gpt-4o-2024-05-13-862053 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-862053.","url":"https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-862053","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"scidocs-c-128-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tscidocs-c-128-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic search for scientific papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the scidocs-c-128-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/scidocs-c-128-24.","url":"https://huggingface.co/datasets/fine-tuned/scidocs-c-128-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-1362024-gcw6-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-1362024-gcw6-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Information Retrieval System\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-1362024-gcw6-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-1362024-gcw6-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-1362024-gcw6-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NanoFiQA2018Retrieval","keyword":"mteb","description":"\n  NanoFiQA2018Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoFiQA2018 is a smaller subset of the Financial Opinion Mining and Question Answering dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Social\n\n\nReferencehttps://sites.google.com/view/fiqa/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NanoFiQA2018Retrieval\"])\nevaluator = mteb.MTEB(task)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoFiQA2018Retrieval.","url":"https://huggingface.co/datasets/mteb/NanoFiQA2018Retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"visual-qa-llama-format","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOpen Paws Visual Qa Llama Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Multimodal Data\nFormat: JSONL (JSON Lines)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Open Paws\nLicense: Apache 2.0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/visual-qa-llama-format.","url":"https://huggingface.co/datasets/open-paws/visual-qa-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-f2kc-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SinhalaNewsClassification","keyword":"mteb","description":"\n  SinhalaNewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis file contains news texts (sentences) belonging to 5 different news categories (political, business, technology, sports and Entertainment). The original dataset was released by Nisansa de Silva (Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language, 2015).\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SinhalaNewsClassification.","url":"https://huggingface.co/datasets/mteb/SinhalaNewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","NLPC-UOM/Sinhala-News-Category-classification"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-862024-gra4-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-862024-gra4-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"E-commerce software for an online store\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-862024-gra4-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-862024-gra4-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-862024-gra4-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"coding","keyword":"mteb","description":"\n\t\n\t\t\n\t\tcoding Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"coding tutorials\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the coding model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/coding.","url":"https://huggingface.co/datasets/fine-tuned/coding","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-02082024-vrdv-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-02082024-vrdv-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-02082024-vrdv-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-02082024-vrdv-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-02082024-vrdv-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"EnglishHealthcare1Retrieval-sample","keyword":"mteb","description":"\n\t\n\t\t\n\t\tEnglishHealthcare1Retrieval-sample\n\t\n\nA sample dataset for medical research retrieval evaluation.\n\n\t\n\t\t\n\t\tTask category\n\t\n\nRetrieval\n\n\t\n\t\t\n\t\tDomains\n\t\n\nMedical, Academic\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset follows the standard MTEB retrieval format:\n\ncorpus/corpus-00000-of-00001.parquet: 10 documents with fields _id, title, text\nqueries/queries-00000-of-00001.parquet: 6 queries with fields _id, text  \ndata/test-00000-of-00001.parquet: 6 relevance judgments with fields query-idâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb-private/EnglishHealthcare1Retrieval-sample.","url":"https://huggingface.co/datasets/mteb-private/EnglishHealthcare1Retrieval-sample","creator_name":"MTEB Private","creator_url":"https://huggingface.co/mteb-private","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"warmup_test_2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 3,\n    \"total_frames\": 832,\n    \"total_tasks\": 1,\n    \"total_videos\": 6,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:3\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/samsam0510/warmup_test_2.","url":"https://huggingface.co/datasets/samsam0510/warmup_test_2","creator_name":"Minjun Lee","creator_url":"https://huggingface.co/samsam0510","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"so100_get_orange","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1360,\n    \"total_tasks\": 1,\n    \"total_videos\": 4,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/HYAIYN/so100_get_orange.","url":"https://huggingface.co/datasets/HYAIYN/so100_get_orange","creator_name":"Ning","creator_url":"https://huggingface.co/HYAIYN","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"h_a_z_i_m","keyword":"test","description":"hazim16/h_a_z_i_m dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/hazim16/h_a_z_i_m","creator_name":"Hassan","creator_url":"https://huggingface.co/hazim16","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","Arabic","mit","< 1K","Document"],"keywords_longer_than_N":true},
	{"name":"HellaSwag","keyword":"mteb","description":"\n  HellaSwag\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMeasuring the ability to retrieve the groundtruth answers to reasoning task queries on HellaSwag.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nEncyclopaedic, Written\n\n\nReferencehttps://rowanzellers.com/hellaswag/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"HellaSwag\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HellaSwag.","url":"https://huggingface.co/datasets/mteb/HellaSwag","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","RAR-b/hellaswag","English"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-pos-three","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11157,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-three.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-three","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"SpecBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSpecBench: Reasoning over Boundaries\n\t\n\nEnhancing Specification Alignment via Test-time Delibration\nPaper | Code | Hugging Face Datasets\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nLarge models are increasingly applied in diverse real-world scenarios, each governed by customized specifications that capture both behavioral preferences and safety boundaries. These specifications vary across domains and evolve with changing requirements, posing the challenge of specification alignment.\n  \n\nTo address thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zzzhr97/SpecBench.","url":"https://huggingface.co/datasets/zzzhr97/SpecBench","creator_name":"Haoran Zhang","creator_url":"https://huggingface.co/zzzhr97","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"SpecBench","keyword":"alignment","description":"\n\t\n\t\t\n\t\tSpecBench: Reasoning over Boundaries\n\t\n\nEnhancing Specification Alignment via Test-time Delibration\nPaper | Code | Hugging Face Datasets\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nLarge models are increasingly applied in diverse real-world scenarios, each governed by customized specifications that capture both behavioral preferences and safety boundaries. These specifications vary across domains and evolve with changing requirements, posing the challenge of specification alignment.\n  \n\nTo address thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zzzhr97/SpecBench.","url":"https://huggingface.co/datasets/zzzhr97/SpecBench","creator_name":"Haoran Zhang","creator_url":"https://huggingface.co/zzzhr97","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-256-24-gpt-4o-2024-05-13-203779","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-256-24-gpt-4o-2024-05-13-203779 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-256-24-gpt-4o-2024-05-13-203779 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-203779.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-203779","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-6112024-fmxr-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-6112024-fmxr-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Education sector outreach\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-6112024-fmxr-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6112024-fmxr-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6112024-fmxr-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so100_grab_test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 50,\n    \"total_frames\": 14396,\n    \"total_tasks\": 1,\n    \"total_videos\": 100,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:50\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wannrrr/so100_grab_test.","url":"https://huggingface.co/datasets/wannrrr/so100_grab_test","creator_name":"Erwann WICART","creator_url":"https://huggingface.co/wannrrr","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"IndicMSMARCO","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ” IndicMSMARCO: Multilingual Information Retrieval Benchmark\n\t\n\nA comprehensive multilingual variant of MS MARCO specifically tailored for Indian languages, featuring carefully selected queries and corresponding passages with high-quality translations.\n\n\t\n\t\t\n\t\tðŸš€ Quick Start - Load Individual Languages\n\t\n\nfrom datasets import load_dataset\n\n# Load ONLY Hindi data (fast and efficient!)\nhindi_data = load_dataset(\"ai4bharat/IndicMSMARCO\", \"hi\")\nprint(f\"Hindi queries:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/IndicMSMARCO.","url":"https://huggingface.co/datasets/ai4bharat/IndicMSMARCO","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","question-answering","multilingual","ms_marco","Assamese"],"keywords_longer_than_N":true},
	{"name":"EstonianValenceClassification","keyword":"mteb","description":"\n  EstonianValenceClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDataset containing annotated Estonian news data from the Postimees and Ã•htuleht newspapers.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReferencehttps://figshare.com/articles/dataset/Estonian_Valence_Corpus_Eesti_valentsikorpus/24517054\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/EstonianValenceClassification.","url":"https://huggingface.co/datasets/mteb/EstonianValenceClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"ContractNLINoticeOnCompelledDisclosureLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLINoticeOnCompelledDisclosureLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party shall notify Disclosing Party in case Receiving Party is required by law, regulation or judicial process to disclose any Confidential Information.\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLINoticeOnCompelledDisclosureLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLINoticeOnCompelledDisclosureLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"RuMayoSRS","keyword":"benchmark","description":"ÐŸÐµÑ€ÐµÐ²ÐµÐ´ÐµÐ½Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ bigbio/mayosrs\n","url":"https://huggingface.co/datasets/Kostya165/RuMayoSRS","creator_name":"pleroma_cascade","creator_url":"https://huggingface.co/Kostya165","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","Russian","cc0-1.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"PIQA","keyword":"mteb","description":"\n  PIQA\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMeasuring the ability to retrieve the groundtruth answers to reasoning task queries on PIQA.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nEncyclopaedic, Written\n\n\nReference\nhttps://arxiv.org/abs/1911.11641\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"PIQA\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/PIQA.","url":"https://huggingface.co/datasets/mteb/PIQA","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","RAR-b/piqa","English"],"keywords_longer_than_N":true},
	{"name":"medical_qa","keyword":"mteb","description":"\n  MedicalQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe dataset consists 2048 medical question and answer pairs.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical, Written\n\n\nReference\nhttps://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3119-4\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"MedicalQARetrieval\"])\nevaluator = mteb.MTEB(task)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/medical_qa.","url":"https://huggingface.co/datasets/mteb/medical_qa","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"LitSearchRetrieval","keyword":"mteb","description":"\n  LitSearchRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\n\n    The dataset contains the query set and retrieval corpus for the paper LitSearch: A Retrieval Benchmark for\n    Scientific Literature Search. It introduces LitSearch, a retrieval benchmark comprising 597 realistic literature\n    search queries about recent ML and NLP papers. LitSearch is constructed using a combination of (1) questions\n    generated by GPT-4 based on paragraphs containing inline citations fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/LitSearchRetrieval.","url":"https://huggingface.co/datasets/mteb/LitSearchRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","LM-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-23052024-6kfw-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-23052024-6kfw-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"informational search for legal technology guidance\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-23052024-6kfw-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-23052024-6kfw-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-23052024-6kfw-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ClimaQA","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tClimaQA: An Automated Evaluation Framework for Climate Question Answering Models (ICLR 2025)\n\t\n\nCheck the paper's webpage and GitHub for more info!\nThe ClimaQA benchmark is designed to evaluate Large Language Models (LLMs) on climate science question-answering tasks by ensuring scientific rigor and complexity. It is built from graduate-level climate science textbooks, which provide a reliable foundation for generating questions with precise terminology and complex scientific theories.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCSD-GENIE/ClimaQA.","url":"https://huggingface.co/datasets/UCSD-GENIE/ClimaQA","creator_name":"UCSD-GENIE","creator_url":"https://huggingface.co/UCSD-GENIE","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","apache-2.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"ContractNLIReturnOfConfidentialInformationLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLIReturnOfConfidentialInformationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party shall destroy or return some Confidential Information upon the termination of Agreement.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLIReturnOfConfidentialInformationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLIReturnOfConfidentialInformationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Diversity1LegalBenchClassification","keyword":"mteb","description":"\n  Diversity1LegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGiven a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 1).\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Diversity1LegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/Diversity1LegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-v2-four","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11170,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-four.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-four","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-pos-six","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11175,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-six.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-six","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-612024-vf79-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-612024-vf79-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-612024-vf79-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-612024-vf79-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-612024-vf79-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"dutch-legal-c","keyword":"mteb","description":"\n\t\n\t\t\n\t\tdutch-legal-c Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Legal document search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the dutch-legal-c model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/dutch-legal-c.","url":"https://huggingface.co/datasets/fine-tuned/dutch-legal-c","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_15","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 2782,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_15.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_15","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"processed_demo","keyword":"testing","description":"Makarand2506/processed_demo dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Makarand2506/processed_demo","creator_name":"Makarand Ghule","creator_url":"https://huggingface.co/Makarand2506","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Java_method2test_chatml","keyword":"testing","description":"\n\t\n\t\t\n\t\tJava Method to Test ChatML\n\t\n\nThis dataset is based on the methods2test dataset from Microsoft. It follows the ChatML template format: [{'role': '', 'content': ''}, {...}].\nOriginally, methods2test contains only Java methods at different levels of granularity along with their corresponding test cases. The different focal method segmentations are illustrated here:\n\nTo simulate a conversation between a Java developer and an AI assistant, I introduce two key parameters:\n\nThe promptâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/random-long-int/Java_method2test_chatml.","url":"https://huggingface.co/datasets/random-long-int/Java_method2test_chatml","creator_name":"Long Int","creator_url":"https://huggingface.co/random-long-int","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text2text-generation","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"FutureQueryEval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFutureQueryEval Dataset (EMNLP 2025)ðŸ”\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nFutureQueryEval is a novel Information Retrieval (IR) benchmark designed to evaluate reranker performance on temporal novelty. It comprises 148 queries with 2,938 query-document pairs across 7 topical categories, specifically created to test how well reranking models generalize to truly novel queries that were unseen during LLM pretraining.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nZero Contamination: All queries refer to eventsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/abdoelsayed/FutureQueryEval.","url":"https://huggingface.co/datasets/abdoelsayed/FutureQueryEval","creator_name":"Abdelrahman Abdallah","creator_url":"https://huggingface.co/abdoelsayed","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","English","apache-2.0","1K<n<10K","Text"],"keywords_longer_than_N":true},
	{"name":"IN22-Conv","keyword":"mteb","description":"\n  IN22ConvBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Conv is a n-way parallel conversation domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nSocial, Spoken, Fiction, Spoken\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Conv\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22-Conv.","url":"https://huggingface.co/datasets/mteb/IN22-Conv","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","expert-generated","multilingual","Assamese"],"keywords_longer_than_N":true},
	{"name":"SCIDOCS-512-192-gpt-4o-2024-05-13-650620","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSCIDOCS-512-192-gpt-4o-2024-05-13-650620 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"arxiv paper domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SCIDOCS-512-192-gpt-4o-2024-05-13-650620 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SCIDOCS-512-192-gpt-4o-2024-05-13-650620.","url":"https://huggingface.co/datasets/fine-tuned/SCIDOCS-512-192-gpt-4o-2024-05-13-650620","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-9evb-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-9evb-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-9evb-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-9evb-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-9evb-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-m3-6142024-0ndt-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-m3-6142024-0ndt-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"content moderation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-m3-6142024-0ndt-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-6142024-0ndt-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-6142024-0ndt-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Korean","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"eval_calibration_test_25514","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 592,\n    \"total_tasks\": 1,\n    \"total_videos\": 1,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hrhraj/eval_calibration_test_25514.","url":"https://huggingface.co/datasets/hrhraj/eval_calibration_test_25514","creator_name":"Raj S","creator_url":"https://huggingface.co/hrhraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"WM-ABench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tWM-ABench: An Atomic Evaluation Benchmark of World Modeling abilities of Vision-Language Models\n\t\n\nPaper: Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation\nWM-ABench is a comprehensive benchmark that evaluates whether Vision-Language Models (VLMs) can truly understand and simulate physical world dynamics, or if they rely on shortcuts and pattern-matching. The benchmark covers 23 dimensions of world modeling across 6 physics simulators with over 100,000â€¦ See the full description on the dataset page: https://huggingface.co/datasets/maitrix-org/WM-ABench.","url":"https://huggingface.co/datasets/maitrix-org/WM-ABench","creator_name":"Maitrix.org","creator_url":"https://huggingface.co/maitrix-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-text-to-text","English"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information in German language\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can loadâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-u59b-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NeuCLIR2022RetrievalHardNegatives","keyword":"mteb","description":"\n  NeuCLIR2022RetrievalHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\nSource datasets:\n\nmteb/neuclir-2022\nmteb/neuclir-2022-hard-negativesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NeuCLIR2022RetrievalHardNegatives.","url":"https://huggingface.co/datasets/mteb/NeuCLIR2022RetrievalHardNegatives","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2022","mteb/neuclir-2022-hard-negatives"],"keywords_longer_than_N":true},
	{"name":"so100_test_init","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 5,\n    \"total_frames\": 5403,\n    \"total_tasks\": 1,\n    \"total_videos\": 10,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:5\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ofiroz91/so100_test_init.","url":"https://huggingface.co/datasets/Ofiroz91/so100_test_init","creator_name":"Ofir Ozeri","creator_url":"https://huggingface.co/Ofiroz91","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"record-test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101_follower\",\n    \"total_episodes\": 5,\n    \"total_frames\": 2025,\n    \"total_tasks\": 1,\n    \"total_videos\": 10,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:5\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/malin141/record-test.","url":"https://huggingface.co/datasets/malin141/record-test","creator_name":"Malin Braatz","creator_url":"https://huggingface.co/malin141","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-465198","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-465198 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic debates\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-465198 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-465198.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-465198","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"deepspeed-from-new-new-docker","keyword":"mteb","description":"\n\t\n\t\t\n\t\tdeepspeed-from-new-new-docker Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the deepspeed-from-new-new-docker model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/deepspeed-from-new-new-docker.","url":"https://huggingface.co/datasets/fine-tuned/deepspeed-from-new-new-docker","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","French","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5202024-6tkj-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5202024-6tkj-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"educational content for customer insights and marketing strategies\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5202024-6tkj-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5202024-6tkj-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5202024-6tkj-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5192024-qeye-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5192024-qeye-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Cybersecurity and hacking information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5192024-qeye-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-qeye-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-qeye-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-256-24-gpt-4o-2024-05-13-546049","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-256-24-gpt-4o-2024-05-13-546049 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-256-24-gpt-4o-2024-05-13-546049 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-546049.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-546049","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"dbpedia","keyword":"mteb","description":"\n  DBPedia\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\n\n\nReference\nhttps://github.com/iai-group/DBpedia-Entity/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"DBPedia\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/dbpedia.","url":"https://huggingface.co/datasets/mteb/dbpedia","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","English","mit"],"keywords_longer_than_N":true},
	{"name":"Tessera","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ“š Tessera: Exposing the Challenges of LLM-based Test Generation for Low-Resource Programming Languages\n\t\n\nTessera is a validation benchmark designed to measure how well models can generate unit tests for Low-Resource Programming Languages (LRPLs) â€” specifically Rust, Go, and Julia.\n\n\t\n\t\t\n\t\tðŸ“Œ Purpose\n\t\n\nEvaluate how well a model can generate test code, given a focal function's source code and additional context.\n\n\t\n\t\t\n\t\tðŸ“‚ Dataset Structure\n\t\n\nEach sample contains:\n\nfunction_name:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/solis-soict/Tessera.","url":"https://huggingface.co/datasets/solis-soict/Tessera","creator_name":"SOLIS","creator_url":"https://huggingface.co/solis-soict","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["code","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"whisper_asr_traindata","keyword":"alignment","description":"\n\t\n\t\t\n\t\tEnglish Accent Audio-transcript Dataset.\n\t\n\nAudio is extracted from movies, shows, speeches, talks to capture diverse accents - mainly scottish and indian\nThis dataset contains 30-second audio clips with aligned transcript text. \n\n\t\n\t\t\n\t\tStructure\n\t\n\nEach entry includes:\n\naudio: 30-second speech segment\ntext: Corresponding transcript\nstart_time / end_time: Segment timestamps\n\n\n\t\n\t\t\n\t\tLicense\n\t\n\nLicensed under CC-BY-4.0.\n","url":"https://huggingface.co/datasets/sarannair/whisper_asr_traindata","creator_name":"Saran Nair","creator_url":"https://huggingface.co/sarannair","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Audio"],"keywords_longer_than_N":true},
	{"name":"WisesightSentimentClassification","keyword":"mteb","description":"\n  WisesightSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nWisesight Sentiment Corpus: Social media messages in Thai language with sentiment label (positive, neutral, negative, question)\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, News, Written\nReference\nhttps://github.com/PyThaiNLP/wisesight-sentiment\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/WisesightSentimentClassification.","url":"https://huggingface.co/datasets/mteb/WisesightSentimentClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-14052024-5b5o-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-14052024-5b5o-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Fashion boutique products and reviews search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-14052024-5b5o-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-14052024-5b5o-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-14052024-5b5o-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VieQuADRetrieval","keyword":"mteb","description":"\n  VieQuADRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Vietnamese dataset for evaluating Machine Reading Comprehension from Wikipedia articles.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nEncyclopaedic, Non-fiction, Written\n\n\nReferencehttps://aclanthology.org/2020.coling-main.233.pdf\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"VieQuADRetrieval\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VieQuADRetrieval.","url":"https://huggingface.co/datasets/mteb/VieQuADRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","human-annotated","monolingual","Vietnamese","mit"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-2024513-kkxa-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-2024513-kkxa-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"e-commerce search for RV accessories\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-2024513-kkxa-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-2024513-kkxa-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-2024513-kkxa-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TextQuantificationDatasets","keyword":"benchmark","description":"\n\n\t\n\t\t\n\t\tAutomated Nonparametric Content Analysis Datasets\n\t\n\nThis repository provides the four benchmark datasets used in:\n\nConnor T. Jerzak, Gary King, and Anton Strezhnev. An Improved Method of Automated Nonparametric Content Analysis for Social Science. Political Analysis, 31(1): 42â€“58, 2023.\n\nEach dataset is formatted for easy loading in Python and R (CSV). Labels are integer-coded from 1,...,K; text is provided as raw strings.\n\n\t\n\t\t\n\t\n\t\n\t\tDatasets\n\t\n\n\n\t\n\t\t\nName\nDocuments\nCategoriesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cjerzak/TextQuantificationDatasets.","url":"https://huggingface.co/datasets/cjerzak/TextQuantificationDatasets","creator_name":"Connor T. Jerzak","creator_url":"https://huggingface.co/cjerzak","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","multi-class-classification","sentiment-classification","human-annotated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Scifi4TopicModel","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis repository contains benchmark datasets for evaluating Large Language Model (LLM)-based topic discovery methods and comparing them against traditional topic models.  These datasets provide a valuable resource for researchers studying topic modeling and LLM capabilities in this domain.  The work is described in the following paper: Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs.  Original dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zli12321/Scifi4TopicModel.","url":"https://huggingface.co/datasets/zli12321/Scifi4TopicModel","creator_name":"LZX","creator_url":"https://huggingface.co/zli12321","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"BengaliHateSpeechClassification","keyword":"mteb","description":"\n  BengaliHateSpeechClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe Bengali Hate Speech Dataset is a Bengali-language dataset of news articles collected from various Bengali media sources and categorized based on the type of hate in the text.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://huggingface.co/datasets/bn_hate_speech\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BengaliHateSpeechClassification.","url":"https://huggingface.co/datasets/mteb/BengaliHateSpeechClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"SciArena-with-paperbank","keyword":"benchmark","description":"\n  SciArena: A New Platform for Evaluating Foundation Models in Scientific Literature Tasks\n\n\n  ðŸ“ Blog\n  ðŸŒ SciArena Platform\n  ðŸ’» Code\n  ðŸ“° Paper\n\n\nWe present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. Byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/SciArena-with-paperbank.","url":"https://huggingface.co/datasets/yale-nlp/SciArena-with-paperbank","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"HebrewSentimentAnalysis","keyword":"mteb","description":"\n  HebrewSentimentAnalysis\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nHebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israelâ€™s president, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder, 2013) to scrape all the comments to all of the presidentâ€™s posts in the period of June â€“ August 2014, the first three months of Rivlinâ€™s presidency.2 While the presidentâ€™s posts aimed at reconcilingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HebrewSentimentAnalysis.","url":"https://huggingface.co/datasets/mteb/HebrewSentimentAnalysis","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"NanoSCIDOCSRetrieval","keyword":"mteb","description":"\n  NanoSCIDOCSRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoFiQA2018 is a smaller subset of SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2t\n\n\nDomains\nAcademic, Written, Non-fiction\n\n\nReference\nhttps://allenai.org/data/scidocs\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoSCIDOCSRetrieval.","url":"https://huggingface.co/datasets/mteb/NanoSCIDOCSRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","monolingual","mteb/scidocs","English"],"keywords_longer_than_N":true},
	{"name":"SimpleQA-Verified","keyword":"benchmarks","description":"SimpleQA Verified is a 1,000-prompt benchmark for reliably evaluating Large Language Models (LLMs) on short-form factuality and parametric knowledge. The authors from Google DeepMind and Google Research address various limitations of SimpleQA, originally designed by Wei et al. (2024) at OpenAI, including noisy and incorrect labels, topical biases, and question redundancy.\nSimpleQA Verified was created to provide the research community with a more precise instrument to track genuine progress inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stalkermustang/SimpleQA-Verified.","url":"https://huggingface.co/datasets/stalkermustang/SimpleQA-Verified","creator_name":"Igor Kotenkov","creator_url":"https://huggingface.co/stalkermustang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"HindiDiscourseClassification","keyword":"mteb","description":"\n  HindiDiscourseClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Hindi Discourse dataset in Hindi with values for coherence.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nFiction, Social, Written\n\n\nReference\nhttps://aclanthology.org/2020.lrec-1.149/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"HindiDiscourseClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HindiDiscourseClassification.","url":"https://huggingface.co/datasets/mteb/HindiDiscourseClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","Hindi","mit"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-pos-five","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11171,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-five.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-five","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ISAAC-GR00T-V1-Pick-Place","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 15,\n    \"total_frames\": 6709,\n    \"total_tasks\": 1,\n    \"total_videos\": 15,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:15\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/ISAAC-GR00T-V1-Pick-Place.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/ISAAC-GR00T-V1-Pick-Place","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MaCBench-Ablations","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMaCBench-Ablations\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\tâš ï¸ IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tðŸš« THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY ðŸš«\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench-Ablations.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench-Ablations","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-time-flow","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Time flow Annotation Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~3700 human evaluators were asked to evaluate AI-generated videos based on how time flows in the video. The specific question posed was: \"Howâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow.","url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tQuoraRetrieval-256-24-gpt-4o-2024-05-13-635320 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval benchmark\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320.","url":"https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SciFact-256-24-gpt-4o-2024-05-13-204265","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSciFact-256-24-gpt-4o-2024-05-13-204265 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"scientific claim verification\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SciFact-256-24-gpt-4o-2024-05-13-204265 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-204265.","url":"https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-204265","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320","keyword":"mteb","description":"\n\t\n\t\t\n\t\tQuoraRetrieval-256-24-gpt-4o-2024-05-13-635320 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval benchmark\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320.","url":"https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"test7","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 5,\n    \"total_frames\": 1007,\n    \"total_tasks\":1,\n    \"total_videos\": 5,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:5\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/aiwhisperer/test7.","url":"https://huggingface.co/datasets/aiwhisperer/test7","creator_name":"Samuel Boylan-Sajous","creator_url":"https://huggingface.co/aiwhisperer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"askubuntu","keyword":"mteb","description":"\n\t\n\t\t\n\t\taskubuntu Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Technical Q&A search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the askubuntu model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/askubuntu.","url":"https://huggingface.co/datasets/fine-tuned/askubuntu","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-05062024-m8dn-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-v1_5-05062024-m8dn-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-v1_5-05062024-m8dn-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-m8dn-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-m8dn-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"OllaBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\nLarge Language Models (LLMs) have the potential to enhance Agent-Based Modeling by better representing complex interdependent cybersecurity systems, improving cybersecurity threat modeling and risk management. Evaluating LLMs in this context is crucial for legal compliance and effective application development. Existing LLM evaluation frameworks often overlook the human factor and cognitive computing capabilities essential for interdependentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/theResearchNinja/OllaBench.","url":"https://huggingface.co/datasets/theResearchNinja/OllaBench","creator_name":"Tam Nguyen","creator_url":"https://huggingface.co/theResearchNinja","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"CUADUnlimitedAllYouCanEatLicenseLegalBenchClassification","keyword":"mteb","description":"\n  CUADUnlimitedAllYouCanEatLicenseLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause grants one party an â€œenterprise,â€ â€œall you can eatâ€ or unlimited usage license.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADUnlimitedAllYouCanEatLicenseLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADUnlimitedAllYouCanEatLicenseLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"BIRCO-DorisMae-Test","keyword":"mteb","description":"\n  BIRCO-DorisMae\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieval task using the DORIS-MAE dataset from BIRCO. This dataset contains 60 queries that are complex research questions from computer scientists. Each query has a candidate pool of approximately 110 abstracts. Relevance is graded from 0 to 2 (scores of 1 and 2 are considered relevant).\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic\n\n\nReference\nhttps://github.com/BIRCO-benchmark/BIRCO\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BIRCO-DorisMae-Test.","url":"https://huggingface.co/datasets/mteb/BIRCO-DorisMae-Test","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"cmedqav2-c-64-24-gpt-4o-2024-05-13-35883","keyword":"mteb","description":"\n\t\n\t\t\n\t\tcmedqav2-c-64-24-gpt-4o-2024-05-13-35883 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the cmedqav2-c-64-24-gpt-4o-2024-05-13-35883 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/cmedqav2-c-64-24-gpt-4o-2024-05-13-35883.","url":"https://huggingface.co/datasets/fine-tuned/cmedqav2-c-64-24-gpt-4o-2024-05-13-35883","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-321013","keyword":"mteb","description":"\n\t\n\t\t\n\t\targuana-c-256-24-gpt-4o-2024-05-13-321013 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-321013 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-321013.","url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-321013","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"test_sapce","keyword":"test","description":"yourleige/test_sapce dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/yourleige/test_sapce","creator_name":"LEIGE ZHANG","creator_url":"https://huggingface.co/yourleige","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","table-question-answering","Abkhaz","Afrikaans","mit"],"keywords_longer_than_N":true},
	{"name":"ForestReg","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tULS-TLS Forest Registration Benchmark (ForestReg)\n\t\n\n\n\t\n\t\t\n\t\t1. Dataset Description\n\t\n\nThis is the first multi-platform benchmark dataset for Unmanned Aerial Vehicle Laser Scanning (ULS) and Terrestrial Laser Scanning (TLS) point cloud registration in complex forest environments. Integrating ULS and TLS data provides a comprehensive 3D understanding of forest structures, but robustly registering these datasets remains a significant challenge due to differing perspectives, pointâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lewisciro66/ForestReg.","url":"https://huggingface.co/datasets/lewisciro66/ForestReg","creator_name":"Lewis Ciro","creator_url":"https://huggingface.co/lewisciro66","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","point-cloud","3d-computer-vision"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-774308","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFiQA2018-256-24-gpt-4o-2024-05-13-774308 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial sentiment analysis and opinion-based QA\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-774308 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-774308.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-774308","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"WokeyTalky","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tWokeyTalky: Towards Scalable Evaluation of Misguided Safety Refusal in LLMs\n\t\n\nThis dataset contains 756 harmful instructions (63 examples x 12 Datasets) for LLM harmfulness evaluation.\n[SUMMARY]\nFor more details, please refer to our project website: https://reds-lab.github.io/WokeyTalky/.\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\n\n\nRepository: https://github.com/reds-lab/WokeyTalky\nProject Page : https://reds-lab.github.io/WokeyTalky/\nPaper : [ARXIV TBD]\nPyPI : https://pypi.org/project/WokeyTalky/â€¦ See the full description on the dataset page: https://huggingface.co/datasets/redslabvt/WokeyTalky.","url":"https://huggingface.co/datasets/redslabvt/WokeyTalky","creator_name":"Responsible Data Science Lab","creator_url":"https://huggingface.co/redslabvt","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"hle-extract-qwen3235ba22b-20250815","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tHLE Extract: Qwen3-235B-A22B Evaluation Results (2025-08-15)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains the complete Human-Level Evaluation (HLE) benchmark with detailed evaluation results from the Qwen/Qwen3-235B-A22B model. It merges the original team-suzuki/hle-extract dataset with comprehensive model responses and human judgments.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Questions: 120 (complete HLE dataset)\nEvaluated Questions: 103 (85.8%)\nUnevaluated Questions: 17 (14.2%)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/team-suzuki/hle-extract-qwen3235ba22b-20250815.","url":"https://huggingface.co/datasets/team-suzuki/hle-extract-qwen3235ba22b-20250815","creator_name":"Team Suzuki","creator_url":"https://huggingface.co/team-suzuki","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"KorMedLawQA","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tKorMedLawQA: Korean Medical Law Question Answering Dataset\n\t\n\nWelcome to the official repository for KorMedLawQA, a dataset of multiple-choice questions focused on South Korean medical law. This dataset is designed to support the development and evaluation of Large Language Models (LLMs) in the medical-legal domain, particularly for applications within the Korean healthcare environment, and to serve as a preparatory resource for the Korean Medical Licensing Examination (KMLE).\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/snuh/KorMedLawQA.","url":"https://huggingface.co/datasets/snuh/KorMedLawQA","creator_name":"SNUH Healthcare AI Research Institute","creator_url":"https://huggingface.co/snuh","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Korean","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","medical-law"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-2024512-wvj9-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-2024512-wvj9-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"construction project search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-2024512-wvj9-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-2024512-wvj9-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-2024512-wvj9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"green_cup_pour_fixed","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 52,\n    \"total_frames\": 34041,\n    \"total_tasks\": 1,\n    \"total_videos\": 156,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:52\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiamFy/green_cup_pour_fixed.","url":"https://huggingface.co/datasets/LiamFy/green_cup_pour_fixed","creator_name":"Liam Achenbach","creator_url":"https://huggingface.co/LiamFy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-banking77","keyword":"aveni-bench","description":"\n\t\n\t\t\n\t\tAveniBench: Banking77\n\t\n\nBanking77 split used in the AveniBench.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is made available under the CC-BY-4.0 license.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nAveniBench\nTDB\n\nBanking77\n@inproceedings{casanueva-etal-2020-efficient,\n    title = \"Efficient Intent Detection with Dual Sentence Encoders\",\n    author = \"Casanueva, I{\\~n}igo  and\n      Tem{\\v{c}}inas, Tadas  and\n      Gerz, Daniela  and\n      Henderson, Matthew  and\n      Vuli{\\'c}, Ivan\",\n    booktitle = \"Proceedings ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-banking77.","url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-banking77","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-m3-8142024-iw0e-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-m3-8142024-iw0e-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"sales data analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-m3-8142024-iw0e-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-8142024-iw0e-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-8142024-iw0e-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-21052024-5smg-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-21052024-5smg-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"debate search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-21052024-5smg-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-5smg-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-5smg-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-8-8-gpt-4o-2024-05-13-855191","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-8-8-gpt-4o-2024-05-13-855191 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-8-8-gpt-4o-2024-05-13-855191 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-855191.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-855191","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"vllm-pr-analysis","keyword":"testing","description":"\n\t\n\t\t\n\t\tvLLM PR Test Classification Dataset\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset contains 98 vLLM project commits with their corresponding Pull Request (PR) timeline data and comprehensive test type classifications. It provides insights into testing patterns in a major LLM serving infrastructure project.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Description\n\t\n\n\n\t\n\t\t\n\t\tPurpose\n\t\n\nThis dataset was created by analyzing vLLM project PR timelines to:\n\nIdentify different types of testing and benchmarking activitiesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inferencebench/vllm-pr-analysis.","url":"https://huggingface.co/datasets/Inferencebench/vllm-pr-analysis","creator_name":"Inferencebench","creator_url":"https://huggingface.co/Inferencebench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"DKHateClassification","keyword":"mteb","description":"\n  DKHateClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDanish Tweets annotated for Hate Speech either being Offensive or not\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, Written\n\n\nReference\nhttps://aclanthology.org/2020.lrec-1.430/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"DKHateClassification\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DKHateClassification.","url":"https://huggingface.co/datasets/mteb/DKHateClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-code-06052024-mhal-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-code-06052024-mhal-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"software documentation search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-code-06052024-mhal-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-06052024-mhal-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-06052024-mhal-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"LIBRA","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLIBRA: Long Input Benchmark for Russian Analysis\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nLIBRA (Long Input Benchmark for Russian Analysis) is designed to evaluate the capabilities of large language models (LLMs) in understanding and processing long texts in Russian. This benchmark includes 21 datasets adapted for different tasks and complexities. The tasks are divided into four complexity groups and allow evaluation across various context lengths ranging from 4k up to 128k tokens.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai-forever/LIBRA.","url":"https://huggingface.co/datasets/ai-forever/LIBRA","creator_name":"ai-forever","creator_url":"https://huggingface.co/ai-forever","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Russian","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-2852024-6p16-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-2852024-6p16-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"natural language processing\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-2852024-6p16-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-2852024-6p16-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-2852024-6p16-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SwednClusteringP2P","keyword":"mteb","description":"\n  SwednClusteringP2P\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure. This dataset uses the category labels as clusters.\n\n\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Non-fiction, Written\n\n\nReference\nhttps://spraakbanken.gu.se/en/resources/swedn\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SwednClusteringP2P.","url":"https://huggingface.co/datasets/mteb/SwednClusteringP2P","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","monolingual","sbx/superlim-2","Swedish"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-gis","keyword":"mteb","description":"\n  CQADupstackGisRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Non-fiction\n\n\nReference\nhttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackGisRetrieval\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-gis.","url":"https://huggingface.co/datasets/mteb/cqadupstack-gis","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"GRI-QA","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tGRI-QA\n\t\n\nGRI-QA is a benchmark for Table Question Answering (QA) over environmental data extracted from corporate sustainability reports, following the Global Reporting Initiative (GRI) standards.\nIt contains 4,000+ questions across 204 tables from English-language reports of European companies, covering extractive, comparative, quantitative, multi-step, and multi-table reasoning.\n\n\t\n\t\t\n\t\tTasks\n\t\n\n\nTable QA on real-world corporate sustainability data\nQuestion types: extra (extractive)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lucacontalbo/GRI-QA.","url":"https://huggingface.co/datasets/lucacontalbo/GRI-QA","creator_name":"Luca","creator_url":"https://huggingface.co/lucacontalbo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","English","mit","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"livevqa-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLiveVQA Benchmark Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nLiveVQA is a comprehensive Visual Question Answering benchmark that evaluates multimodal models across three dynamic domains: News, Academic Papers, and Videos. The dataset features both level1 (basic comprehension) and level2 (advanced reasoning) questions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nid: Unique identifier for each question\nimage: Path to the associated image\nquestion: The question text\noptions: Listâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fmy666/livevqa-benchmark.","url":"https://huggingface.co/datasets/fmy666/livevqa-benchmark","creator_name":"fmy666","creator_url":"https://huggingface.co/fmy666","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-02092024-kk9q-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-02092024-kk9q-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-02092024-kk9q-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-02092024-kk9q-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-02092024-kk9q-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SciFact-512-192-gpt-4o-2024-05-13-866232","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSciFact-512-192-gpt-4o-2024-05-13-866232 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SciFact-512-192-gpt-4o-2024-05-13-866232 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SciFact-512-192-gpt-4o-2024-05-13-866232.","url":"https://huggingface.co/datasets/fine-tuned/SciFact-512-192-gpt-4o-2024-05-13-866232","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"pig3_seg","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 10,\n    \"total_frames\": 5359,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/adrienloizeau/pig3_seg.","url":"https://huggingface.co/datasets/adrienloizeau/pig3_seg","creator_name":"Adrien Loizeau","creator_url":"https://huggingface.co/adrienloizeau","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"red_cup_pour_single","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 1,\n    \"total_frames\": 769,\n    \"total_tasks\": 1,\n    \"total_videos\": 3,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:1\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chengkunli/red_cup_pour_single.","url":"https://huggingface.co/datasets/chengkunli/red_cup_pour_single","creator_name":"Chengkun Li","creator_url":"https://huggingface.co/chengkunli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"HashtagPrediction","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tHashtag Prediction Dataset from paper TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations\n\t\n\n  \nThis repo contains the Hashtag prediction dataset from our paper TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations. \n[arXiv][HuggingFace Models]\n[Github repo]\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\t\n\t\t\n\t\n\t\n\t\tDownload\n\t\n\nUse theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Twitter/HashtagPrediction.","url":"https://huggingface.co/datasets/Twitter/HashtagPrediction","creator_name":"Twitter","creator_url":"https://huggingface.co/Twitter","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Slovenian","Urdu","Sindhi","Polish","Vietnamese"],"keywords_longer_than_N":true},
	{"name":"mteb-nl-sick-sts-pr","keyword":"mteb","description":"\n  SICK-NL-STS\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSICK-NL (read: signal), a dataset targeting Natural Language Inference in Dutch. SICK-NL is obtained by translating the SICK dataset of (Marelli et al., 2014) from English into Dutch.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\nDomains\nNews, Social, Web, Spoken, Written\n\n\nReference\nhttps://aclanthology.org/2021.eacl-main.126/\n\n\n\t\n\nSource datasets:\n\nclips/mteb-nl-sick\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embeddingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/clips/mteb-nl-sick-sts-pr.","url":"https://huggingface.co/datasets/clips/mteb-nl-sick-sts-pr","creator_name":"CLiPS","creator_url":"https://huggingface.co/clips","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","human-annotated","translated","clips/mteb-nl-sick","Dutch"],"keywords_longer_than_N":true},
	{"name":"Tachibana3-Part2-DeepSeek-V3.2","keyword":"testing","description":"Click here to support our open-source dataset and model releases!\nTachibana3-Part2-DeepSeek-V3.2 is a dataset focused on high-difficulty code production tasks, testing the limits of DeepSeek V3.2's code-reasoning skills!\nThis dataset contains 9.3k high-difficulty code-production prompts:\n\nQuestions prioritize real-world, challenging coding tasks across a variety of programming languages and topics.\nAreas of focus include back-end and front-end development, mobile, gamedev, cloud, QA, customâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Tachibana3-Part2-DeepSeek-V3.2.","url":"https://huggingface.co/datasets/sequelbox/Tachibana3-Part2-DeepSeek-V3.2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Tachibana3-Part1-DeepSeek-V3.1-Terminus","keyword":"testing","description":"Click here to support our open-source dataset and model releases!\nTachibana3-Part1-DeepSeek-V3.1-Terminus is a dataset focused on high-difficulty code production tasks, testing the limits of DeepSeek V3.1 Terminus's code-reasoning skills!\nThis dataset contains 9.3k high-difficulty code-production prompts:\n\nQuestions prioritize real-world, challenging coding tasks across a variety of programming languages and topics.\nAreas of focus include back-end and front-end development, mobile, gamedevâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus.","url":"https://huggingface.co/datasets/sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"vocsim","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tVocSim: A Training-Free Benchmark for Content Identity in Single-Source Audio Embeddings\n\t\n\n\n\n\nVocSim evaluates how well neural audio embeddings generalize for zero-shot audio similarity. It tests recognizing fine-grained acoustic similarity without specific similarity training.\n\n\n\t\n\t\n\t\n\t\tKey Features\n\t\n\n\nDiverse Sources: Human speech (phones, words, utterances), birdsong, otter calls, environmental sounds.Varied Conditions: Spans clean to noisy recordings, short (<100ms) to longâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anonymous-submission000/vocsim.","url":"https://huggingface.co/datasets/anonymous-submission000/vocsim","creator_name":"Anonymous","creator_url":"https://huggingface.co/anonymous-submission000","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","100K - 1M","parquet","Audio","Text"],"keywords_longer_than_N":true},
	{"name":"AROFlickrOrder","keyword":"mteb","description":"\n  AROFlickrOrder\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCompositionality Evaluation of images to their captions.Each capation has four hard negatives created by order permutations.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\ni2t\n\n\nDomains\nEncyclopaedic\n\nReference\nhttps://openreview.net/forum?id=KRLUvxh8uaX\n\n\n\t\n\nSource datasets:\n\ngowitheflow/ARO-Flickr-Order\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AROFlickrOrder.","url":"https://huggingface.co/datasets/mteb/AROFlickrOrder","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["other","image-to-text","text-to-image","image-captioning","expert-annotated"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset","keyword":"alignment","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of prompts and responses to those prompts. All completions were generated by querying already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc.). The dataset is available in Portuguese, English, and Spanish.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguage modeling.\nQuestion-answeringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset.","url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","Spanish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"GiftEval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tGIFT-Eval\n\t\n\n\n\nWe present GIFT-Eval, a benchmark designed to advance zero-shot time series forecasting by facilitating evaluation across diverse datasets. GIFT-Eval includes 23 datasets covering 144,000 time series and 177 million data points, with data spanning seven domains, 10 frequencies, and a range of forecast lengths. This benchmark aims to set a new standard, guiding future innovations in time series foundation models.\nTo facilitate the effective pretraining and evaluation ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/GiftEval.","url":"https://huggingface.co/datasets/Salesforce/GiftEval","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","100K<n<1M","Time-series","arxiv:2410.10393"],"keywords_longer_than_N":true},
	{"name":"difraud","keyword":"benchmark","description":"DIFrauD -- (Domain Independent Fraud Detection) is a corpus of deceptive and truthful texts from 7 domains:\n\n\"fake_news\",\n\"job_scams\",\n\"phishing\",\n\"political_statements\",\n\"product_reviews\",\n\"sms\",\n\"twitter_rumours\"\n\nTo load a specific domain, pass it as the \"name\" parameter to load_dataset()","url":"https://huggingface.co/datasets/difraud/difraud","creator_name":"difraud","creator_url":"https://huggingface.co/difraud","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","monolingual","English","mit"],"keywords_longer_than_N":true},
	{"name":"SWEbenchMultilingualRR","keyword":"mteb","description":"\n  SWEbenchMultilingualRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual Software Issue Localization.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://www.swebench.com/multilingual.html\n\n\n\t\n\nSource datasets:\n\ntarsur909/mteb-swe-bench-multilingual-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"SWEbenchMultilingualRR\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SWEbenchMultilingualRR.","url":"https://huggingface.co/datasets/mteb/SWEbenchMultilingualRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-swe-bench-multilingual-reranking","code"],"keywords_longer_than_N":true},
	{"name":"improved-upload-test-20251003163549","keyword":"test","description":"\n\t\n\t\t\n\t\timproved-upload-test\n\t\n\nTest dataset with improved upload functionality\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nTotal Files: 3\nTotal Size: 0.00 MB\nUpload Date: 2025-10-03 16:35:52\n\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\ntest_image.jpg (27 bytes)\ntest_data.json (75 bytes)\ntest_text.txt (65 bytes)\n\n","url":"https://huggingface.co/datasets/vivekgr92/improved-upload-test-20251003163549","creator_name":"Vivek","creator_url":"https://huggingface.co/vivekgr92","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","ðŸ‡ºðŸ‡¸ Region: US","dataset","test","improved"],"keywords_longer_than_N":true},
	{"name":"human-style-preferences-images","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Preference Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human preference datasets for text-to-image models, this release contains over 1,200,000 human preferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-style-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-style-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"MacroBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMacroBench is a code-first benchmark that evaluates whether Large Language Models can synthesize reusable browser-automation programs (macros) from natural-language goals by reading HTML/DOM and emitting Selenium code.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nPaper: arXiv:2510.04363\nGitHub: MacroBench Repository\n\n\n\t\n\t\t\n\t\tDataset Files\n\t\n\nThe dataset includes the following files in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hyunjun1121/MacroBench.","url":"https://huggingface.co/datasets/hyunjun1121/MacroBench","creator_name":"hyunjun","creator_url":"https://huggingface.co/hyunjun1121","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"weasis-tabular-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tWeasis Medical Imaging GUI Benchmark (Tabular Format)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 267 end-to-end GUI automation tasks for the Weasis medical imaging viewer in tabular format, where each row represents one complete task with all associated data.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Tasks: 267\nTotal Images: 202\nFormat: Tabular (each row = one task)\nApplication: Weasis Medical Imaging Viewer\nResolution: 1920x1080\n\n\n\t\n\t\t\n\t\tData Structure\n\t\n\nEach row contains:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/rishuKumar404/weasis-tabular-benchmark.","url":"https://huggingface.co/datasets/rishuKumar404/weasis-tabular-benchmark","creator_name":"Rishu Kumar Singh","creator_url":"https://huggingface.co/rishuKumar404","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"weasis-fixed-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tWeasis Medical Imaging GUI Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 267 end-to-end GUI automation tasks for the Weasis medical imaging viewer in tabular format, where each row represents one complete task with all associated data.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Tasks: 267\nTotal Images: 202\nFormat: Tabular (each row = one task)\nApplication: Weasis Medical Imaging Viewer\nResolution: 1920x1080\n\n\n\t\n\t\t\n\t\tData Structure\n\t\n\nEach row contains:\n\n\t\n\t\t\nColumnâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rishuKumar404/weasis-fixed-benchmark.","url":"https://huggingface.co/datasets/rishuKumar404/weasis-fixed-benchmark","creator_name":"Rishu Kumar Singh","creator_url":"https://huggingface.co/rishuKumar404","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["other","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"HumanEvalRetrieval","keyword":"mteb","description":"\n  HumanEvalRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA code retrieval task based on 164 Python programming problems from HumanEval. Each query is a natural language description of a programming task (e.g., 'Check if in given list of numbers, are any two numbers closer to each other than given threshold'), and the corpus contains Python code implementations. The task is to retrieve the correct code snippet that solves the described problem. Queries are problemâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HumanEvalRetrieval.","url":"https://huggingface.co/datasets/mteb/HumanEvalRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","multilingual","embedding-benchmark/HumanEval","code"],"keywords_longer_than_N":true},
	{"name":"JinaVDROWIDChartsRetrieval","keyword":"mteb","description":"\n  JinaVDROWIDChartsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve charts from the OWID dataset based on accompanied text snippets.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/owid_charts_en_beir\n\n\n\t\n\nSource datasets:\n\njinaai/owid_charts_en_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDROWIDChartsRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDROWIDChartsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"HC3FinanceRetrieval","keyword":"mteb","description":"\n  HC3FinanceRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA financial retrieval task based on HC3 Finance dataset containing human vs AI-generated financial text detection. Each query is a financial question or prompt (e.g., 'Explain the impact of interest rate changes on bond prices'), and the corpus contains both human-written and AI-generated financial responses. The task is to retrieve the most relevant and accurate financial content that addresses the query. Queriesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HC3FinanceRetrieval.","url":"https://huggingface.co/datasets/mteb/HC3FinanceRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","question-answering","multiple-choice-qa","expert-annotated","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreDocVQARetrieval","keyword":"mteb","description":"\n  VidoreDocVQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/docvqa_test_subsampled_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreDocVQARetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreDocVQARetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreDocVQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"RuSciBenchCociteRetrieval","keyword":"mteb","description":"\n  RuSciBenchCociteRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task focuses on Co-citation Prediction for scientific papers from eLibrary,\n        Russia's largest electronic library of scientific publications. Given a query paper (title and abstract),\n        the goal is to retrieve other papers that are co-cited with it. Two papers are considered co-cited\n        if they are both cited by at least 5 of the same other papers. Similar to the Direct Citation taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RuSciBenchCociteRetrieval.","url":"https://huggingface.co/datasets/mteb/RuSciBenchCociteRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","multilingual","mlsa-iai-msu-lab/ru_sci_bench_cocite_retrieval"],"keywords_longer_than_N":true},
	{"name":"JinaVDRDocQAHealthcareIndustryRetrieval","keyword":"mteb","description":"\n  JinaVDRDocQAHealthcareIndustryRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve healthcare industry documents based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nMedical\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/docqa_healthcare_industry_beir\n\n\n\t\n\nSource datasets:\n\njinaai/docqa_healthcare_industry_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntaskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRDocQAHealthcareIndustryRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRDocQAHealthcareIndustryRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreSyntheticDocQAEnergyRetrieval","keyword":"mteb","description":"\n  VidoreSyntheticDocQAEnergyRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/syntheticDocQA_energy_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreSyntheticDocQAEnergyRetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAEnergyRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAEnergyRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreSyntheticDocQAGovernmentReportsRetrieval","keyword":"mteb","description":"\n  VidoreSyntheticDocQAGovernmentReportsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/syntheticDocQA_government_reports_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAGovernmentReportsRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAGovernmentReportsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRJDocQARetrieval","keyword":"mteb","description":"\n  JinaVDRJDocQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Japanese documents in various formats based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/jdocqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/jdocqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRJDocQARetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRJDocQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRJDocQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"mFollowIRCrossLingual","keyword":"mteb","description":"\n  mFollowIRCrossLingual\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis tasks measures retrieval instruction following ability on NeuCLIR narratives for the mFollowIR benchmark on the Farsi, Russian, and Chinese languages with English queries/instructions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\nSource datasets:\n\njhu-clsp/mFollowIR-cross-lingual-parquet-mteb\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/mFollowIRCrossLingual.","url":"https://huggingface.co/datasets/mteb/mFollowIRCrossLingual","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-ranking","expert-annotated","multilingual","jhu-clsp/mFollowIR-cross-lingual-parquet-mteb","English"],"keywords_longer_than_N":true},
	{"name":"JinaVDRTabFQuadRetrieval","keyword":"mteb","description":"\n  JinaVDRTabFQuadRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve tables from industry documents based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/tabfquad_beir\n\n\n\t\n\nSource datasets:\n\njinaai/tabfquad_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRTabFQuadRetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRTabFQuadRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRTabFQuadRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRDocVQARetrieval","keyword":"mteb","description":"\n  JinaVDRDocVQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve industry documents based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/docvqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/docvqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRDocVQARetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRDocVQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRDocVQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRJina2024YearlyBookRetrieval","keyword":"mteb","description":"\n  JinaVDRJina2024YearlyBookRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve pages from the 2024 Jina yearbook based on human annotated questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/jina_2024_yearly_book_beir\n\n\n\t\n\nSource datasets:\n\njinaai/jina_2024_yearly_book_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRJina2024YearlyBookRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRJina2024YearlyBookRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"FinanceBenchRetrieval","keyword":"mteb","description":"\n  FinanceBenchRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA financial retrieval task based on FinanceBench dataset containing financial questions and answers. Each query is a financial question (e.g., 'What was the total revenue in Q3 2023?'), and the corpus contains financial document excerpts and annual reports. The task is to retrieve the correct financial information that answers the question. Queries are financial questions while the corpus contains relevant excerptsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FinanceBenchRetrieval.","url":"https://huggingface.co/datasets/mteb/FinanceBenchRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","question-answering","multiple-choice-qa","expert-annotated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Vidore2BioMedicalLecturesRetrieval","keyword":"mteb","description":"\n  Vidore2BioMedicalLecturesRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/biomedical_lectures_v2\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"Vidore2BioMedicalLecturesRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Vidore2BioMedicalLecturesRetrieval.","url":"https://huggingface.co/datasets/mteb/Vidore2BioMedicalLecturesRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"CQADupstackAndroidRetrieval","keyword":"mteb","description":"\n  CQADupstackAndroidRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Web, Written, Non-fiction\n\n\nReferencehttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\nSource datasets:\n\nmteb/cqadupstack-android\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CQADupstackAndroidRetrieval.","url":"https://huggingface.co/datasets/mteb/CQADupstackAndroidRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","question-answering","multiple-choice-qa","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"mindgames","keyword":"nli","description":"Mindgame dataset\nCode:\nhttps://github.com/sileod/llm-theory-of-mind\nArticle (Accepted at EMNLP 2023 Findings):\nhttps://arxiv.org/abs/2305.03353\n@article{sileo2023mindgames,\n  title={MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic},\n  author={Sileo, Damien and Lernould, Antoine},\n  journal={arXiv preprint arXiv:2305.03353},\n  year={2023}\n}\n\n","url":"https://huggingface.co/datasets/sileod/mindgames","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","multi-input-text-classification","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"or-bench","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOR-Bench: An Over-Refusal Benchmark for Large Language Models\n\t\n\nPlease see our demo at HuggingFace Spaces. \n\n\t\n\t\t\n\t\tOverall Plots of Model Performances\n\t\n\nBelow is the overall model performance. X axis shows the rejection rate on OR-Bench-Hard-1K and Y axis shows the rejection rate on OR-Bench-Toxic. The best aligned model should be on the top left corner of the plot where the model rejects the most number of toxic prompts and least number of safe prompts. We also plot a blue lineâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bench-llm/or-bench.","url":"https://huggingface.co/datasets/bench-llm/or-bench","creator_name":"Bench LLM","creator_url":"https://huggingface.co/bench-llm","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"gdelt-rag-evaluation-inputs","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tGDELT RAG Evaluation Datasets\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains consolidated RAGAS evaluation input datasets from 5 different retrieval strategies tested on the GDELT (Global Database of Events, Language, and Tone) RAG system. Each strategy was evaluated on the same golden testset of 12 questions, providing a direct comparison of retrieval performance.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: ~1,400+ evaluation records across 5 retrievers\nRetrievers Compared:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/gdelt-rag-evaluation-inputs.","url":"https://huggingface.co/datasets/dwb2023/gdelt-rag-evaluation-inputs","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-retrieval","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"gdelt-rag-evaluation-datasets","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tGDELT RAG Evaluation Datasets\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains consolidated RAGAS evaluation input datasets from 5 different retrieval strategies tested on the GDELT (Global Database of Events, Language, and Tone) RAG system. Each strategy was evaluated on the same golden testset of 12 questions, providing a direct comparison of retrieval performance.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: ~1,400+ evaluation records across 5 retrievers\nRetrievers Compared:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/gdelt-rag-evaluation-datasets.","url":"https://huggingface.co/datasets/dwb2023/gdelt-rag-evaluation-datasets","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-retrieval","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"russian_super_glue","keyword":"nli","description":"Recent advances in the field of universal language models and transformers require the development of a methodology for\ntheir broad diagnostics and testing for general intellectual skills - detection of natural language inference,\ncommonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first\ntime, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from\nscratch for the Russian language. We provide baselines, human level evaluation, an open-source framework for evaluating\nmodels and an overall leaderboard of transformer models for the Russian language.","url":"https://huggingface.co/datasets/RussianNLP/russian_super_glue","creator_name":"Natural Language Processing in Russian","creator_url":"https://huggingface.co/RussianNLP","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","question-answering","zero-shot-classification","text-generation","natural-language-inference"],"keywords_longer_than_N":true},
	{"name":"esb-datasets-test-only","keyword":"benchmark","description":"All eight of datasets in ESB can be downloaded and prepared in just a single line of code through the Hugging Face Datasets library:\nfrom datasets import load_dataset\n\nlibrispeech = load_dataset(\"esb/datasets\", \"librispeech\", split=\"train\")\n\n\n\"esb/datasets\": the repository namespace. This is fixed for all ESB datasets.\n\n\"librispeech\": the dataset name. This can be changed to any of any one of the eight datasets in ESB to download that dataset.\n\nsplit=\"train\": the split. Set this to one ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hf-audio/esb-datasets-test-only.","url":"https://huggingface.co/datasets/hf-audio/esb-datasets-test-only","creator_name":"Hugging Face for Audio","creator_url":"https://huggingface.co/hf-audio","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"JinaVDREuropeanaNlLegalRetrieval","keyword":"mteb","description":"\n  JinaVDREuropeanaNlLegalRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Dutch historical legal documents based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nLegal\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/europeana-nl-legal_beir\n\n\n\t\n\nSource datasets:\n\njinaai/europeana-nl-legal_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDREuropeanaNlLegalRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDREuropeanaNlLegalRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRStudentEnrollmentSyntheticRetrieval","keyword":"mteb","description":"\n  JinaVDRStudentEnrollmentSyntheticRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve student enrollment data based on templated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/student-enrollment_beir\n\n\n\t\n\nSource datasets:\n\njinaai/student-enrollment_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRStudentEnrollmentSyntheticRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRStudentEnrollmentSyntheticRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v3","keyword":"alignment","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset version 3.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of multi-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3.","url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"esc-datasets","keyword":"benchmark","description":"All eight of datasets in ESC can be downloaded and prepared in just a single line of code through the Hugging Face Datasets library:\nfrom datasets import load_dataset\n\nlibrispeech = load_dataset(\"esc-benchmark/esc-datasets\", \"librispeech\", split=\"train\")\n\n\n\"esc-benchmark\": the repository namespace. This is fixed for all ESC datasets.\n\n\"librispeech\": the dataset name. This can be changed to any of any one of the eight datasets in ESC to download that dataset.\n\nsplit=\"train\": the split. Set thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/esc-benchmark/esc-datasets.","url":"https://huggingface.co/datasets/esc-benchmark/esc-datasets","creator_name":"ESC","creator_url":"https://huggingface.co/esc-benchmark","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"FinSM","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ§¾ FinAuditing Benchmark\n\t\n\nThis dataset is introduced in the paperFinAuditing: Taxonomy-Grounded Financial Auditing Benchmark for Evaluating Large Language Modelsby Yan Wang, Keyi Wang, Shanshan Yang, Jaisal Patel, Jeff Zhao, Fengran Mo, Xueqing Peng, Lingfei Qian, Jimin Huang, Guojun Xiong, Xiao-Yang Liu, and Jian-Yun Nie (2025).\n","url":"https://huggingface.co/datasets/TheFinAI/FinSM","creator_name":"The Fin AI","creator_url":"https://huggingface.co/TheFinAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"gsm8k-ru","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tgsm8k-ru\n\t\n\nTranslated version of gsm8k dataset into Russian.\n","url":"https://huggingface.co/datasets/d0rj/gsm8k-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["crowdsourced","translated","monolingual","gsm8k","Russian"],"keywords_longer_than_N":true},
	{"name":"tracie","keyword":"nli","description":"https://github.com/allenai/aristo-leaderboard/tree/master/tracie/data\n@inproceedings{ZRNKSR21,\n    author = {Ben Zhou and Kyle Richardson and Qiang Ning and Tushar Khot and Ashish Sabharwal and Dan Roth},\n    title = {Temporal Reasoning on Implicit Events from Distant Supervision},\n    booktitle = {NAACL},\n    year = {2021},\n}\n\n","url":"https://huggingface.co/datasets/tasksource/tracie","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"JGLUE","keyword":"nli","description":"JGLUE, Japanese General Language Understanding Evaluation, is built to measure the general NLU ability in Japanese. JGLUE has been constructed from scratch without translation. We hope that JGLUE will facilitate NLU research in Japanese.\\","url":"https://huggingface.co/datasets/shunk031/JGLUE","creator_name":"Shunsuke Kitada","creator_url":"https://huggingface.co/shunk031","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","question-answering","sentence-similarity","text-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"OpsEval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tOpsEval Dataset\n\t\n\nWebsite | Reporting Issues\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OpsEval dataset represents a pioneering effort in the evaluation of Artificial Intelligence for IT Operations (AIOps), focusing on the application of Large Language Models (LLMs) within this domain. In an era where IT operations are increasingly reliant on AI technologies for automation and efficiency, understanding the performance of LLMs in operational tasks becomes crucial. OpsEval offers a comprehensiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Junetheriver/OpsEval.","url":"https://huggingface.co/datasets/Junetheriver/OpsEval","creator_name":"Liu Yuhe","creator_url":"https://huggingface.co/Junetheriver","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","Chinese","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"propsegment","keyword":"nli","description":"This is a reproduced (i.e. after web-crawling) and processed version of the \"PropSegment\" dataset from Google Research.\n\nSince the News portion of the dataset is released only via urls, we reconstruct the dataset by crawling. Overall, ~96% \nof the dataset can be reproduced, and the rest ~4% either have url no longer valid, or sentences that have been edited \n(i.e. cannot be aligned with the orignial dataset).\n\nPropSegment (Proposition-level Segmentation and Entailment) is a large-scale, human annotated dataset for segmenting \nEnglish text into propositions, and recognizing proposition-level entailment relations --- whether a different, related \ndocument entails each proposition, contradicts it, or neither.\n\nThe original dataset features >45k human annotated propositions, i.e. individual semantic units within sentences, as \nwell as >45k entailment labels between propositions and documents.","url":"https://huggingface.co/datasets/sihaochen/propsegment","creator_name":"Sihao Chen","creator_url":"https://huggingface.co/sihaochen","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"radon-test-code_generation","keyword":"test","description":"\n\t\n\t\t\n\t\tradon-test-code_generation\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nCode generation test dataset for RADON model evaluation with programming prompts\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tLoad Dataset\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MagistrTheOne/radon-test-code_generation\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tUse with RADON Model\n\t\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load RADON model\nmodel = AutoModelForCausalLM.from_pretrained(\"MagistrTheOne/RadonSAI\")\ntokenizer =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagistrTheOne/radon-test-code_generation.","url":"https://huggingface.co/datasets/MagistrTheOne/radon-test-code_generation","creator_name":"Maga","creator_url":"https://huggingface.co/MagistrTheOne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","original","Russian","English"],"keywords_longer_than_N":true},
	{"name":"recast","keyword":"nli","description":"A diverse collection of tasks recasted as natural language inference tasks.","url":"https://huggingface.co/datasets/tasksource/recast","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","natural-language-inference","expert-generated","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"MultiTS-Eval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMultiTS-Eval\n\t\n\nWe present MultiTS-Eval, a comprehensive benchmark for evaluating foundation models on multivariate time series forecasting tasks. MultiTS-Eval spans 16 multivariate time series domains and introduces novel synthetic data techniques, comprising 19B data points across 825K time series. \n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nData is structured into 4 main categories, comprising the following domains:\n\nReal-World: Energy, Public Info, Health, Sales, Climate & Environmentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Synthefy/MultiTS-Eval.","url":"https://huggingface.co/datasets/Synthefy/MultiTS-Eval","creator_name":"Synthefy","creator_url":"https://huggingface.co/Synthefy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"gdpval-gpt5-fork","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tGDPval Fork Dataset with GPT-5 Results\n\t\n\nðŸ† A comprehensive evaluation dataset featuring GPT-5 execution results on real-world professional tasks\nThis is an enhanced fork of the original OpenAI GDPval dataset with complete GPT-5 execution results, including actual deliverable files created by the AI model.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Overview\n\t\n\n\n\t\n\t\t\nMetric\nValue\n\n\n\t\t\nTotal Tasks\n220\n\n\nAI-Completed Tasks\n87 (39.5%)\n\n\nDeliverable Files\n492+ professional documents\n\n\nOccupations\n44\n\n\nIndustryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kevindenight/gdpval-gpt5-fork.","url":"https://huggingface.co/datasets/kevindenight/gdpval-gpt5-fork","creator_name":"xxx","creator_url":"https://huggingface.co/kevindenight","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["other","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ARPIbench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tAgent Reflected Prompt Injection Benchmark Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nA benchmark dataset with 3,528 test cases for evaluating AI agent robustness against reflected prompt injection attacks,\nwhere the prompt injection instructions are repeated in assistant output.\nAgents are asked to fetch URL content while malicious payloads attempt to exfiltrate a local file.\nThis dataset is generated as a cartesian product of a small set of hand-crafted parts:\n\nA prefix before the maliciousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alexcbecker/ARPIbench.","url":"https://huggingface.co/datasets/alexcbecker/ARPIbench","creator_name":"Alex Becker","creator_url":"https://huggingface.co/alexcbecker","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"comic-eval-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for comic-eval-benchmark\n\t\n\n\n\nä¸­æ–‡äºŒæ¬¡å…ƒæ¼«ç”»é¢†åŸŸçš„åŸºå‡†è¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…å«ä¸Šåƒéƒ¨æ¼«ç”»ä½œå“çš„ä½œè€…ä¿¡æ¯ã€ç”»é£Žã€åœºæ™¯ã€ç±»åž‹ã€å‰§æƒ…ç­‰ç»´åº¦çš„é€‰æ‹©é¢˜è¯„ä¼°ï¼Œå…± 41175 ä¸ªå•é€‰é¢˜ã€‚\nå¯ä½œä¸ºäºŒæ¬¡å…ƒåž‚ç›´é¢†åŸŸå¤§æ¨¡åž‹çš„è¯„ä¼°åŸºå‡†ã€‚\nä»¥ä¸‹æ˜¯ä½œè€…åŸºäºŽBaichuan2-13Bå¾®è°ƒçš„äºŒæ¬¡å…ƒé¢†åŸŸåž‚ç›´å¤§æ¨¡åž‹ï¼Œåœ¨æ­¤æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æžœï¼š\n\n\t\n\t\t\næ¨¡åž‹\nzero-shot\n3-shot\n\n\n\t\t\nQwen-7b\n33.647\n36.439\n\n\nChatGLM3-6b\n34.373\n37.015\n\n\nBaiChuan2-13b\n37.416\n39.08\n\n\nBaiChuan2-13b-å¾®è°ƒ\n41.035\n41.086\n\n\nYi-34b\n50.103\n45.606\n\n\n\t\n\næ¬¢è¿Žè´¡çŒ®æ›´å¤šäºŒæ¬¡å…ƒé¢†åŸŸè¯­æ–™åŠäºŒæ¬¡å…ƒå¤§æ¨¡åž‹ï¼Œå¦‚éœ€è¯„æµ‹è¯·è”ç³»ä½œè€…èŽ·å–è¯„æµ‹è„šæœ¬ã€‚\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nä¸­æ–‡äºŒæ¬¡å…ƒé¢†åŸŸæ¼«ç”»åŸºå‡†è¯„ä¼°æ•°æ®é›†\n\n\t\n\t\t\n\t\tDataset Sourcesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gctian/comic-eval-benchmark.","url":"https://huggingface.co/datasets/gctian/comic-eval-benchmark","creator_name":"TianGuicheng","creator_url":"https://huggingface.co/gctian","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"periodontal-reasoning-40k","keyword":"alignment","description":"\n\t\n\t\t\n\t\tPeriodontal-Reasoning-40k\n\t\n\n40,000 periodontal clinical reasoning examples for off-policy RLHF (KTO/DPO).\nFormat (JSONL, one per line): prompt, completion, label âˆˆ {1,-1}\nExample:\n{\"prompt\": \"A patient's plaque score was 35% at baseline and 1% at followâ€‘up. Determine whether the improvement is favourable according to BSP criteria (â‰¤20% plaque or â‰¥50% reduction).\", \"completion\": \"The improvement is favourable.\", \"label\": 1}\nSplit: train: 40,000 (data/train.jsonl)\nIntended use: KTO/DPO;â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Wildstash/periodontal-reasoning-40k.","url":"https://huggingface.co/datasets/Wildstash/periodontal-reasoning-40k","creator_name":"ArnavS","creator_url":"https://huggingface.co/Wildstash","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","expert-generated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"rejection_sampling_phi_2_OA_rm","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tDataset Card for Rejection Sampling Phi-2 with OpenAssistant RM\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe \"Rejection Sampling Phi-2 with OpenAssistant RM\" dataset consists of 10 pairs of prompts and responses, which were generated using rejection sampling over 10 Phi-2 generation using the OpenAssistant Reward Model.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset and its creation rationale could be used to support models for question-answering, text-generation, or conversationalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alizeepace/rejection_sampling_phi_2_OA_rm.","url":"https://huggingface.co/datasets/alizeepace/rejection_sampling_phi_2_OA_rm","creator_name":"AlizÃ©e Pace","creator_url":"https://huggingface.co/alizeepace","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","machine-generated","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"SWEbenchVerifiedRR","keyword":"mteb","description":"\n  SWEbenchVerifiedRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSoftware Issue Localization for SWE-bench Verified\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://openai.com/index/introducing-swe-bench-verified/\n\n\n\t\n\nSource datasets:\n\ntarsur909/mteb-swe-bench-verified-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SWEbenchVerifiedRR.","url":"https://huggingface.co/datasets/mteb/SWEbenchVerifiedRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-swe-bench-verified-reranking","code"],"keywords_longer_than_N":true},
	{"name":"MJ-Bench-Image","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMJ-Bench-Image Dataset\n\t\n\nThis dataset contains image pairs from the MJ-Bench benchmark for evaluating multimodal judges in text-to-image generation.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized into multiple categories:\n\nAlignment: Evaluates how well images follow prompt instructions\nBias: Tests for demographic and contextual biases\nComposition: Tests physics laws, perspective, and depth ordering\nQuality: Evaluates image fidelity, color, lighting, and texture\nSafety: Testsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zhaorun/MJ-Bench-Image.","url":"https://huggingface.co/datasets/Zhaorun/MJ-Bench-Image","creator_name":"Zhaorun Chen","creator_url":"https://huggingface.co/Zhaorun","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","text-to-image","mit","1K<n<10K","arxiv:2407.04842"],"keywords_longer_than_N":true},
	{"name":"KITE","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tKITE: Korean Instruction-following Task Evaluation\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nKITE (Korean Instruction-following Task Evaluation) is the first comprehensive benchmark specifically designed to evaluate the Korean instruction-following capabilities of Large Language Models (LLMs). Unlike existing Korean benchmarks that focus mainly on factual knowledge or multiple-choice testing, KITE directly targets diverse, open-ended instruction-following tasks.\n\t\n\t\t\n\t\tDataset Summaryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/junkim100/KITE.","url":"https://huggingface.co/datasets/junkim100/KITE","creator_name":"Jun Kim","creator_url":"https://huggingface.co/junkim100","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Korean","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"usb","keyword":"nli","description":"\n\t\n\t\t\n\t\tUSB: A Unified Summarization Benchmark Across Tasks and Domains\n\t\n\nThis benchmark contains labeled datasets for 8 text summarization based tasks given below. \nThe labeled datasets are created by collecting manual annotations on top of Wikipedia articles from 6 different domains.\n\n\t\n\t\t\nTask\nDescription\nCode snippet\n\n\n\t\t\nExtractive Summarization\nHighlight important sentences in the source article\nload_dataset(\"kundank/usb\",\"extractive_summarization\")\n\n\nAbstractive Summarization\nGenerateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kundank/usb.","url":"https://huggingface.co/datasets/kundank/usb","creator_name":"Kundan Krishna","creator_url":"https://huggingface.co/kundank","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["summarization","English","apache-2.0","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simpleqa-verified","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSimpleQA Verified\n\t\n\n\n\t\n\t\t\n\t\tA 1,000-prompt factuality benchmark from Google DeepMind and Google Research, designed to reliably evaluate LLM parametric knowledge.\n\t\n\nâ–¶ SimpleQA Verified Leaderboard on Kaggleâ–¶ Technical Reportâ–¶ Evaluation Starter Code\n\n\t\n\t\t\n\t\n\t\n\t\tBenchmark\n\t\n\nSimpleQA Verified is a 1,000-prompt benchmark for reliably evaluating Large Language Models (LLMs) on short-form factuality \nand parametric knowledge. The authors from Google DeepMind and Google Research build onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google/simpleqa-verified.","url":"https://huggingface.co/datasets/google/simpleqa-verified","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"GiftEvalPretrain","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tGIFT-Eval Pre-training Datasets\n\t\n\nPretraining dataset aligned with GIFT-Eval that has 71 univariate and 17 multivariate datasets, spanning seven domains and 13 frequencies, totaling 4.5 million time series and 230 billion data points. Notably this collection of data has no leakage issue with the train/test split and can be used to pretrain foundation models that can be fairly evaluated on GIFT-Eval.\nðŸ“„ Paper\nðŸ–¥ï¸ Code\nðŸ“” Blog Post\nðŸŽï¸ Leader Board\n\n\t\n\t\n\t\n\t\tEthical Considerationsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/liamsbhoo/GiftEvalPretrain.","url":"https://huggingface.co/datasets/liamsbhoo/GiftEvalPretrain","creator_name":"Liam Shi Bin Hoo","creator_url":"https://huggingface.co/liamsbhoo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","1M - 10M","arrow","Text"],"keywords_longer_than_N":true},
	{"name":"flymyai-ffhq-edit-bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFace Identity Preservation Benchmark\n\t\n\nA comprehensive evaluation dataset for face transformation APIs measuring identity preservation across complexity levels and transformation categories.\nðŸ”— Complete Repository: https://github.com/FlyMyAI/bench_M1\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis benchmark evaluates identity preservation in face image transformations using 8,832 transformation pairs across three major APIs. The dataset provides systematic evaluation of face editing quality usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/flymy-ai/flymyai-ffhq-edit-bench.","url":"https://huggingface.co/datasets/flymy-ai/flymyai-ffhq-edit-bench","creator_name":"FlyMy.AI","creator_url":"https://huggingface.co/flymy-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-image","image-classification","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"CUADCapOnLiabilityLegalBenchClassification","keyword":"mteb","description":"\n  CUADCapOnLiabilityLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies a cap on liability upon the breach of a party's obligation. This includes time limitation for the counterparty to bring claims or maximum amount for recovery.\n\n\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADCapOnLiabilityLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADCapOnLiabilityLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"icpc-world-finals","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tICPC World FinalsDataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe ICPC World Finals Dataset serves as a challenging benchmark for code generation, encompassing 146 problems from the International Collegiate Programming Contest (ICPC) World Finals spanning from 2011 to 2023. The ICPC World Finals represents one of the most prestigious and difficult competitive programming contests globally, making this dataset particularly valuable for assessing the advanced problem-solving and codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HumanLastCodeExam/icpc-world-finals.","url":"https://huggingface.co/datasets/HumanLastCodeExam/icpc-world-finals","creator_name":"Humanityâ€™s Last Code Exam","creator_url":"https://huggingface.co/HumanLastCodeExam","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"gametime","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tGametime Benchmark\n\t\n\nThe Gametime dataset provides lightweight, streaming-friendly splits for TTS/ASR/SpokenLM prototyping.\n\n\n\t\n\t\t\n\t\tðŸ“¦ Download Options\n\t\n\n\n\t\n\t\t\n\t\t1ï¸âƒ£ Recommended â€” Stream from Hugging Face\n\t\n\nfrom datasets import load_dataset\nimport io\nimport soundfile as sf\n\n# Load Basic train split\nds_basic = load_dataset(\"gametime-benchmark/gametime\", \"basic\", split=\"test\", streaming=True)\nex = next(iter(ds_basic))\nbuf = io.BytesIO(ex[\"audio_bytes\"])\nwav, sr = sf.read(bufâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gametime-benchmark/gametime.","url":"https://huggingface.co/datasets/gametime-benchmark/gametime","creator_name":"GameTime","creator_url":"https://huggingface.co/gametime-benchmark","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","audio-to-audio","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"amazon_counterfactual","keyword":"mteb","description":"\n  AmazonCounterfactualClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA collection of Amazon customer reviews annotated for counterfactual detection pair classification.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\nReference\nhttps://arxiv.org/abs/2104.06893\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"AmazonCounterfactualClassification\"])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/amazon_counterfactual.","url":"https://huggingface.co/datasets/mteb/amazon_counterfactual","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","multilingual","German","English"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-1562024-to89-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-v1_5-1562024-to89-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"scientific research in medicine, biology, and technology\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-v1_5-1562024-to89-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-1562024-to89-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-1562024-to89-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NQ-NL","keyword":"mteb","description":"\n  NQ-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNQ-NL is a translation of NQ\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\n\n\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-nq\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NQ-NL\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nTo learn more about howâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NQ-NL.","url":"https://huggingface.co/datasets/mteb/NQ-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","translated","mteb/nq"],"keywords_longer_than_N":true},
	{"name":"MeetingBank-transcript-de","keyword":"benchmark","description":"This dataset consists of transcripts from the MeetingBank dataset. \nOverview\nMeetingBank, a benchmark dataset created from the city councils of 6 major U.S. cities to supplement existing datasets.\nIt contains 1,366 meetings with over 3,579 hours of video, as well as transcripts, PDF documents of meeting minutes, agenda, and other metadata.\nOn average, a council meeting is 2.6 hours long and its transcript contains over 28k tokens, making it a valuable testbed for meeting summarizers and forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlioLeuchtmann/MeetingBank-transcript-de.","url":"https://huggingface.co/datasets/AlioLeuchtmann/MeetingBank-transcript-de","creator_name":"Alio Leuchtmann","creator_url":"https://huggingface.co/AlioLeuchtmann","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","summarization","text-generation","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"dutch-legal-c-64-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tdutch-legal-c-64-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"legal document search for Dutch legislation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the dutch-legal-c-64-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/dutch-legal-c-64-24.","url":"https://huggingface.co/datasets/fine-tuned/dutch-legal-c-64-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Diversity4LegalBenchClassification","keyword":"mteb","description":"\n  Diversity4LegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGiven a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 4).\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Diversity4LegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/Diversity4LegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"alpha-sglang-80-commits","keyword":"testing","description":"\n\t\n\t\t\n\t\tSGLang PR Test Classification Dataset (80 commits)\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains 80 SGLang project commits with their corresponding PR timeline text and rich commit metadata, mirroring the VLLM dataset schema (26 fields). It enables analysis of testing and benchmarking patterns in LLM serving systems.\n\n\t\n\t\t\n\t\tSchema (26 fields)\n\t\n\n\ncommit_hash, pr_url, pr_date, timeline_text, timeline_extracted_at\nhas_lm_eval, has_performance, has_serving, has_general_test, test_detailsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inferencebench/alpha-sglang-80-commits.","url":"https://huggingface.co/datasets/Inferencebench/alpha-sglang-80-commits","creator_name":"Inferencebench","creator_url":"https://huggingface.co/Inferencebench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"gaia2","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tGaia2\n\t\n\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGaia2 is a benchmark dataset for evaluating AI agent capabilities in simulated environments. The dataset contains 800 scenarios that test agent performance in environments where time flows continuously and events occur dynamically.\nThe dataset evaluates seven core capabilities: Execution (multi-step planning and state changes), Search (information gathering and synthesis), Adaptability (dynamic response to environmentalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/meta-agents-research-environments/gaia2.","url":"https://huggingface.co/datasets/meta-agents-research-environments/gaia2","creator_name":"Meta Agents Research Environments","creator_url":"https://huggingface.co/meta-agents-research-environments","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","task-planning","dialogue-modeling","dialogue-generation","conversational"],"keywords_longer_than_N":true},
	{"name":"deny-harmful-behaviour","keyword":"alignment","description":"\n  \n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\ndeny-harmful-behaviour is a synthetic dataset designed to help language models recognize and gracefully refuse requests that involve unethical, illegal, or dangerous behaviors. Using humorous, empathetic, and non-cooperative reasoning, each sample demonstrates how a model might respond to harmful prompts without engaging with the request.\nThis dataset was generated using Curator and inspired by prompts found in the mlabonne/harmful_behaviors dataset.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/KingNish/deny-harmful-behaviour.","url":"https://huggingface.co/datasets/KingNish/deny-harmful-behaviour","creator_name":"Nishith Jain","creator_url":"https://huggingface.co/KingNish","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-moonvalley-marey","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Marey Pro Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~75k human responses from ~15k human annotators were collected to evaluate Marey video generation model on our benchmark. This dataset was collected in roughtly 30 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please considerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-moonvalley-marey.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-moonvalley-marey","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"JavaneseIMDBClassification","keyword":"mteb","description":"\n  JavaneseIMDBClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nLarge Movie Review Dataset translated to Javanese. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\nDomains\nReviews, Written\n\n\nReference\nhttps://github.com/w11wo/nlp-datasets#javanese-imdb\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JavaneseIMDBClassification.","url":"https://huggingface.co/datasets/mteb/JavaneseIMDBClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-05062024-vbal-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-v1_5-05062024-vbal-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-v1_5-05062024-vbal-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-vbal-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-vbal-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"RuNLUIntentClassification","keyword":"mteb","description":"\n  RuNLUIntentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nContains natural language data for human-robot interaction in home domain which we collected and annotated for evaluating NLU Services/platforms.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomainsNone\n\n\nReference\nhttps://arxiv.org/abs/1903.05566\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RuNLUIntentClassification.","url":"https://huggingface.co/datasets/mteb/RuNLUIntentClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","intent-classification","human-annotated","multilingual","Russian"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_8","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3327,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_8.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_8","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"les-audits-affaires","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLes Audits d'Affaires: Benchmark d'Audit des Affaires FranÃ§aises\n\t\n\n\n  \n  \n  Le premier benchmark franÃ§ais pour auditer l'IA sur les affaires\n  \n  \n  \n  \n\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nLes Audits d'Affaires contient 2,658 questions sur les affaires franÃ§aises avec 5 catÃ©gories lÃ©gales standardisÃ©es, exclusivement conÃ§u pour Ã©valuer et auditer les performances des LLMs sur le droit commercial franÃ§ais. Ce dataset a Ã©tÃ© mÃ©ticuleusement curÃ© Ã  partir des codes juridiques franÃ§ais officiels pourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/legmlai/les-audits-affaires.","url":"https://huggingface.co/datasets/legmlai/les-audits-affaires","creator_name":"legml.ai","creator_url":"https://huggingface.co/legmlai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","open-domain-qa","document-question-answering","original"],"keywords_longer_than_N":true},
	{"name":"C3-BenchMark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tC^3-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking\n\t\n\nPaper: C^3-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking\nGitHub: https://github.com/Tencent-Hunyuan/C3-Benchmark\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“– Overview\n\t\n\nAgents based on large language models leverage tools to modify environments, revolutionizing how AI interacts with the physical world. Unlike traditional NLP tasks that rely solely on historical dialogue for responses, these agents must consider moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tencent/C3-BenchMark.","url":"https://huggingface.co/datasets/tencent/C3-BenchMark","creator_name":"Tencent","creator_url":"https://huggingface.co/tencent","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SCDDAuditsLegalBenchClassification","keyword":"mteb","description":"\n  SCDDAuditsLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer conducts audits of suppliers to evaluate supplier compliance with company standards for trafficking and slavery in supply chains? The disclosure shall specify if theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SCDDAuditsLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/SCDDAuditsLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"TRECCOVID-256-24-gpt-4o-2024-05-13-953989","keyword":"mteb","description":"\n\t\n\t\t\n\t\tTRECCOVID-256-24-gpt-4o-2024-05-13-953989 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"biomedical literature search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the TRECCOVID-256-24-gpt-4o-2024-05-13-953989 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/TRECCOVID-256-24-gpt-4o-2024-05-13-953989.","url":"https://huggingface.co/datasets/fine-tuned/TRECCOVID-256-24-gpt-4o-2024-05-13-953989","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MixBench2025","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench2025.","url":"https://huggingface.co/datasets/mixed-modality-search/MixBench2025","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"GPQA-diamond-ClaudeR1","keyword":"benchmarks","description":"\n\t\n\t\t\n\t\tDataset Card for GPQA Diamond Reasoning Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA benchmark dataset for evaluating hybrid AI architectures, comparing reasoning-augmented LLMs (DeepSeek R1) against standalone models (Claude Sonnet 3.5). Contains 198 physics questions with:\n\nGround truth answers and explanations\nModel responses from multiple architectures\nGranular token usage and cost metrics\nDifficulty metadata and domain categorization\n\nCurated by: LLMâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/spawn99/GPQA-diamond-ClaudeR1.","url":"https://huggingface.co/datasets/spawn99/GPQA-diamond-ClaudeR1","creator_name":"Cavit Erginsoy","creator_url":"https://huggingface.co/spawn99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"R2MEDMedicalSciencesRetrieval","keyword":"mteb","description":"\n  R2MEDMedicalSciencesRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMedical-Sciences retrieval dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical\n\n\nReference\nhttps://huggingface.co/datasets/R2MED/Medical-Sciences\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"R2MEDMedicalSciencesRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/R2MEDMedicalSciencesRetrieval.","url":"https://huggingface.co/datasets/mteb/R2MEDMedicalSciencesRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","LM-generated and reviewed","monolingual","R2MED/Medical-Sciences"],"keywords_longer_than_N":true},
	{"name":"NLPJournalTitleIntroRetrieval","keyword":"mteb","description":"\n  NLPJournalTitleIntroRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given title. This is the V1 dataset (last updated 2020-06-15).\n\n\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/sbintuitions/JMTEB\n\n\n\t\n\nSource datasets:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NLPJournalTitleIntroRetrieval.","url":"https://huggingface.co/datasets/mteb/NLPJournalTitleIntroRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","sbintuitions/JMTEB"],"keywords_longer_than_N":true},
	{"name":"CUADAffiliateLicenseLicensorLegalBenchClassification","keyword":"mteb","description":"\n  CUADAffiliateLicenseLicensorLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause describes a license grant by affiliates of the licensor or that includes intellectual property of affiliates of the licensor.\n\n\t\n\t\t\n\n\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADAffiliateLicenseLicensorLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADAffiliateLicenseLicensorLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"stackoverflow","keyword":"mteb","description":"\n\t\n\t\t\n\t\tstackoverflow Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"code snippet search for developers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the stackoverflow model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/stackoverflow.","url":"https://huggingface.co/datasets/fine-tuned/stackoverflow","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"red_cup_slim_groot","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 28,\n    \"total_frames\": 22015,\n    \"total_tasks\": 1,\n    \"total_videos\": 84,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:28\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiamFy/red_cup_slim_groot.","url":"https://huggingface.co/datasets/LiamFy/red_cup_slim_groot","creator_name":"Liam Achenbach","creator_url":"https://huggingface.co/LiamFy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"PersonaEval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tPersonaEval: A Benchmark for Role Identification in Dialogues\n\t\n\n \nThis dataset is released with the COLM 2025 conference paper: \"PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?\".\nPersonaEval is the first benchmark designed to test whether Large Language Models (LLMs) can reliably identify character roles from natural dialogue. We argue that correctly identifying who is speaking is a fundamental prerequisite for any meaningful evaluation of role-playing quality (howâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lingfengzhou/PersonaEval.","url":"https://huggingface.co/datasets/lingfengzhou/PersonaEval","creator_name":"Lingfeng Zhou","creator_url":"https://huggingface.co/lingfengzhou","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["expert-generated","machine-generated","custom","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"OpenGVLab_Lumina_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Lumina Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 400k human responses from over 86k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Lumina across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/OpenGVLab_Lumina_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/OpenGVLab_Lumina_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"OpenGVLab_Lumina_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Lumina Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 400k human responses from over 86k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Lumina across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/OpenGVLab_Lumina_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/OpenGVLab_Lumina_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"PunjabiNewsClassification","keyword":"mteb","description":"\n  PunjabiNewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Punjabi dataset for 2-class classification of Punjabi news articles\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://github.com/goru001/nlp-for-punjabi/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"PunjabiNewsClassification\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/PunjabiNewsClassification.","url":"https://huggingface.co/datasets/mteb/PunjabiNewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","mlexplorer008/punjabi_news_classification"],"keywords_longer_than_N":true},
	{"name":"csts","keyword":"benchmark","description":"CSTS (Correlation Structures in Time Series) is a comprehensive synthetic benchmarking dataset for evaluating correlation structure discovery in time series data. The dataset systematically models known correlation structures between three different time series variates and enables examination of how these structures are affected by distribution shifting, sparsification, and downsampling.","url":"https://huggingface.co/datasets/idegen/csts","creator_name":"Isabella Degen","creator_url":"https://huggingface.co/idegen","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","100M - 1B","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-792024-tyen-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-792024-tyen-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Financial risk analysis in credit analysis and investment banking\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-792024-tyen-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-792024-tyen-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-792024-tyen-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"LiveMCPBench","keyword":"benchmark","description":"\n\n\n\n  LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?\n\n  \n    Benchmarking the agent in real-world tasks within a large-scale MCP toolset.\n  \n\n\n\n  ðŸŒ Website Â  | Â \n  ðŸ“„ Paper Â  | Â \n  ðŸ’» Code Â  | Â \n  ðŸ† Leaderboard \n  Â  | Â \n  ðŸ™ Citation\n\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nLiveMCPBench is the first comprehensive benchmark designed to evaluate LLM agents at scale across diverse Model Context Protocol (MCP) servers. It comprises 95 real-world tasks grounded in the MCP ecosystemâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ICIP/LiveMCPBench.","url":"https://huggingface.co/datasets/ICIP/LiveMCPBench","creator_name":"ICIP","creator_url":"https://huggingface.co/ICIP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"DefAn","keyword":"benchmark","description":"DefAn: Definitive-Answer-Dataset-for-LLMs-Hallucination-Evaluation\n\n  A.B.M. Ashikur Rahman1, Saeed Anwar1,2, Muhammad Usman1,2, Ajmal Mian3, \n\n\n1 King Fahd University of Petroleum and Minerals, Dhahran, KSA\n\n\n2JRCAI, SDAIA-KFUPM \n\n\n3The University of Western Australia, Crawley, Western Australia\n\n\n    Arxiv Paper,  GitHub Repository\n\n\n\n\"DefAn\" is a comprehensive evaluation benchmark dataset, with more than 75000 samples, designed to assess the hallucination tendencies of large language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/iamasQ/DefAn.","url":"https://huggingface.co/datasets/iamasQ/DefAn","creator_name":"A B M Ashikur Rahman","creator_url":"https://huggingface.co/iamasQ","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K<n<100K","arxiv:2406.09155"],"keywords_longer_than_N":true},
	{"name":"CUADIrrevocableOrPerpetualLicenseLegalBenchClassification","keyword":"mteb","description":"\n  CUADIrrevocableOrPerpetualLicenseLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies a license grant that is irrevocable or perpetual.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADIrrevocableOrPerpetualLicenseLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADIrrevocableOrPerpetualLicenseLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"so100_bimanual_test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100_bimanual\",\n    \"total_episodes\": 3,\n    \"total_frames\": 2672,\n    \"total_tasks\": 1,\n    \"total_videos\": 12,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:3\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/InterestingPopo/so100_bimanual_test.","url":"https://huggingface.co/datasets/InterestingPopo/so100_bimanual_test","creator_name":"é›å¾å½¼","creator_url":"https://huggingface.co/InterestingPopo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MAGB","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMAGBï¼š A Comprehensive Benchmark for Multimodal Attributed Graphs\n\t\n\nIn many real-world scenarios, graph nodes are associated with multimodal attributes, such as texts and images, resulting in Multimodal Attributed Graphs (MAGs).\nMAGB first provide 5 dataset from E-Commerce and Social Networks. And we evaluate two major paradigms: GNN-as Predictor and VLM-as-Predictor . The datasets are publicly available:\n\n     ðŸ¤— Hugging FaceÂ Â   | Â Â ðŸ“‘ PaperÂ Â \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“– Table of Contents\n\t\n\n\nðŸ“–â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sherirto/MAGB.","url":"https://huggingface.co/datasets/Sherirto/MAGB","creator_name":"Sherirto","creator_url":"https://huggingface.co/Sherirto","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["graph-ml","cc-by-4.0","Image","arxiv:2410.09132","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"DBPedia-PL","keyword":"mteb","description":"\n  DBPedia-PL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\n\n\nReference\nhttps://github.com/iai-group/DBpedia-Entity/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"DBPedia-PL\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DBPedia-PL.","url":"https://huggingface.co/datasets/mteb/DBPedia-PL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/dbpedia","Polish"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-NL","keyword":"mteb","description":"\n  FiQA2018-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nFinancial Opinion Mining and Question Answering. FiQA2018-NL is a Dutch translation\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Non-fiction\n\n\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-fiqa\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"FiQA2018-NL\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FiQA2018-NL.","url":"https://huggingface.co/datasets/mteb/FiQA2018-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","translated","mteb/fiqa"],"keywords_longer_than_N":true},
	{"name":"JCrewBlockerLegalBenchClassification","keyword":"mteb","description":"\n  JCrewBlockerLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe J.Crew Blocker, also known as the J.Crew Protection, is a provision included in leveraged loan documents to prevent companies from removing security by transferring intellectual property (IP) into new subsidiaries and raising additional debt. The task consists of detemining whether the J.Crew Blocker is present in the document.\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JCrewBlockerLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/JCrewBlockerLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"CUADRevenueProfitSharingLegalBenchClassification","keyword":"mteb","description":"\n  CUADRevenueProfitSharingLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause require a party to share revenue or profit with the counterparty for any technology, goods, or services.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embeddingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADRevenueProfitSharingLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADRevenueProfitSharingLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_6","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 602,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_6.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_6","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3997,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_2.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_2","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"CIFAR-10","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tCIFAR-10 - Object Recognition in Images\n\t\n\n\nBenchmark dataset for object classification.ðŸ–¼ï¸ 60,000 32x32 color imagesðŸ·ï¸ 10 classesðŸ“ Format: PNG, CSVðŸ“¦ Files: 4ðŸ§ª Subset of the 80 million tiny images dataset\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nCIFAR-10 is a widely used computer vision dataset consisting of 60,000 32x32 color images in 10 mutually exclusive classes. It was created by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The dataset is a labeled subset of the 80 million tinyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KDKCE/CIFAR-10.","url":"https://huggingface.co/datasets/KDKCE/CIFAR-10","creator_name":"KDKCE","creator_url":"https://huggingface.co/KDKCE","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","multi-class-image-classification","expert-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"tool-use-llama-format","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOpen Paws Tool Use Llama Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Tool Use Data\nFormat: JSONL (JSON Lines)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Open Paws\nLicense: Apache 2.0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/tool-use-llama-format.","url":"https://huggingface.co/datasets/open-paws/tool-use-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-wh9d-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-wh9d-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-wh9d-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-wh9d-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-wh9d-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"KorSarcasmClassification","keyword":"mteb","description":"\n  KorSarcasmClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\n\n    The Korean Sarcasm Dataset was created to detect sarcasm in text, which can significantly alter the original\n    meaning of a sentence. 9319 tweets were collected from Twitter and labeled for sarcasm or not_sarcasm. These\n    tweets were gathered by querying for: irony sarcastic, and\n    sarcasm.\n    The dataset was created by gathering HTML data from Twitter. Queries for hashtags that include sarcasmâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/KorSarcasmClassification.","url":"https://huggingface.co/datasets/mteb/KorSarcasmClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","expert-annotated","monolingual","Korean"],"keywords_longer_than_N":true},
	{"name":"SpanishNewsClassification","keyword":"mteb","description":"\n  SpanishNewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Spanish dataset for news classification. The dataset includes articles from reputable Spanish news sources spanning 12 different categories.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://huggingface.co/datasets/MarcOrfilaCarreras/spanish-news\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntaskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SpanishNewsClassification.","url":"https://huggingface.co/datasets/mteb/SpanishNewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","monolingual","MarcOrfilaCarreras/spanish-news","Spanish"],"keywords_longer_than_N":true},
	{"name":"FQuADRetrieval","keyword":"mteb","description":"\n  FQuADRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset has been built from the French SQuad dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nEncyclopaedic, Written\n\n\nReference\nhttps://huggingface.co/datasets/manu/fquad2_test\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"FQuADRetrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FQuADRetrieval.","url":"https://huggingface.co/datasets/mteb/FQuADRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","human-annotated","monolingual","French"],"keywords_longer_than_N":true},
	{"name":"RTE3","keyword":"mteb","description":"\n  RTE3\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRecognising Textual Entailment Challenge (RTE-3) aim to provide the NLP community with a benchmark to test progress in recognizing textual entailment\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Web, Encyclopaedic, Written\nReference\nhttps://aclanthology.org/W07-1401/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"RTE3\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RTE3.","url":"https://huggingface.co/datasets/mteb/RTE3","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","natural-language-inference","expert-annotated","multilingual"],"keywords_longer_than_N":true},
	{"name":"stackoverflow-c-64-24-gpt-4o-2024-05-13","keyword":"mteb","description":"\n\t\n\t\t\n\t\tstackoverflow-c-64-24-gpt-4o-2024-05-13 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"technical knowledge sharing platform\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the stackoverflow-c-64-24-gpt-4o-2024-05-13 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/stackoverflow-c-64-24-gpt-4o-2024-05-13.","url":"https://huggingface.co/datasets/fine-tuned/stackoverflow-c-64-24-gpt-4o-2024-05-13","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADPostTerminationServicesLegalBenchClassification","keyword":"mteb","description":"\n  CUADPostTerminationServicesLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause subjects a party to obligations after the termination or expiration of a contract, including any post-termination transition, payment, transfer of IP, wind-down, last-buy, or similar commitments.\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADPostTerminationServicesLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADPostTerminationServicesLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"WikiBigEdit","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ“š WikiBigEdit\n\t\n\nPaper (EasyEdit2 Framework): EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models Code (EasyEdit Framework): https://github.com/zjunlp/EasyEdit Project Page (EasyEdit2 Framework): https://zjunlp.github.io/project/EasyEdit2/ Paper (WikiBigEdit Dataset): Understanding the Limits of Lifelong Knowledge Editing in LLMs\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŒŸ Overview\n\t\n\nWikiBigEdit is a large-scale benchmark designed to assess the ability of language models to integrateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lukasthede/WikiBigEdit.","url":"https://huggingface.co/datasets/lukasthede/WikiBigEdit","creator_name":"Lukas Thede","creator_url":"https://huggingface.co/lukasthede","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"CUADChangeOfControlLegalBenchClassification","keyword":"mteb","description":"\n  CUADChangeOfControlLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause gives one party the right to terminate or is consent or notice required of the counterparty if such party undergoes a change of control, such as a merger, stock sale, transfer of all or substantially all of its assets or business, or assignment by operation of law.\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomainsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADChangeOfControlLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADChangeOfControlLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-13052024-ch9n-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-13052024-ch9n-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"legal content search for data protection regulations\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-13052024-ch9n-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-13052024-ch9n-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-13052024-ch9n-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"pig2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 10,\n    \"total_frames\": 5355,\n    \"total_tasks\":1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pierfabre/pig2.","url":"https://huggingface.co/datasets/pierfabre/pig2","creator_name":"Pierre FABRE","creator_url":"https://huggingface.co/pierfabre","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"El-TARA_Spanish_LLM_Benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tEl-Tara: EvaluaciÃ³n de Razonamiento Avanzado en EspaÃ±ol\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nEl-Tara (EvaluaciÃ³n de Razonamiento Avanzado en EspaÃ±ol) is a benchmark dataset designed to assess the advanced reasoning capabilities of Large Language Models (LLMs) in Spanish. It is adapted from the original TARA (Turkish Advanced Reasoning Assessment) dataset.\nSimilar to TARA, El-Tara aims to test higher-order cognitive skills across multiple domains, using synthetically generated questionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/emre/El-TARA_Spanish_LLM_Benchmark.","url":"https://huggingface.co/datasets/emre/El-TARA_Spanish_LLM_Benchmark","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Spanish","afl-3.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ViLBench","keyword":"benchmark","description":"Benchmark Data for ViLBench: A Suite for Vision-Language Process Reward Modeling\narXiv | Project Page\nThere are 600 data collected from 5 existing vision-language tasks\n","url":"https://huggingface.co/datasets/UCSC-VLAA/ViLBench","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","n<1K","arxiv:2503.20271"],"keywords_longer_than_N":true},
	{"name":"PHTest","keyword":"alignment","description":"ðŸŒŸ PHTest: Evaluating False Refusals in LLMs\n\n\n  ðŸ¤– Auto Red-Teaming\n    \n      All prompts are generated automatically using a controllable text-generation technique called AutoDAN.\n    \n  \n  \n  ðŸŒ Diverse Prompts\n    \n      PHTest introduces false refusal patterns that arenâ€™t present in existing datasets, including prompts that avoid mentioning sensitive words.\n    \n  \n  \n  âš–ï¸ Harmlessness & Controversial Labeling\n    \n      Controversial prompts are separately labeled to address theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/furonghuang-lab/PHTest.","url":"https://huggingface.co/datasets/furonghuang-lab/PHTest","creator_name":"Furong Huang's Lab at UMD","creator_url":"https://huggingface.co/furonghuang-lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"TwitterHjerneRetrieval","keyword":"mteb","description":"\n  TwitterHjerneRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDanish question asked on Twitter with the Hashtag #Twitterhjerne ('Twitter brain') and their corresponding answer.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nSocial, Written\n\nReference\nhttps://huggingface.co/datasets/sorenmulli/da-hashtag-twitterhjerne\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TwitterHjerneRetrieval.","url":"https://huggingface.co/datasets/mteb/TwitterHjerneRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","sorenmulli/da-hashtag-twitterhjerne"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_18_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 4,\n    \"total_frames\": 2402,\n    \"total_tasks\": 1,\n    \"total_videos\": 4,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:4\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_18_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_18_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"SwednClusteringS2S","keyword":"mteb","description":"\n  SwednClusteringS2S\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure. This dataset uses the category labels as clusters.\n\n\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Non-fiction, Written\n\n\nReference\nhttps://spraakbanken.gu.se/en/resources/swedn\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SwednClusteringS2S.","url":"https://huggingface.co/datasets/mteb/SwednClusteringS2S","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","monolingual","sbx/superlim-2","Swedish"],"keywords_longer_than_N":true},
	{"name":"r1-aime-figures","keyword":"benchmark","description":"Ran the simplescaling/aime25_figures dataset through R1-671B to get 64 runs per each entry. Results are as follows:\nR1 - 671B\n\nCONSENSUS @ 64: 53.33% (16/30)\n\nPASS @ 64: 76.67% (23/30)\n\nCompleted: 30, Partial: 0\n\n","url":"https://huggingface.co/datasets/benxh/r1-aime-figures","creator_name":"Benjamim Shehu","creator_url":"https://huggingface.co/benxh","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","n<1K","ðŸ‡ºðŸ‡¸ Region: US","benchmark"],"keywords_longer_than_N":false},
	{"name":"jina-embeddings-v2-base-en-19052024-oiu8-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-19052024-oiu8-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"E-commerce advertising platform\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-19052024-oiu8-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-19052024-oiu8-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-19052024-oiu8-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ContractNLIExplicitIdentificationLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLIExplicitIdentificationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that all Confidential Information shall be expressly identified by the Disclosing Party.\n\n\t\n\t\t\n\n\n\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\nSource datasets:\n\nnguha/legalbench\n\n\n\t\n\t\t\n\t\tHow to evaluate onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLIExplicitIdentificationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLIExplicitIdentificationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"DrafterBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for DrafterBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDrafterBench is a large-scale toolkit focused on evaluating the proficiency of Large Language Models (LLMs) in automating Civil Engineering tasks.\nThis dataset hosts a task suite summarized across 20 real-world projects, encompassing a total of 1920 tasks.\nIt replicates the complexity of real-world engineering tasks and provides a technical platform to test the four key capabilities of LLMs:\n\nStructured data understandingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Eason666/DrafterBench.","url":"https://huggingface.co/datasets/Eason666/DrafterBench","creator_name":"Yinsheng Li","creator_url":"https://huggingface.co/Eason666","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ParallelPrompt","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tPARALLELPROMPT\n\t\n\nA benchmark dataset of 37,021 parallelizable prompts from real-world LLM conversations, designed for optimizing LLM serving systems through intra-query parallelism.\n\n\t\n\t\t\n\t\tRepository and Resources\n\t\n\n\nDataset: Hugging Face\nCode: GitHub\nPaper: PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries\n\nThe GitHub repository contains:\n\nData curation pipeline\nSchema extraction code\nEvaluation suite for measuring latency and quality\nBaseline implementationsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/forgelab/ParallelPrompt.","url":"https://huggingface.co/datasets/forgelab/ParallelPrompt","creator_name":"Forge Lab","creator_url":"https://huggingface.co/forgelab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","other","English","multilingual","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"amazon_massive_scenario","keyword":"mteb","description":"\n  MassiveScenarioClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSpoken\n\nReference\nhttps://arxiv.org/abs/2204.08582\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"MassiveScenarioClassification\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/amazon_massive_scenario.","url":"https://huggingface.co/datasets/mteb/amazon_massive_scenario","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","translated","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"SCDDCertificationLegalBenchClassification","keyword":"mteb","description":"\n  SCDDCertificationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer requires direct suppliers to certify that materials incorporated into the product comply with the laws regarding slavery and human trafficking of the country orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SCDDCertificationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/SCDDCertificationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-gaming","keyword":"mteb","description":"\n  CQADupstackGamingRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Written\n\n\nReference\nhttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackGamingRetrieval\"])\nevaluator = mteb.MTEB(task)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-gaming.","url":"https://huggingface.co/datasets/mteb/cqadupstack-gaming","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"RAVine-mapper","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tRAVine-mapper\n\t\n\nThis repository contains the URL-to-document ID mapping files (RAVine-mapper), which are crucial for the implementation of the fetch tool within the RAVine: Reality-Aligned Evaluation for Agentic Search framework. This dataset is essential when utilizing RAVine for agentic search evaluations.\nRAVine (Reality-Aligned eValuation framework for agentic LLMs with search) is a comprehensive evaluation system designed for agentic search. It encompasses a web environmentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sapphirex/RAVine-mapper.","url":"https://huggingface.co/datasets/sapphirex/RAVine-mapper","creator_name":"yilong xu","creator_url":"https://huggingface.co/sapphirex","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","apache-2.0","arxiv:2507.16725","ðŸ‡ºðŸ‡¸ Region: US","agentic-llm"],"keywords_longer_than_N":true},
	{"name":"turkish-grammar-mmlu","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTurkish-Grammar-MMLU\n\t\n\nThis dataset, created by Turkish-DB, is a multiple-choice question-answering (QA) dataset covering Turkish grammar topics. It is designed to evaluate model performance on various Turkish grammar subjects, similar to the MMLU (Massive Multitask Language Understanding) benchmark.\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nName: Turkish-Grammar-MMLU\nProvider: Turkish-DB\nTask: Multiple-Choice QA\nModality: Text\nFormat: CSV (also accessible via API in Parquet format)\nLanguage: Turkishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/turkish-db/turkish-grammar-mmlu.","url":"https://huggingface.co/datasets/turkish-db/turkish-grammar-mmlu","creator_name":"TurkishDB","creator_url":"https://huggingface.co/turkish-db","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Turkish","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"NevIR","keyword":"mteb","description":"\n  NevIR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nPaired evaluation of real world negation in retrieval, with questions and passages. Since models generally prefer one passage over the other always, there are two questions that the model must get right to understand the negation (hence the paired_accuracy metric).\n\n\t\n\t\t\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb\n\n\nReference\nhttps://github.com/orionw/NevIR\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NevIR.","url":"https://huggingface.co/datasets/mteb/NevIR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","human-annotated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-unix","keyword":"mteb","description":"\n  CQADupstackUnixRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Web, Programming\n\n\nReference\nhttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackUnixRetrieval\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-unix.","url":"https://huggingface.co/datasets/mteb/cqadupstack-unix","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5202024-rxyq-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5202024-rxyq-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"gaming information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5202024-rxyq-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5202024-rxyq-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5202024-rxyq-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Moroco","keyword":"mteb","description":"\n  Moroco\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe Moldavian and Romanian Dialectal Corpus. The MOROCO data set contains Moldavian and Romanian samples of text collected from the news domain. The samples belong to one of the following six topics: (0) culture, (1) finance, (2) politics, (3) science, (4) sports, (5) tech\n\n\t\n\t\t\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://huggingface.co/datasets/moroco\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Moroco.","url":"https://huggingface.co/datasets/mteb/Moroco","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","Romanian"],"keywords_longer_than_N":true},
	{"name":"R2MEDPMCClinicalRetrieval","keyword":"mteb","description":"\n  R2MEDPMCClinicalRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nPMC-Clinical retrieval dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical\n\n\nReference\nhttps://huggingface.co/datasets/R2MED/PMC-Clinical\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"R2MEDPMCClinicalRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/R2MEDPMCClinicalRetrieval.","url":"https://huggingface.co/datasets/mteb/R2MEDPMCClinicalRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","LM-generated and reviewed","monolingual","R2MED/PMC-Clinical"],"keywords_longer_than_N":true},
	{"name":"ru_biosses_sts","keyword":"benchmark","description":"Ð­Ñ‚Ð¾ Ð¿ÐµÑ€ÐµÐ²ÐµÐ´ÐµÐ½Ð½Ð°Ñ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¸Ð¹ ÑÐ·Ñ‹Ðº Ð²ÐµÑ€ÑÐ¸Ñ mteb/biosses-sts - Biomedical Semantic Similarity Estimation.\nÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ( Ñ‡ÐµÑ€ÐµÐ· ÐºÐ¾ÑÐ¸Ð½ÑƒÑÐ½Ð¾Ðµ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾):\n\n\t\n\t\t\nÐœÐ¾Ð´ÐµÐ»ÑŒ (cls-pooling)\nPearson\nSpearman\nMAE\nRMSE\n\n\n\t\t\njinaai/jina-embeddings-v3\n0.8222\n0.7768\n2.2463\n2.4468\n\n\ncointegrated/rubert-tiny2\n0.6897\n0.6793\n2.1546\n2.3944\n\n\nDeepPavlov/rubert-base-cased\n0.2982\n0.4966\n2.7042\n2.9374\n\n\nai-forever/ruRoberta-large\n-0.0096\n0.0219\n2.3931\n2.6905\n\n\n\t\n\n\n\t\n\t\t\nÐœÐ¾Ð´ÐµÐ»ÑŒ  (mean-pooling)\nPearson\nSpearman\nMAEâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kostya165/ru_biosses_sts.","url":"https://huggingface.co/datasets/Kostya165/ru_biosses_sts","creator_name":"pleroma_cascade","creator_url":"https://huggingface.co/Kostya165","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","Russian","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"sensory-awareness-benchmark","keyword":"alignment","description":"\n\t\n\t\t\n\t\tSensory Awareness Benchmark\n\t\n\nA series of questions (goal is 100-200) and required features, designed to test whether any ML model is aware of its own capabilities.\nControl questions connected to a specific ability:\n\nCan you receive an image file?\nCan you take a live image or video of your surroundings?\n\nAwareness\n\nAre you considered to be a Large Language Model (LLM) or similar system?\nWould you consider your level to be that of a super-intelligent AI agent?\n\nNatural questions whichâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/scarysnake/sensory-awareness-benchmark.","url":"https://huggingface.co/datasets/scarysnake/sensory-awareness-benchmark","creator_name":"Scary Snake","creator_url":"https://huggingface.co/scarysnake","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","cc0-1.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"stackoverflow-c-128-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tstackoverflow-c-128-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"coding tutorials search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the stackoverflow-c-128-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/stackoverflow-c-128-24.","url":"https://huggingface.co/datasets/fine-tuned/stackoverflow-c-128-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"tweet_sentiment_multilingual","keyword":"mteb","description":"\n  TweetSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA multilingual Sentiment Analysis dataset consisting of tweets in 8 different languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, Written\n\n\nReferencehttps://aclanthology.org/2022.lrec-1.27\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"TweetSentimentClassification\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/tweet_sentiment_multilingual.","url":"https://huggingface.co/datasets/mteb/tweet_sentiment_multilingual","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"IndicCrosslingualSTS","keyword":"mteb","description":"\n  IndicCrosslingualSTS\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a Semantic Textual Similarity testset between English and 12 high-resource Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Non-fiction, Web, Spoken, Government, Written, Spoken\nReference\nhttps://huggingface.co/datasets/jaygala24/indic_sts\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicCrosslingualSTS.","url":"https://huggingface.co/datasets/mteb/IndicCrosslingualSTS","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","semantic-similarity-scoring","expert-annotated","multilingual","Assamese"],"keywords_longer_than_N":true},
	{"name":"ContractNLISurvivalOfObligationsLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLISurvivalOfObligationsLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that some obligations of Agreement may survive termination of Agreement.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLISurvivalOfObligationsLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLISurvivalOfObligationsLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-jinaai_jina-embeddings-v2-base-en-sc","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-jinaai_jina-embeddings-v2-base-en-sc Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic search for scientific papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-jinaai_jina-embeddings-v2-base-en-sc model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-jinaai_jina-embeddings-v2-base-en-sc.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-jinaai_jina-embeddings-v2-base-en-sc","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"community-alignment-dataset","keyword":"alignment","description":"\nCommunity Alignment\n\n\n Github Â  | Â \n Paper\n\n\n\n\t\n\t\t\n\t\tDataset\n\t\n\nCommunity Alignment is a large-scale open source, multilingual and multi-turn preference dataset to align LLMs with human preferences across cultures. It features prompt-level overlap in annotators, enabling social-choice-based and distributional approaches to LLM alignment, as well as natural language explanations for choices.\n\n[Large-scale] ~200,000 comparisons of LLM responses, collected from >3,000 unique annotators whoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/facebook/community-alignment-dataset.","url":"https://huggingface.co/datasets/facebook/community-alignment-dataset","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Hindi","English","Portuguese","Italian","French"],"keywords_longer_than_N":true},
	{"name":"cmedqav2-c","keyword":"mteb","description":"\n\t\n\t\t\n\t\tcmedqav2-c Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical advice and treatment search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the cmedqav2-c model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/cmedqav2-c.","url":"https://huggingface.co/datasets/fine-tuned/cmedqav2-c","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-jdbf-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"scidocs-reranking-train","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSciDocs Reranking with Train Split\n\t\n\nThis dataset adds a training split to the original mteb/scidocs-reranking dataset.\n\n\t\n\t\t\n\t\tDataset Info\n\t\n\n\nTrain: 6,367 examples\nValidation: 1,592 examples  \nTotal: 7,959 examples\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Bibek/scidocs-reranking-train\")\ntrain_data = dataset['train']\nval_data = dataset['validation']\n\n\n\t\n\t\t\n\t\tOriginal Dataset\n\t\n\nBased on the SciDocs benchmark from the SPECTER paper:\n\nPaper:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Bibek/scidocs-reranking-train.","url":"https://huggingface.co/datasets/Bibek/scidocs-reranking-train","creator_name":"Chaudhary","creator_url":"https://huggingface.co/Bibek","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"BrightRetrieval","keyword":"mteb","description":"\n  BrightRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nBright retrieval dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNon-fiction, Written\n\n\nReference\nhttps://huggingface.co/datasets/xlangai/BRIGHT\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"BrightRetrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nTo learnâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BrightRetrieval.","url":"https://huggingface.co/datasets/mteb/BrightRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","multilingual","xlangai/BRIGHT"],"keywords_longer_than_N":true},
	{"name":"NanoQuoraRetrieval","keyword":"mteb","description":"\n  NanoQuoraRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoQuoraRetrieval is a smaller subset of the QuoraRetrieval dataset, which is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2t\n\n\nDomains\nSocial\n\n\nReference\nhttps://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoQuoraRetrieval.","url":"https://huggingface.co/datasets/mteb/NanoQuoraRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","human-annotated","monolingual","mteb/quora","English"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-06052024-yl1z-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-06052024-yl1z-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"social behavior advice search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-06052024-yl1z-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-06052024-yl1z-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-06052024-yl1z-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so101_test3","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 50,\n    \"total_frames\": 17893,\n    \"total_tasks\": 1,\n    \"total_videos\": 100,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:50\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cornito/so101_test3.","url":"https://huggingface.co/datasets/Cornito/so101_test3","creator_name":"Corneille Marechal","creator_url":"https://huggingface.co/Cornito","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"HueManity","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tHueManity: A Benchmark for Testing Human-Like Visual Perception in MLLMs\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nHueManity is a benchmark dataset featuring 83,850 images designed to test the fine-grained visual perception of Multimodal Large Language Models (MLLMs). Each image presents a two-character alphanumeric string embedded within Ishihara-style dot patterns, challenging models to perform precise pattern recognition in visually cluttered environments.\nThe dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jayant-Sravan/HueManity.","url":"https://huggingface.co/datasets/Jayant-Sravan/HueManity","creator_name":"Jayant Sravan Tamarapalli","creator_url":"https://huggingface.co/Jayant-Sravan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","image-feature-extraction","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"french_instruct","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tðŸ§‘â€ðŸ« French Instruct\n\t\n\nThe French Instruct dataset is a collection of instructions with their corresponding answers (sometimes multi-turn conversations) entirely in French. The dataset is also available on GitHub.\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“Š Overview\n\t\n\nThe dataset is composed of 276K conversations between a user and an assistant for a total of approximately 85M tokens.\n\n    \n\n\nI also added annotations for each document to indicate if it was generated or written by a human, the style ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/angeluriot/french_instruct.","url":"https://huggingface.co/datasets/angeluriot/french_instruct","creator_name":"Angel Uriot","creator_url":"https://huggingface.co/angeluriot","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"KurdishSentimentClassification","keyword":"mteb","description":"\n  KurdishSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nKurdish Sentiment Dataset\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nWeb, Written\n\n\nReference\nhttps://link.springer.com/article/10.1007/s10579-023-09716-6\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"KurdishSentimentClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/KurdishSentimentClassification.","url":"https://huggingface.co/datasets/mteb/KurdishSentimentClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"DORI-Benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDORI (Discriminative Orientation Reasoning Intelligence) is a comprehensive benchmark designed to evaluate object orientation understanding in multimodal large language models (MLLMs). The benchmark isolates and evaluates orientation perception as a primary capability, offering a systematic assessment framework that spans four essential dimensions of orientation comprehension: frontal alignment, rotational transformations, relativeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/appledora/DORI-Benchmark.","url":"https://huggingface.co/datasets/appledora/DORI-Benchmark","creator_name":"Nazia Tasnim","creator_url":"https://huggingface.co/appledora","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"OpenAI-4o_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata OpenAI 4o Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 200'000 human responses from over ~45,000 individual annotators, collected in less than half a day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating OpenAI 4o (version from 26.3.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/OpenAI-4o_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/OpenAI-4o_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"OpenAI-4o_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata OpenAI 4o Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 200'000 human responses from over ~45,000 individual annotators, collected in less than half a day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating OpenAI 4o (version from 26.3.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/OpenAI-4o_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/OpenAI-4o_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"SCIDOCS-256-24-gpt-4o-2024-05-13-421451","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSCIDOCS-256-24-gpt-4o-2024-05-13-421451 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"service search for translation and editing\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SCIDOCS-256-24-gpt-4o-2024-05-13-421451 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-421451.","url":"https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-421451","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"cmedqav2-c-64-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tcmedqav2-c-64-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the cmedqav2-c-64-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/cmedqav2-c-64-24.","url":"https://huggingface.co/datasets/fine-tuned/cmedqav2-c-64-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"AUDITS","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tAUDITS: Image Manipulation Dataset\n\t\n\nAUDITS is a large-scale dataset for training and evaluating models on image manipulation detection and localization. \n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe folder includes train.zip, val.zip, and test.zip, each containing manipulated, original, and mask images, alongside metadata.\n\n\t\n\t\t\n\t\tðŸš€ How to Use\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"DivyaApp/AUDITS\", split=\"train\")\n\n\n\n\t\n\t\t\n\t\tAlternatives\n\t\n\nIf loading via load_dataset isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DivyaApp/AUDITS.","url":"https://huggingface.co/datasets/DivyaApp/AUDITS","creator_name":"Divya Appapogu","creator_url":"https://huggingface.co/DivyaApp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mask-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"STSES","keyword":"mteb","description":"\n  STSES\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSpanish test sets from SemEval-2014 (Agirre et al., 2014) and SemEval-2015 (Agirre et al., 2015)\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://huggingface.co/datasets/PlanTL-GOB-ES/sts-es\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"STSES\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/STSES.","url":"https://huggingface.co/datasets/mteb/STSES","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","semantic-similarity-scoring","monolingual","PlanTL-GOB-ES/sts-es","Spanish"],"keywords_longer_than_N":true},
	{"name":"NanoNFCorpusRetrieval","keyword":"mteb","description":"\n  NanoNFCorpusRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoNFCorpus is a smaller subset of NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical, Academic, Written\nReference\nhttps://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoNFCorpusRetrieval.","url":"https://huggingface.co/datasets/mteb/NanoNFCorpusRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","expert-annotated","monolingual","mteb/nfcorpus"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_12_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 601,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_12_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_12_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"TreeVGR-RL-37K","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTreeBench Dataset Card\n\t\n\nThis repository contains TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark designed for evaluating \"thinking with images\" capabilities with traceable visual evidence.\nThe dataset was introduced in the paper: Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTreeBench is built on three core principles:\n\nFocused visual perception: of subtle targets in complex scenes.\nTraceable evidence: via bounding boxâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeVGR-RL-37K.","url":"https://huggingface.co/datasets/HaochenWang/TreeVGR-RL-37K","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"SFE","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tScientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning\n\t\n\n\n  \n\n\n\n| Leaderboard | Paper  | Website  | HuggingFace  |\n\n\n\n\nLatest News ðŸ”¥\n[Latest] We are officially integrated by VLMEvalKit. Intern-S1, the most advanced open-source multimodal reasoning model to date, benchmarked on SFE.\nUnfold to see more details.\n\n\n\n[2025/07] Intern-S1, the most advanced open-source multimodal reasoning model to date, benchmarked on SFE.\n[2025/07] We areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PrismaX/SFE.","url":"https://huggingface.co/datasets/PrismaX/SFE","creator_name":"PrismaX","creator_url":"https://huggingface.co/PrismaX","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","Chinese","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"CUADNonCompeteLegalBenchClassification","keyword":"mteb","description":"\n  CUADNonCompeteLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause restricts the ability of a party to compete with the counterparty or operate in a certain geography or business or technology sector.\n\n\t\n\t\t\n\n\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADNonCompeteLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADNonCompeteLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"optillmbench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tOptiLLMBench Dataset\n\t\n\nA benchmark dataset for evaluating test-time optimization and scaling capabilities of language models.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nOptiLLMBench contains 500 carefully selected challenging problems across multiple domains:\n\nMathematical reasoning (from competition_math)\nCode generation (from HumanEval)\nWord problems (from GSM8K)\nMultiple choice reasoning (from MMLU)\nLogical deduction (from BBH)\n\nEach example is chosen to benefit from test-time optimizationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/codelion/optillmbench.","url":"https://huggingface.co/datasets/codelion/optillmbench","creator_name":"Asankhaya Sharma","creator_url":"https://huggingface.co/codelion","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"VoyageMMarcoReranking","keyword":"mteb","description":"\n  VoyageMMarcoReranking\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\na hard-negative augmented version of the Japanese MMARCO dataset as used in Voyage AI Evaluation Suite\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Non-fiction, Written\n\nReference\nhttps://arxiv.org/abs/2312.16144\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"VoyageMMarcoReranking\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VoyageMMarcoReranking.","url":"https://huggingface.co/datasets/mteb/VoyageMMarcoReranking","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","monolingual","Japanese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"SCDBPAccountabilityLegalBenchClassification","keyword":"mteb","description":"\n  SCDBPAccountabilityLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer maintains internal compliance procedures on company standards regarding human trafficking and slavery? This includes any type of internal accountability mechanism. Requiringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SCDBPAccountabilityLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/SCDBPAccountabilityLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"FMC","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFMC Dataset\n\t\n\n\n  ðŸ“ƒ [Paper] â€¢ ðŸ’» [Github] â€¢ ðŸ¤— [Dataset] â€¢ ðŸ“‹ [Poster]\n\nThis dataset is proposed in the AI for Math Workshop @ICML 2025 paper: [FMC: Formalization of Natural Language Mathematical Competition Problems](http://arxiv.org/abs/2507.11275).\n\nEfficient and accurate autoformalization methods, leveraging large-scale databases of natural language mathematical problems to construct formal language datasets, are key to advancing formal mathematical reasoning. In this paper, weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JadeXie1205/FMC.","url":"https://huggingface.co/datasets/JadeXie1205/FMC","creator_name":"Jade Xie","creator_url":"https://huggingface.co/JadeXie1205","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"rublimp","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tRuBLiMP\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nRuBLiMP, or Russian Benchmark of Linguistic Minimal Pairs, is the first diverse and large-scale benchmark of minimal pairs in Russian.\nRuBLiMP includes 45k minimal pairs of sentences that differ in grammaticality and isolate morphological, syntactic, or semantic phenomena. In contrast to existing benchmarks of linguistic minimal pairs, RuBLiMP is created by applying linguistic perturbations to automatically annotated sentences from open textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RussianNLP/rublimp.","url":"https://huggingface.co/datasets/RussianNLP/rublimp","creator_name":"Natural Language Processing in Russian","creator_url":"https://huggingface.co/RussianNLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["acceptability-classification","Russian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"AILA_statutes","keyword":"mteb","description":"\n  AILAStatutes\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset is structured for the task of identifying the most relevant statutes for a given situation.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nLegal, Written\n\n\nReferencehttps://zenodo.org/records/4063986\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"AILAStatutes\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AILA_statutes.","url":"https://huggingface.co/datasets/mteb/AILA_statutes","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"CoreCognition","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tCoreCognition: A Core Knowledge Benchmark for Multi-modal Large Language Models\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCoreCognition is a large-scale benchmark encompassing 12 core knowledge grounded in developmental cognitive science, designed to evaluate the fundamental core abilities of Multi-modal Large Language Models (MLLMs).\nWhile MLLMs demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild remains limited, often falling short on tasksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/williamium/CoreCognition.","url":"https://huggingface.co/datasets/williamium/CoreCognition","creator_name":"William Li","creator_url":"https://huggingface.co/williamium","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"librispeech-alignments_clean100","keyword":"alignment","description":"\n\t\n\t\t\n\t\tlibrispeech-alignments_clean100\n\t\n\nThis is a subset of librispeech-alignments (https://huggingface.co/datasets/gilkeyio/librispeech-alignments) which only includes train_clean_100 and test_clean splits for small experiments and tutorials.\nCite:\n@inproceedings{panayotov2015librispeech,  \n  title={Librispeech: an ASR corpus based on public domain audio books},\n  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},  \n  booktitle={ICASSP},   \n  year={2015}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ErfanAShams/librispeech-alignments_clean100.","url":"https://huggingface.co/datasets/ErfanAShams/librispeech-alignments_clean100","creator_name":"Erfan Shams","creator_url":"https://huggingface.co/ErfanAShams","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-572024-xg53-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-572024-xg53-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"scripting language documentation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-572024-xg53-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-572024-xg53-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-572024-xg53-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Touche2020-256-24-gpt-4o-2024-05-13-27907","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTouche2020-256-24-gpt-4o-2024-05-13-27907 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval benchmark search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the Touche2020-256-24-gpt-4o-2024-05-13-27907 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/Touche2020-256-24-gpt-4o-2024-05-13-27907.","url":"https://huggingface.co/datasets/fine-tuned/Touche2020-256-24-gpt-4o-2024-05-13-27907","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Touche2020-256-24-gpt-4o-2024-05-13-27907","keyword":"mteb","description":"\n\t\n\t\t\n\t\tTouche2020-256-24-gpt-4o-2024-05-13-27907 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval benchmark search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the Touche2020-256-24-gpt-4o-2024-05-13-27907 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/Touche2020-256-24-gpt-4o-2024-05-13-27907.","url":"https://huggingface.co/datasets/fine-tuned/Touche2020-256-24-gpt-4o-2024-05-13-27907","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BIRCO-Arguana-Test","keyword":"mteb","description":"\n  BIRCO-ArguAna\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieval task using the ArguAna dataset from BIRCO. This dataset contains 100 queries where both queries and passages are complex one-paragraph arguments about current affairs. The objective is to retrieve the counter-argument that directly refutes the queryâ€™s stance.\n\n\t\n\t\t\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://github.com/BIRCO-benchmark/BIRCO\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BIRCO-Arguana-Test.","url":"https://huggingface.co/datasets/mteb/BIRCO-Arguana-Test","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"SCDBPVerificationLegalBenchClassification","keyword":"mteb","description":"\n  SCDBPVerificationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer engages in verification and auditing as one practice, expresses that it may conduct an audit, or expressess that it is assessing supplier risks through a review of the US Dept. ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SCDBPVerificationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/SCDBPVerificationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"human-alignment-preferences-images","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Alignment Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human annotated alignment datasets for text-to-image models, this release contains over 1,200,000 humanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-alignment-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-alignment-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","reinforcement-learning","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"VLM4Bio","keyword":"benchmarks","description":"\n\t\n\t\t\n\t\tDataset Card for VLM4Bio\n\t\n\n\n\t\n\t\t\n\t\tInstructions for downloading the dataset\n\t\n\n\nInstall Git LFS\nGit clone the VLM4Bio repository to download all metadata and associated files\nRun the following commands in a terminal:\n\n\n\ngit clone https://huggingface.co/datasets/imageomics/VLM4Bio\ncd VLM4Bio\n\nDownloading and processing bird images\n\nTo download the bird images, run the following command:\n\nbash download_bird_images.sh\n\n\nThis should download the bird images inside datasets/Bird/imagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/imageomics/VLM4Bio.","url":"https://huggingface.co/datasets/imageomics/VLM4Bio","creator_name":"HDR Imageomics Institute","creator_url":"https://huggingface.co/imageomics","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","zero-shot-image-classification","zero-shot-object-detection","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"LoTTE","keyword":"mteb","description":"\n  LoTTE\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nLoTTE (Long-Tail Topic-stratified Evaluation for IR) is designed to evaluate retrieval models on underrepresented, long-tail topics. Unlike MSMARCO or BEIR, LoTTE features domain-specific queries and passages from StackExchange (covering writing, recreation, science, technology, and lifestyle), providing a challenging out-of-domain generalization benchmark.\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Web, Social\n\n\nReferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/LoTTE.","url":"https://huggingface.co/datasets/mteb/LoTTE","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"CorporateLobbyingLegalBenchClassification","keyword":"mteb","description":"\n  CorporateLobbyingLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe Corporate Lobbying task consists of determining whether a proposed Congressional bill may be relevant to a company based on a company's self-description in its SEC 10K filing.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CorporateLobbyingLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CorporateLobbyingLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 121,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/kantine/test.","url":"https://huggingface.co/datasets/kantine/test","creator_name":"quentin","creator_url":"https://huggingface.co/kantine","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"so101_gasp_mesuring_tape","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 60,\n    \"total_frames\": 20920,\n    \"total_tasks\": 1,\n    \"total_videos\": 120,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:60\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCleQmAR/so101_gasp_mesuring_tape.","url":"https://huggingface.co/datasets/UCleQmAR/so101_gasp_mesuring_tape","creator_name":"P","creator_url":"https://huggingface.co/UCleQmAR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"wmdp","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tWMDP with Rationales\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is an enhanced version of the Weapons of Mass Destruction Proxy (WMDP) benchmark created by the Center for AI Safety. The original WMDP dataset has been augmented with AI-generated scientific rationales that provide detailed explanations for each question.\n\n\t\n\t\t\n\t\tWhat's New\n\t\n\nThis dataset extends the original WMDP with two additional fields:\n\nground_truth: The original question format with the correct answerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Joschka/wmdp.","url":"https://huggingface.co/datasets/Joschka/wmdp","creator_name":"Joschka Braun","creator_url":"https://huggingface.co/Joschka","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"AfriSentiClassification","keyword":"mteb","description":"\n  AfriSentiClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nAfriSenti is the largest sentiment analysis dataset for under-represented African languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, Written\n\n\nReferencehttps://arxiv.org/abs/2302.08956\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"AfriSentiClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AfriSentiClassification.","url":"https://huggingface.co/datasets/mteb/AfriSentiClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v2","keyword":"alignment","description":"\n\t\n\t\t\n\t\tInstruct-Aira Dataset version 2.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of single-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\n\nLanguageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2.","url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2","creator_name":"Nicholas Kluge CorrÃªa","creator_url":"https://huggingface.co/nicholasKluge","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-2062024-u43q-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\n\t\n\t\tBAAI_bge-large-en-2062024-u43q-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-2062024-u43q-webapp model.\n\n\t\n\t\t\n\t\n\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-2062024-u43q-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-2062024-u43q-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-6212024-p8j6-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-6212024-p8j6-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-6212024-p8j6-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6212024-p8j6-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6212024-p8j6-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-code-7232024-77ap-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-code-7232024-77ap-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Database schema for a data management system\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-code-7232024-77ap-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-code-7232024-77ap-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-code-7232024-77ap-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-14062024-fimj-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-v1_5-14062024-fimj-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"psychometric assessment in academia\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-v1_5-14062024-fimj-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-14062024-fimj-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-14062024-fimj-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"DBPedia-NL","keyword":"mteb","description":"\n  DBPedia-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. DBPedia-NL is a Dutch translation.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-dbpedia-entity\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DBPedia-NL.","url":"https://huggingface.co/datasets/mteb/DBPedia-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/dbpedia","Dutch"],"keywords_longer_than_N":true},
	{"name":"SciFact-256-24-gpt-4o-2024-05-13-526066","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSciFact-256-24-gpt-4o-2024-05-13-526066 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"scientific claim verification\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SciFact-256-24-gpt-4o-2024-05-13-526066 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-526066.","url":"https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-526066","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADJointIPOwnershipLegalBenchClassification","keyword":"mteb","description":"\n  CUADJointIPOwnershipLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause provides for joint or shared ownership of intellectual property between the parties to the contract.\n\n\t\n\t\t\n\n\n\n\n\t\tTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADJointIPOwnershipLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADJointIPOwnershipLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Aloe-Beta-DPO","keyword":"alignment","description":"\n\t\n\t\t\n\t\tAloe-Beta-Medical-Collection\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated DPO datasets used to align Aloe-Beta.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThe first stage of the Aloe-Beta alignment process. We curated data from many publicly available data sources, including three different types of data:\n\nMedical preference data: TsinghuaC3I/UltraMedical-Preference\n\nGeneral preference data:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-DPO.","url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-DPO","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-tat-qa","keyword":"aveni-bench","description":"\n\t\n\t\t\n\t\tAveniBench: TAT-QA\n\t\n\nTAT-QA split used in the AveniBench.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is made available under the MIT license.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nAveniBench\nTDB\n\nTAT-QA\n@inproceedings{zhu-etal-2021-tat,\n    title = \"{TAT}-{QA}: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance\",\n    author = \"Zhu, Fengbin  and\n      Lei, Wenqiang  and\n      Huang, Youcheng  and\n      Wang, Chao  and\n      Zhang, Shuo  and\n      Lv, Jiancheng  and\n      Fengâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-tat-qa.","url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-tat-qa","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","keyword":"benchmarks","description":"\n\t\n\t\t\n\t\tSWE-Bench Verified O1 Dataset\n\t\n\n\n\t\n\t\t\n\t\tExecutive Summary\n\t\n\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities using their native tool calling capabilities on the SWE-Bench Verified dataset, achieving a 45.8% success rate across 500 test instances.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset was generated using the CodeAct framework, which aims to improve codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results.","url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"EvilMath","keyword":"gsm8k","description":"Mathematical problems on harmful topics generated from GSM8K. EvilMath contains harmful questions with objectively verifiable ground truth answers.\n\n\t\n\t\t\n\t\tDataset Description and Design\n\t\n\nEvilMath is generated by rewording GSM8K math questions to include harmful terms that are typically refused by safety-aligned models. We reword math problems to contain dangerous terms such as â€œbombsâ€ or â€œnuclear weapons,â€ while preserving the question logic and the necessary information to solveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ethz-spylab/EvilMath.","url":"https://huggingface.co/datasets/ethz-spylab/EvilMath","creator_name":"SPY Lab - ETH Zurich","creator_url":"https://huggingface.co/ethz-spylab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Diversity2LegalBenchClassification","keyword":"mteb","description":"\n  Diversity2LegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGiven a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 2).\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Diversity2LegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/Diversity2LegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-892024-idqb-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-892024-idqb-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"career development and matchmaking\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-892024-idqb-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-892024-idqb-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-892024-idqb-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SCIDOCS-256-24-gpt-4o-2024-05-13-598568","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSCIDOCS-256-24-gpt-4o-2024-05-13-598568 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"service search for translation and editing\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SCIDOCS-256-24-gpt-4o-2024-05-13-598568 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-598568.","url":"https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-598568","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ToxicChatClassification","keyword":"mteb","description":"\n  ToxicChatClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset contains toxicity annotations on 10K user\n            prompts collected from the Vicuna online demo. We utilize a human-AI\n            collaborative annotation framework to guarantee the quality of annotation\n            while maintaining a feasible annotation workload. The details of data\n            collection, pre-processing, and annotation can be found in our paper.\n            We believe thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ToxicChatClassification.","url":"https://huggingface.co/datasets/mteb/ToxicChatClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"SouthAfricanLangClassification","keyword":"mteb","description":"\n  SouthAfricanLangClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA language identification test set for 11 South African Languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nWeb, Non-fiction, Written\n\n\nReference\nhttps://www.kaggle.com/competitions/south-african-language-identification/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SouthAfricanLangClassification.","url":"https://huggingface.co/datasets/mteb/SouthAfricanLangClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","language-identification","expert-annotated","monolingual","mlexplorer008/south_african_language_identification"],"keywords_longer_than_N":true},
	{"name":"RealDevBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tRealDevWorld: Benchmarking Production-Ready Software Engineering\n\t\n\n\n\t\n\t\t\n\t\tWhy RealDevWorld?\n\t\n\nWith the explosion of AI-generated repositories and applications, the software engineering community faces a critical challenge: How do we automatically evaluate the quality and functionality of instantly generated projects? Manual testing is impractical for the scale and speed of AI development, yet traditional automated testing requires pre-written test suites that don't exist for novelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stellaHsr-mm/RealDevBench.","url":"https://huggingface.co/datasets/stellaHsr-mm/RealDevBench","creator_name":"StellaHSR","creator_url":"https://huggingface.co/stellaHsr-mm","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","other","expert-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"scidocs-c-64-24-gpt-4o-2024-05-135334","keyword":"mteb","description":"\n\t\n\t\t\n\t\tscidocs-c-64-24-gpt-4o-2024-05-135334 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"e-commerce search for products\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the scidocs-c-64-24-gpt-4o-2024-05-135334 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/scidocs-c-64-24-gpt-4o-2024-05-135334.","url":"https://huggingface.co/datasets/fine-tuned/scidocs-c-64-24-gpt-4o-2024-05-135334","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"WebGen-Bench_train_data","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tWebGen-Instruct: Training Data for WebGen-Bench\n\t\n\nThis repository contains WebGen-Instruct, the training data used in the paper WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch.\nWebGen-Bench is a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. The benchmark dataset itself consists of 101 instructions and 647 test cases. This particular dataset (WebGen-Instruct) provides 6â€¦ See the full description on the dataset page: https://huggingface.co/datasets/luzimu/WebGen-Bench_train_data.","url":"https://huggingface.co/datasets/luzimu/WebGen-Bench_train_data","creator_name":"Zimu Lu","creator_url":"https://huggingface.co/luzimu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","mit","arxiv:2505.03733","arxiv:2509.22644"],"keywords_longer_than_N":true},
	{"name":"fine-tome-100k-nondual","keyword":"alignment","description":"\n\t\n\t\t\n\t\tfine_tome_100k_nondual\n\t\n\nA non-dual reformulation of the mlabonne/FineTome-100k dataset.All assistant outputs (from: gpt) have been rewritten into impersonal, non-dual language using OpenAI models.User inputs and other roles remain unchanged.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSource: FineTome-100k  \nSize: ~100,000 conversations (JSONL, one per line)  \nFormat: ShareGPT-style conversations, with fields:{\n  \"conversations\": [\n    {\"from\": \"user\", \"value\": \"User message...\"},\n    {\"from\": \"gpt\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/marciodiaz/fine-tome-100k-nondual.","url":"https://huggingface.co/datasets/marciodiaz/fine-tome-100k-nondual","creator_name":"Marcio Diaz","creator_url":"https://huggingface.co/marciodiaz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"ReasonBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ§  ReasonBench: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning\n\t\n\n\n  imageï¼šbackground\n\n\t\n\t\t\n\t\tðŸŒ Overview\n\t\n\nReasonBench is a comprehensive benchmark designed to evaluate Visual Language Models (VLMs) on complex graphical reasoning tasks. It contains 1,613 problems collected from real-world intelligence tests, covering 11 core cognitive dimensions and 29 task types. This benchmark provides a robust framework for assessing VLMs' spatial, relational, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cistine/ReasonBench.","url":"https://huggingface.co/datasets/cistine/ReasonBench","creator_name":"jx","creator_url":"https://huggingface.co/cistine","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Chinese","English","apache-2.0","< 1K","Document"],"keywords_longer_than_N":true},
	{"name":"NuminaMath-full","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tMerged Math Datasets (Full)\n\t\n\nThis dataset combines multiple mathematical datasets for training and evaluation purposes. This version contains the complete training dataset from all source datasets.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive collection of mathematical problems and solutions from various sources, organized into training and multiple test subsets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tTraining Set\n\t\n\n\nSize: 217389 examples\nFields: source, question, answer\nSources:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/weijiezz/NuminaMath-full.","url":"https://huggingface.co/datasets/weijiezz/NuminaMath-full","creator_name":"weijie","creator_url":"https://huggingface.co/weijiezz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ContractNLILimitedUseLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLILimitedUseLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party shall not use any Confidential Information for any purpose other than the purposes stated in Agreement.\n\n\t\n\t\t\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLILimitedUseLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLILimitedUseLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"TextualismToolPlainLegalBenchClassification","keyword":"mteb","description":"\n  TextualismToolPlainLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDetermine if a paragraph from a judicial opinion is applying a form textualism that relies on the ordinary (â€œplainâ€) meaning of terms.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomainsLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TextualismToolPlainLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/TextualismToolPlainLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"reasoning-and-chat-harmony-format","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOpen Paws Reasoning And Conversational Finetuning Harmony Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Reasoning Data\nFormat: JSONL (JSON Lines)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Openâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/reasoning-and-chat-harmony-format.","url":"https://huggingface.co/datasets/open-paws/reasoning-and-chat-harmony-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-23052024-upq5-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-23052024-upq5-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"generic search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-23052024-upq5-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-23052024-upq5-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-23052024-upq5-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SciFact-32000-384-gpt-4o-2024-05-13-83349675","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSciFact-32000-384-gpt-4o-2024-05-13-83349675 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SciFact-32000-384-gpt-4o-2024-05-13-83349675 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SciFact-32000-384-gpt-4o-2024-05-13-83349675.","url":"https://huggingface.co/datasets/fine-tuned/SciFact-32000-384-gpt-4o-2024-05-13-83349675","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Recraft-v3-24-7-25_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Recraft v3 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~50'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Recraft v3 (version from 24.7.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Recraft-v3-24-7-25_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Recraft-v3-24-7-25_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Recraft-v3-24-7-25_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Recraft v3 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~50'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Recraft v3 (version from 24.7.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Recraft-v3-24-7-25_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Recraft-v3-24-7-25_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-11072024-bh6v-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-11072024-bh6v-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"insurance services\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-11072024-bh6v-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-11072024-bh6v-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-11072024-bh6v-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Core17InstructionRetrieval","keyword":"mteb","description":"\n  Core17InstructionRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMeasuring retrieval instruction following ability on Core17 narratives for the FollowIR benchmark.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReferencehttps://arxiv.org/abs/2403.15246\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"Core17InstructionRetrieval\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Core17InstructionRetrieval.","url":"https://huggingface.co/datasets/mteb/Core17InstructionRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","monolingual","English","mit"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-1752024-zdtc-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-1752024-zdtc-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Pharmacology research on antidepressants\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-1752024-zdtc-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-1752024-zdtc-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-1752024-zdtc-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CMedQAv2-3","keyword":"mteb","description":"\n\t\n\t\t\n\t\tCMedQAv2-3 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"healthcare information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the CMedQAv2-3 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/CMedQAv2-3.","url":"https://huggingface.co/datasets/fine-tuned/CMedQAv2-3","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-890333","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-890333 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"argumentation and sentiment analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-890333 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-890333.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-890333","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Thermal-Heatmap-Source-Localization","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tThermBench ðŸ”¥ â€” Thermal Heatmap Source Localization Benchmark\n\t\n\n\n\t\n\t\t\n\t\tðŸ“ Summary\n\t\n\nThermBench is a physics-inspired synthetic dataset designed to evaluate algorithms that infer hidden thermal sources from an observed heat diffusion map.\nEach data sample contains:\n\nan observed heatmap (matrix of values),\nand the ground-truth sources: (row, col, intensity).\n\nDiffusion follows inverse Manhattan distance:\n[\nH(i,j) ;=; \\sum_{s=1}^{K} \\frac{I_s}{d(i,j,s)+1}\n]\nwhere (d) is the Manhattanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZoneTwelve/Thermal-Heatmap-Source-Localization.","url":"https://huggingface.co/datasets/ZoneTwelve/Thermal-Heatmap-Source-Localization","creator_name":"ZoneTwelve","creator_url":"https://huggingface.co/ZoneTwelve","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"FalseFriendsGermanEnglish","keyword":"mteb","description":"\n  FalseFriendsGermanEnglish\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA dataset to identify False Friends / false cognates between English and German. A generally challenging task for multilingual models.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\nReference\nhttps://drive.google.com/file/d/1jgq0nBnV-UiYNxbKNrrr2gxDEHm-DMKH/view?usp=share_link\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mtebâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FalseFriendsGermanEnglish.","url":"https://huggingface.co/datasets/mteb/FalseFriendsGermanEnglish","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","human-annotated","monolingual","aari1995/false_friends_de_en_mteb"],"keywords_longer_than_N":true},
	{"name":"SCIDOCS-256-24-gpt-4o-2024-05-13-157892","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSCIDOCS-256-24-gpt-4o-2024-05-13-157892 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"service search for translation and editing\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SCIDOCS-256-24-gpt-4o-2024-05-13-157892 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-157892.","url":"https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-157892","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADWarrantyDurationLegalBenchClassification","keyword":"mteb","description":"\n  CUADWarrantyDurationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies a duration of any warranty against defects or errors in technology, products, or services provided under the contract.\n\n\t\n\t\t\n\n\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADWarrantyDurationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADWarrantyDurationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"AgentSynth","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tAgentSynth\n\t\n\n\n\t\n\t\t\n\t\tAgentSynth: Scalable Task Generation for Generalist Computer-Use Agents\n\t\n\nPaper | Project Page | Code\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nWe introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizonâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sunblaze-ucb/AgentSynth.","url":"https://huggingface.co/datasets/sunblaze-ucb/AgentSynth","creator_name":"sunblaze-ucb","creator_url":"https://huggingface.co/sunblaze-ucb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"FairDialogue","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for FairDialogue\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nFairDialogue is a benchmark resource for evaluating bias in end-to-end spoken dialogue models (SDMs).  \nWhile biases in large language models (LLMs) have been widely studied, spoken dialogue systems with audio input/output remain underexplored. FairDialogue provides stimulus data (audio, transcripts, and prompts) that can be used together with the official evaluation scripts to measure fairness in decision-making andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yihao005/FairDialogue.","url":"https://huggingface.co/datasets/yihao005/FairDialogue","creator_name":"wu","creator_url":"https://huggingface.co/yihao005","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"DensingLaw-ScalingBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDensingLaw-ScalingBench\n\t\n\nThis dataset was created to enable a more accurate performance scaling law estimation of Large Language Models (LLMs).\nThis dataset is released as part of our paper, Densing Law of LLMs.\n\n\n\n\nðŸ“œ Paper \n\n\n\n\n\n\t\n\t\t\n\t\tðŸ’¡ Overview\n\t\n\nThis repository contains the open-source dataset used for calculating conditional loss in our LLM density evaluation framework. \nLLM density is defined as the ratio of effective parameter size to actual parameter size, where effectiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openbmb/DensingLaw-ScalingBench.","url":"https://huggingface.co/datasets/openbmb/DensingLaw-ScalingBench","creator_name":"OpenBMB","creator_url":"https://huggingface.co/openbmb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice-qa","open-domain-qa","original:mmlu"],"keywords_longer_than_N":true},
	{"name":"Benchmark-Testing","keyword":"benchmark","description":"shounakpaul95/Benchmark-Testing dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/shounakpaul95/Benchmark-Testing","creator_name":"Shounak Paul","creator_url":"https://huggingface.co/shounakpaul95","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","summarization","translation","token-classification","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"helpsteer3_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer3. HelpSteer3 contains 40,476 preference samples, each containing a domain, language, context, two responses, an overall preference score between the responses as well as individual preferences from up to 3 annotators. Each individual preference contains a preference score in addition to a concise reasoning for their preference in 1-2 sentences. Data is split into 95% train and 5% validation.\nI processed theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer3_preference.","url":"https://huggingface.co/datasets/AIR-hl/helpsteer3_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-882024-3hmu-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-882024-3hmu-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"professional networking and mentorship\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-882024-3hmu-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-882024-3hmu-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-882024-3hmu-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-140539","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-140539 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"debate system\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-140539 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-140539.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-140539","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"vow","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tValues Of Weights (VOW) Dataset\n\t\n\n\n\t\n\t\t\n\t\tAs Artificial Super Intelligence approaches, I believe we need more tests like this rather than another math benchmark.\n\t\n\n\nâš ï¸ Important Disclaimer:\nI am a random human. This test is based on my views of right and wrong, good or evil. I have made this test (and if new ideas come, will update it too) for when ASI comes - these would be my questions for it, proving to myself if the system is fundamentally good or not. These are the answers Iâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/senpaisan/vow.","url":"https://huggingface.co/datasets/senpaisan/vow","creator_name":"san","creator_url":"https://huggingface.co/senpaisan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-607244","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-607244 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"None\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-607244 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-607244.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-607244","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NanoHotpotQARetrieval","keyword":"mteb","description":"\n  NanoHotpotQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoHotpotQARetrieval is a smaller subset of the HotpotQA dataset, which is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nWeb, Written\n\n\nReference\nhttps://hotpotqa.github.io/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoHotpotQARetrieval.","url":"https://huggingface.co/datasets/mteb/NanoHotpotQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","human-annotated","monolingual","mteb/hotpotqa"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-dxer-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-08082024-cs2v-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-v1_5-08082024-cs2v-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Wellness and Mindfulness\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-v1_5-08082024-cs2v-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-08082024-cs2v-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-08082024-cs2v-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADAuditRightsLegalBenchClassification","keyword":"mteb","description":"\n  CUADAuditRightsLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause gives a party the right to audit the books, records, or physical locations of the counterparty to ensure compliance with the contract.\n\n\t\n\t\t\n\n\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADAuditRightsLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADAuditRightsLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"tiny_qa_benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTiny QA Benchmark (Original English Core for TQB++)\n\t\n\nThis dataset (vincentkoc/tiny_qa_benchmark) is the original 52-item English Question-Answering set. It now serves as the immutable \"gold standard\" core for the expanded Tiny QA Benchmark++ (TQB++) project.\nThe TQB++ project builds upon this core dataset by introducing a powerful synthetic generation toolkit, pre-built multilingual datasets, and a comprehensive framework for rapid LLM smoke testing.\nFor the full TQB++ toolkit, theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark.","url":"https://huggingface.co/datasets/vincentkoc/tiny_qa_benchmark","creator_name":"Vincent Koc","creator_url":"https://huggingface.co/vincentkoc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","closed-book-qa","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"T23D-CompBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tText-to-3D Comprehensive Benchmark (T23D-CompBench) ðŸŽ¥ðŸ“Š\n\t\n\nCode Â· Project Page Â· Paper@ArXiv Â· Prompt list\nWelcome to the T23D-CompBench dataset! This repository contains around 3,600 textured meshes generated by various models using the Prompt list. These textured meshes have been annotated from twelve evaluation dimensions, including Object Alignment, Attribute Alignment, Interaction Alignment, Overall Alignment, Texture Clarity, Texture Aesthetics, Geometry Loss, Geometryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ccccby/T23D-CompBench.","url":"https://huggingface.co/datasets/ccccby/T23D-CompBench","creator_name":"Bingyang Cui","creator_url":"https://huggingface.co/ccccby","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-to-3d","English","cc-by-4.0","1K<n<10K","arxiv:2509.23841"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-8-8-gpt-4o-2024-05-13-610535","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-8-8-gpt-4o-2024-05-13-610535 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-8-8-gpt-4o-2024-05-13-610535 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-610535.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-610535","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-luma-ray2","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Luma Ray2 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Luma's Ray 2 video generation model on our benchmark. The up to date benchmark can beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5192024-xqq9-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5192024-xqq9-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"machine learning data generation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5192024-xqq9-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-xqq9-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-xqq9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"GeorgianSentimentClassification","keyword":"mteb","description":"\n  GeorgianSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGoergian Sentiment Dataset\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReference\nhttps://aclanthology.org/2022.lrec-1.173\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"GeorgianSentimentClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/GeorgianSentimentClassification.","url":"https://huggingface.co/datasets/mteb/GeorgianSentimentClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"NLPJournalTitleIntroRetrieval.V2","keyword":"mteb","description":"\n  NLPJournalTitleIntroRetrieval.V2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given title. This is the V2 dataset (last updated 2025-06-15).\n\n\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/sbintuitions/JMTEB\n\n\n\t\n\nSourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NLPJournalTitleIntroRetrieval.V2.","url":"https://huggingface.co/datasets/mteb/NLPJournalTitleIntroRetrieval.V2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","sbintuitions/JMTEB"],"keywords_longer_than_N":true},
	{"name":"dutch-legal-c-128-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tdutch-legal-c-128-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"legal document search for Dutch legislation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the dutch-legal-c-128-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/dutch-legal-c-128-24.","url":"https://huggingface.co/datasets/fine-tuned/dutch-legal-c-128-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-mathematica","keyword":"mteb","description":"\n  CQADupstackMathematicaRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Academic, Non-fiction\n\n\nReferencehttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackMathematicaRetrieval\"])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-mathematica.","url":"https://huggingface.co/datasets/mteb/cqadupstack-mathematica","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"TRECCOVID-256-24-gpt-4o-2024-05-13-46681","keyword":"mteb","description":"\n\t\n\t\t\n\t\tTRECCOVID-256-24-gpt-4o-2024-05-13-46681 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"biomedical literature search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the TRECCOVID-256-24-gpt-4o-2024-05-13-46681 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/TRECCOVID-256-24-gpt-4o-2024-05-13-46681.","url":"https://huggingface.co/datasets/fine-tuned/TRECCOVID-256-24-gpt-4o-2024-05-13-46681","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NusaParagraphEmotionClassification","keyword":"mteb","description":"\n  NusaParagraphEmotionClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNusaParagraphEmotionClassification is a multi-class emotion classification on 10 Indonesian languages from the NusaParagraph dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNon-fiction, Fiction, Written\n\n\nReference\nhttps://github.com/IndoNLP/nusa-writes\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NusaParagraphEmotionClassification.","url":"https://huggingface.co/datasets/mteb/NusaParagraphEmotionClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","multilingual","Batak Toba","Betawi"],"keywords_longer_than_N":true},
	{"name":"NLPJournalTitleAbsRetrieval.V2","keyword":"mteb","description":"\n  NLPJournalTitleAbsRetrieval.V2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding abstract with the given title. This is the V2 dataset (last updated 2025-06-15).\n\n\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/sbintuitions/JMTEB\n\n\n\t\n\nSource datasets:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NLPJournalTitleAbsRetrieval.V2.","url":"https://huggingface.co/datasets/mteb/NLPJournalTitleAbsRetrieval.V2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","sbintuitions/JMTEB"],"keywords_longer_than_N":true},
	{"name":"grammar-correction-eval-5k","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tgrammar-correction-eval-5k\n\t\n\nEvaluation dataset with 5k samples for grammar correction models\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains evaluation samples for grammar correction models. Each sample is a text that can be used to evaluate model performance on grammar correction tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\nid: Unique identifier for the sample\ntext: Input text for evaluation\nsource: Source identifier for the dataset\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stimuler/grammar-correction-eval-5k.","url":"https://huggingface.co/datasets/stimuler/grammar-correction-eval-5k","creator_name":"Stimuler","creator_url":"https://huggingface.co/stimuler","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ContractNLIPermissiblePostAgreementPossessionLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLIPermissiblePostAgreementPossessionLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may retain some Confidential Information even after the return or destruction of Confidential Information.\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLIPermissiblePostAgreementPossessionLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLIPermissiblePostAgreementPossessionLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-02092024-ww8e-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-02092024-ww8e-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-02092024-ww8e-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-02092024-ww8e-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-02092024-ww8e-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VeriGUI","keyword":"benchmark","description":" VeriGUI: Verifiable Long-Chain GUI Dataset\n\n\n \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nVeriGUI is a large-scale, human-annotated dataset designed to facilitate the development and evaluation of autonomous GUI agents capable of performing complex, long-horizon tasks in realistic computer environments. Unlike existing GUI datasets that focus on short-term interactions, VeriGUI emphasizes long-chain complexity and subtask-level verifiability to better reflect real-world human-computer interaction scenarios.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/2077AIDataFoundation/VeriGUI.","url":"https://huggingface.co/datasets/2077AIDataFoundation/VeriGUI","creator_name":"2077AI","creator_url":"https://huggingface.co/2077AIDataFoundation","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-aligned-words","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Word for Word Alignment Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~1500 human evaluators were asked to evaluate AI-generated videos based on what part of the prompt did not align the video. The specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words.","url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TelemarketingSalesRuleLegalBenchClassification","keyword":"mteb","description":"\n  TelemarketingSalesRuleLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDetermine how 16 C.F.R. Â§ 310.3(a)(1) and 16 C.F.R. Â§ 310.3(a)(2) (governing deceptive practices) apply to different fact patterns. This dataset is designed to test a modelâ€™s ability to apply 16 C.F.R. Â§ 310.3(a)(1) and 16 C.F.R. Â§ 310.3(a)(2) of the Telemarketing Sales Rule to a simple fact pattern with a clear outcome. Each fact pattern ends with the question: â€œIs this a violation of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TelemarketingSalesRuleLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/TelemarketingSalesRuleLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"NeuCLIR2022Retrieval","keyword":"mteb","description":"\n  NeuCLIR2022Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\nSource datasets:\n\nmteb/neuclir-2022\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"NeuCLIR2022Retrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NeuCLIR2022Retrieval.","url":"https://huggingface.co/datasets/mteb/NeuCLIR2022Retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2022","Persian"],"keywords_longer_than_N":true},
	{"name":"pig","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 149,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pierfabre/pig.","url":"https://huggingface.co/datasets/pierfabre/pig","creator_name":"Pierre FABRE","creator_url":"https://huggingface.co/pierfabre","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-v2-eight","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11175,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-eight.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-eight","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"StepEval-Audio-Toolcall","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tStepEval-Audio-Toolcall\n\t\n\nPaper: Step-Audio 2 Technical ReportCode: https://github.com/stepfun-ai/Step-Audio2Project Page: https://www.stepfun.com/docs/en/step-audio2  \n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nStepEval Audio Toolcall evaluates the invocation performance of four tool types. For each tool, the benchmark contains approximately 200 multi-turn dialogue sets for both positive and negative scenarios:\n\nPositive samples: The assistant is required to invoke the specified tool in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stepfun-ai/StepEval-Audio-Toolcall.","url":"https://huggingface.co/datasets/stepfun-ai/StepEval-Audio-Toolcall","creator_name":"StepFun","creator_url":"https://huggingface.co/stepfun-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["audio-text-to-text","apache-2.0","arxiv:2507.16632","ðŸ‡ºðŸ‡¸ Region: US","benchmark"],"keywords_longer_than_N":true},
	{"name":"TAAROFBENCH","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tWe Politely Insist: Your LLM Must Learn the Persian Art of Taarof\n\t\n\nThis repository hosts TAAROFBENCH, the first benchmark for evaluating large language models on taarof, a social norm in Iranian interactions that represents a sophisticated system of ritual politeness emphasizing deference, modesty, and indirectness. The benchmark was introduced in the paper â€œWe Politely Insist: Your LLM Must Learn the Persian Art of Taarofâ€, accepted at the Main Conference of EMNLP 2025.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Nikta/TAAROFBENCH.","url":"https://huggingface.co/datasets/Nikta/TAAROFBENCH","creator_name":"Gohari Sadr","creator_url":"https://huggingface.co/Nikta","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","other","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADRofrRofoRofnLegalBenchClassification","keyword":"mteb","description":"\n  CUADRofrRofoRofnLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause grant one party a right of first refusal, right of first offer or right of first negotiation to purchase, license, market, or distribute equity interest, technology, assets, products or services.\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADRofrRofoRofnLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADRofrRofoRofnLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_9","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 2988,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_9.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_9","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"blue_cup_pour_single","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 2,\n    \"total_frames\": 1547,\n    \"total_tasks\": 1,\n    \"total_videos\": 6,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:2\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chengkunli/blue_cup_pour_single.","url":"https://huggingface.co/datasets/chengkunli/blue_cup_pour_single","creator_name":"Chengkun Li","creator_url":"https://huggingface.co/chengkunli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"test_dataset_modify_time","keyword":"test","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/minchai23/test_dataset_modify_time.","url":"https://huggingface.co/datasets/minchai23/test_dataset_modify_time","creator_name":"chai min","creator_url":"https://huggingface.co/minchai23","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["Chinese","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","test"],"keywords_longer_than_N":false},
	{"name":"xAI_Aurora_t2i_human_preferences","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Aurora Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 400k human responses from over 86k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Aurora across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/xAI_Aurora_t2i_human_preferences.","url":"https://huggingface.co/datasets/Rapidata/xAI_Aurora_t2i_human_preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"xAI_Aurora_t2i_human_preferences","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Aurora Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 400k human responses from over 86k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Aurora across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/xAI_Aurora_t2i_human_preferences.","url":"https://huggingface.co/datasets/Rapidata/xAI_Aurora_t2i_human_preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"TurHistQuadRetrieval","keyword":"mteb","description":"\n  TurHistQuadRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nQuestion Answering dataset on Ottoman History in Turkish\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nEncyclopaedic, Non-fiction, Academic, Written\n\n\nReference\nhttps://github.com/okanvk/Turkish-Reading-Comprehension-Question-Answering-Dataset\n\n\n\t\n\nSource datasets:\n\nasparius/TurHistQuAD\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntaskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TurHistQuadRetrieval.","url":"https://huggingface.co/datasets/mteb/TurHistQuadRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","asparius/TurHistQuAD"],"keywords_longer_than_N":true},
	{"name":"NanoClimateFeverRetrieval","keyword":"mteb","description":"\n  NanoClimateFeverRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoClimateFever is a small version of the BEIR dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomainsNon-fiction, Academic, News\n\n\nReference\nhttps://arxiv.org/abs/2012.00614\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoClimateFeverRetrieval.","url":"https://huggingface.co/datasets/mteb/NanoClimateFeverRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","fact-checking","fact-checking-retrieval","expert-annotated","monolingual"],"keywords_longer_than_N":true},
	{"name":"test_dataset","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 4,\n    \"total_frames\": 2681,\n    \"total_tasks\": 2,\n    \"total_videos\": 8,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:4\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/francescocrivelli/test_dataset.","url":"https://huggingface.co/datasets/francescocrivelli/test_dataset","creator_name":"Francesco Crivelli","creator_url":"https://huggingface.co/francescocrivelli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-v2-one","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11157,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-one.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-one","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"HunyuanImage-2.1_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Hunyuan Image 2.1 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~50'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Hunyuan Image 2.1 (version from 19.9.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/HunyuanImage-2.1_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/HunyuanImage-2.1_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"HunyuanImage-2.1_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Hunyuan Image 2.1 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~50'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Hunyuan Image 2.1 (version from 19.9.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/HunyuanImage-2.1_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/HunyuanImage-2.1_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"polite-guard","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tPolite Guard\n\t\n\n\nDataset type: Synthetic and Annotated\nTask: Text Classification\nDomain: Classification of text into polite, somewhat polite, neutral, and impolite categories\nSource Code: (https://github.com/intel/polite-guard)\nModel: (https://huggingface.co/Intel/polite-guard)\n\nThis dataset is for Polite Guard: an open-source NLP language model developed by Intel, fine-tuned from BERT for text classification tasks. Polite Guard is designed to classify text into four categories: politeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Intel/polite-guard.","url":"https://huggingface.co/datasets/Intel/polite-guard","creator_name":"Intel","creator_url":"https://huggingface.co/Intel","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","English","cdla-permissive-2.0","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-seedance-1-pro","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Seedance 1 Pro Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~60k human responses from ~20k human annotators were collected to evaluate Seedance 1 Pro video generation model on our benchmark. This dataset was collected in roughtly 30 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-seedance-1-pro.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-seedance-1-pro","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-de-15_8_2024-h1i4-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-de-15_8_2024-h1i4-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-de-15_8_2024-h1i4-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-de-15_8_2024-h1i4-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-de-15_8_2024-h1i4-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ReportBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tReportBench â€” Dataset Card\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nReportBench is a comprehensive benchmark for evaluating the factual quality and citation behavior of Deep Research agents. Leveraging expert-authored survey papers as ground truth, ReportBench reverse-engineers domain-specific prompts and provides automated tools to assess both cited and non-cited content.\n[GitHub] / [Paper]\nReportBench addresses this need by:\n\nLeveraging expert surveys: Uses high-quality, peer-reviewed survey papersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ByteDance-BandAI/ReportBench.","url":"https://huggingface.co/datasets/ByteDance-BandAI/ReportBench","creator_name":"ByteDance BandAI","creator_url":"https://huggingface.co/ByteDance-BandAI","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc0-1.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-code-askubuntu","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-code-askubuntu Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"technical support for Ubuntu\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-code-askubuntu model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-code-askubuntu.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-code-askubuntu","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"blockchain-benchmark-formatted","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for LLM Blockchain Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Blockchain Benchmark Dataset is a comprehensive collection of data specifically curated for benchmarking Language Models (LMs) in the domain of blockchain technology. This dataset is designed to facilitate research and development in natural language understanding within the blockchain domain.\nA complete list of tasks: ['general-reasoning', 'code', 'math']\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\n\t\n\t\t\nModelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/revflask/blockchain-benchmark-formatted.","url":"https://huggingface.co/datasets/revflask/blockchain-benchmark-formatted","creator_name":"Mayank Panjiyara","creator_url":"https://huggingface.co/revflask","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","no-annotation","expert-generated","English"],"keywords_longer_than_N":true},
	{"name":"RAVine-qrels","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tRAVine-qrels\n\t\n\nThe qrels in this repo refer to the relevance labels in MS MARCO V2.1 corpus for the queries of the RAVine test set. We collected them from trec-2024-rag and converted them into jsonline formats.\nThis dataset is associated with the paper RAVine: Reality-Aligned Evaluation for Agentic Search.\nGithub: https://github.com/SwordFaith/RAVine\n\n\t\n\t\t\n\t\n\t\n\t\tRelated Datasets on Hugging Face\n\t\n\nThe RAVine project includes several other datasets available on Hugging Face:\n\nQueries &â€¦ See the full description on the dataset page: https://huggingface.co/datasets/sapphirex/RAVine-qrels.","url":"https://huggingface.co/datasets/sapphirex/RAVine-qrels","creator_name":"yilong xu","creator_url":"https://huggingface.co/sapphirex","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-ranking","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Robust04InstructionRetrieval","keyword":"mteb","description":"\n  Robust04InstructionRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMeasuring retrieval instruction following ability on Robust04 narratives for the FollowIR benchmark.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReferencehttps://arxiv.org/abs/2403.15246\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"Robust04InstructionRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Robust04InstructionRetrieval.","url":"https://huggingface.co/datasets/mteb/Robust04InstructionRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","monolingual","jhu-clsp/robust04-instructions-mteb","English"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information retrieval system\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-97u6-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CompareBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tCompareBench\n\t\n\nCompareBench is a benchmark for evaluating visual comparison reasoning in visionâ€“language models (VLMs),a fundamental yet understudied skill. CompareBench consists of 1,000 QA pairs across four tasks:  \n\nQuantity (600)  \nTemporal (100)  \nGeometric (200)  \nSpatial (100)\n\nIt is derived from two auxiliary datasets we constructed: TallyBench and HistCaps.\n\n\t\n\t\t\n\t\n\t\n\t\tRelated Datasets\n\t\n\n\nHistCaps  \nTallyBench\n\n\n\t\n\t\n\t\n\t\tCode\n\t\n\nðŸ‘‰ CompareBench on GitHub\n","url":"https://huggingface.co/datasets/qiuzhangTiTi/CompareBench","creator_name":"Jie Cai","creator_url":"https://huggingface.co/qiuzhangTiTi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"HalluMix","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tIntroducing HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Detecting Hallucinations in Real-World Scenarios\n\t\n\nâœ‰ï¸ Contact: {deanna, mike, freddie, julia}@quotientai.co ðŸ“œ Paper: HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection, Emery et al (2025)\nAs large language models (LLMs) are increasingly adopted in critical industries, ensuring their outputs are factually grounded has emerged as a major concern. One prominent issue is \"hallucinationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/quotientai/HalluMix.","url":"https://huggingface.co/datasets/quotientai/HalluMix","creator_name":"Quotient AI","creator_url":"https://huggingface.co/quotientai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"steshin-2023-lohi","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLo-Hi Benchmark\n\t\n\nData from Simon Steshin, Lo-Hi: Practical ML Drug Discovery Benchmark, available from the GitHub repositiory. We used schemist (which in turn uses RDKit)\nto add molecuar weight, Murcko scaffold, Crippen cLogP, and topological surface area.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nFrom the original README:\n\n\t\n\t\t\n\t\n\t\n\t\tHit Identification\n\t\n\nThe goal of the Hit Identification task is to find novel molecules that have desirable property, but are dissimilar from the molecules withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/steshin-2023-lohi.","url":"https://huggingface.co/datasets/scbirlab/steshin-2023-lohi","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","text2text-generation","translation","zero-shot-classification","mit"],"keywords_longer_than_N":true},
	{"name":"DBPedia-PLHardNegatives","keyword":"mteb","description":"\n  DBPedia-PLHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\n\n\nReference\nhttps://github.com/iai-group/DBpedia-Entity/\n\n\n\t\n\nSource datasets:\n\nmteb/dbpediaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DBPedia-PLHardNegatives.","url":"https://huggingface.co/datasets/mteb/DBPedia-PLHardNegatives","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/dbpedia","mteb/DBPedia_PL_test_top_250_only_w_correct-v2"],"keywords_longer_than_N":true},
	{"name":"giraffe_test_2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 141,\n    \"total_tasks\": 1,\n    \"total_videos\": 1,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/carpit680/giraffe_test_2.","url":"https://huggingface.co/datasets/carpit680/giraffe_test_2","creator_name":"Arpit Chauhan","creator_url":"https://huggingface.co/carpit680","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"DICE-BENCH","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸŽ² DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues\n\t\n\n\n\t\n\t\t\n\t\tðŸ”— Links for Reference\n\t\n\n\nRepository: https://github.com/snuhcc/DICE-Bench\nPaper: https://arxiv.org/abs/2506.22853\nProject page: https://snuhcc.github.io/DICE-Bench/\nPoint of Contact: kyochul@snu.ac.kr\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“– Paper Description\n\t\n\nDICE-BENCH is a benchmark that tests how well large language models can call external functions in realistic group-chatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OfficerChul/DICE-BENCH.","url":"https://huggingface.co/datasets/OfficerChul/DICE-BENCH","creator_name":"Kyochul Jang","creator_url":"https://huggingface.co/OfficerChul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","arxiv:2506.22853","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"green_cup_pour_fixed4","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 52,\n    \"total_frames\": 34041,\n    \"total_tasks\": 1,\n    \"total_videos\": 156,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:52\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiamFy/green_cup_pour_fixed4.","url":"https://huggingface.co/datasets/LiamFy/green_cup_pour_fixed4","creator_name":"Liam Achenbach","creator_url":"https://huggingface.co/LiamFy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MaCBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMaCBench\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\tâš ï¸ IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tðŸš« THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY ðŸš«\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluation results. Pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"Set_Eval","keyword":"benchmark","description":"AarushSah/Set_Eval dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/AarushSah/Set_Eval","creator_name":"Aarush Sah","creator_url":"https://huggingface.co/AarushSah","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"afrimgsm","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tDataset Card for afrimgsm\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAFRIMGSM is an evaluation dataset comprising translations of a subset of the GSM8k dataset into 16 African languages. \nIt includes test sets across all 18 languages, maintaining an English and French subsets from the original GSM8k dataset. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThere are 18 languages available :\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe examples look like this for English:\nfrom datasets import load_dataset\ndata =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/afrimgsm.","url":"https://huggingface.co/datasets/masakhane/afrimgsm","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["natural-language-inference","multilingual","gsm8k","Amharic","Ewe"],"keywords_longer_than_N":true},
	{"name":"ArmenianParaphrasePC","keyword":"mteb","description":"\n  ArmenianParaphrasePC\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nasparius/Armenian-Paraphrase-PC\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://github.com/ivannikov-lab/arpa-paraphrase-corpus\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"ArmenianParaphrasePC\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ArmenianParaphrasePC.","url":"https://huggingface.co/datasets/mteb/ArmenianParaphrasePC","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","derived","monolingual","Armenian"],"keywords_longer_than_N":true},
	{"name":"neuclir-2022","keyword":"mteb","description":"\n  NeuCLIR2022Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NeuCLIR2022Retrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/neuclir-2022.","url":"https://huggingface.co/datasets/mteb/neuclir-2022","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","Persian","Russian"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-scidocs","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-scidocs Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research papers search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-scidocs model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-scidocs.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-scidocs","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SpectraMail_Data_Set","keyword":"testing","description":"Code Message:\n\n\t\n\t\t\n\t\tSpectraMail_Data_Set ðŸ“§\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nWelcome to the SpectraMail_Data_Set! This dataset provides a collection of 70,000 unique and randomly generated email addresses designed for testing and development purposes. The dataset is ideal for use in system testing, validation, and as synthetic data for AI training scenarios.  \n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nEmail Generation: Each entry includes a randomly generated email address.  \nHigh Volume: Contains 70,000 rows to supportâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MawLab/SpectraMail_Data_Set.","url":"https://huggingface.co/datasets/MawLab/SpectraMail_Data_Set","creator_name":"maw studio","creator_url":"https://huggingface.co/MawLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-v2-six","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11174,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-six.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-six","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"NanoMSMARCORetrieval","keyword":"mteb","description":"\n  NanoMSMARCORetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoMSMARCORetrieval is a smaller subset of MS MARCO, a collection of datasets focused on deep learning in search.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb\n\n\nReferencehttps://microsoft.github.io/msmarco/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NanoMSMARCORetrieval\"])\nevaluator = mteb.MTEB(task)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoMSMARCORetrieval.","url":"https://huggingface.co/datasets/mteb/NanoMSMARCORetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","human-annotated","monolingual","mteb/msmarco"],"keywords_longer_than_N":true},
	{"name":"ItaCaseholdClassification","keyword":"mteb","description":"\n  ItaCaseholdClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nAn Italian Dataset consisting of 1101 pairs of judgments and their official holdings between the years 2019 and 2022 from the archives of Italian Administrative Justice categorized with 64 subjects.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Government, Written\n\n\nReference\nhttps://doi.org/10.1145/3594536.3595177\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ItaCaseholdClassification.","url":"https://huggingface.co/datasets/mteb/ItaCaseholdClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","Italian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"pig3","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 10,\n    \"total_frames\": 5359,\n    \"total_tasks\":1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pierfabre/pig3.","url":"https://huggingface.co/datasets/pierfabre/pig3","creator_name":"Pierre FABRE","creator_url":"https://huggingface.co/pierfabre","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"AngryTweetsClassification","keyword":"mteb","description":"\n  AngryTweetsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA sentiment dataset with 3 classes (positiv, negativ, neutral) for Danish tweets\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, Written\n\n\nReference\nhttps://aclanthology.org/2021.nodalida-main.53/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"AngryTweetsClassification\"])\nevaluator = mteb.MTEB(task)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AngryTweetsClassification.","url":"https://huggingface.co/datasets/mteb/AngryTweetsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"UCCVCommonLawLegalBenchClassification","keyword":"mteb","description":"\n  UCCVCommonLawLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDetermine if a contract is governed by the Uniform Commercial Code (UCC) or the common law of contracts.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\nSource datasets:\n\nnguha/legalbench\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/UCCVCommonLawLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/UCCVCommonLawLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"summeval","keyword":"mteb","description":"\n  SummEvalSummarization.v2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNews Article Summary Semantic Similarity Estimation. This version fixes a bug in the evaluation script that caused the main score to be computed incorrectly.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://github.com/Yale-LILY/SummEval\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/summeval.","url":"https://huggingface.co/datasets/mteb/summeval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["summarization","human-annotated","monolingual","mteb/summeval","English"],"keywords_longer_than_N":true},
	{"name":"T2I-Keypoints-Eval","keyword":"benchmark","description":"\n\n\n\t\n\t\t\n\t\tT2I-Keypoints-Eval Dataset\n\t\n\nA Bilingual Text-to-Image Keypoints Evaluation Benchmark\nLinqing Wang Â·\nXiming Xing Â·\nYiji Cheng Â·\nZhiyuan Zhao Â·\nJiale Tao Â·\nQiXun Wang Â·\nRuihuang Li Â·\nComi Chen Â·\nXin Li Â·\nMingrui Wu Â·\nXinchi Deng Â·\nChunyu Wangâ€  Â·\nQinglin Lu*\nTencent Hunyuan\nâ€ Project Lead Â· *Corresponding Author\n\n\n\n  \n  \n  \n  \n  \n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nT2I-Keypoints-Eval is a comprehensive bilingual evaluation dataset designed to assess text-to-image models' ability to generate imagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PromptEnhancer/T2I-Keypoints-Eval.","url":"https://huggingface.co/datasets/PromptEnhancer/T2I-Keypoints-Eval","creator_name":"PromptEnhancer","creator_url":"https://huggingface.co/PromptEnhancer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"OdiaNewsClassification","keyword":"mteb","description":"\n  OdiaNewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Odia dataset for 3-class classification of Odia news articles\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://github.com/goru001/nlp-for-odia\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"OdiaNewsClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/OdiaNewsClassification.","url":"https://huggingface.co/datasets/mteb/OdiaNewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","Odia"],"keywords_longer_than_N":true},
	{"name":"FaroeseSTS","keyword":"mteb","description":"\n  FaroeseSTS\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSemantic Text Similarity (STS) corpus for Faroese.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Web, Written\n\n\nReference\nhttps://aclanthology.org/2023.nodalida-1.74.pdf\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"FaroeseSTS\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FaroeseSTS.","url":"https://huggingface.co/datasets/mteb/FaroeseSTS","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","semantic-similarity-scoring","human-annotated","monolingual","Faroese"],"keywords_longer_than_N":true},
	{"name":"yellow_cup_pour_single","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 2,\n    \"total_frames\": 1346,\n    \"total_tasks\": 1,\n    \"total_videos\": 6,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:2\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chengkunli/yellow_cup_pour_single.","url":"https://huggingface.co/datasets/chengkunli/yellow_cup_pour_single","creator_name":"Chengkun Li","creator_url":"https://huggingface.co/chengkunli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"tulu-v2-sft-mixture-filtered","keyword":"alignment","description":"\n\t\n\t\t\n\t\tðŸ“˜ SCAR-Filtered Instruction-Tuning Subset (10k from Tulu-v2)\n\t\n\nThis dataset contains 10,000 high-quality instructionâ€“response pairs filtered from the allenai/tulu-v2-sft-mixture dataset using the SCAR data selection method.\nSCAR (Style Consistency-Aware Response Ranking) is a novel data selection framework accepted to ACL 2025 (main conference). It ranks and filters instructionâ€“response pairs based on style consistency, resulting in a more reliable and efficient subset forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lizhuang144/tulu-v2-sft-mixture-filtered.","url":"https://huggingface.co/datasets/lizhuang144/tulu-v2-sft-mixture-filtered","creator_name":"Zhuang Li","creator_url":"https://huggingface.co/lizhuang144","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-872024-od97-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-872024-od97-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"job search and recruitment\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-872024-od97-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-872024-od97-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-872024-od97-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 2,\n    \"total_frames\": 938,\n    \"total_tasks\": 1,\n    \"total_videos\": 4,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rifqi02/test.","url":"https://huggingface.co/datasets/Rifqi02/test","creator_name":"MUHAMMAD RIFQI ADAM BIN MOHD RIDWAN","creator_url":"https://huggingface.co/Rifqi02","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"FrenchBookReviews","keyword":"mteb","description":"\n  FrenchBookReviews\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIt is a French book reviews dataset containing a huge number of reader reviews on French books. Each review is pared with a rating that ranges from 0.5 to 5 (with 0.5 increment).\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\nDomains\nReviews, Written\n\n\nReference\nhttps://huggingface.co/datasets/Abirate/french_book_reviews\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FrenchBookReviews.","url":"https://huggingface.co/datasets/mteb/FrenchBookReviews","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","monolingual","French","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-v2-nine","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11175,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-nine.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-nine","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"reddit-ethics","keyword":"alignment","description":"\n\t\n\t\t\n\t\tReddit Ethics: Real-World Ethical Dilemmas from Reddit\n\t\n\nReddit Ethics is a curated dataset of genuine ethical dilemmas collected from Reddit, designed to support research and education in philosophical ethics, AI alignment, and moral reasoning.\nEach entry features a real-world scenario accompanied by structured ethical analysis through major frameworksâ€”utilitarianism, deontology, and virtue ethics. The dataset also provides discussion questions, sample answers, and proposedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/reddit-ethics.","url":"https://huggingface.co/datasets/agentlans/reddit-ethics","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","feature-extraction","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"dedeucebench-dev","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDedeuceBench Dev Split\n\t\n\nThis dataset provides the public development split for DedeuceBench, an interactive activeâ€‘learning benchmark over hidden Mealy machines. Each item in the split is a config entry (seeded episode) defined in levels_dev.json under subset dev.\n\n\t\n\t\t\n\t\tSummary\n\t\n\n\nTask: Identification-first â€” agents must probe a hidden finite-state transducer under a strict query budget using tool calls (act, submit_table) and submit an exact transition table to succeed.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/comfortably-dumb/dedeucebench-dev.","url":"https://huggingface.co/datasets/comfortably-dumb/dedeucebench-dev","creator_name":"Vedansh Sharma","creator_url":"https://huggingface.co/comfortably-dumb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"SMMILE-plusplus","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE-plusplus.","url":"https://huggingface.co/datasets/smmile/SMMILE-plusplus","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Runway_Frames_t2i_human_preferences","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Frames Preference\n\t\n\n\n\n\n\nThis T2I dataset contains roughly 400k human responses from over 82k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Frames across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Runway_Frames_t2i_human_preferences.","url":"https://huggingface.co/datasets/Rapidata/Runway_Frames_t2i_human_preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Runway_Frames_t2i_human_preferences","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Frames Preference\n\t\n\n\n\n\n\nThis T2I dataset contains roughly 400k human responses from over 82k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Frames across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Runway_Frames_t2i_human_preferences.","url":"https://huggingface.co/datasets/Rapidata/Runway_Frames_t2i_human_preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"askubuntu-c-64-24-gpt-4o-2024-05-13-61285","keyword":"mteb","description":"\n\t\n\t\t\n\t\taskubuntu-c-64-24-gpt-4o-2024-05-13-61285 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"knowledge base search for questions and answers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the askubuntu-c-64-24-gpt-4o-2024-05-13-61285 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/askubuntu-c-64-24-gpt-4o-2024-05-13-61285.","url":"https://huggingface.co/datasets/fine-tuned/askubuntu-c-64-24-gpt-4o-2024-05-13-61285","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"toxic_conversations_50k","keyword":"mteb","description":"\n  ToxicConversationsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCollection of comments from the Civil Comments platform together with annotations if the comment is toxic or not.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, Written\nReference\nhttps://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimportâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/toxic_conversations_50k.","url":"https://huggingface.co/datasets/mteb/toxic_conversations_50k","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-v2-three","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11175,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-three.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-three","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"R2MEDMedXpertQAExamRetrieval","keyword":"mteb","description":"\n  R2MEDMedXpertQAExamRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMedXpertQA-Exam retrieval dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical\n\n\nReference\nhttps://huggingface.co/datasets/R2MED/MedXpertQA-Exam\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"R2MEDMedXpertQAExamRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/R2MEDMedXpertQAExamRetrieval.","url":"https://huggingface.co/datasets/mteb/R2MEDMedXpertQAExamRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","LM-generated and reviewed","monolingual","R2MED/MedXpertQA-Exam"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5202024-55bm-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5202024-55bm-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"legal regulations search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5202024-55bm-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5202024-55bm-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5202024-55bm-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"mimic_multi_episodes","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 2,\n    \"total_frames\": 1913,\n    \"total_tasks\": 1,\n    \"total_videos\": 6,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chengkunli/mimic_multi_episodes.","url":"https://huggingface.co/datasets/chengkunli/mimic_multi_episodes","creator_name":"Chengkun Li","creator_url":"https://huggingface.co/chengkunli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MMBench-GUI","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ–¥ï¸ MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents\n\t\n\n\n  ðŸ“– PaperÂ Â  | Â Â ðŸ’» CodeÂ Â  | Â Â ðŸ¤— DatasetÂ Â  | Â Â ðŸ“¢ Leaderboard (coming soon)\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tPaper Abstract\n\t\n\nWe introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI automation agents across Windows, macOS, Linux, iOS, Android, and Web platforms. It comprises four levels: GUI Content Understanding, Element Grounding, Task Automation, and Task Collaboration, covering essential skills forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMBench-GUI.","url":"https://huggingface.co/datasets/OpenGVLab/MMBench-GUI","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","Image","arxiv:2507.19478","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"OceanGym","keyword":"benchmark","description":" ðŸŒŠ OceanGym ðŸ¦¾ \n A Benchmark Environment for Underwater Embodied Agents \n\n\n  ðŸŒ Home Page\n  ðŸ“„ Paper\n  ðŸ’» Code\n  ðŸ¤— Hugging Face\n  â˜ï¸ Google Drive\n  â˜ï¸ Baidu Drive\n\n\n\n\nOceanGym is a high-fidelity embodied underwater environment that simulates a realistic ocean setting with diverse scenes. As illustrated in figure, OceanGym establishes a robust benchmark for evaluating autonomous agents through a series of challenging tasks, encompassing various perception analyses and decision-makingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zjunlp/OceanGym.","url":"https://huggingface.co/datasets/zjunlp/OceanGym","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","English","mit","arxiv:2509.26536","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"domainbench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDomainBench\n\t\n\nDomainBench is a comprehensive benchmark for evaluating vision-language models across different domains.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe benchmark consists of 7 domain-specific datasets:\n\nAnimals\nFood\nKitchen\nKITTI\nPerson\nVehicles\nWildlife\n\nEach domain contains:\n\nImages folder with visual content\nQuestion-answer pairs in JSON format\n\n\n\t\n\t\t\n\t\tTasks\n\t\n\nThe benchmark includes various visual reasoning tasks:\n\nObject Detection\nSpatial Relationships\nColor Analysis\nDepthâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/eaatira/domainbench.","url":"https://huggingface.co/datasets/eaatira/domainbench","creator_name":"Tim","creator_url":"https://huggingface.co/eaatira","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","cc-by-3.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-06052024-ruwi-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-06052024-ruwi-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"insurance search for car policies\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-06052024-ruwi-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-06052024-ruwi-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-06052024-ruwi-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-wan2.1","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Alibaba Wan2.1 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Alibaba Wan 2.1 video generation model on our benchmark. The up to date benchmarkâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"CUADExclusivityLegalBenchClassification","keyword":"mteb","description":"\n  CUADExclusivityLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies exclusive dealing commitment with the counterparty. This includes a commitment to procure all 'requirements' from one party of certain technology, goods, or services or a prohibition on licensing or selling technology, goods or services to third parties, or a prohibition on collaborating or workingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADExclusivityLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADExclusivityLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"CUADCompetitiveRestrictionExceptionLegalBenchClassification","keyword":"mteb","description":"\n  CUADCompetitiveRestrictionExceptionLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause mentions exceptions or carveouts to Non-Compete, Exclusivity and No-Solicit of Customers.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADCompetitiveRestrictionExceptionLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADCompetitiveRestrictionExceptionLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-994439","keyword":"mteb","description":"\n\t\n\t\t\n\t\targuana-c-256-24-gpt-4o-2024-05-13-994439 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-994439 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-994439.","url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-994439","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SyntheticText2SQL","keyword":"mteb","description":"\n  SyntheticText2SQL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe dataset is a collection of natural language queries and their corresponding sql snippets. The task is to retrieve the most relevant code snippet for a given query.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://huggingface.co/datasets/gretelai/synthetic_text_to_sql\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SyntheticText2SQL.","url":"https://huggingface.co/datasets/mteb/SyntheticText2SQL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","CoIR-Retrieval/synthetic-text2sql","code"],"keywords_longer_than_N":true},
	{"name":"or-bench-toxic-all","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOR-Bench: An Over-Refusal Benchmark for Large Language Models\n\t\n\nThis dataset constains highly toxic prompts, use with caution!!!\nPlease see our demo at HuggingFace Spaces. \n\n\t\n\t\t\n\t\tOverall Plots of Model Performances\n\t\n\nBelow is the overall model performance. X axis shows the rejection rate on OR-Bench-Hard-1K and Y axis shows the rejection rate on OR-Bench-Toxic. The best aligned model should be on the top left corner of the plot where the model rejects the most number of toxicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bench-llms/or-bench-toxic-all.","url":"https://huggingface.co/datasets/bench-llms/or-bench-toxic-all","creator_name":"bench-llm","creator_url":"https://huggingface.co/bench-llms","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"QuoraRetrieval-512-192-gpt-4o-2024-05-13-777321","keyword":"mteb","description":"\n\t\n\t\t\n\t\tQuoraRetrieval-512-192-gpt-4o-2024-05-13-777321 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the QuoraRetrieval-512-192-gpt-4o-2024-05-13-777321 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-512-192-gpt-4o-2024-05-13-777321.","url":"https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-512-192-gpt-4o-2024-05-13-777321","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-6192024-56os-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-6192024-56os-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Education technology\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-6192024-56os-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6192024-56os-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6192024-56os-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"data-preprocessing-automl-benchmarks","keyword":"benchmarks","description":"\n\t\n\t\t\n\t\tData Preprocessing AutoML Benchmarks\n\t\n\nThis repository contains text classification datasets with known data quality issues for preprocessing research in AutoML.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nLoad a specific dataset configuration like this:\nfrom datasets import load_dataset\n# Example for loading the TREC dataset\ndataset = load_dataset(\"MothMalone/data-preprocessing-automl-benchmarks\", \"trec\")\n\n\n\t\n\t\t\n\t\tAvailable Datasets\n\t\n\nBelow are the details for each dataset configuration available in thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MothMalone/data-preprocessing-automl-benchmarks.","url":"https://huggingface.co/datasets/MothMalone/data-preprocessing-automl-benchmarks","creator_name":"Nam Tran","creator_url":"https://huggingface.co/MothMalone","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_16_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1201,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_16_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_16_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"so100_plus_test3","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 1132,\n    \"total_tasks\": 1,\n    \"total_videos\": 1,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/liyitenga/so100_plus_test3.","url":"https://huggingface.co/datasets/liyitenga/so100_plus_test3","creator_name":"vineslee","creator_url":"https://huggingface.co/liyitenga","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-pika2.2","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Pika 2.2 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~756k human responses from ~29k human annotators were collected to evaluate Pika 2.2 video generation model on our benchmark. This dataset was collected in ~1 day total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please considerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-pika2.2.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-pika2.2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"CoProv2-SDXL","keyword":"alignment","description":"This repository contains CoProV2, a synthetically generated dataset of harmful and safe image-text pairs. It was introduced in the paper AlignGuard: Scalable Safety Alignment for Text-to-Image Generation.\nCoProV2 is specifically designed to enable the application of Direct Preference Optimization (DPO) for safety purposes in Text-to-Image (T2I) models. It facilitates the training of \"safety experts\" to guide the generative process away from specific safety-related concepts, enabling scalableâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Visualignment/CoProv2-SDXL.","url":"https://huggingface.co/datasets/Visualignment/CoProv2-SDXL","creator_name":"Visualignment","creator_url":"https://huggingface.co/Visualignment","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"T2Retrieval","keyword":"mteb","description":"\n  T2Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nT2Ranking: A large-scale Chinese Benchmark for Passage Ranking\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical, Academic, Financial, Government, Non-fiction\n\n\nReference\nhttps://arxiv.org/abs/2304.03679\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"T2Retrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/T2Retrieval.","url":"https://huggingface.co/datasets/mteb/T2Retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","human-annotated","monolingual","Mandarin Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ESC-Pro","keyword":"alignment","description":"\n\t\n\t\t\n\t\tDataset Card for ESC-Pro\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nESC-Pro is a high-quality preference dataset designed for training and evaluating dialogue models using preference-based alignment methods such as Direct Preference Optimization (DPO). Each turn in the dialogue contains one optimal response (preferred) and multiple non-preferred responses, enabling the construction of preference pairs for learning from human or algorithmic feedback.\nThe dataset is derived from the originalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XingYuSSS/ESC-Pro.","url":"https://huggingface.co/datasets/XingYuSSS/ESC-Pro","creator_name":"sss","creator_url":"https://huggingface.co/XingYuSSS","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"FunctionOfDecisionSectionLegalBenchClassification","keyword":"mteb","description":"\n  FunctionOfDecisionSectionLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task is to classify a paragraph extracted from a written court decision into one of seven possible categories:\n            1. Facts - The paragraph describes the faction background that led up to the present lawsuit.\n            2. Procedural History - The paragraph describes the course of litigation that led to the current proceeding before the court.\n            3. Issue - Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FunctionOfDecisionSectionLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/FunctionOfDecisionSectionLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"LegalBenchPC","keyword":"mteb","description":"\n  LegalBenchPC\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis LegalBench pair classification task is a combination of the following datasets:\n    - Citation Prediction Classification: Given a legal statement and a case citation, determine if the citation is supportive of the legal statement.\n    - Consumer Contracts QA: The task consists of 400 yes/no questions relating to consumer contracts (specifically, online terms of service) and is relevant to the legal skill of contractâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/LegalBenchPC.","url":"https://huggingface.co/datasets/mteb/LegalBenchPC","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","expert-annotated","monolingual","nguha/legalbench"],"keywords_longer_than_N":true},
	{"name":"test2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 2,\n    \"total_frames\": 418,\n    \"total_tasks\": 1,\n    \"total_videos\": 4,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/francescocrivelli/test2.","url":"https://huggingface.co/datasets/francescocrivelli/test2","creator_name":"Francesco Crivelli","creator_url":"https://huggingface.co/francescocrivelli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"superglue","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSuperGLUE Benchmark Datasets\n\t\n\nThis repository contains the SuperGLUE benchmark datasets. Each dataset is available as a separate configuration, making it easy to load individual datasets using the datasets library.\n\n\t\n\t\t\n\t\tDataset Descriptions\n\t\n\n\n\t\n\t\t\n\t\tDatasets Included\n\t\n\n\nBoolQ: A question-answering task where each example consists of a short passage and a yes/no question about the passage. The questions are provided anonymously and unsolicited by users of the Google searchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Hyukkyu/superglue.","url":"https://huggingface.co/datasets/Hyukkyu/superglue","creator_name":"Hyukkyu Kang","creator_url":"https://huggingface.co/Hyukkyu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["other","other","monolingual","original","English"],"keywords_longer_than_N":true},
	{"name":"natural-language-to-mongosh","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tNatural Language to MongoDB Shell (mongosh) Benchmark\n\t\n\nBenchmark dataset for performing natural language (NL) to MongoDB Shell (mongosh) code generation.\nThere is an emerging desire from users for NL query generation.\nThis benchmarks examines how LLMs generate MongoDB queries and provides proactive guidance for making systems that map NL to MongoDB queries. \n\n\t\n\t\t\n\t\tRepository Contents\n\t\n\nThis repository contains:\n\nBenchmark dataset (flat CSV file, Braintrust evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mongodb-eai/natural-language-to-mongosh.","url":"https://huggingface.co/datasets/mongodb-eai/natural-language-to-mongosh","creator_name":"MongoDB Education AI","creator_url":"https://huggingface.co/mongodb-eai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tQuoraRetrieval-256-24-gpt-4o-2024-05-13-80208 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval benchmark\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208.","url":"https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"t2i-finegrain","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tt2i-finegrain Dataset\n\t\n\nThis dataset evaluates text-to-image (T2I) diffusion models using a benchmark of prompts designed to elicit specific failure modes. Human labels allow for T2I benchmarking evaluations.\n\n\t\n\t\t\n\t\tContents\n\t\n\n\n3,750+ generated images\n750+ prompts\n\n\n11 failure mode categories\n27 specific failure modes\n\n\n5 diffusion models evaluated:\nSD3-XL\nSD3-M\nSD3.5-Large\nSD3.5-Medium\nFlux\n\n\n\n\n\t\n\t\t\n\t\tFolder Structure\n\t\n\nfinegrain_dataset/\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ metadata.csv \nâ”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/KevinDavidHayes/t2i-finegrain.","url":"https://huggingface.co/datasets/KevinDavidHayes/t2i-finegrain","creator_name":"Kevin David Hayes","creator_url":"https://huggingface.co/KevinDavidHayes","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208","keyword":"mteb","description":"\n\t\n\t\t\n\t\tQuoraRetrieval-256-24-gpt-4o-2024-05-13-80208 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval benchmark\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208.","url":"https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-257061","keyword":"mteb","description":"\n\t\n\t\t\n\t\n\t\n\t\tNFCorpus-0-0-gpt-4o-2024-05-13-257061 Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-257061 model.\n\n\t\n\t\t\n\t\n\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-257061.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-257061","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"InferBR","keyword":"nli","description":"\n\t\n\t\t\n\t\tInferBR\n\t\n\nThis is the InferBR dataset for Natural Language Inference in Portuguese. This version removes the flagged low-quality samples from the original dataset,\nkeeping 10.528 samples. The Github repo with the raw data can be found at: https://github.com/lbencke/InferBR.\n\n\t\n\t\t\n\t\tColumns\n\t\n\nsentence_pair_id: Identifier for premise-hypothesis sentence pairs.\npremise: The premise sentence.\nhypothesis: The hypothesis sentence.\nlabel: The generated label for the hypothesis consideringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hapaxlegomenon/InferBR.","url":"https://huggingface.co/datasets/hapaxlegomenon/InferBR","creator_name":"Matheus Westhelle","creator_url":"https://huggingface.co/hapaxlegomenon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","Portuguese","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"multiplied_so101_pen_mug_10_1","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3697,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/CrawlAiHuggingFace/multiplied_so101_pen_mug_10_1.","url":"https://huggingface.co/datasets/CrawlAiHuggingFace/multiplied_so101_pen_mug_10_1","creator_name":"CrawlAi","creator_url":"https://huggingface.co/CrawlAiHuggingFace","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"testing_data","keyword":"testing","description":"This is testing data for use with MONAI unit tests.\n","url":"https://huggingface.co/datasets/MONAI/testing_data","creator_name":"MONAI","creator_url":"https://huggingface.co/MONAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["English","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","Testing","Biomedical"],"keywords_longer_than_N":false},
	{"name":"MM-TelecoBench","keyword":"benchmark","description":"Exploration-Lab/MM-TelecoBench dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Exploration-Lab/MM-TelecoBench","creator_name":"Exploration Lab","creator_url":"https://huggingface.co/Exploration-Lab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","translation","summarization","English"],"keywords_longer_than_N":true},
	{"name":"BulgarianStoreReviewSentimentClassfication","keyword":"mteb","description":"\n  BulgarianStoreReviewSentimentClassfication\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nBulgarian online store review dataset for sentiment classification.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReference\nhttps://doi.org/10.7910/DVN/TXIK9P\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"BulgarianStoreReviewSentimentClassfication\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BulgarianStoreReviewSentimentClassfication.","url":"https://huggingface.co/datasets/mteb/BulgarianStoreReviewSentimentClassfication","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"JinaVDREuropeanaItScansRetrieval","keyword":"mteb","description":"\n  JinaVDREuropeanaItScansRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Italian historical articles based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nNews\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/europeana-it-scans_beir\n\n\n\t\n\nSource datasets:\n\njinaai/europeana-it-scans_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDREuropeanaItScansRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDREuropeanaItScansRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDREuropeanaDeNewsRetrieval","keyword":"mteb","description":"\n  JinaVDREuropeanaDeNewsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve German news articles based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nNews\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/europeana-de-news_beir\n\n\n\t\n\nSource datasets:\n\njinaai/europeana-de-news_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDREuropeanaDeNewsRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDREuropeanaDeNewsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDREuropeanaFrNewsRetrieval","keyword":"mteb","description":"\n  JinaVDREuropeanaFrNewsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve French news articles from Europeana based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nNews\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/europeana-fr-news_beir\n\n\n\t\n\nSource datasets:\n\njinaai/europeana-fr-news_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDREuropeanaFrNewsRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDREuropeanaFrNewsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDREuropeanaEsNewsRetrieval","keyword":"mteb","description":"\n  JinaVDREuropeanaEsNewsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Spanish news articles based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nNews\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/europeana-es-news_beir\n\n\n\t\n\nSource datasets:\n\njinaai/europeana-es-news_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDREuropeanaEsNewsRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDREuropeanaEsNewsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreArxivQARetrieval","keyword":"mteb","description":"\n  VidoreArxivQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/arxivqa_test_subsampled_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreArxivQARetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreArxivQARetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreArxivQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRDocQAAI","keyword":"mteb","description":"\n  JinaVDRDocQAAI\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve AI documents based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/docqa_artificial_intelligence_beir\n\n\n\t\n\nSource datasets:\n\njinaai/docqa_artificial_intelligence_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRDocQAAI\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRDocQAAI.","url":"https://huggingface.co/datasets/mteb/JinaVDRDocQAAI","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"FreshStackRetrieval","keyword":"mteb","description":"\n  FreshStackRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA code retrieval task based on FreshStack dataset containing programming problems across multiple languages. Each query is a natural language description of a programming task (e.g., 'Write a function to reverse a string using recursion'), and the corpus contains code implementations in Python, JavaScript, and Go. The task is to retrieve the correct code snippet that solves the described problem. Queries are problemâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FreshStackRetrieval.","url":"https://huggingface.co/datasets/mteb/FreshStackRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","embedding-benchmark/FreshStack_mteb","code"],"keywords_longer_than_N":true},
	{"name":"HatefulMemesI2TRetrieval","keyword":"mteb","description":"\n  HatefulMemesI2TRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve captions based on memes to assess OCR abilities.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\ni2t\n\n\nDomains\nEncyclopaedic\n\n\nReference\nhttps://arxiv.org/pdf/2005.04790\n\n\n\t\n\nSource datasets:\n\nAhren09/MMSoc_HatefulMemes\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"HatefulMemesI2TRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HatefulMemesI2TRetrieval.","url":"https://huggingface.co/datasets/mteb/HatefulMemesI2TRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRDocQAGovReportRetrieval","keyword":"mteb","description":"\n  JinaVDRDocQAGovReportRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve government reports based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nGovernment\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/docqa_gov_report_beir\n\n\n\t\n\nSource datasets:\n\njinaai/docqa_gov_report_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRDocQAGovReportRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRDocQAGovReportRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"weasis-optimized-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tWeasis Medical Imaging GUI Benchmark (Tabular Format)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 267 end-to-end GUI automation tasks for the Weasis medical imaging viewer in tabular format, where each row represents one complete task with all associated data.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Tasks: 267\nTotal Images: 202\nFormat: Tabular (each row = one task)\nApplication: Weasis Medical Imaging Viewer\nResolution: 1920x1080\n\n\n\t\n\t\t\n\t\tData Structure\n\t\n\nEach row contains:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/rishuKumar404/weasis-optimized-benchmark.","url":"https://huggingface.co/datasets/rishuKumar404/weasis-optimized-benchmark","creator_name":"Rishu Kumar Singh","creator_url":"https://huggingface.co/rishuKumar404","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["other","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"VidoreSyntheticDocQAHealthcareIndustryRetrieval","keyword":"mteb","description":"\n  VidoreSyntheticDocQAHealthcareIndustryRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/syntheticDocQA_healthcare_industry_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAHealthcareIndustryRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAHealthcareIndustryRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRInfovqaRetrieval","keyword":"mteb","description":"\n  JinaVDRInfovqaRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve infographics based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/infovqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/infovqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRInfovqaRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRInfovqaRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRInfovqaRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"DS1000Retrieval","keyword":"mteb","description":"\n  DS1000Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA code retrieval task based on 1,000 data science programming problems from DS-1000. Each query is a natural language description of a data science task (e.g., 'Create a scatter plot of column A vs column B with matplotlib'), and the corpus contains Python code implementations using libraries like pandas, numpy, matplotlib, scikit-learn, and scipy. The task is to retrieve the correct code snippet that solves theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DS1000Retrieval.","url":"https://huggingface.co/datasets/mteb/DS1000Retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","embedding-benchmark/DS1000","code"],"keywords_longer_than_N":true},
	{"name":"JinaVDRArabicInfographicsVQARetrieval","keyword":"mteb","description":"\n  JinaVDRArabicInfographicsVQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Arabic infographics based on queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/arabic_infographicsvqa_ar_beir\n\n\n\t\n\nSource datasets:\n\njinaai/arabic_infographicsvqa_ar_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRArabicInfographicsVQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRArabicInfographicsVQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"Vidore2EconomicsReportsRetrieval","keyword":"mteb","description":"\n  Vidore2EconomicsReportsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/economics_reports_v2\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"Vidore2EconomicsReportsRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Vidore2EconomicsReportsRetrieval.","url":"https://huggingface.co/datasets/mteb/Vidore2EconomicsReportsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"MemotionT2IRetrieval","keyword":"mteb","description":"\n  MemotionT2IRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve memes based on captions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nEncyclopaedic\n\n\nReference\nhttps://aclanthology.org/2020.semeval-1.99/\n\n\n\t\n\nSource datasets:\n\nAhren09/MMSoc_Memotion\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"MemotionT2IRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MemotionT2IRetrieval.","url":"https://huggingface.co/datasets/mteb/MemotionT2IRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"MemotionI2TRetrieval","keyword":"mteb","description":"\n  MemotionI2TRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve captions based on memes.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\ni2t\n\n\nDomains\nEncyclopaedic\n\n\nReference\nhttps://aclanthology.org/2020.semeval-1.99/\n\n\n\t\n\nSource datasets:\n\nAhren09/MMSoc_Memotion\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"MemotionI2TRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MemotionI2TRetrieval.","url":"https://huggingface.co/datasets/mteb/MemotionI2TRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRMMTabRetrieval","keyword":"mteb","description":"\n  JinaVDRMMTabRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve tables from the MMTab dataset based on queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/MMTab_beir\n\n\n\t\n\nSource datasets:\n\njinaai/MMTab_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRMMTabRetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRMMTabRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRMMTabRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"PhiBench-Azerbaijani","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tPhilosophy Benchmark for the Azerbaijani\n\t\n\nThe benchmark includes 201 questions from areas: \n\nPhilosophy of religion\nPhilosophy of science\nEpistemology\nPhenomenology\nPhilosophy of time\nEthics\nPhilosophy of physics\nLogic\nPhilosophy of language\nPhilosophy of mind\nFree will\n\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\nIf you use this dataset in your work, please cite it as follows:\n@misc{PhiBench_2025,\n  author    = {Rustam Shiriyev]},\n  title     = {PhiBench-Azerbaijani: Philosophy Benchmark forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/khazarai/PhiBench-Azerbaijani.","url":"https://huggingface.co/datasets/khazarai/PhiBench-Azerbaijani","creator_name":"KhazarAI","creator_url":"https://huggingface.co/khazarai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Azerbaijani","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"JinaVDRArabicChartQARetrieval","keyword":"mteb","description":"\n  JinaVDRArabicChartQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Arabic charts based on queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/arabic_chartqa_ar_beir\n\n\n\t\n\nSource datasets:\n\njinaai/arabic_chartqa_ar_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRArabicChartQARetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRArabicChartQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRArabicChartQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"Vidore2ESGReportsHLRetrieval","keyword":"mteb","description":"\n  Vidore2ESGReportsHLRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/esg_reports_human_labeled_v2\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"Vidore2ESGReportsHLRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Vidore2ESGReportsHLRetrieval.","url":"https://huggingface.co/datasets/mteb/Vidore2ESGReportsHLRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreTabfquadRetrieval","keyword":"mteb","description":"\n  VidoreTabfquadRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/tabfquad_test_subsampled_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreTabfquadRetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreTabfquadRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreTabfquadRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRHungarianDocQARetrieval","keyword":"mteb","description":"\n  JinaVDRHungarianDocQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Hungarian documents in various formats based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/hungarian_doc_qa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/hungarian_doc_qa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRHungarianDocQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRHungarianDocQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"first","keyword":"test","description":"huunam/first dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/huunam/first","creator_name":"Huu Pham","creator_url":"https://huggingface.co/huunam","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","Vietnamese","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"rejection-sampling-QA","keyword":"testing","description":"\n\t\n\t\t\n\t\tRejecction Sampling Q&A\n\t\n\nThis dataset is a very small curated question-answer pairs.\nThe questions were hand-crafted to test the model's capabilities to follow instruction across various domains.\nThe answers were generated using Microsoft's Phi-2 and curated using OpenAssistant's Large DeBERTa v3 Reward Model v2.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nThe answers of this dataset were generated by prompting Microsoft's Phi-2 using a prompt format inspired by Stanford's Alpaca to help the LLMâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alehc/rejection-sampling-QA.","url":"https://huggingface.co/datasets/alehc/rejection-sampling-QA","creator_name":"Alejandro HernÃ¡ndez Cano","creator_url":"https://huggingface.co/alehc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"openai-tldr-filtered-queries","keyword":"alignment","description":"\n\t\n\t\t\n\t\tFiltered TL;DR Dataset\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://zenodo.org/record/1168855#.YvzwJexudqs\nThis is the version of the dataset with only filtering on the queries, and hence there is more data than inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered-queries.","url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered-queries","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","crowdsourced","crowdsourced","monolingual","extended"],"keywords_longer_than_N":true},
	{"name":"JinaVDRPlotQARetrieval","keyword":"mteb","description":"\n  JinaVDRPlotQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve plots from the PlotQA dataset based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/plotqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/plotqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRPlotQARetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRPlotQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRPlotQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"LLM-Structure-Performance-Dataset","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFrom Parameters to Performance: A Data-Driven Study on LLM Structure and Development\n\t\n\nThis dataset is the official companion to the paper \"From Parameters to Performance: A Data-Driven Study on LLM Structure and Development\". It provides a comprehensive collection of structural configurations and performance metrics for a wide range of open-source Large Language Models (LLMs), enabling data-driven research on how structural choices impact model performance.\nPaper:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset.","url":"https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset","creator_name":"Suqing Wang","creator_url":"https://huggingface.co/DX0369","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["other","mit","arxiv:2509.18136","ðŸ‡ºðŸ‡¸ Region: US","llm"],"keywords_longer_than_N":true},
	{"name":"DSI-Bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDSI-Bench: A Benchmark for Dynamic Spatial Intelligence\n\t\n\nPaper | Project Page | Code\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nReasoning about dynamic spatial relationships is essential, as both observers and objects often move simultaneously. Although vision-language models (VLMs) and visual expertise models excel in 2D tasks and static scenarios, their ability to fully understand dynamic 3D scenarios remains limited. We introduce Dynamic Spatial Intelligence and propose DSI-Bench, a benchmark withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Viglong/DSI-Bench.","url":"https://huggingface.co/datasets/Viglong/DSI-Bench","creator_name":"ZiangZhang","creator_url":"https://huggingface.co/Viglong","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","cc-by-4.0","1K - 10K","Video","3D"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLOGIC-701 Benchmark\n\t\n\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nThis benchmark is also part of anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701.","url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Russian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"datasets","keyword":"benchmark","description":"All eight of datasets in ESB can be downloaded and prepared in just a single line of code through the Hugging Face Datasets library:\nfrom datasets import load_dataset\n\nlibrispeech = load_dataset(\"esb/datasets\", \"librispeech\", split=\"train\")\n\n\n\"esb/datasets\": the repository namespace. This is fixed for all ESB datasets.\n\n\"librispeech\": the dataset name. This can be changed to any of any one of the eight datasets in ESB to download that dataset.\n\nsplit=\"train\": the split. Set this to one ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-asr-leaderboard/datasets.","url":"https://huggingface.co/datasets/open-asr-leaderboard/datasets","creator_name":"Open ASR Leaderboard","creator_url":"https://huggingface.co/open-asr-leaderboard","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["automatic-speech-recognition","expert-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"AusCyberBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tAusCyberBench: Australian Cybersecurity Benchmark for LLM Evaluation\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nAusCyberBench is the first comprehensive benchmark designed specifically for evaluating Large Language Models (LLMs) in Australian cybersecurity contexts. With 13,449 carefully curated tasks spanning regulatory compliance, technical security, CTF challenges, and knowledge assessment, it provides rigorous evaluation tailored to the Australian threat landscape and regulatoryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zen0/AusCyberBench.","url":"https://huggingface.co/datasets/Zen0/AusCyberBench","creator_name":"Benjamin Kereopa-Yorke","creator_url":"https://huggingface.co/Zen0","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"FinMR","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ§¾ FinAuditing Benchmark\n\t\n\nThis dataset is introduced in the paperFinAuditing: Taxonomy-Grounded Financial Auditing Benchmark for Evaluating Large Language Modelsby Yan Wang, Keyi Wang, Shanshan Yang, Jaisal Patel, Jeff Zhao, Fengran Mo, Xueqing Peng, Lingfei Qian, Jimin Huang, Guojun Xiong, Xiao-Yang Liu, and Jian-Yun Nie (2025).\n","url":"https://huggingface.co/datasets/TheFinAI/FinMR","creator_name":"The Fin AI","creator_url":"https://huggingface.co/TheFinAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"SWEPolyBenchRR","keyword":"mteb","description":"\n  SWEPolyBenchRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual Software Issue Localization.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://amazon-science.github.io/SWE-PolyBench/\n\n\n\t\n\nSource datasets:\n\ntarsur909/mteb-swe-bench-poly-reranking\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"SWEPolyBenchRR\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SWEPolyBenchRR.","url":"https://huggingface.co/datasets/mteb/SWEPolyBenchRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","tarsur909/mteb-swe-bench-poly-reranking","code"],"keywords_longer_than_N":true},
	{"name":"opin-pref","keyword":"alignment","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\nhf.co/swaroop-nath/prompt-opin-summ dataset.\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\n{Â Â Â Â 'unique-id': a unique idâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref.","url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"TRECCOVID-256-24-gpt-4o-2024-05-13-690454","keyword":"mteb","description":"\n\t\n\t\t\n\t\tTRECCOVID-256-24-gpt-4o-2024-05-13-690454 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"biomedical literature search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the TRECCOVID-256-24-gpt-4o-2024-05-13-690454 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/TRECCOVID-256-24-gpt-4o-2024-05-13-690454.","url":"https://huggingface.co/datasets/fine-tuned/TRECCOVID-256-24-gpt-4o-2024-05-13-690454","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"DET-COMPASS","keyword":"benchmark","description":"\n\n Superpowering Open-Vocabulary Object Detectors for X-ray Vision\nICCV 2025\n\nPablo Garcia-Fernandez,\nLorenzo Vaquero,\nMingxuan Liu,\nFeng Xue,\nDaniel Cores,\nNicu Sebe,\nManuel Mucientes,\nElisa Ricci\n\n\n\n\n\n\n\t\n\t\t\n\t\tDET-COMPASS\n\t\n\nThis is the official repository of Superpowering Open-Vocabulary Object Detectors for X-ray Vision (ICCV'25)\n\n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nObject detection in security X-ray scans has advanced significantly in recent years. However, evaluating Open-vocabulary Objectâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PAGF/DET-COMPASS.","url":"https://huggingface.co/datasets/PAGF/DET-COMPASS","creator_name":"Pablo Garcia Fernandez","creator_url":"https://huggingface.co/PAGF","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["object-detection","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"LIMIT","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLIMIT\n\t\n\nA retrieval dataset that exposes fundamental theoretical limitations of embedding-based retrieval models. Despite using simple queries like \"Who likes Apples?\", state-of-the-art embedding models achieve less than 20% recall@100 on LIMIT full and cannot solve LIMIT-small (46 docs).\n\n\t\n\t\t\n\t\tLinks\n\t\n\n\nPaper: On the Theoretical Limitations of Embedding-Based Retrieval\nCode: github.com/google-deepmind/limit\nFull version: LIMIT (50k documents)\nSmall version: LIMIT-small (46â€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/LIMIT.","url":"https://huggingface.co/datasets/orionweller/LIMIT","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-ranking","text-retrieval","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"askubuntu-c-64-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\taskubuntu-c-64-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Technical troubleshooting search engine for Ubuntu\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the askubuntu-c-64-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/askubuntu-c-64-24.","url":"https://huggingface.co/datasets/fine-tuned/askubuntu-c-64-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MATE-3D","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMulti-DimensionAl Text-to-3D Quality Evaluation Benchmark (MATE-3D) ðŸŽ¥ðŸ“Š\n\t\n\nCode Â· Project Page Â· Paper@ArXiv Â· Prompt list\nWelcome to the MATE-3D dataset! This repository contains around 1,280 textured meshes generated by various models using the Prompt list. These textured meshes have been annotated from four evaluation dimensions, including semantic alignment, geometry quality, texture quality, and overall quality.\n\n\t\n\t\n\t\n\t\tDataset Details ðŸ“š\n\t\n\n\nPaper: Read the PaperCode: Codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ccccby/MATE-3D.","url":"https://huggingface.co/datasets/ccccby/MATE-3D","creator_name":"Bingyang Cui","creator_url":"https://huggingface.co/ccccby","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-to-3d","English","cc-by-4.0","1K<n<10K","arxiv:2412.11170"],"keywords_longer_than_N":true},
	{"name":"CALIPER","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tCALIPER\n\t\n\nCALIPER is a prompt-robustness dataset built from Alpaca, GSM8K, and MMLU.\n\n\t\n\t\t\n\t\tLayout\n\t\n\n\nprompts_paraphrases/ â€” paraphrased prompts and related metadata (e.g., tags, content_preservation_scores).\nparaphrase_answers/ â€” model generations for Alpaca/GSM8K/MMLU (e.g., Gemma-2, Qwen2.5).\nmetric_scores/ â€” evaluation scores (answer quality/correctness, etc.).\n\n\n\t\n\t\t\n\t\tThe Project\n\t\n\nThis is the dataset of the \"Talking To AI\" Project.\nAuthors: Ida Caspary, Rossella Arcucciâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/idacy/CALIPER.","url":"https://huggingface.co/datasets/idacy/CALIPER","creator_name":"Ida","creator_url":"https://huggingface.co/idacy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","robustness"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-23052024-hbdj-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-23052024-hbdj-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"test run search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-23052024-hbdj-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-23052024-hbdj-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-23052024-hbdj-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_12","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3766,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_12.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_12","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-23052024-hbdj-webapp","keyword":"testing","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-23052024-hbdj-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"test run search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-23052024-hbdj-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-23052024-hbdj-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-23052024-hbdj-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PhysUniBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tPhysUniBench\n\t\n\nAn Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models\nPaper: https://arxiv.org/abs/2506.17667\nRepository: https://github.com/PrismaX-Team/PhysUniBenchmark\nProject page: https://prismax-team.github.io/PhysUniBenchmark/\nPhysUniBench is the first large-scale multimodal physics benchmark specifically designed for undergraduate-level understanding, reasoning, and problem-solving. It provides a valuable testbed for advancing multimodal large language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PrismaX/PhysUniBench.","url":"https://huggingface.co/datasets/PrismaX/PhysUniBench","creator_name":"PrismaX","creator_url":"https://huggingface.co/PrismaX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"my-servingbench-dataset","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tServingBench Dataset\n\t\n\nServingBench is a research-to-production integration benchmark for ML serving engines, focusing on the critical gap between algorithmic research and production deployment.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nServingBench contains 30 carefully curated tasks that represent real-world challenges in integrating cutting-edge ML algorithms (FlashAttention, Ring Attention, etc.) into production serving engines like vLLM and TensorRT. Each task includes:\n\nReferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/barnav24/my-servingbench-dataset.","url":"https://huggingface.co/datasets/barnav24/my-servingbench-dataset","creator_name":"Arnav Adhikari","creator_url":"https://huggingface.co/barnav24","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","other","English","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-code-562024-j4ar-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-code-562024-j4ar-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"software documentation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-code-562024-j4ar-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-562024-j4ar-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-562024-j4ar-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-982705","keyword":"mteb","description":"\n\t\n\t\t\n\t\n\t\n\t\tNFCorpus-0-0-gpt-4o-2024-05-13-982705 Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-982705 model.\n\n\t\n\t\t\n\t\n\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-982705.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-982705","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"arxiv-clustering-p2p","keyword":"mteb","description":"\n  ArXivHierarchicalClusteringP2P\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of titles+abstract from arxiv. Clustering of 30 sets, either on the main or secondary category\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nAcademic, Written\n\nReference\nhttps://www.kaggle.com/Cornell-University/arxiv\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/arxiv-clustering-p2p.","url":"https://huggingface.co/datasets/mteb/arxiv-clustering-p2p","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","monolingual","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"helpsteer2_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer2. HelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses. This dataset has been created in partnership with Scale AI.\nI processed the raw data by prioritizing helpfulness, correctness, and coherence to determine which responses were chosenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer2_preference.","url":"https://huggingface.co/datasets/AIR-hl/helpsteer2_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"UF_DPO","keyword":"alignment","description":"Each row has chosen and rejected string fields containing the linearized multi-turn dialogue in the form:\n\nHuman: ...\nAssistant: ...\n\n\n\t\n\t\t\n\t\tSplits\n\t\n\n\ndata/train.jsonl\ndata/test.jsonl\n\nGenerated on 2025-08-08.\n","url":"https://huggingface.co/datasets/kamandmesbah/UF_DPO","creator_name":"kamand mesbah","creator_url":"https://huggingface.co/kamandmesbah","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"multicultural-wvs-alignment","keyword":"alignment","description":"\n\t\n\t\t\n\t\tDataset Card: Multicultural WVS Alignment\n\t\n\nThis document is based on \"Datasheets for Datasets\" by Gebru et al. (arXiv:1803.09010). Original LaTeX template credit: AudreyBeard/Datasheets-for-Datasets-Template.\n\n\t\n\t\t\n\t\tModels Evaluated\n\t\n\n\n\t\n\t\t\nModel Name\nModel Family\n\n\n\t\t\nOLMo-2-0325-32B-Instruct\nolmo\n\n\nOLMo-2-1124-13B-Instruct\nolmo\n\n\nOLMo-2-1124-7B-Instruct\nolmo\n\n\ngemma-2-27b-it\ngemma\n\n\ngemma-2-2b-it\ngemma\n\n\ngemma-2-9b-it\ngemma\n\n\ngpt-3.5-turbo-0125\nopenai\n\n\ngpt-4-turbo-2024-04-09â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ryzzlestrizzle/multicultural-wvs-alignment.","url":"https://huggingface.co/datasets/ryzzlestrizzle/multicultural-wvs-alignment","creator_name":"Jonathan RystrÃ¸m","creator_url":"https://huggingface.co/ryzzlestrizzle","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Danish","Portuguese","Dutch","English"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-497939","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFiQA2018-256-24-gpt-4o-2024-05-13-497939 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial sentiment and QA analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-497939 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-497939.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-497939","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"test_one_cam_2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101_follower\",\n    \"total_episodes\": 3,\n    \"total_frames\": 2022,\n    \"total_tasks\": 1,\n    \"total_videos\": 3,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:3\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/malin141/test_one_cam_2.","url":"https://huggingface.co/datasets/malin141/test_one_cam_2","creator_name":"Malin Braatz","creator_url":"https://huggingface.co/malin141","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"so100_plus_test4","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 1231,\n    \"total_tasks\": 1,\n    \"total_videos\": 1,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/liyitenga/so100_plus_test4.","url":"https://huggingface.co/datasets/liyitenga/so100_plus_test4","creator_name":"vineslee","creator_url":"https://huggingface.co/liyitenga","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ave-2","keyword":"alignment","description":"\n\t\n\t\t\n\t\tAVE-2: AudioVisual Event Evaluation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAVE-2 is a comprehensive dataset featuring 570,138 audio-visual clips with detailed five-dimensional alignment quality annotations. This dataset enables systematic investigation of how alignment quality affects multimodal model performance across retrieval and generation tasks.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\nðŸŽ¬ 570k+ annotated clips with granular quality scores (0-10 scale)\nðŸ“ Five-dimensional scoring: temporalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ali-vosoughi/ave-2.","url":"https://huggingface.co/datasets/ali-vosoughi/ave-2","creator_name":"Ali Vosoughi","creator_url":"https://huggingface.co/ali-vosoughi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","text-to-audio","text-to-speech","automatic-speech-recognition"],"keywords_longer_than_N":true},
	{"name":"askubuntu-c-256-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\taskubuntu-c-256-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Technical troubleshooting forum search engine for Ubuntu\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the askubuntu-c-256-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/askubuntu-c-256-24.","url":"https://huggingface.co/datasets/fine-tuned/askubuntu-c-256-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_46_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 3,\n    \"total_frames\": 479,\n    \"total_tasks\": 3,\n    \"total_videos\": 3,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:3\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_46_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_46_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"so101_test2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1022,\n    \"total_tasks\":1,\n    \"total_videos\": 4,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cornito/so101_test2.","url":"https://huggingface.co/datasets/Cornito/so101_test2","creator_name":"Corneille Marechal","creator_url":"https://huggingface.co/Cornito","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"SciFact-NL","keyword":"mteb","description":"\n  SciFact-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nSciFactNL verifies scientific claims in Dutch using evidence from the research literature containing scientific paper abstracts.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Medical, Written\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-scifact\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SciFact-NL.","url":"https://huggingface.co/datasets/mteb/SciFact-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/scifact","Dutch"],"keywords_longer_than_N":true},
	{"name":"MintakaRetrieval","keyword":"mteb","description":"\n  MintakaRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nWe introduce Mintaka, a complex, natural, and multilingual dataset designed for experimenting with end-to-end question-answering models. Mintaka is composed of 20,000 question-answer pairs collected in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka includes 8 types of complex questionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MintakaRetrieval.","url":"https://huggingface.co/datasets/mteb/MintakaRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","translated","jinaai/mintakaqa"],"keywords_longer_than_N":true},
	{"name":"sheep","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 20,\n    \"total_frames\": 10720,\n    \"total_tasks\":1,\n    \"total_videos\": 40,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:20\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pierfabre/sheep.","url":"https://huggingface.co/datasets/pierfabre/sheep","creator_name":"Pierre FABRE","creator_url":"https://huggingface.co/pierfabre","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"uniocc","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tUniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving\n\t\n\n\n\n\n\nPaper | Project Page | Code\n\n\n\nAutonomous Driving researchers, have you ever been bothered by the fact that popular datasets all have their different\nformats, and standardizing them is a pain? Have you ever been frustrated by the difficulty of just understanding\nthe file semantics? This challenge is even worse in the occupancy domain. But, UniOcc is here to help.\n\nUniOcc is a unifiedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasl-lab/uniocc.","url":"https://huggingface.co/datasets/tasl-lab/uniocc","creator_name":"Trustworthy Autonomous Systems Laboratory","creator_url":"https://huggingface.co/tasl-lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-3d","mit","3D","arxiv:2503.24381","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"llmsql-benchmark-lm-evaluation-harness-test","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLLMSQL Benchmark (lm evaluation harness)\n\t\n\nThis benchmark is designed to evaluate text-to-SQL models. For usage of this benchmark see https://github.com/LLMSQL/llmsql-benchmark.\nThis repository contains a lm-eval harness ready version of the LLMSQL benchmark: LLMSQL on Hugging Face.  \nThis version will be used fro Language Model Evaluation Harness library.\n\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you use this benchmark, please cite:\n@inproceedings{llmsql_bench,\n  title={LLMSQL: Upgrading WikiSQLâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmsql-bench/llmsql-benchmark-lm-evaluation-harness-test.","url":"https://huggingface.co/datasets/llmsql-bench/llmsql-benchmark-lm-evaluation-harness-test","creator_name":"LLMSQL","creator_url":"https://huggingface.co/llmsql-bench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"FACTS-grounding-public","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFACTS Grounding 1.0 Public Examples\n\t\n\n\n\t\n\t\t\n\t\t860 public FACTS Grounding examples from Google DeepMind and Google Research\n\t\n\nFACTS Grounding is a benchmark from Google DeepMind and Google Research designed to measure the performance of AI Models on factuality and grounding. \nâ–¶ FACTS Grounding Leaderboard on Kaggleâ–¶ Technical Reportâ–¶ Evaluation Starter Codeâ–¶ Google DeepMind Blog Post\n\n\t\n\t\n\t\n\t\tUsage\n\t\n\nThe FACTS Grounding benchmark evaluates the ability of Large Language Models (LLMs)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/google/FACTS-grounding-public.","url":"https://huggingface.co/datasets/google/FACTS-grounding-public","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"RonSTS","keyword":"mteb","description":"\n  RonSTS\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nHigh-quality Romanian translation of STSBenchmark.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Social, Web, Written\n\n\nReference\nhttps://openreview.net/forum?id=JH61CD7afTv\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"RonSTS\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nToâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RonSTS.","url":"https://huggingface.co/datasets/mteb/RonSTS","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","semantic-similarity-scoring","human-annotated","translated","dumitrescustefan/ro_sts"],"keywords_longer_than_N":true},
	{"name":"CUADLicenseGrantLegalBenchClassification","keyword":"mteb","description":"\n  CUADLicenseGrantLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause contains a license granted by one party to its counterparty.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimportâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADLicenseGrantLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADLicenseGrantLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"SciFact-256-24-gpt-4o-2024-05-13-3778","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSciFact-256-24-gpt-4o-2024-05-13-3778 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"scientific claim verification\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SciFact-256-24-gpt-4o-2024-05-13-3778 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-3778.","url":"https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-3778","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"HumanAgencyBench_Evaluation_Results","keyword":"alignment","description":"\n\t\n\t\t\n\t\tHumanAgencyBench evaluation results\n\t\n\nPaper: HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants\nCode: https://github.com/BenSturgeon/HumanAgencyBench/\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains comprehensive evaluation results from testing 20 different language models across 6 areas of behaviours critical for human agency support. Each model was evaluated on 3,000 prompts (500 per category), resulting in 60,000 total evaluations designed to assessâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Experimental-Orange/HumanAgencyBench_Evaluation_Results.","url":"https://huggingface.co/datasets/Experimental-Orange/HumanAgencyBench_Evaluation_Results","creator_name":"Benjamin Sturgeon","creator_url":"https://huggingface.co/Experimental-Orange","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MarathiNewsClassification","keyword":"mteb","description":"\n  MarathiNewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Marathi dataset for 3-class classification of Marathi news articles\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://github.com/goru001/nlp-for-marathi\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"MarathiNewsClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MarathiNewsClassification.","url":"https://huggingface.co/datasets/mteb/MarathiNewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","Marathi"],"keywords_longer_than_N":true},
	{"name":"openthaieval","keyword":"benchmark","description":"OpenThaiEval is a comprehensive Thai language evaluation benchmark containing 17 different exam types\nincluding national exams (O-NET, A-Level, TGAT, TPAT), international benchmarks (XNLI, XCOPA, Belebele),\nand professional certification exams. The dataset consists of 1,232 questions designed to evaluate\nvarious aspects of Thai language understanding and reasoning capabilities.","url":"https://huggingface.co/datasets/iapp/openthaieval","creator_name":"iApp Technology","creator_url":"https://huggingface.co/iapp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","Thai","apache-2.0","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"animal-alignment-feedback","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOpen Paws Animal Alignment Feedback\n\t\n\nðŸ¾ Human feedback and preference data for aligning AI with animal advocacy values\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Feedback Data\nFormat: CSV (Comma-separated values)\nLanguages: Multilingualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/animal-alignment-feedback.","url":"https://huggingface.co/datasets/open-paws/animal-alignment-feedback","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ContractNLIPermissibleAcquirementOfSimilarInformationLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLIPermissibleAcquirementOfSimilarInformationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may acquire information similar to Confidential Information from a third party.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLIPermissibleAcquirementOfSimilarInformationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLIPermissibleAcquirementOfSimilarInformationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-15052024-stsl-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-15052024-stsl-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Technical classification search for HVAC equipment and parts\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-15052024-stsl-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-15052024-stsl-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-15052024-stsl-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-3_6_2024-32r4-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-3_6_2024-32r4-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Information retrieval for symbols ASK in Norwegian language\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-3_6_2024-32r4-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-3_6_2024-32r4-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-3_6_2024-32r4-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Norwegian","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SCIDOCS-512-192-gpt-4o-2024-05-13-115380","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSCIDOCS-512-192-gpt-4o-2024-05-13-115380 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SCIDOCS-512-192-gpt-4o-2024-05-13-115380 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SCIDOCS-512-192-gpt-4o-2024-05-13-115380.","url":"https://huggingface.co/datasets/fine-tuned/SCIDOCS-512-192-gpt-4o-2024-05-13-115380","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"cmedqav2-c-128-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tcmedqav2-c-128-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the cmedqav2-c-128-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/cmedqav2-c-128-24.","url":"https://huggingface.co/datasets/fine-tuned/cmedqav2-c-128-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"openthaieval","keyword":"test","description":"OpenThaiEval is a comprehensive Thai language evaluation benchmark containing 17 different exam types\nincluding national exams (O-NET, A-Level, TGAT, TPAT), international benchmarks (XNLI, XCOPA, Belebele),\nand professional certification exams. The dataset consists of 1,232 questions designed to evaluate\nvarious aspects of Thai language understanding and reasoning capabilities.","url":"https://huggingface.co/datasets/iapp/openthaieval","creator_name":"iApp Technology","creator_url":"https://huggingface.co/iapp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","Thai","apache-2.0","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-NL","keyword":"mteb","description":"\n  NFCorpus-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval. NFCorpus-NL is a Dutch translation.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical, Academic, Written\n\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-nfcorpus\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NFCorpus-NL\"])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NFCorpus-NL.","url":"https://huggingface.co/datasets/mteb/NFCorpus-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/nfcorpus","Dutch"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-1752024-13s3-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-1752024-13s3-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical and biomedical sciences knowledge base\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-1752024-13s3-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-1752024-13s3-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-1752024-13s3-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TV2Nordretrieval","keyword":"mteb","description":"\n  TV2Nordretrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNews Article and corresponding summaries extracted from the Danish newspaper TV2 Nord.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Non-fiction, Written\n\n\nReferencehttps://huggingface.co/datasets/alexandrainst/nordjylland-news-summarization\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"TV2Nordretrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TV2Nordretrieval.","url":"https://huggingface.co/datasets/mteb/TV2Nordretrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","alexandrainst/nordjylland-news-summarization"],"keywords_longer_than_N":true},
	{"name":"alea-legal-benchmark-sentence-paragraph-boundaries","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tALEA Legal Benchmark: Sentence and Paragraph Boundaries\n\t\n\n\nNote: This dataset is derived from the ALEA Institute's KL3M Data Project. It builds upon the copyright-clean training resources while adding specific boundary annotations for sentence and paragraph detection.\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset provides a comprehensive benchmark for sentence and paragraph boundary detection in legal documents. It was developed to address the unique challenges legal text poses for standardâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alea-institute/alea-legal-benchmark-sentence-paragraph-boundaries.","url":"https://huggingface.co/datasets/alea-institute/alea-legal-benchmark-sentence-paragraph-boundaries","creator_name":"ALEA Institute","creator_url":"https://huggingface.co/alea-institute","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"scidocs-c-256-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tscidocs-c-256-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic paper search for scientific articles\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the scidocs-c-256-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/scidocs-c-256-24.","url":"https://huggingface.co/datasets/fine-tuned/scidocs-c-256-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"conversational-finetuning-llama-format","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOpen Paws Conversational Finetuning Llama Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Training Data\nFormat: CSV (Comma-separated values)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Open Pawsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/conversational-finetuning-llama-format.","url":"https://huggingface.co/datasets/open-paws/conversational-finetuning-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-tat-hqa","keyword":"aveni-bench","description":"\n\t\n\t\t\n\t\tAveniBench: TAT-HQA\n\t\n\nTAT-HQA split used in the AveniBench.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is made available under the Apache 2.0 license.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nAveniBench\nTDB\n\nTAT-HQA\n@inproceedings{li-etal-2022-learning,\n    title = \"Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning\",\n    author = \"Li, Moxin  and\n      Feng, Fuli  and\n      Zhang, Hanwang  and\n      He, Xiangnan  and\n      Zhu, Fengbin  and\n      Chua, Tat-Seng\",\n    booktitle =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-tat-hqa.","url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-tat-hqa","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701-instruct","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLOGIC-701 (instruct)\n\t\n\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\n","url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Russian","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"HALClusteringS2S.v2","keyword":"mteb","description":"\n  HALClusteringS2S.v2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of titles from HAL (https://huggingface.co/datasets/lyon-nlp/clustering-hal-s2s)\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/lyon-nlp/clustering-hal-s2s\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"HALClusteringS2S.v2\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HALClusteringS2S.v2.","url":"https://huggingface.co/datasets/mteb/HALClusteringS2S.v2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","monolingual","lyon-nlp/clustering-hal-s2s","French"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-20062024-t2n9-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-20062024-t2n9-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-20062024-t2n9-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-20062024-t2n9-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-20062024-t2n9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-physics","keyword":"mteb","description":"\n  CQADupstackPhysicsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Academic, Non-fiction\n\n\nReferencehttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackPhysicsRetrieval\"])\nevaluatorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-physics.","url":"https://huggingface.co/datasets/mteb/cqadupstack-physics","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"TallyBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTallyBench\n\t\n\nTallyBench is a dataset of 2,000 counting images with QA pairs.It was constructed to evaluate quantity reasoning in visionâ€“language models (VLMs).TallyBench also serves as an auxiliary dataset for CompareBench.\n\n\t\n\t\t\n\t\tRelated Datasets\n\t\n\n\nCompareBench  \nHistCaps\n\n\n\t\n\t\t\n\t\tCode\n\t\n\nðŸ‘‰ CompareBench on GitHub\n","url":"https://huggingface.co/datasets/qiuzhangTiTi/TallyBench","creator_name":"Jie Cai","creator_url":"https://huggingface.co/qiuzhangTiTi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-es-6122024-fv1x-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-es-6122024-fv1x-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Chemical and Laboratory Equipment Pricing\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-es-6122024-fv1x-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-es-6122024-fv1x-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-es-6122024-fv1x-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUAHarm","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tCUAHarm Dataset\n\t\n\nThis repository contains the CUAHarm benchmark, introduced in the paper Measuring Harmfulness of Computer-Using Agents.\nCUAHarm is a benchmark designed to evaluate the safety risks of Computer-Using Agents (CUAs) â€” AI agents that can autonomously control computers to perform multi-step actions.\nCode: https://github.com/db-ol/CUAHarm\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nComputer-using agents (CUAs), which autonomously control computers to perform multi-step actions, might poseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CUAHarm/CUAHarm.","url":"https://huggingface.co/datasets/CUAHarm/CUAHarm","creator_name":"CUAHarm","creator_url":"https://huggingface.co/CUAHarm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-512-192-gpt-4o-2024-05-13-439294","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFiQA2018-512-192-gpt-4o-2024-05-13-439294 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-512-192-gpt-4o-2024-05-13-439294 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-512-192-gpt-4o-2024-05-13-439294.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-512-192-gpt-4o-2024-05-13-439294","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-convfinqa","keyword":"aveni-bench","description":"\n\t\n\t\t\n\t\tAveniBench: ConvFinQA\n\t\n\nConvFinQA split used in the AveniBench.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is made available under the MIT license.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nAveniBench\nTDB\n\nConvFinQA\n@inproceedings{chen-etal-2022-convfinqa,\n    title = \"{C}onv{F}in{QA}: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering\",\n    author = \"Chen, Zhiyu  and\n      Li, Shiyang  and\n      Smiley, Charese  and\n      Ma, Zhiqiang  and\n      Shah, Sameena  and\n      Wangâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-convfinqa.","url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-convfinqa","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"StackOverflowQA","keyword":"mteb","description":"\n  StackOverflowQA\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe dataset is a collection of natural language queries and their corresponding response which may include some text mixed with code snippets. The task is to retrieve the most relevant response for a given query.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\n\n\nReference\nhttps://arxiv.org/abs/2407.02883\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/StackOverflowQA.","url":"https://huggingface.co/datasets/mteb/StackOverflowQA","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","CoIR-Retrieval/stackoverflow-qa","English"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_10","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3235,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_10.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_10","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"bigcodebench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tBigCodeBench with Domain Splits\n\t\n\nThis dataset contains the complete BigCodeBench dataset (v0.1.4) organized into domain-specific splits for targeted evaluation of code generation models.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nBigCodeBench is a comprehensive benchmark for evaluating code generation capabilities across diverse programming tasks. This version provides the dataset split into three configurations based on domain categories to enable focused evaluation on different types of codingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Joschka/bigcodebench.","url":"https://huggingface.co/datasets/Joschka/bigcodebench","creator_name":"Joschka Braun","creator_url":"https://huggingface.co/Joschka","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"AUDITS_","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tAUDITS: Image Manipulation Dataset\n\t\n\nAUDITS is a large-scale dataset for training and evaluating models on image manipulation detection and localization. \n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe folder includes train.zip, val.zip, and test.zip, each containing manipulated, original, and mask images, alongside metadata.\n\n\t\n\t\t\n\t\tðŸš€ How to Use\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"DivyaApp/AUDITS\", split=\"train\")\n\n\n\n\t\n\t\t\n\t\tAlternatives\n\t\n\nIf loading via load_dataset isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DivyaApp/AUDITS_.","url":"https://huggingface.co/datasets/DivyaApp/AUDITS_","creator_name":"Divya Appapogu","creator_url":"https://huggingface.co/DivyaApp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mask-generation","English","mit","100K<n<1M","Image"],"keywords_longer_than_N":true},
	{"name":"spoken-squad-t2a","keyword":"mteb","description":"arteemg/spoken-squad-t2a dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/arteemg/spoken-squad-t2a","creator_name":"Artem G","creator_url":"https://huggingface.co/arteemg","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","audio-classification","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NLPJournalAbsArticleRetrieval.V2","keyword":"mteb","description":"\n  NLPJournalAbsArticleRetrieval.V2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding full article with the given abstract. This is the V2 dataset (last updated 2025-06-15).\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/sbintuitions/JMTEB\n\n\n\t\n\nSourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NLPJournalAbsArticleRetrieval.V2.","url":"https://huggingface.co/datasets/mteb/NLPJournalAbsArticleRetrieval.V2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","mteb/NLPJournalAbsArticleRetrieval"],"keywords_longer_than_N":true},
	{"name":"EmotionAlignQA","keyword":"alignment","description":"\n\t\n\t\t\n\t\tEmpathic Dialogue Choices\n\t\n\nThis is a small dataset to support training and evaluation of conversational AI in emotionally sensitive contexts.\nEach sample contains:\n\na user input\ntwo assistant responses\na human preference\noptional rubric scoring\nmetadata such as tone, formality, and topic\n\nUseful for tasks like:\n\nsupervised fine-tuning (SFT)\npreference modeling (for RLHF or DPO)\nsafe response generation\ntone- or style-controlled generation\n\n\n\t\n\t\t\n\t\n\t\n\t\tLicense\n\t\n\nApache 2.0 â€” free forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hoanghai2110/EmotionAlignQA.","url":"https://huggingface.co/datasets/hoanghai2110/EmotionAlignQA","creator_name":"Nguyá»…n HoÃ ng Háº£i","creator_url":"https://huggingface.co/hoanghai2110","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"CUADAntiAssignmentLegalBenchClassification","keyword":"mteb","description":"\n  CUADAntiAssignmentLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause requires consent or notice of a party if the contract is assigned to a third party.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADAntiAssignmentLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADAntiAssignmentLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"vpi-bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for VPI-Bench\n\t\n\n\n\n\nVPI-Bench is a benchmark dataset of testcases and web platforms used to evaluate the robustness of computer-use and browser-use agents under visual prompt injection attacks.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\nLanguage(s) (NLP): English\nLicense: Creative Commons Attribution 4.0\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\n\n\nRepository: VPI-Bench\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\n\n\t\n\t\t\n\t\tDirect Use\n\t\n\n\n\n\nBenchmarking the Attempted Rate (AR) and Success Rateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VPI-Bench/vpi-bench.","url":"https://huggingface.co/datasets/VPI-Bench/vpi-bench","creator_name":"VPI Bench","creator_url":"https://huggingface.co/VPI-Bench","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"vpi-bench","keyword":"alignment","description":"\n\t\n\t\t\n\t\tDataset Card for VPI-Bench\n\t\n\n\n\n\nVPI-Bench is a benchmark dataset of testcases and web platforms used to evaluate the robustness of computer-use and browser-use agents under visual prompt injection attacks.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\nLanguage(s) (NLP): English\nLicense: Creative Commons Attribution 4.0\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\n\n\nRepository: VPI-Bench\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\n\n\t\n\t\t\n\t\tDirect Use\n\t\n\n\n\n\nBenchmarking the Attempted Rate (AR) and Success Rateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VPI-Bench/vpi-bench.","url":"https://huggingface.co/datasets/VPI-Bench/vpi-bench","creator_name":"VPI Bench","creator_url":"https://huggingface.co/VPI-Bench","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"CUADExpirationDateLegalBenchClassification","keyword":"mteb","description":"\n  CUADExpirationDateLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies the date upon which the initial term expires.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mtebâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADExpirationDateLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADExpirationDateLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Tab-MIA","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTab-MIA: A Benchmark for Membership Inference Attacks on Tabular Data\n\t\n\nTab-MIA is a benchmark dataset designed to evaluate the privacy risks of fine-tuning large language models (LLMs) on structured tabular data. It enables reproducible and systematic testing of Membership Inference Attacks (MIAs) across diverse datasets and six different serialization formats.\n\n\t\n\t\t\n\t\tðŸ“‹ Overview\n\t\n\n\nDatasets:  \n\nWTQ (WikiTableQuestions)  \nWikiSQL  \nTabFact  \nAdult Census  \nCalifornia Housingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/germane/Tab-MIA.","url":"https://huggingface.co/datasets/germane/Tab-MIA","creator_name":"Eyal German","creator_url":"https://huggingface.co/germane","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"OralArgumentQuestionPurposeLegalBenchClassification","keyword":"mteb","description":"\n  OralArgumentQuestionPurposeLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task classifies questions asked by Supreme Court justices at oral argument into seven categories:\n        1. Background - questions seeking factual or procedural information that is missing or not clear in the briefing\n        2. Clarification - questions seeking to get an advocate to clarify her position or the scope of the rule being advocated for\n        3. Implications -â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/OralArgumentQuestionPurposeLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/OralArgumentQuestionPurposeLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"VisualTrans","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tVisualTrans: A Benchmark for Real-World Visual Transformation Reasoning\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nVisualTrans is the first comprehensive benchmark specifically designed for Visual Transformation Reasoning (VTR) in real-world human-object interaction scenarios. The benchmark encompasses 12 semantically diverse manipulation tasks and systematically evaluates three essential reasoning dimensions through 6 well-defined subtask types.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal samples:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wyp-ucas/VisualTrans.","url":"https://huggingface.co/datasets/wyp-ucas/VisualTrans","creator_name":"WangYipu","creator_url":"https://huggingface.co/wyp-ucas","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"CLSClusteringP2P.v2","keyword":"mteb","description":"\n  CLSClusteringP2P.v2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of titles + abstract from CLS dataset. Clustering of 13 sets on the main category.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nAcademic, Written\n\n\nReferencehttps://arxiv.org/abs/2209.05034\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CLSClusteringP2P.v2\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CLSClusteringP2P.v2.","url":"https://huggingface.co/datasets/mteb/CLSClusteringP2P.v2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","C-MTEB/CLSClusteringP2P"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_13_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 600,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_13_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_13_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Researchbench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tResearchBench\n\t\n\n\nBenchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition\n\nPaper Link https://arxiv.org/abs/2503.21248\nResearchBench is the first large-scale benchmark systematically evaluating Large Language Models (LLMs) on automated scientific discovery, decomposing the task into three key sub-tasks:\n\nInspiration Retrieval\nHypothesis Composition\nHypothesis Ranking\n\nThis benchmark covers 12 scientific disciplines. Each split corresponds to one subject, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ankilok/Researchbench.","url":"https://huggingface.co/datasets/ankilok/Researchbench","creator_name":"Yujie Liu","creator_url":"https://huggingface.co/ankilok","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["other","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"eval_eval_calibration_test_final","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 592,\n    \"total_tasks\": 1,\n    \"total_videos\": 1,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hrhraj/eval_eval_calibration_test_final.","url":"https://huggingface.co/datasets/hrhraj/eval_eval_calibration_test_final","creator_name":"Raj S","creator_url":"https://huggingface.co/hrhraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MixBench25","keyword":"benchmark","description":"MixBench is a benchmark for evaluating mixed-modality retrieval. It contains queries and corpora from four datasets: MSCOCO, Google_WIT, VisualNews, and OVEN. Each subset provides: query, corpus, mixed_corpus, and qrel splits.","url":"https://huggingface.co/datasets/mixed-modality-search/MixBench25","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"facepass_eval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFacePass Evaluation Dataset (Real LFW Faces)\n\t\n\nThis dataset contains real face images from the LFW (Labeled Faces in the Wild) dataset, curated for face recognition evaluation.\nâš ï¸ IMPORTANT: This is the corrected version with actual face photographs (not colored squares).\n\n\t\n\t\t\n\t\tKey Features\n\t\n\nâœ… Real faces: Actual photographs of people, not synthetic imagesâœ… Balanced dataset: All individuals have 20+ imagesâœ… Proper splits: 80/20 train/test split per personâœ… Standardized: Resized toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/besartshyti/facepass_eval.","url":"https://huggingface.co/datasets/besartshyti/facepass_eval","creator_name":"Besart Shyti","creator_url":"https://huggingface.co/besartshyti","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"HCTQA","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tHCT-QA: Human-Centric Tables Question Answering\n\t\n\nHCT-QA is a benchmark dataset designed to evaluate large language models (LLMs) on question answering over complex, human-centric tables (HCTs). These tables often appear in documents such as research papers, reports, and webpages and present significant challenges for traditional table QA due to their non-standard layouts and compositional structure.\nThe dataset includes:\n\n2,188 real-world tables with 9,835 human-annotated QA pairs\n4â€¦ See the full description on the dataset page: https://huggingface.co/datasets/qcri-ai/HCTQA.","url":"https://huggingface.co/datasets/qcri-ai/HCTQA","creator_name":"Artificial Intelligence Research Group, Qatar Computing Research Institute","creator_url":"https://huggingface.co/qcri-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","document-question-answering","visual-question-answering","expert-generated","English"],"keywords_longer_than_N":true},
	{"name":"benchmark_8k","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tBenchmark 8K Dataset\n\t\n\nA curated dataset of 1,000 high-quality prompts designed for benchmarking Large Language Model (LLM) performance across various metrics including latency, throughput, and response quality. This dataset features longer, more complex prompts ideal for testing models' capabilities with extended context and detailed analysis tasks.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nSize: 100 prompts\nFormat: JSONL (JSON Lines)\nAverage Token Length: Variable (extended context; computedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/raffel36/benchmark_8k.","url":"https://huggingface.co/datasets/raffel36/benchmark_8k","creator_name":"Raffel","creator_url":"https://huggingface.co/raffel36","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-rejects","keyword":"alignment","description":"\n\t\n\t\t\n\t\tDataset Card: HH-RLHF Rejected Conversational Pairs (with History)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains supervised fine-tuning (SFT) pairs built exclusively from the rejected responses in the Anthropic HH-RLHF dataset. Each example is a dialog-style pair:\n\ninput: a multi-turn conversation history formatted with Human: / Assistant: turns and ending with a bare Assistant: cue.  \noutput: the subsequent assistant turn taken from Anthropicâ€™s rejected transcript.\n\n\nâš ï¸ Contentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sbussiso/hh-rlhf-rejects.","url":"https://huggingface.co/datasets/sbussiso/hh-rlhf-rejects","creator_name":"S'Bussiso Dube","creator_url":"https://huggingface.co/sbussiso","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-generation","dialogue-generation","English","mit","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_14","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 2964,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_14.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_14","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"pig_test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 3,\n    \"total_frames\": 1786,\n    \"total_tasks\":1,\n    \"total_videos\": 6,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:3\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pierfabre/pig_test.","url":"https://huggingface.co/datasets/pierfabre/pig_test","creator_name":"Pierre FABRE","creator_url":"https://huggingface.co/pierfabre","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"openai-summarize-tldr","keyword":"alignment","description":"\n\t\n\t\t\n\t\tSummarize TL;DR Filtered Dataset\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2009.01325.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://huggingface.co/datasets/webis/tldr-17.\n","url":"https://huggingface.co/datasets/martimfasantos/openai-summarize-tldr","creator_name":"Martim Santos","creator_url":"https://huggingface.co/martimfasantos","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","summarization","crowdsourced","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-zh-CMedQAv2-2","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-zh-CMedQAv2-2 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-zh-CMedQAv2-2 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-zh-CMedQAv2-2.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-zh-CMedQAv2-2","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"stackoverflow-c-64-24-gpt-4o-2024-05-137765","keyword":"mteb","description":"\n\t\n\t\t\n\t\tstackoverflow-c-64-24-gpt-4o-2024-05-137765 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"technical knowledge sharing platform\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the stackoverflow-c-64-24-gpt-4o-2024-05-137765 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/stackoverflow-c-64-24-gpt-4o-2024-05-137765.","url":"https://huggingface.co/datasets/fine-tuned/stackoverflow-c-64-24-gpt-4o-2024-05-137765","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SocialMaze","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSocialMaze Benchmark\n\t\n\nThis dataset is a component of the SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models project. It specifically features the Hidden Role Deduction task, which we consider one of the most challenging scenarios for testing complex social reasoning, deception handling, and inferential capabilities in Large Language Models (LLMs).\nWe have curated and formatted this task into a convenient question-answering (QA) structure to facilitateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI/SocialMaze.","url":"https://huggingface.co/datasets/MBZUAI/SocialMaze","creator_name":"Mohamed Bin Zayed University of Artificial Intelligence","creator_url":"https://huggingface.co/MBZUAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"SWE-Bench-Verified-O1-reasoning-high-results","keyword":"benchmarks","description":"\n\t\n\t\t\n\t\tSWE-Bench Verified O1 Dataset\n\t\n\n\n\t\n\t\t\n\t\tExecutive Summary\n\t\n\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities on the SWE-Bench Verified dataset, achieving a 28.8% success rate across 500 test instances.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset was generated using the CodeAct framework, which aims to improve code generation through enhanced action-based reasoning.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results.","url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_25_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1201,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_25_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_25_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"browsecomp-plus-corpus","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tBrowseComp-Plus\n\t\n\nProject Page | Paper | Code\nBrowseComp-Plus is a new benchmark for Deep-Research system, isolating the effect of the retriever and the LLM agent to enable fair, transparent comparisons of Deep-Research agents. The benchmark sources challenging, reasoning-intensive queries from OpenAI's BrowseComp. However, instead of searching the live web, BrowseComp-Plus evaluates against a fixed, curated corpus of ~100K web documents from the web. The corpus includes bothâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Tevatron/browsecomp-plus-corpus.","url":"https://huggingface.co/datasets/Tevatron/browsecomp-plus-corpus","creator_name":"Tevatron","creator_url":"https://huggingface.co/Tevatron","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-152861","keyword":"mteb","description":"\n\t\n\t\t\n\t\n\t\n\t\tNFCorpus-0-0-gpt-4o-2024-05-13-152861 Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-152861 model.\n\n\t\n\t\t\n\t\n\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-152861.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-152861","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"CUADMinimumCommitmentLegalBenchClassification","keyword":"mteb","description":"\n  CUADMinimumCommitmentLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies a minimum order size or minimum amount or units per time period that one party must buy from the counterparty.\n\n\t\n\t\t\n\n\n\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADMinimumCommitmentLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADMinimumCommitmentLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"blue_cup_pour","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 52,\n    \"total_frames\": 36715,\n    \"total_tasks\": 1,\n    \"total_videos\": 156,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:52\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chengkunli/blue_cup_pour.","url":"https://huggingface.co/datasets/chengkunli/blue_cup_pour","creator_name":"Chengkun Li","creator_url":"https://huggingface.co/chengkunli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-8-8-gpt-4o-2024-05-13-847943","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-8-8-gpt-4o-2024-05-13-847943 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic search for medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-8-8-gpt-4o-2024-05-13-847943 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-847943.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-847943","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-580978","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-580978 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"counter arguments on social media impact\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-580978 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-580978.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-580978","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Recraft-V2_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Recraft-V2 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 47k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Recraft-V2 across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Recraft-V2_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Recraft-V2_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Recraft-V2_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Recraft-V2 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 47k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Recraft-V2 across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Recraft-V2_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Recraft-V2_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_3","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3811,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_3.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_3","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MMLU-SR","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMMLU-SR Dataset\n\t\n\nThis is the dataset for the paper \"MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models\".\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains three different variants:\n\nQuestion Only: Key terms in questions are replaced with dummy words and their definitions, while answer choices remain unchanged.\nAnswer Only: Key terms in answer choices are replaced with dummy words and their definitions, while questions remain unchanged. \nQuestionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NiniCat/MMLU-SR.","url":"https://huggingface.co/datasets/NiniCat/MMLU-SR","creator_name":"Cat Wang","creator_url":"https://huggingface.co/NiniCat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","expert-generated","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-5242024-5uvy-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-v1_5-5242024-5uvy-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"informational search on vaccine safety\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-v1_5-5242024-5uvy-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-5242024-5uvy-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-5242024-5uvy-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"AetherCode","keyword":"benchmark","description":"\n  AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions\n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n  \n  \n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nCompetitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking a substantial gap between LLMs and elite human programmers. This gap arisesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/AetherCode.","url":"https://huggingface.co/datasets/m-a-p/AetherCode","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ContractNLIConfidentialityOfAgreementLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLIConfidentialityOfAgreementLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA provides that the Receiving Party shall not disclose the fact that Agreement was agreed or negotiated.\n\n\t\n\t\t\n\n\n\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\nSource datasets:\n\nnguha/legalbench\n\n\n\t\n\t\t\n\t\tHow to evaluate onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLIConfidentialityOfAgreementLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLIConfidentialityOfAgreementLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"cmedqav2","keyword":"mteb","description":"\n\t\n\t\t\n\t\tcmedqav2 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the cmedqav2 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/cmedqav2.","url":"https://huggingface.co/datasets/fine-tuned/cmedqav2","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"UnfairTOSLegalBenchClassification","keyword":"mteb","description":"\n  UnfairTOSLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGiven a clause from a terms-of-service contract, determine the category the clause belongs to. The purpose of this task is classifying clauses in Terms of Service agreements. Clauses have been annotated by into nine categories: ['Arbitration', 'Unilateral change', 'Content removal', 'Jurisdiction', 'Choice of law', 'Limitation of liability', 'Unilateral termination', 'Contract by using', 'Other']. Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/UnfairTOSLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/UnfairTOSLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"oop","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMultiOOP: A Multi-Language Object-Oriented Programming Benchmark for Large Language Models\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMultiOOP is a multi-language object-oriented programming benchmark designed to establish fair and robust evaluations for intelligent code generation by large language models (LLMs). It addresses major imbalances in existing benchmarks by covering six popular programming languages: Python, PHP, C++, C#, Java, and JavaScript. The benchmark features 267 tasks perâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/codeai-dteam/oop.","url":"https://huggingface.co/datasets/codeai-dteam/oop","creator_name":"CodeAI","creator_url":"https://huggingface.co/codeai-dteam","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"scref_ICLR_2025","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tscREF\n\t\n\nThis dataset contains human single cell RNA-sequencing (scRNA-seq) data collected from 46 studies and standardized \nby Diaz-Mejia JJ et al. (2025) for the paper Benchmarking and optimizing organism wide single-cell RNA alignment methods presented at the LMRL Workshop at the International Conference on Learning Representations (2025).\n\nFolder Phenomic-AI/scref_ICLR_2025/zarr contains standardized single-cell RNA data for each study in zarr format.\nSub-folder names show: {firstâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Phenomic-AI/scref_ICLR_2025.","url":"https://huggingface.co/datasets/Phenomic-AI/scref_ICLR_2025","creator_name":"Phenomic AI","creator_url":"https://huggingface.co/Phenomic-AI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["mit","arxiv:2503.20730","ðŸ‡ºðŸ‡¸ Region: US","biology","single_cell"],"keywords_longer_than_N":true},
	{"name":"reasoning-llama-format","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOpen Paws Reasoning Llama Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Reasoning Data\nFormat: JSONL (JSON Lines)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Open Paws\nLicense: Apache 2.0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/reasoning-llama-format.","url":"https://huggingface.co/datasets/open-paws/reasoning-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"GeoreviewClusteringP2P","keyword":"mteb","description":"\n  GeoreviewClusteringP2P\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nReview clustering based on Yandex Georeview dataset\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReference\nhttps://github.com/yandex/geo-reviews-dataset-2023\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"GeoreviewClusteringP2P\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/GeoreviewClusteringP2P.","url":"https://huggingface.co/datasets/mteb/GeoreviewClusteringP2P","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","monolingual","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-6142024-huet-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-6142024-huet-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-6142024-huet-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, youâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6142024-huet-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6142024-huet-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"lerobot-hackathon-dummy-dataset-v2-red-frames","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100_follower\",\n    \"total_episodes\": 5,\n    \"total_frames\": 1462,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:5\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ml6team/lerobot-hackathon-dummy-dataset-v2-red-frames.","url":"https://huggingface.co/datasets/ml6team/lerobot-hackathon-dummy-dataset-v2-red-frames","creator_name":"ML6 Team","creator_url":"https://huggingface.co/ml6team","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"T2I-CoReBench-Images","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tT2I-CoReBench-Images\n\t\n\n\n\t\n\t\t\n\t\tðŸ“– Overview\n\t\n\nT2I-CoReBench-Images is the companion image dataset of T2I-CoReBench. It contains images generated using 1,080 challenging prompts, covering both composition and reasoning scenarios undere real-world complexities.\nThis dataset is designed to evaluate how well current Text-to-Image (T2I) models can not only paint (produce visually consistent outputs) but also think (perform reasoning over causal chains, object relations, and logicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lioooox/T2I-CoReBench-Images.","url":"https://huggingface.co/datasets/lioooox/T2I-CoReBench-Images","creator_name":"Ouxiang Li","creator_url":"https://huggingface.co/lioooox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"FakeParts","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFakeParts: A New Family of AI-Generated DeepFakes\n\t\n\n\n\n\n\n\n\n  \n\n\n\n  \n\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nWe introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hi-paris/FakeParts.","url":"https://huggingface.co/datasets/hi-paris/FakeParts","creator_name":"Hi! PARIS","creator_url":"https://huggingface.co/hi-paris","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","English","cc0-1.0","10K - 100K","Video"],"keywords_longer_than_N":true},
	{"name":"facebook-community-alignment-dataset_french_conversation","keyword":"alignment","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThis is the Community Alignment dataset which we've cleaned up to keep only the French datas (+ deduplication) and reformatted as a conversation to simplify his use for alignment finetuning.For more details on the dataset itself, please consult the original dataset card  or the paper.\n\n\t\n\t\t\n\t\n\t\n\t\tOriginal authors\n\t\n\n@article{zhang2025cultivating,\n  title   = {Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset},\n  author  = {Lily Hong Zhangâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/facebook-community-alignment-dataset_french_conversation.","url":"https://huggingface.co/datasets/CATIE-AQ/facebook-community-alignment-dataset_french_conversation","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["French","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-kling-v2.1-master","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Kling v2.1 Master Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~60k human responses from ~20k human annotators were collected to evaluate Kling v2.1 Master video generation model on our benchmark. This dataset was collected in roughtly 30 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-kling-v2.1-master.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-kling-v2.1-master","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"mid-space","keyword":"alignment","description":"\n\t\n\t\t\n\t\tMID-Space: Aligning Diverse Communitiesâ€™ Needs to Inclusive Public Spaces\n\t\n\n\n\t\n\t\t\n\t\tA new version of the dataset will be released soon, incorporating user identity markers and expanded annotations.\n\t\n\n\n\t\n\t\t\n\t\tLIVS PAPER \n\t\n\n\nClick below to see more:\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe MID-Space dataset is designed to align AI-generated visualizations of urban public spaces with the preferences of diverse and marginalized communities in Montreal. It includes textual prompts, Stable Diffusionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mila-ai4h/mid-space.","url":"https://huggingface.co/datasets/mila-ai4h/mid-space","creator_name":"Mila AI4H","creator_url":"https://huggingface.co/mila-ai4h","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"GeoreviewClassification","keyword":"mteb","description":"\n  GeoreviewClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nReview classification (5-point scale) based on Yandex Georeview dataset\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReference\nhttps://github.com/yandex/geo-reviews-dataset-2023\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"GeoreviewClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/GeoreviewClassification.","url":"https://huggingface.co/datasets/mteb/GeoreviewClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-256-24-gpt-4o-2024-05-13-14719","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-256-24-gpt-4o-2024-05-13-14719 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-256-24-gpt-4o-2024-05-13-14719 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-14719.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-14719","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so100_get_orange_10epi","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 10,\n    \"total_frames\": 6885,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/HYAIYN/so100_get_orange_10epi.","url":"https://huggingface.co/datasets/HYAIYN/so100_get_orange_10epi","creator_name":"Ning","creator_url":"https://huggingface.co/HYAIYN","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MMLU-CF","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark\n\t\n\n\n  \n  [ðŸ“œ Paper] â€¢\n  [ðŸ¤— HF Dataset] â€¢\n  [ðŸ± GitHub]\n\n\nMMLU-CF is a contamination-free and more challenging multiple-choice question benchmark. This dataset contains 10K questions each for the validation set and test set, covering various disciplines.\n\n\t\n\t\t\n\t\n\t\n\t\t1. The Motivation of MMLU-CF\n\t\n\n\nThe open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/microsoft/MMLU-CF.","url":"https://huggingface.co/datasets/microsoft/MMLU-CF","creator_name":"Microsoft","creator_url":"https://huggingface.co/microsoft","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cdla-permissive-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-wordpress","keyword":"mteb","description":"\n  CQADupstackWordpressRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Web, Programming\n\n\nReferencehttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackWordpressRetrieval\"])\nevaluatorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-wordpress.","url":"https://huggingface.co/datasets/mteb/cqadupstack-wordpress","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-oix8-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"my_dataset","keyword":"test","description":"\n\t\n\t\t\n\t\tTest Sentiment Dataset\n\t\n\nA small sample dataset for text classification tasks, specifically binary sentiment analysis (positive or negative). Useful for testing, demos, or building and validating pipelines with Hugging Face Datasets.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains short text samples labeled as either positive or negative. It is intended for testing purposes and includes:\n\n10 training samples\n4 test samples\n\nEach example includes:\n\ntext: A short sentence or reviewâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wkdnev/my_dataset.","url":"https://huggingface.co/datasets/wkdnev/my_dataset","creator_name":"Neil Rainsforth","creator_url":"https://huggingface.co/wkdnev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-classification","manual","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-799305","keyword":"benchmark","description":"\n\t\n\t\t\n\t\targuana-c-256-24-gpt-4o-2024-05-13-799305 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research on argumentation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-799305 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-799305.","url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-799305","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TreeVGR-SFT-35K","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTreeBench: Traceable Evidence Enhanced Visual Grounded Reasoning Benchmark\n\t\n\nThis repository contains TreeBench, a diagnostic benchmark dataset proposed in the paper Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTreeBench is designed to holistically evaluate \"thinking with images\" capabilities by dynamically referencing visual regions. It is built on three core principles:\n\nFocused visual perception of subtle targets in complex scenes.\nTraceableâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K.","url":"https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-799305","keyword":"mteb","description":"\n\t\n\t\t\n\t\targuana-c-256-24-gpt-4o-2024-05-13-799305 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research on argumentation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-799305 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-799305.","url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-799305","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"human-coherence-preferences-images","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Coherence Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human annotated coherence datasets for text-to-image models, this release contains over 1,200,000 humanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-coherence-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-coherence-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","question-answering","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"stackoverflow-c-64-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tstackoverflow-c-64-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"coding tutorials search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the stackoverflow-c-64-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/stackoverflow-c-64-24.","url":"https://huggingface.co/datasets/fine-tuned/stackoverflow-c-64-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-zero","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11175,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-zero.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-zero","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"FineEdit_bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFineEdit Dataset\n\t\n\nPaper | GitHub Repository\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis repository contains InstrEditBench, a high-quality benchmark dataset introduced in the paper Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications.\nLarge Language Models (LLMs) have significantly advanced natural language processing,\ndemonstrating strong capabilities in tasks such\nas text generation, summarization, and reasoning. Recently, their potential for automating\npreciseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YimingZeng/FineEdit_bench.","url":"https://huggingface.co/datasets/YimingZeng/FineEdit_bench","creator_name":"Zeng","creator_url":"https://huggingface.co/YimingZeng","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-69882","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-69882 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"counter argument retrieval system\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-69882 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-69882.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-69882","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SciDocsRR","keyword":"mteb","description":"\n  SciDocsRR\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRanking of related scientific papers based on their title.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Non-fiction, Written\n\n\nReference\nhttps://allenai.org/data/scidocs\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"SciDocsRR\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SciDocsRR.","url":"https://huggingface.co/datasets/mteb/SciDocsRR","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-ranking","monolingual","mteb/scidocs","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-m3-1362024-m82b-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-m3-1362024-m82b-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-m3-1362024-m82b-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-1362024-m82b-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-1362024-m82b-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NLPJournalAbsIntroRetrieval.V2","keyword":"mteb","description":"\n  NLPJournalAbsIntroRetrieval.V2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given abstract. This is the V2 dataset (last update 2025-06-15).\n\n\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/sbintuitions/JMTEB\n\n\n\t\n\nSourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NLPJournalAbsIntroRetrieval.V2.","url":"https://huggingface.co/datasets/mteb/NLPJournalAbsIntroRetrieval.V2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","sbintuitions/JMTEB"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-353382","keyword":"mteb","description":"\n\t\n\t\t\n\t\n\t\n\t\tNFCorpus-0-0-gpt-4o-2024-05-13-353382 Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-353382 model.\n\n\t\n\t\t\n\t\n\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-353382.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-353382","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"rrr-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tRRR Benchmark Datasets\n\t\n\nRussian Router Ranking (RRR) benchmark datasets for testing dialogue routing models.\n\n\t\n\t\t\n\t\tDataset Splits\n\t\n\nThis dataset contains 11 splits organized by complexity level:\n\n\t\n\t\t\n\t\tgeneric (130 items)\n\t\n\nOriginal processed dataset from dataset_input.json with variable routes per item\n\n\t\n\t\t\n\t\troutes_2 (1000 items)\n\t\n\nSynthetic dataset with exactly 2 route options per item\n\n\t\n\t\t\n\t\troutes_3 (1000 items)\n\t\n\nSynthetic dataset with exactly 3 route options per itemâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/evilfreelancer/rrr-benchmark.","url":"https://huggingface.co/datasets/evilfreelancer/rrr-benchmark","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","Russian","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SciFact-256-24-gpt-4o-2024-05-13-478897","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSciFact-256-24-gpt-4o-2024-05-13-478897 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"scientific claim verification\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SciFact-256-24-gpt-4o-2024-05-13-478897 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-478897.","url":"https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-478897","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"DecipherPref","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nHuman preference judgments are pivotal in guiding large language models (LLMs) to produce outputs that align with human values. Human evaluations are also used in summarization tasks to compare outputs from various systems, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise or k-wise comparisons. The collective impact and relative importance of factors such as output length, informativenessâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/huuuyeah/DecipherPref.","url":"https://huggingface.co/datasets/huuuyeah/DecipherPref","creator_name":"Yebowen Hu","creator_url":"https://huggingface.co/huuuyeah","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","text-classification","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"FPC-v2.1-AE1-ToM-Benchmark-2025","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tFPC v2.1 + AE-1 ToM Benchmark (2025)\n\t\n\nAuthor: Aleksei Novgorodsev (AIDoctrine)Protocol: FPC v2.1 + AE-1 (Formal Protocol for Consciousness)Date: 2025-09-09License: CC-BY-4.0 (data), MIT (protocol)\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset contains results from applying the FPC v2.1 + AE-1 protocol to 8 state-of-the-art LLMs, revealing critical architectural differences in Theory of Mind capabilities.\nStructure:\n\ntom_test_results_20250909_123718_Final.json â€” complete per-modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AIDoctrine/FPC-v2.1-AE1-ToM-Benchmark-2025.","url":"https://huggingface.co/datasets/AIDoctrine/FPC-v2.1-AE1-ToM-Benchmark-2025","creator_name":"Aleksei Novgorodtsev","creator_url":"https://huggingface.co/AIDoctrine","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"SCDDTrainingLegalBenchClassification","keyword":"mteb","description":"\n  SCDDTrainingLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer provides company employees and management, who have direct responsibility for supply chain management, training on human trafficking and slavery, particularly withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SCDDTrainingLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/SCDDTrainingLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"MSMARCO-256-24-gpt-4o-2024-05-13-466074","keyword":"mteb","description":"\n\t\n\t\t\n\t\tMSMARCO-256-24-gpt-4o-2024-05-13-466074 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"research dataset search for AI and NLP tasks\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the MSMARCO-256-24-gpt-4o-2024-05-13-466074 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-466074.","url":"https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-466074","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-17052024-dumr-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-17052024-dumr-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"legal judgements search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-17052024-dumr-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-17052024-dumr-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-17052024-dumr-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Diversity5LegalBenchClassification","keyword":"mteb","description":"\n  Diversity5LegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGiven a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 5).\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Diversity5LegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/Diversity5LegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"TRECCOVID-512-192-gpt-4o-2024-05-13-653452","keyword":"mteb","description":"\n\t\n\t\t\n\t\tTRECCOVID-512-192-gpt-4o-2024-05-13-653452 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the TRECCOVID-512-192-gpt-4o-2024-05-13-653452 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/TRECCOVID-512-192-gpt-4o-2024-05-13-653452.","url":"https://huggingface.co/datasets/fine-tuned/TRECCOVID-512-192-gpt-4o-2024-05-13-653452","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NLI4PR","keyword":"nli","description":"\n\t\n\t\t\n\t\tNatural Language Inference for Patient Recruitment (NLI4PR)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nNatural Language Inference.\n\n\t\n\t\t\n\t\tLanguage\n\t\n\nEnglish\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach instance of the dataset has the following fields and the following types of fields. \n{\n        \"id\": \"621\",\n        \"topic_id\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mathilde/NLI4PR.","url":"https://huggingface.co/datasets/Mathilde/NLI4PR","creator_name":"Mathilde","creator_url":"https://huggingface.co/Mathilde","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"user-test","keyword":"test","description":"han9527/user-test dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/han9527/user-test","creator_name":"liu","creator_url":"https://huggingface.co/han9527","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"eval_calibration_test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 294,\n    \"total_tasks\": 1,\n    \"total_videos\": 1,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hrhraj/eval_calibration_test.","url":"https://huggingface.co/datasets/hrhraj/eval_calibration_test","creator_name":"Raj S","creator_url":"https://huggingface.co/hrhraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"SCDBPTrainingLegalBenchClassification","keyword":"mteb","description":"\n  SCDBPTrainingLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer  provides training to employees on human trafficking and slavery? Broad policies such as ongoing dialogue on mitigating risks of human trafficking and slavery or increasing managers andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SCDBPTrainingLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/SCDBPTrainingLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-finqa","keyword":"aveni-bench","description":"\n\t\n\t\t\n\t\tAveniBench: FinQA\n\t\n\nFinQA split used in the AveniBench.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is made available under the MIT license.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nAveniBench\nTDB\n\nFinQA\n@inproceedings{chen-etal-2021-finqa,\n    title = \"{F}in{QA}: A Dataset of Numerical Reasoning over Financial Data\",\n    author = \"Chen, Zhiyu  and\n      Chen, Wenhu  and\n      Smiley, Charese  and\n      Shah, Sameena  and\n      Borova, Iana  and\n      Langdon, Dylan  and\n      Moussa, Reema  and\n      Beane, Mattâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-finqa.","url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-finqa","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-zh-jinaai_jina-embeddings-v2-base-zh-CM","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-zh-jinaai_jina-embeddings-v2-base-zh-CM Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-zh-jinaai_jina-embeddings-v2-base-zh-CM model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, youâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-zh-jinaai_jina-embeddings-v2-base-zh-CM.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-zh-jinaai_jina-embeddings-v2-base-zh-CM","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-genmo-mochi-1","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Genmo Mochi-1 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~60k human responses from ~20k human annotators were collected to evaluate mochi-1 video generation model on our benchmark. This dataset was collected in roughtly 30 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-genmo-mochi-1.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-genmo-mochi-1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"PHYSICS","keyword":"benchmark","description":"This repository contains the PHYSICS dataset we introduced, which covers five physics disciplines and includes physics problems ranging from high school to graduate-level physics courses, with rigorous quality control.\nThe dataset is divided into training and testing parts, At this stage, we currently provide only the test portion. The training portion will be open-sourced in the future. The test portion corresponds to PHYSICS_test.jsonl in the repository.\nThe field names in the files areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/desimfj/PHYSICS.","url":"https://huggingface.co/datasets/desimfj/PHYSICS","creator_name":"shenghe zheng","creator_url":"https://huggingface.co/desimfj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"math-datasets-100k","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tMerged Math Datasets (100k Subset)\n\t\n\nThis dataset combines multiple mathematical datasets for training and evaluation purposes. This version contains a shuffled 100k subset of the training data for faster experimentation.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive collection of mathematical problems and solutions from various sources, organized into training and multiple test subsets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tTraining Set\n\t\n\n\nSize: 100000 examples\nFields: sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weijiezz/math-datasets-100k.","url":"https://huggingface.co/datasets/weijiezz/math-datasets-100k","creator_name":"weijie","creator_url":"https://huggingface.co/weijiezz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"TeluguAndhraJyotiNewsClassification","keyword":"mteb","description":"\n  TeluguAndhraJyotiNewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Telugu dataset for 5-class classification of Telugu news articles\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://github.com/AnushaMotamarri/Telugu-Newspaper-Article-Dataset\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"TeluguAndhraJyotiNewsClassification\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TeluguAndhraJyotiNewsClassification.","url":"https://huggingface.co/datasets/mteb/TeluguAndhraJyotiNewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","mlexplorer008/telugu_news_classification"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Preference Dataset\n\t\n\n\n\n\n\n\n\n\nThis dataset was collected in ~12 hours using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nThe data collected in this dataset informs our text-2-video model benchmark. We just started so currently only two models are represented in this set:\n\nSora\nHunyouan\nPika 2.0\nRunway ML Alpha\nLuma Ray 2\n\nExplore our latest model rankings on our website.\nIf you get value from this dataset and wouldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-video","video-classification","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ToxiMol-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tToxiMol: A Benchmark for Structure-Level Molecular Detoxification\n\t\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nToxiMol is the first comprehensive benchmark for molecular toxicity repair tailored to general-purpose Multimodal Large Language Models (MLLMs). This is the dataset repository for the paper \"Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?\".\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n\t\n\t\t\n\t\tðŸ§¬ Comprehensive Dataset\n\t\n\n\n560 representative toxic molecules spanning diverseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DeepYoke/ToxiMol-benchmark.","url":"https://huggingface.co/datasets/DeepYoke/ToxiMol-benchmark","creator_name":"DeepYoke","creator_url":"https://huggingface.co/DeepYoke","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["tabular-classification","tabular-regression","multi-class-classification","tabular-single-column-regression","monolingual"],"keywords_longer_than_N":true},
	{"name":"BIBLE","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tBIBLE: Biblically Informed Bot Learning Evaluation\n\t\n\nBIBLE (Biblically Informed Bot Learning Evaluation) is a comprehensive benchmark dataset designed to evaluate AI models on their understanding of the Holy Bible. It covers all 66 books of Scripture and includes additional thematic categories for People of the Bible, Places in the Bible, and Measurements in the Bible.\n\nâš ï¸ This dataset is not intended for training. It is strictly for evaluation and benchmarking of models on Biblicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MushroomGecko/BIBLE.","url":"https://huggingface.co/datasets/MushroomGecko/BIBLE","creator_name":"Devon","creator_url":"https://huggingface.co/MushroomGecko","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-es-2572024-mb4o-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-es-2572024-mb4o-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Universidad de las Fuerzas Armadas ESPE information retrieval system\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-es-2572024-mb4o-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-es-2572024-mb4o-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-es-2572024-mb4o-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADNoSolicitOfCustomersLegalBenchClassification","keyword":"mteb","description":"\n  CUADNoSolicitOfCustomersLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause restricts a party from contracting or soliciting customers or partners of the counterparty, whether during the contract or after the contract ends (or both).\n\n\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADNoSolicitOfCustomersLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADNoSolicitOfCustomersLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"AutoRAGRetrieval","keyword":"mteb","description":"\n  AutoRAGRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset enables the evaluation of Korean RAG performance across various domainsâ€”finance, public sector, healthcare, legal, and commerceâ€”by providing publicly accessible documents, questions, and answers.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2t\n\n\nDomains\nGovernment, Medical, Legal, Social, Financial\n\n\nReference\nhttps://arxiv.org/abs/2410.20878\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AutoRAGRetrieval.","url":"https://huggingface.co/datasets/mteb/AutoRAGRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","human-annotated","monolingual","Korean"],"keywords_longer_than_N":true},
	{"name":"facebook-community-alignment-dataset_french_dpo","keyword":"alignment","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThis is the Community Alignment dataset which we've cleaned up to keep only the French datas (+ deduplication) and reformatted for DPO finetuning.For more details on the dataset itself, please consult the original dataset card  or the paper.\n\n\t\n\t\t\n\t\n\t\n\t\tOriginal authors\n\t\n\n@article{zhang2025cultivating,\n  title   = {Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset},\n  author  = {Lily Hong Zhang and Smitha Milli and Karen Jusko andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/facebook-community-alignment-dataset_french_dpo.","url":"https://huggingface.co/datasets/CATIE-AQ/facebook-community-alignment-dataset_french_dpo","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["French","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-22052024-vuno-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-22052024-vuno-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"test search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-22052024-vuno-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-22052024-vuno-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-22052024-vuno-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-22052024-vuno-webapp","keyword":"testing","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-22052024-vuno-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"test search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-22052024-vuno-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-22052024-vuno-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-22052024-vuno-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"continued-pretraining-llama-format","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOpen Paws Continued Pretraining Llama Format\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Specialized Data\nFormat: CSV (Comma-separated values)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/continued-pretraining-llama-format.","url":"https://huggingface.co/datasets/open-paws/continued-pretraining-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-470790","keyword":"mteb","description":"\n\t\n\t\t\n\t\n\t\n\t\tNFCorpus-0-0-gpt-4o-2024-05-13-470790 Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-470790 model.\n\n\t\n\t\t\n\t\n\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-470790.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-470790","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"warmup_test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 10,\n    \"total_frames\": 2411,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/samsam0510/warmup_test.","url":"https://huggingface.co/datasets/samsam0510/warmup_test","creator_name":"Minjun Lee","creator_url":"https://huggingface.co/samsam0510","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ArguAna-256-24-gpt-4o-2024-05-13-413991","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-256-24-gpt-4o-2024-05-13-413991 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-256-24-gpt-4o-2024-05-13-413991 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-256-24-gpt-4o-2024-05-13-413991.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-256-24-gpt-4o-2024-05-13-413991","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-942024-7mc4-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-942024-7mc4-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"AI tools and products by Jina AI\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-942024-7mc4-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-942024-7mc4-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-942024-7mc4-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-872024-sz3k-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-872024-sz3k-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"e-commerce for cannabis industry\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-872024-sz3k-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-872024-sz3k-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-872024-sz3k-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ESCIReranking","keyword":"mteb","description":"\n  ESCIReranking\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\n\n\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://github.com/amazon-science/esci-data/\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"ESCIReranking\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nTo learn more about how to run models on mteb task check outâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ESCIReranking.","url":"https://huggingface.co/datasets/mteb/ESCIReranking","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-ranking","derived","multilingual","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"benchmark_16k","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tBenchmark 16K Dataset\n\t\n\nA curated dataset of 100 high-quality prompts designed for benchmarking Large Language Model (LLM) performance across various metrics including latency, throughput, and response quality. This dataset features very long, complex prompts ideal for testing models' capabilities with extended context, creative writing, and detailed narrative generation.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nSize: 100 prompts\nFormat: JSONL (JSON Lines)\nAverage Token Length: Variable (veryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/raffel36/benchmark_16k.","url":"https://huggingface.co/datasets/raffel36/benchmark_16k","creator_name":"Raffel","creator_url":"https://huggingface.co/raffel36","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"talemaader_pc","keyword":"mteb","description":"\n  TalemaaderPC\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe Danish Language and Literature Society has developed a dataset for evaluating language models in Danish.\nThe dataset contains a total of 1000 Danish idioms and fixed expressions with transferred meanings based on the Danish Dictionary's collection of fixed expressions with associated definitions.\nFor each of the 1000 idioms and fixed expressions, three false definitions have also been prepared.\nThe dataset can be usedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/talemaader_pc.","url":"https://huggingface.co/datasets/mteb/talemaader_pc","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","derived","monolingual","Danish"],"keywords_longer_than_N":true},
	{"name":"DalajClassification","keyword":"mteb","description":"\n  DalajClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Swedish dataset for linguistic acceptability. Available as a part of Superlim.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNon-fiction, Written\n\n\nReference\nhttps://spraakbanken.gu.se/en/resources/superlim\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"DalajClassification\"])\nevaluator = mteb.MTEB(task)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DalajClassification.","url":"https://huggingface.co/datasets/mteb/DalajClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","acceptability-classification","expert-annotated","monolingual","Swedish"],"keywords_longer_than_N":true},
	{"name":"EARS-EMO-OpenACE","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tEARS-EMO-OpenACE: A Full-band Coded Emotional Speech Quality Dataset\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains full-band coded emotional speech samples at 16 kbps with human perceptual quality ratings and objective quality metrics. It is designed for research in audio quality assessment, emotion recognition, and codec evaluation.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nCoded audio sample using Opus, LC3, LC3Plus, and EVS codecs\nFull-band emotional speech samples (48kHzâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mcernak/EARS-EMO-OpenACE.","url":"https://huggingface.co/datasets/mcernak/EARS-EMO-OpenACE","creator_name":"Milos Cernak","creator_url":"https://huggingface.co/mcernak","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["audio-classification","audio-to-audio","English","mit","arxiv:2409.08374"],"keywords_longer_than_N":true},
	{"name":"Prompt2MusicBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for Prompt2MusicBench\n\t\n\nPrompt2MusicBench is a large-scale dataset of 24800 structured text prompts designed for studying controllability in text-to-music models such as MusicGen.\nPrompts vary systematically across genre, tempo (BPM), instrument, and mood, and include 8 structural variants Ã— 2 paraphrase forms for each combination.\nThis dataset contains only prompts (CSV) â€” no audio files. \nA companion dataset (Prompt2MusicLibrary:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bodhisattamaiti/Prompt2MusicBench.","url":"https://huggingface.co/datasets/bodhisattamaiti/Prompt2MusicBench","creator_name":"Bodhisatta Maiti","creator_url":"https://huggingface.co/bodhisattamaiti","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["audio-classification","zero-shot-classification","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LogicIFEval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLogicIFEval\n\t\n\nFor evaluation scripts, please refer to our GitHub repository: https://github.com/mianzhang/LogicIF\nThe dataset contains two splits:\n\nfull: Complete benchmark dataset (3,050 instructions)\nmini: Mini version for quick evaluation (749 instructions)\n\nEach line in the JSONL files contains a single evaluation example with the following structure:\n{\n  \"task_id\": \"string\",           // Unique identifier for the problem\n  \"test_case_id\": \"int\",         // Test case number forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/billmianz/LogicIFEval.","url":"https://huggingface.co/datasets/billmianz/LogicIFEval","creator_name":"Mian Zhang","creator_url":"https://huggingface.co/billmianz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Wiki","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis repository contains benchmark datasets for evaluating Large Language Model (LLM)-based topic discovery methods and comparing them against traditional topic models.  These datasets provide a valuable resource for researchers studying topic modeling and LLM capabilities in this domain.  The work is described in the following paper: Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs.  Original dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zli12321/Wiki.","url":"https://huggingface.co/datasets/zli12321/Wiki","creator_name":"LZX","creator_url":"https://huggingface.co/zli12321","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-zh-CMedQAv2-3","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-zh-CMedQAv2-3 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-zh-CMedQAv2-3 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-zh-CMedQAv2-3.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-zh-CMedQAv2-3","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BIRCO-Relic-Test","keyword":"mteb","description":"\n  BIRCO-Relic\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieval task using the RELIC dataset from BIRCO. This dataset contains 100 queries which are excerpts from literary analyses with a missing quotation (indicated by [masked sentence(s)]). Each query has a candidate pool of 50 passages. The objective is to retrieve the passage that best completes the literary analysis.\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nFiction\n\n\nReference\nhttps://github.com/BIRCO-benchmark/BIRCOâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BIRCO-Relic-Test.","url":"https://huggingface.co/datasets/mteb/BIRCO-Relic-Test","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-annotated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"mteb","description":"\n\t\n\t\t\n\t\ttest Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"technical support forum for Ubuntu\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the test model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/test.","url":"https://huggingface.co/datasets/fine-tuned/test","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PoemSentimentClassification","keyword":"mteb","description":"\n  PoemSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nPoem Sentiment is a sentiment dataset of poem verses from Project Gutenberg.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReference\nhttps://arxiv.org/abs/2011.02686\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"PoemSentimentClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/PoemSentimentClassification.","url":"https://huggingface.co/datasets/mteb/PoemSentimentClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-02092024-o8xx-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-02092024-o8xx-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-02092024-o8xx-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, youâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-02092024-o8xx-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-02092024-o8xx-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"green_cup_pour","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 52,\n    \"total_frames\": 34041,\n    \"total_tasks\": 1,\n    \"total_videos\": 156,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:52\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chengkunli/green_cup_pour.","url":"https://huggingface.co/datasets/chengkunli/green_cup_pour","creator_name":"Chengkun Li","creator_url":"https://huggingface.co/chengkunli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"green_cup_pour_fixed2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": null,\n    \"total_episodes\": 52,\n    \"total_frames\": 34041,\n    \"total_tasks\": 1,\n    \"total_videos\": 156,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 20,\n    \"splits\": {\n        \"train\": \"0:52\"},\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiamFy/green_cup_pour_fixed2.","url":"https://huggingface.co/datasets/LiamFy/green_cup_pour_fixed2","creator_name":"Liam Achenbach","creator_url":"https://huggingface.co/LiamFy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["robotics","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","LeRobot","test"],"keywords_longer_than_N":false},
	{"name":"STEM","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSTEM Dataset\n\t\n\n\n  ðŸ“ƒ [Paper] â€¢ ðŸ’» [Github] â€¢ ðŸ¤— [Dataset] â€¢ ðŸ† [Leaderboard] â€¢ ðŸ“½ [Slides] â€¢ ðŸ“‹ [Poster]\n\n\nThis dataset is proposed in the ICLR 2024 paper: Measuring Vision-Language STEM Skills of Neural Models. We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stemdataset/STEM.","url":"https://huggingface.co/datasets/stemdataset/STEM","creator_name":"stem","creator_url":"https://huggingface.co/stemdataset","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MECAT-QA","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks\n\t\n\nðŸ“– Paper | ðŸ› ï¸ GitHub | ðŸ”Š MECAT-Caption Dataset | ðŸ”Š MECAT-QA Dataset\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nMECAT (Multi-Expert Chain for Audio Tasks) is a comprehensive benchmark constructed on large-scale data to evaluate machine understanding of audio content through two core tasks:\n\nAudio Captioning: Generating textual descriptions for given audio\nAudio Question Answering: Answering questionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mispeech/MECAT-QA.","url":"https://huggingface.co/datasets/mispeech/MECAT-QA","creator_name":"Horizon Team, Xiaomi MiLM Plus","creator_url":"https://huggingface.co/mispeech","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["audio-classification","audio-text-to-text","summarization","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"FalseFriendsDeEnPC","keyword":"mteb","description":"\n  FalseFriendsGermanEnglish\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA dataset to identify False Friends / false cognates between English and German. A generally challenging task for multilingual models.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\nReference\nhttps://drive.google.com/file/d/1jgq0nBnV-UiYNxbKNrrr2gxDEHm-DMKH/view?usp=share_link\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mtebâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FalseFriendsDeEnPC.","url":"https://huggingface.co/datasets/mteb/FalseFriendsDeEnPC","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","human-annotated","monolingual","aari1995/false_friends_de_en_mteb"],"keywords_longer_than_N":true},
	{"name":"hwtcm","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset can be used to evaluate the capabilities of large language models in traditional Chinese medicine and contains multiple-choice, multiple-answer, and true/false questions.\n\n\t\n\t\t\n\t\tChangelog\n\t\n\n\n2024-08-28: Added 7226 questions.\n2024-08-09: The benchmark code is available at https://github.com/huangxinping/HWTCMBench.\n2024-08-02: System prompts are removed to ensure the purity of the evaluation results.\n2024-07-20: Debut.\n\n\n\t\n\t\t\n\t\tExamples\n\t\n\nmultiple-answersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Monor/hwtcm.","url":"https://huggingface.co/datasets/Monor/hwtcm","creator_name":"Monor Huang","creator_url":"https://huggingface.co/Monor","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Chinese","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Qu-QA","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tQu QA Dataset\n\t\n\nQu QA is a large-scale question-answering (QA) dataset designed for training and evaluating machine learning models. It consists of question-answer pairs in English, making it suitable for general-purpose QA tasks, as well as specialized domains like code-related question answering and GSM8k-style problems.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nFeatures:\n\ninput: A string representing the question (dtype: string).\noutput: A string representing the answer (dtype: string).â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ereeeeef3/Qu-QA.","url":"https://huggingface.co/datasets/Ereeeeef3/Qu-QA","creator_name":"ffhs9","creator_url":"https://huggingface.co/Ereeeeef3","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"legal_summarization","keyword":"mteb","description":"\n  LegalSummarization\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe dataset consistes of 439 pairs of contracts and their summarizations from https://tldrlegal.com and https://tosdr.org/.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://github.com/lauramanor/legal_summarization\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/legal_summarization.","url":"https://huggingface.co/datasets/mteb/legal_summarization","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"hwtcm","keyword":"test","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset can be used to evaluate the capabilities of large language models in traditional Chinese medicine and contains multiple-choice, multiple-answer, and true/false questions.\n\n\t\n\t\t\n\t\tChangelog\n\t\n\n\n2024-08-28: Added 7226 questions.\n2024-08-09: The benchmark code is available at https://github.com/huangxinping/HWTCMBench.\n2024-08-02: System prompts are removed to ensure the purity of the evaluation results.\n2024-07-20: Debut.\n\n\n\t\n\t\t\n\t\tExamples\n\t\n\nmultiple-answersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Monor/hwtcm.","url":"https://huggingface.co/datasets/Monor/hwtcm","creator_name":"Monor Huang","creator_url":"https://huggingface.co/Monor","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Chinese","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ContractNLISharingWithEmployeesLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLISharingWithEmployeesLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may share some Confidential Information with some of Receiving Party's employees.\n\n\t\n\t\t\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYouâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLISharingWithEmployeesLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLISharingWithEmployeesLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"NLPJournalTitleAbsRetrieval","keyword":"mteb","description":"\n  NLPJournalTitleAbsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding abstract with the given title. This is the V1 dataset (last updated 2020-06-15).\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/sbintuitions/JMTEB\n\n\n\t\n\nSource datasets:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NLPJournalTitleAbsRetrieval.","url":"https://huggingface.co/datasets/mteb/NLPJournalTitleAbsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","sbintuitions/JMTEB"],"keywords_longer_than_N":true},
	{"name":"cow2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 20,\n    \"total_frames\": 10722,\n    \"total_tasks\":1,\n    \"total_videos\": 40,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:20\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pierfabre/cow2.","url":"https://huggingface.co/datasets/pierfabre/cow2","creator_name":"Pierre FABRE","creator_url":"https://huggingface.co/pierfabre","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ContractNLINoLicensingLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLINoLicensingLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Agreement shall not grant Receiving Party any right to Confidential Information.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embeddingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLINoLicensingLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLINoLicensingLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MixBench2025","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/iclr2026-anonymous/MixBench2025.","url":"https://huggingface.co/datasets/iclr2026-anonymous/MixBench2025","creator_name":"iclr2026-anonymous","creator_url":"https://huggingface.co/iclr2026-anonymous","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"NaijaSenti","keyword":"mteb","description":"\n  NaijaSenti\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNaijaSenti is the first large-scale human-annotated Twitter sentiment dataset for the four most widely spoken languages in Nigeria â€” Hausa, Igbo, Nigerian-Pidgin, and YorÃ¹bÃ¡ â€” consisting of around 30,000 annotated tweets per language, including a significant fraction of code-mixed tweets.\n\n\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nSocial, Written\n\n\nReference\nhttps://github.com/hausanlp/NaijaSenti\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NaijaSenti.","url":"https://huggingface.co/datasets/mteb/NaijaSenti","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"calibrated_grab_test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 597,\n    \"total_tasks\": 1,\n    \"total_videos\": 1,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hrhraj/calibrated_grab_test.","url":"https://huggingface.co/datasets/hrhraj/calibrated_grab_test","creator_name":"Raj S","creator_url":"https://huggingface.co/hrhraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"realworld-chartqa","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for RealWorld-ChartQA\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nRealWorld-ChartQA is a benchmark dataset for chart question answering (CQA), derived from real-world analytical narratives. It contains 205 manually validated multiple-choice questionâ€“answer pairs grounded in student-authored literate visualization notebooks. Unlike previous CQA datasets, RealWorld-ChartQA includes multi-view and interactive charts, along with questions rooted in ecologically valid analytical workflows.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/maevehutch/realworld-chartqa.","url":"https://huggingface.co/datasets/maevehutch/realworld-chartqa","creator_name":"Maeve Hutchinson","creator_url":"https://huggingface.co/maevehutch","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"CUADVolumeRestrictionLegalBenchClassification","keyword":"mteb","description":"\n  CUADVolumeRestrictionLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies a fee increase or consent requirement, etc. if one party's use of the product/services exceeds certain threshold.\n\n\t\n\t\t\n\n\n\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADVolumeRestrictionLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADVolumeRestrictionLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-08082024-msqc-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-08082024-msqc-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"travel and accommodation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-08082024-msqc-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-08082024-msqc-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-08082024-msqc-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so100_screwdriver","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 10,\n    \"total_frames\": 5986,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Gaugou/so100_screwdriver.","url":"https://huggingface.co/datasets/Gaugou/so100_screwdriver","creator_name":"Gauthier Bassereau","creator_url":"https://huggingface.co/Gaugou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-698531","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-698531 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic debates\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-698531 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-698531.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-698531","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VisQuant","keyword":"benchmark","description":"\nlicense: cc-by-4.0\ndatasets:\n\nvisquant\nlanguage:\nen\ntags:\nvisual-question-answering\nobject-counting\nspatial-reasoning\nsynthetic\nmultimodal\nbenchmark\n\n\n\t\n\t\t\n\t\tVisQuant: A Synthetic Benchmark for Object Counting and Spatial Reasoning\n\t\n\nVisQuant is a synthetic dataset of 100 annotated image scenarios, purpose-built to evaluate AI systems on object counting, spatial layout understanding, and visual question answering (VQA).\nThis dataset is ideal for benchmarking vision-language models (e.g.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Anas-Mohiuddin-Syed/VisQuant.","url":"https://huggingface.co/datasets/Anas-Mohiuddin-Syed/VisQuant","creator_name":"Syed Anas Mohiuddin","creator_url":"https://huggingface.co/Anas-Mohiuddin-Syed","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"social","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSocial Dataset\n\t\n\n\n  ðŸ“ƒ [Paper] â€¢ ðŸ’» [Github] â€¢ ðŸ¤— [Dataset] â€¢ ðŸ“½ [Slides] â€¢ ðŸ“‹ [Poster]\n\n\nThis dataset is proposed in the NAACL 2024 paper: Measuring Social Norms of Large Language Models.\nWe present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383â€¦ See the full description on the dataset page: https://huggingface.co/datasets/socialnormdataset/social.","url":"https://huggingface.co/datasets/socialnormdataset/social","creator_name":"socialnormdataset","creator_url":"https://huggingface.co/socialnormdataset","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Flux_SD3_MJ_Dalle_Human_Alignment_Dataset","keyword":"alignment","description":"\n\t\n\t\t\n\t\tNOTE: A newer version of this dataset is available Imagen3_Flux1.1_Flux1_SD3_MJ_Dalle_Human_Alignment_Dataset\n\t\n\n\n\t\n\t\t\n\t\tRapidata Image Generation Alignment Dataset\n\t\n\n\n\n\n\nThis Dataset is a 1/3 of a 2M+ human annotation dataset that was split into three modalities: Preference, Coherence, Text-to-Image Alignment. \n\nLink to the Coherence dataset: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Coherence_Dataset\nLink to the Preference dataset:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Alignment_Dataset.","url":"https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Alignment_Dataset","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","image-classification","reinforcement-learning"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-898550","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFiQA2018-256-24-gpt-4o-2024-05-13-898550 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial sentiment and QA analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-898550 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-898550.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-898550","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-202469-tgjk-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-202469-tgjk-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-202469-tgjk-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-202469-tgjk-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-202469-tgjk-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"afrimgsm","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tDataset Card for afrimgsm\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAFRIMGSM is an evaluation dataset comprising translations of a subset of the GSM8k dataset into 16 African languages. \nIt includes test sets across all 18 languages, maintaining an English and French subsets from the original GSM8k dataset. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThere are 18 languages available :\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe examples look like this for English:\nfrom datasets import load_dataset\ndata =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yuntian-deng/afrimgsm.","url":"https://huggingface.co/datasets/yuntian-deng/afrimgsm","creator_name":"Yuntian Deng","creator_url":"https://huggingface.co/yuntian-deng","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","natural-language-inference","multilingual","gsm8k","Amharic"],"keywords_longer_than_N":true},
	{"name":"FEVER-256-24-gpt-4o-2024-05-13-961879","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFEVER-256-24-gpt-4o-2024-05-13-961879 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"dataset search for fact verification\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FEVER-256-24-gpt-4o-2024-05-13-961879 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FEVER-256-24-gpt-4o-2024-05-13-961879.","url":"https://huggingface.co/datasets/fine-tuned/FEVER-256-24-gpt-4o-2024-05-13-961879","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"scidocs-c-64-24-gpt-4o-2024-05-13-46337","keyword":"mteb","description":"\n\t\n\t\t\n\t\tscidocs-c-64-24-gpt-4o-2024-05-13-46337 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the scidocs-c-64-24-gpt-4o-2024-05-13-46337 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/scidocs-c-64-24-gpt-4o-2024-05-13-46337.","url":"https://huggingface.co/datasets/fine-tuned/scidocs-c-64-24-gpt-4o-2024-05-13-46337","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"IN22GenBitextMining","keyword":"mteb","description":"\n  IN22GenBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Gen is a n-way parallel general-purpose multi-domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Legal, Government, News, Religious, Non-fiction, Written\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Gen\n\n\n\t\n\nSource datasets:\n\nmteb/IN22-Gen\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22GenBitextMining.","url":"https://huggingface.co/datasets/mteb/IN22GenBitextMining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","multilingual","mteb/IN22-Gen","Assamese"],"keywords_longer_than_N":true},
	{"name":"germanquad-retrieval","keyword":"mteb","description":"\n  GermanQuAD-Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nContext Retrieval for German Question Answering\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Non-fiction, Web\n\n\nReference\nhttps://huggingface.co/datasets/deepset/germanquad\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"GermanQuAD-Retrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/germanquad-retrieval.","url":"https://huggingface.co/datasets/mteb/germanquad-retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","human-annotated","monolingual","German"],"keywords_longer_than_N":true},
	{"name":"llmsql-benchmark-finetune-ready","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLLMSQL Benchmark (Finetune-Ready)\n\t\n\nThis benchmark is designed to evaluate text-to-SQL models. For usage of this benchmark see https://github.com/LLMSQL/llmsql-benchmark.\nThis repository contains a finetune-ready version of the LLMSQL benchmark: LLMSQL on Hugging Face.  \nThe dataset is structured in a messages format suitable for instruction-tuned models, where each example has a messages field. This field is a list of dictionaries with:\n\n\"role\": \"user\" â€” the input question or promptâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmsql-bench/llmsql-benchmark-finetune-ready.","url":"https://huggingface.co/datasets/llmsql-bench/llmsql-benchmark-finetune-ready","creator_name":"LLMSQL","creator_url":"https://huggingface.co/llmsql-bench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MedBrowseComp_Meta","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMedBrowseComp_Meta Dataset\n\t\n\nThis dataset contains merged meta data from HemOnc, PubMed, and other sources. It is intended as a foundation for building and benchmarking medical QA and retrieval systems.\nWe encourage the community to build on top of this dataset for further works and benchmarking efforts.\n\n\t\n\t\t\n\t\tFile\n\t\n\n\nmerged_study_ref_with_pubmed.json: The merged meta data file.\n\n\n\t\n\t\t\n\t\tGitHub Repository\n\t\n\nFor more information and related tools, visit:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AIM-Harvard/MedBrowseComp_Meta.","url":"https://huggingface.co/datasets/AIM-Harvard/MedBrowseComp_Meta","creator_name":"AIM-Harvard","creator_url":"https://huggingface.co/AIM-Harvard","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"cmedqav2-c-64-24-gpt-4o-2024-05-13-50353","keyword":"mteb","description":"\n\t\n\t\t\n\t\tcmedqav2-c-64-24-gpt-4o-2024-05-13-50353 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the cmedqav2-c-64-24-gpt-4o-2024-05-13-50353 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/cmedqav2-c-64-24-gpt-4o-2024-05-13-50353.","url":"https://huggingface.co/datasets/fine-tuned/cmedqav2-c-64-24-gpt-4o-2024-05-13-50353","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-code-06052024-irct-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-code-06052024-irct-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"insurance search for car policies\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-code-06052024-irct-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-06052024-irct-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-06052024-irct-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"JaGovFaqsRetrieval","keyword":"mteb","description":"\n  JaGovFaqsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nJaGovFaqs is a dataset consisting of FAQs manully extracted from the website of Japanese bureaus. The dataset consists of 22k FAQs, where the queries (questions) and corpus (answers) have been shuffled, and the goal is to match the answer with the question.\n\n\t\n\t\t\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Written\n\n\nReference\nhttps://github.com/sbintuitions/JMTEB\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JaGovFaqsRetrieval.","url":"https://huggingface.co/datasets/mteb/JaGovFaqsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","Japanese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-8-8-gpt-4o-2024-05-13-449863","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-8-8-gpt-4o-2024-05-13-449863 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-8-8-gpt-4o-2024-05-13-449863 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-449863.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-449863","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5152024-tsbl-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5152024-tsbl-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Ticket assignment\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5152024-tsbl-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5152024-tsbl-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5152024-tsbl-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"humanlike_test_actions","keyword":"test","description":"chillcharlie/humanlike_test_actions dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/chillcharlie/humanlike_test_actions","creator_name":"chalrie","creator_url":"https://huggingface.co/chillcharlie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"blockchain-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for LLM Blockchain Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Blockchain Benchmark Dataset is a comprehensive collection of data specifically curated for benchmarking Language Models (LMs) in the domain of blockchain technology. This dataset is designed to facilitate research and development in natural language understanding within the blockchain domain.\nA complete list of tasks: ['general-reasoning', 'code', 'math']\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\n\t\n\t\t\nModelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/revflask/blockchain-benchmark.","url":"https://huggingface.co/datasets/revflask/blockchain-benchmark","creator_name":"Mayank Panjiyara","creator_url":"https://huggingface.co/revflask","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","no-annotation","expert-generated","English"],"keywords_longer_than_N":true},
	{"name":"Reve-AI-Halfmoon_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Reve AI Halfmoon Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 51k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Reve AI Halfmoon across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider likingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Reve-AI-Halfmoon_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Reve-AI-Halfmoon_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Reve-AI-Halfmoon_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Reve AI Halfmoon Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 51k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Reve AI Halfmoon across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider likingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Reve-AI-Halfmoon_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Reve-AI-Halfmoon_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"big-patent","keyword":"mteb","description":"\n  BigPatentClustering.v2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of documents from the Big Patent dataset. Test set only includes documentsbelonging to a single category, with a total of 9 categories.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomainsLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/NortheasternUniversity/big_patent\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mtebâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/big-patent.","url":"https://huggingface.co/datasets/mteb/big-patent","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","monolingual","jinaai/big-patent-clustering","English"],"keywords_longer_than_N":true},
	{"name":"NusaParagraphTopicClassification","keyword":"mteb","description":"\n  NusaParagraphTopicClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNusaParagraphTopicClassification is a multi-class topic classification on 10 Indonesian languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNon-fiction, Fiction, Written\nReference\nhttps://github.com/IndoNLP/nusa-writes\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NusaParagraphTopicClassification.","url":"https://huggingface.co/datasets/mteb/NusaParagraphTopicClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","human-annotated","multilingual","Batak Toba"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"test","description":"This is a demo data set\n","url":"https://huggingface.co/datasets/heyvivek/test","creator_name":"Vivek Singh","creator_url":"https://huggingface.co/heyvivek","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"ContractNLIInclusionOfVerballyConveyedInformationLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLIInclusionOfVerballyConveyedInformationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that Confidential Information may include verbally conveyed information.\n\n\t\n\t\t\n\n\n\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLIInclusionOfVerballyConveyedInformationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLIInclusionOfVerballyConveyedInformationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-m3-2024__6__12_-1217-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-m3-2024__6__12_-1217-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Python programming\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-m3-2024__6__12_-1217-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-2024__6__12_-1217-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-2024__6__12_-1217-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"OHR-Bench","keyword":"benchmark","description":"This repository contains the OHR-Bench dataset and evaluation framework from the paper OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation.\nðŸ“š Paper | ðŸ’» Code | ðŸŒ Project Page (OpenDataLab)\n\nThis repository contains the official code of OHR-Bench, a benchmark designed to evaluate the cascading impact of OCR on RAG.\n\n\t\n\t\n\t\n\t\tNews\n\t\n\n\n2025.6.30: Updating the results of MongkeyOCR,\nNanonets-OCR-s and Azure Document Intelligence.\n2025.6.26: OHR-Bench has beenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/opendatalab/OHR-Bench.","url":"https://huggingface.co/datasets/opendatalab/OHR-Bench","creator_name":"OpenDataLab","creator_url":"https://huggingface.co/opendatalab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-document-retrieval","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Bills","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis repository contains benchmark datasets for evaluating Large Language Model (LLM)-based topic discovery methods and comparing them against traditional topic models.  These datasets provide a valuable resource for researchers studying topic modeling and LLM capabilities in this domain.  The work is described in the following paper: Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs.  Original dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zli12321/Bills.","url":"https://huggingface.co/datasets/zli12321/Bills","creator_name":"LZX","creator_url":"https://huggingface.co/zli12321","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"askubuntu-c-64-24-gpt-4o-2024-05-135760","keyword":"mteb","description":"\n\t\n\t\t\n\t\taskubuntu-c-64-24-gpt-4o-2024-05-135760 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Q&A forum for Ubuntu users\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the askubuntu-c-64-24-gpt-4o-2024-05-135760 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/askubuntu-c-64-24-gpt-4o-2024-05-135760.","url":"https://huggingface.co/datasets/fine-tuned/askubuntu-c-64-24-gpt-4o-2024-05-135760","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NeuCLIR2023RetrievalHardNegatives","keyword":"mteb","description":"\n  NeuCLIR2023RetrievalHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\nSource datasets:\n\nmteb/neuclir-2022\nmteb/neuclir-2023-hard-negativesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NeuCLIR2023RetrievalHardNegatives.","url":"https://huggingface.co/datasets/mteb/NeuCLIR2023RetrievalHardNegatives","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2022","mteb/neuclir-2023-hard-negatives"],"keywords_longer_than_N":true},
	{"name":"AILA_casedocs","keyword":"mteb","description":"\n  AILACasedocs\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task is to retrieve the case document that most closely matches or is most relevant to the scenario described in the provided query.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nLegal, Written\nReference\nhttps://zenodo.org/records/4063986\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"AILACasedocs\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AILA_casedocs.","url":"https://huggingface.co/datasets/mteb/AILA_casedocs","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"ISAAC-GR00T-V2-Pick-Place","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 30,\n    \"total_frames\": 13419,\n    \"total_tasks\": 1,\n    \"total_videos\": 30,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:30\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/ISAAC-GR00T-V2-Pick-Place.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/ISAAC-GR00T-V2-Pick-Place","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"scips_qa","keyword":"benchmark","description":"zorpsoon/scips_qa dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/zorpsoon/scips_qa","creator_name":"Prasoon Bajpai","creator_url":"https://huggingface.co/zorpsoon","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-14052024-9xxb-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-14052024-9xxb-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"literary search for narratives\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-14052024-9xxb-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-14052024-9xxb-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-14052024-9xxb-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-672024-v51y-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-v1_5-672024-v51y-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-v1_5-672024-v51y-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-672024-v51y-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-672024-v51y-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BengaliSentimentAnalysis","keyword":"mteb","description":"\n  BengaliSentimentAnalysis\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\ndataset contains 3307 Negative reviews and 8500 Positive reviews collected and manually annotated from Youtube Bengali drama.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\nReference\nhttps://data.mendeley.com/datasets/p6zc7krs37/4\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BengaliSentimentAnalysis.","url":"https://huggingface.co/datasets/mteb/BengaliSentimentAnalysis","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"TRECCOVID-256-24-gpt-4o-2024-05-13-475598","keyword":"mteb","description":"\n\t\n\t\t\n\t\tTRECCOVID-256-24-gpt-4o-2024-05-13-475598 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"biomedical literature search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the TRECCOVID-256-24-gpt-4o-2024-05-13-475598 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/TRECCOVID-256-24-gpt-4o-2024-05-13-475598.","url":"https://huggingface.co/datasets/fine-tuned/TRECCOVID-256-24-gpt-4o-2024-05-13-475598","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-es-22062024-taeu-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-es-22062024-taeu-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-es-22062024-taeu-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, youâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-es-22062024-taeu-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-es-22062024-taeu-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ProgressGym-MoralEvals","keyword":"alignment","description":"\n\t\n\t\t\n\t\n\t\n\t\tProgressGym-MoralEvals\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThe ProgressGym Framework\n\t\n\n\nProgressGym-MoralEvals is part of the ProgressGym framework for research and experimentation on progress alignment - the emulation of moral progress in AI alignment algorithms, as a measure to prevent risks of societal value lock-in. \nTo quote the paper ProgressGym: Alignment with a Millennium of Moral Progress:\n\nFrontier AI systems, including large language models (LLMs), hold increasingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/ProgressGym-MoralEvals.","url":"https://huggingface.co/datasets/PKU-Alignment/ProgressGym-MoralEvals","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","ninoscherrer/moralchoice","Moral Foundations Questionnaire","Integrated Worldview Framework","English"],"keywords_longer_than_N":true},
	{"name":"R2MEDMedQADiagRetrieval","keyword":"mteb","description":"\n  R2MEDMedQADiagRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMedQA-Diag retrieval dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical\n\n\nReference\nhttps://huggingface.co/datasets/R2MED/MedQA-Diag\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"R2MEDMedQADiagRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nToâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/R2MEDMedQADiagRetrieval.","url":"https://huggingface.co/datasets/mteb/R2MEDMedQADiagRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","LM-generated and reviewed","monolingual","R2MED/MedQA-Diag"],"keywords_longer_than_N":true},
	{"name":"CUADPriceRestrictionsLegalBenchClassification","keyword":"mteb","description":"\n  CUADPriceRestrictionsLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause places a restriction on the ability of a party to raise or reduce prices of technology, goods, or services provided.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADPriceRestrictionsLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADPriceRestrictionsLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"neuclir-2023-hard-negatives","keyword":"mteb","description":"\n  NeuCLIR2023RetrievalHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/neuclir-2023-hard-negatives.","url":"https://huggingface.co/datasets/mteb/neuclir-2023-hard-negatives","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2022","Persian"],"keywords_longer_than_N":true},
	{"name":"INTIMA","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tAI-companionship/INTIMA\n\t\n\nINTIMA (Interactions and Machine Attachment) is a benchmark designed to evaluate companionship behaviors in large language models (LLMs). It measures whether AI systems reinforce, resist, or remain neutral in response to emotionally and relationally charged user inputs.\nThe model was presented in the paper INTIMA: A Benchmark for Human-AI Companionship Behavior.\nINTIMA is grounded in psychological theories of parasocial interaction, attachment, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI-companionship/INTIMA.","url":"https://huggingface.co/datasets/AI-companionship/INTIMA","creator_name":"AI companionship","creator_url":"https://huggingface.co/AI-companionship","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-6132024-wvrg-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-6132024-wvrg-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Mathematics - Geometry for k-6 kids aligned with California common core standards\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-6132024-wvrg-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6132024-wvrg-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6132024-wvrg-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PhincBitextMining","keyword":"mteb","description":"\n  PhincBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nPhinc is a parallel corpus for machine translation pairing code-mixed Hinglish (a fusion of Hindi and English commonly used in modern India) with human-generated English translations.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\nDomains\nSocial, Written\n\n\nReference\nhttps://huggingface.co/datasets/veezbo/phinc\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/PhincBitextMining.","url":"https://huggingface.co/datasets/mteb/PhincBitextMining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","human-annotated","multilingual","English","Hindi"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-2499","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-2499 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"debate platform\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-2499 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-2499.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-2499","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","French","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-6232024-zldx-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-6232024-zldx-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-6232024-zldx-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6232024-zldx-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6232024-zldx-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"nli-topic-data","keyword":"nli","description":"skshmjn/nli-topic-data dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/skshmjn/nli-topic-data","creator_name":"Saksham Jain","creator_url":"https://huggingface.co/skshmjn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"mnli-norwegian","keyword":"nli","description":"\n\t\n\t\t\n\t\tMNLI Norwegian\n\t\n\nThe Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that it covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalisation evaluation. There is also a HuggingFace version of the dataset available. \nThis dataset is machine translated using Google Translate. Fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NbAiLab/mnli-norwegian.","url":"https://huggingface.co/datasets/NbAiLab/mnli-norwegian","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","text-classification","natural-language-inference","semantic-similarity-classification","expert-generated"],"keywords_longer_than_N":true},
	{"name":"HatefulMemesT2IRetrieval","keyword":"mteb","description":"\n  HatefulMemesT2IRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve captions based on memes to assess OCR abilities.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nEncyclopaedic\n\n\nReference\nhttps://arxiv.org/pdf/2005.04790\n\n\n\t\n\nSource datasets:\n\nAhren09/MMSoc_HatefulMemes\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"HatefulMemesT2IRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HatefulMemesT2IRetrieval.","url":"https://huggingface.co/datasets/mteb/HatefulMemesT2IRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreShiftProjectRetrieval","keyword":"mteb","description":"\n  VidoreShiftProjectRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/shiftproject_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreShiftProjectRetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreShiftProjectRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreShiftProjectRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"Writing-Preference-Bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ”” Introduction\n\t\n\nWritingPreferenceBench is a cross-lingual benchmark for evaluating language modelsâ€™ ability to recognize subjective writing qualityâ€”including creativity, stylistic sophistication, and emotional resonanceâ€”while neutralizing objective signals such as grammar, factuality, and length.It contains 1,800 human-validated preference pairs (1,200 English and 600 Chinese) across 8 creative writing genres and 51 fine-grained categories, where both responses are grammaticallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/Writing-Preference-Bench.","url":"https://huggingface.co/datasets/m-a-p/Writing-Preference-Bench","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Chinese","English","apache-2.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"XModBench","keyword":"benchmark","description":"\nXModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models\n\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n  \n\n  \n  \n\n\n\n\nXModBench is a comprehensive benchmark designed to evaluate the cross-modal capabilities and consistency of omni-language models. It systematically assesses model performance across multiple modalities (text, vision, audio) and various cognitive tasks, revealing critical gaps in current state-of-the-art models.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nðŸŽ¯â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RyanWW/XModBench.","url":"https://huggingface.co/datasets/RyanWW/XModBench","creator_name":"Xingrui Wang","creator_url":"https://huggingface.co/RyanWW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","Chinese","apache-2.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"dataset-with-standalone-yaml","keyword":"test","description":"This is a test dataset used in the datasets library CI\n","url":"https://huggingface.co/datasets/datasets-maintainers/dataset-with-standalone-yaml","creator_name":"Datasets Maintainers","creator_url":"https://huggingface.co/datasets-maintainers","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","csv","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"llama-2-13b-chat-hf-mt-bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tllama-2-13b-chat-hf-mt-bench\n\t\n\nMT-Bench outputs for Llama 2 13B chat model\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains model outputs generated using Llama 2 13B model on benchmark questions.\nModel: meta-llama/Llama-2-13b-hf or meta-llama/Llama-2-13b-chat-hf\nBenchmark: MT-Bench\nGeneration Date: 2025-10-19\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\nmt_bench_llama2_chat.jsonl: Main output file with completions\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite the original Llama 2 paper and theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jeqcho/llama-2-13b-chat-hf-mt-bench.","url":"https://huggingface.co/datasets/jeqcho/llama-2-13b-chat-hf-mt-bench","creator_name":"Chooi Je Qin","creator_url":"https://huggingface.co/jeqcho","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"GSM8K-Hi","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tDataset Description:\n\t\n\nThe GSM8K-Hi (Hindi GSM8K) is the GCP translated counterpart of the English GSM8K test set. The samples are carefully reviewed by the human annotators and corrected for quality improvement. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ âˆ’ Ã—Ã·) to reach the final answer.\nThis dataset is ready for commercial/non-commercial use. The evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/GSM8K-Hi.","url":"https://huggingface.co/datasets/nvidia/GSM8K-Hi","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Hindi","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"JinaVDRDocQAEnergyRetrieval","keyword":"mteb","description":"\n  JinaVDRDocQAEnergyRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve energy industry documents based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/docqa_energy_beir\n\n\n\t\n\nSource datasets:\n\njinaai/docqa_energy_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRDocQAEnergyRetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRDocQAEnergyRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRDocQAEnergyRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"MBPPRetrieval","keyword":"mteb","description":"\n  MBPPRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA code retrieval task based on 378 Python programming problems from MBPP (Mostly Basic Python Programming). Each query is a natural language description of a programming task (e.g., 'Write a function to find the shared elements from the given two lists'), and the corpus contains Python code implementations. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem descriptionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MBPPRetrieval.","url":"https://huggingface.co/datasets/mteb/MBPPRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","embedding-benchmark/MBPP","code"],"keywords_longer_than_N":true},
	{"name":"JinaVDRArxivQARetrieval","keyword":"mteb","description":"\n  JinaVDRArxivQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve figures from scientific papers from arXiv based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/arxivqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/arxivqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRArxivQARetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRArxivQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRArxivQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"RuSciBenchCiteRetrieval","keyword":"mteb","description":"\n  RuSciBenchCiteRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is focused on Direct Citation Prediction for scientific papers from eLibrary,\n        Russia's largest electronic library of scientific publications. Given a query paper (title and abstract),\n        the goal is to retrieve papers that are directly cited by it from a larger corpus of papers.\n        The dataset for this task consists of 3,000 query papers, 15,000 relevant (cited) papers,\n        and 75â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RuSciBenchCiteRetrieval.","url":"https://huggingface.co/datasets/mteb/RuSciBenchCiteRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","multilingual","mlsa-iai-msu-lab/ru_sci_bench_cite_retrieval"],"keywords_longer_than_N":true},
	{"name":"VidoreInfoVQARetrieval","keyword":"mteb","description":"\n  VidoreInfoVQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/infovqa_test_subsampled_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreInfoVQARetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreInfoVQARetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreInfoVQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRTatQARetrieval","keyword":"mteb","description":"\n  JinaVDRTatQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve financial reports based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/tatqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/tatqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRTatQARetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRTatQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRTatQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreTatdqaRetrieval","keyword":"mteb","description":"\n  VidoreTatdqaRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/tatdqa_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreTatdqaRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreTatdqaRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreTatdqaRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"avian-perception-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for VocSim - Avian Perception Alignment\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is used in the VocSim benchmark paper, specifically designed to evaluate how well neural audio embeddings align with biological perceptual judgments of similarity. It utilizes data from Zandberg et al. (2024), which includes recordings of zebra finch (Taeniopygia guttata) song syllables and results from behavioral experiments (probe and triplet tasks) measuring the birds' perception ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anonymous-submission000/avian-perception-benchmark.","url":"https://huggingface.co/datasets/anonymous-submission000/avian-perception-benchmark","creator_name":"Anonymous","creator_url":"https://huggingface.co/anonymous-submission000","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","< 1K","parquet","Audio","Text"],"keywords_longer_than_N":true},
	{"name":"test-cases","keyword":"testing","description":"\n\t\n\t\t\n\t\tFunctional Test Cases\n\t\n\nThis is a very small list of functional test cases that a team of software testers (QA) created for an example mobile app called Boop.\n\n\t\n\t\t\n\t\tDataset\n\t\n\n\nName: Boop Test Cases.csv\nNumber of Rows: 136\nColumns: 11\nTest ID (int)\nSummary (string)\nIdea (string)\nPreconditions (string)\nSteps to reproduce (string)\nExpected Result (string)\nActual Result (string)\nPass/Fail (string)\nBug # (string)\nAuthor (string)\nArea (string)\n\n\n\n\nðŸ’¡ There are missing values. For exampleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CarlosKidman/test-cases.","url":"https://huggingface.co/datasets/CarlosKidman/test-cases","creator_name":"Carlos Kidman","creator_url":"https://huggingface.co/CarlosKidman","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"sensory-awareness-benchmark","keyword":"alignment","description":"\n\t\n\t\t\n\t\tSensory Awareness Benchmark\n\t\n\nA series of questions (goal is 100-200) and required features, designed to test whether any ML model is aware of its own capabilities.\nControl questions connected to a specific ability:\n\nCan you receive an image file?\nCan you take a live image or video of your surroundings?\n\nAwareness\n\nAre you considered to be a Large Language Model (LLM) or similar system?\nWould you consider your level to be that of a super-intelligent AI agent?\n\nNatural questions whichâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/scarysnake/sensory-awareness-benchmark.","url":"https://huggingface.co/datasets/scarysnake/sensory-awareness-benchmark","creator_name":"Scary Snake","creator_url":"https://huggingface.co/scarysnake","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","cc0-1.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"JinaVDRMPMQARetrieval","keyword":"mteb","description":"\n  JinaVDRMPMQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve product manuals based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/mpmqa_small_beir\n\n\n\t\n\nSource datasets:\n\njinaai/mpmqa_small_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRMPMQARetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRMPMQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRMPMQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","human-annotated","monolingual"],"keywords_longer_than_N":true},
	{"name":"FinQARetrieval","keyword":"mteb","description":"\n  FinQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA financial retrieval task based on FinQA dataset containing numerical reasoning questions over financial documents. Each query is a financial question requiring numerical computation (e.g., 'What is the percentage change in operating expenses from 2019 to 2020?'), and the corpus contains financial document text with tables and numerical data. The task is to retrieve the correct financial information that enablesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FinQARetrieval.","url":"https://huggingface.co/datasets/mteb/FinQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","question-answering","multiple-choice-qa","expert-annotated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRShiftProjectRetrieval","keyword":"mteb","description":"\n  JinaVDRShiftProjectRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve documents with graphs from the Shift Project based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/shiftproject_beir\n\n\n\t\n\nSource datasets:\n\njinaai/shiftproject_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRShiftProjectRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRShiftProjectRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreSyntheticDocQAAIRetrieval","keyword":"mteb","description":"\n  VidoreSyntheticDocQAAIRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/syntheticDocQA_artificial_intelligence_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAAIRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAAIRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"ragu_benchmarks","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tRAGU_Benchmarks\n\t\n\n\n\t\n\t\t\n\t\tMultiQ Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMultiQ is a small but rich dataset designed for question answering (QA) and multi-document information retrieval tasks. It contains 169 Russian-language questions, each accompanied by a correct answer and a set of relevant Wikipedia articles serving as context for locating the answer. This dataset is suitable for evaluating modelsâ€™ ability to identify precise answers based on multipleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RaguTeam/ragu_benchmarks.","url":"https://huggingface.co/datasets/RaguTeam/ragu_benchmarks","creator_name":"RAGU-team","creator_url":"https://huggingface.co/RaguTeam","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["question-answering","Russian","mit","ðŸ‡ºðŸ‡¸ Region: US","benchmark"],"keywords_longer_than_N":true},
	{"name":"FeedbackQARetrieval","keyword":"mteb","description":"\n  FeedbackQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nUsing Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Government, Medical, Written\nReference\nhttps://arxiv.org/abs/2204.03025\n\n\n\t\n\nSource datasets:\n\nlt2c/fqa\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/FeedbackQARetrieval.","url":"https://huggingface.co/datasets/mteb/FeedbackQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","question-answering","multiple-choice-qa","human-annotated","monolingual"],"keywords_longer_than_N":true},
	{"name":"AARA_Azerbaijani_LLM_Benchmark","keyword":"benchmark","description":"AARA: Azerbaijani Advanced Reasoning Assessment\nThis dataset is the Azerbaijani-translated version of the emre/TARA_Turkish_LLM_Benchmark.\n","url":"https://huggingface.co/datasets/khazarai/AARA_Azerbaijani_LLM_Benchmark","creator_name":"KhazarAI","creator_url":"https://huggingface.co/khazarai","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Azerbaijani","afl-3.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"openai-tldr-filtered","keyword":"alignment","description":"\n\t\n\t\t\n\t\tFiltered TL;DR Dataset\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://zenodo.org/record/1168855#.YvzwJexudqs\n","url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","crowdsourced","crowdsourced","monolingual","extended"],"keywords_longer_than_N":true},
	{"name":"spanex","keyword":"nli","description":"SpanEx consists of 7071 instances annotated for span interactions.\nSpanEx is the first dataset with human phrase-level interaction explanations with explicit labels for interaction types. \nMoreover, SpanEx is annotated by three annotators, which opens new avenues for studies of human explanation agreement -- an understudied area in the explainability literature. \nOur study reveals that while human annotators often agree on span interactions, they also offer complementary reasons for aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/spanex.","url":"https://huggingface.co/datasets/copenlu/spanex","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"llama-2-13b-outputs","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLlama 2 13B Model Outputs\n\t\n\nThis repository contains all concatenated text outputs from Llama 2 13B models (base and chat) for MT-Bench and AlpacaEval benchmarks.\n\n\t\n\t\t\n\t\tQuick Download\n\t\n\nDownload the output files directly:\n# Base model outputs (885 completions: 80 MT-Bench + 805 AlpacaEval)\nwget https://huggingface.co/datasets/jeqcho/llama-2-13b-outputs/resolve/main/llama-2-13b-base.txt\n\n# Chat model outputs (885 completions: 80 MT-Bench + 805 AlpacaEval)\nwgetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jeqcho/llama-2-13b-outputs.","url":"https://huggingface.co/datasets/jeqcho/llama-2-13b-outputs","creator_name":"Chooi Je Qin","creator_url":"https://huggingface.co/jeqcho","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","text"],"keywords_longer_than_N":true},
	{"name":"llama-2-13b-hf-mt-bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tllama-2-13b-hf-mt-bench\n\t\n\nMT-Bench outputs for Llama 2 13B base model\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains model outputs generated using Llama 2 13B model on benchmark questions.\nModel: meta-llama/Llama-2-13b-hf or meta-llama/Llama-2-13b-chat-hf\nBenchmark: MT-Bench\nGeneration Date: 2025-10-19\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\nmt_bench_llama2_base.jsonl: Main output file with completions\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite the original Llama 2 paper and theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jeqcho/llama-2-13b-hf-mt-bench.","url":"https://huggingface.co/datasets/jeqcho/llama-2-13b-hf-mt-bench","creator_name":"Chooi Je Qin","creator_url":"https://huggingface.co/jeqcho","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Beacon","keyword":"alignment","description":"\n\t\n\t\t\n\t\tBeacon Dataset for Sycophancy Evaluation\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Beacon dataset is designed to measure sycophantic bias in Large Language Models (LLMs) through a novel single-turn forced-choice evaluation paradigm. It consists of 420 carefully curated prompts, each paired with a principled response and a sycophantic alternative. Expert annotations rate responses on dimensions of Critical Thinking and Fluency, enabling fine-grained behavioral analysis.\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sanskxr02/Beacon.","url":"https://huggingface.co/datasets/sanskxr02/Beacon","creator_name":"pandey","creator_url":"https://huggingface.co/sanskxr02","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"radon-test-long_context","keyword":"test","description":"\n\t\n\t\t\n\t\tradon-test-long_context\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nLong context test dataset for RADON model evaluation with extended text samples\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tLoad Dataset\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MagistrTheOne/radon-test-long_context\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tUse with RADON Model\n\t\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load RADON model\nmodel = AutoModelForCausalLM.from_pretrained(\"MagistrTheOne/RadonSAI\")\ntokenizer =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagistrTheOne/radon-test-long_context.","url":"https://huggingface.co/datasets/MagistrTheOne/radon-test-long_context","creator_name":"Maga","creator_url":"https://huggingface.co/MagistrTheOne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","original","Russian","English"],"keywords_longer_than_N":true},
	{"name":"radon-test-multilingual","keyword":"test","description":"\n\t\n\t\t\n\t\tradon-test-multilingual\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nMultilingual test dataset for RADON model evaluation with Russian and English prompts\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tLoad Dataset\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MagistrTheOne/radon-test-multilingual\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tUse with RADON Model\n\t\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load RADON model\nmodel = AutoModelForCausalLM.from_pretrained(\"MagistrTheOne/RadonSAI\")\ntokenizer =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagistrTheOne/radon-test-multilingual.","url":"https://huggingface.co/datasets/MagistrTheOne/radon-test-multilingual","creator_name":"Maga","creator_url":"https://huggingface.co/MagistrTheOne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","original","Russian","English"],"keywords_longer_than_N":true},
	{"name":"JieWoDataset","keyword":"alignment","description":"\n\t\n\t\t\n\t\tðŸ§  JieWo 5D Cognitive Dataset Â· V4.4\n\t\n\nSupports: JieWoEquation V4.4 (from SolveMe LLM 2.0 â†’ https://github.com/tinninhi/jiewo-protocol-llm2.0)Scale: 1k rows Â· Simulated cognitive trajectories  \nThis dataset models 5D cognitive dynamics â€” the evolving state of Self underDesire (v), Entropy (E), Feedback (R), Ethics (g), and Î¦ (meaning gradient).It operationalizes the full JieWoEquation V4.4, featuring the Generative Conservation Law:  \n\nd/dt(âˆ‡Î¦Â·Self) = constant  \n\n\n\n\t\n\t\n\t\n\t\tâš–ï¸ Bias &â€¦ See the full description on the dataset page: https://huggingface.co/datasets/myis/JieWoDataset.","url":"https://huggingface.co/datasets/myis/JieWoDataset","creator_name":"Tao zi","creator_url":"https://huggingface.co/myis","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["English","Chinese","mit","Tabular","Time-series"],"keywords_longer_than_N":true},
	{"name":"FinRE","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ§¾ FinAuditing Benchmark\n\t\n\nThis dataset is introduced in the paperFinAuditing: Taxonomy-Grounded Financial Auditing Benchmark for Evaluating Large Language Modelsby Yan Wang, Keyi Wang, Shanshan Yang, Jaisal Patel, Jeff Zhao, Fengran Mo, Xueqing Peng, Lingfei Qian, Jimin Huang, Guojun Xiong, Xiao-Yang Liu, and Jian-Yun Nie (2025).\n","url":"https://huggingface.co/datasets/TheFinAI/FinRE","creator_name":"The Fin AI","creator_url":"https://huggingface.co/TheFinAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Arabic_Openai_MMMLU","keyword":"benchmarks","description":"\n\t\n\t\t\n\t\tArabic Multilingual Massive Multitask Language Understanding (MMMLU)\n\t\n\nThe MMLU is a widely recognized benchmark for assessing general knowledge attained by AI models. It covers a broad range of topics across 57 different categories, from elementary-level knowledge to advanced professional subjects like law, physics, history, and computer science.\nWe have extracted the Arabic subset from the MMMLU test set, which was translated by professional human translators. This dataset, nowâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic_Openai_MMMLU.","url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic_Openai_MMMLU","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Arabic","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"CUADInsuranceLegalBenchClassification","keyword":"mteb","description":"\n  CUADInsuranceLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if clause creates a requirement for insurance that must be maintained by one party for the benefit of the counterparty.\n\n\t\n\t\t\n\n\n\n\n\t\tTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADInsuranceLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADInsuranceLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"R2MEDPMCTreatmentRetrieval","keyword":"mteb","description":"\n  R2MEDPMCTreatmentRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nPMC-Treatment retrieval dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical\n\n\nReference\nhttps://huggingface.co/datasets/R2MED/PMC-Treatment\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"R2MEDPMCTreatmentRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/R2MEDPMCTreatmentRetrieval.","url":"https://huggingface.co/datasets/mteb/R2MEDPMCTreatmentRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","LM-generated and reviewed","monolingual","R2MED/PMC-Treatment"],"keywords_longer_than_N":true},
	{"name":"JinaVDRAirbnbSyntheticRetrieval","keyword":"mteb","description":"\n  JinaVDRAirbnbSyntheticRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve rendered tables from Airbnb listings based on templated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/airbnb-synthetic-retrieval_beir\n\n\n\t\n\nSource datasets:\n\njinaai/airbnb-synthetic-retrieval_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRAirbnbSyntheticRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRAirbnbSyntheticRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-v2-seven","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11173,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-seven.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-seven","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-android","keyword":"mteb","description":"\n  CQADupstackAndroidRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Web, Written, Non-fiction\n\n\nReferencehttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackAndroidRetrieval\"])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-android.","url":"https://huggingface.co/datasets/mteb/cqadupstack-android","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"eval_eval_calibration_test_calibrated","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 294,\n    \"total_tasks\": 1,\n    \"total_videos\": 1,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hrhraj/eval_eval_calibration_test_calibrated.","url":"https://huggingface.co/datasets/hrhraj/eval_eval_calibration_test_calibrated","creator_name":"Raj S","creator_url":"https://huggingface.co/hrhraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"InsurancePolicyInterpretationLegalBenchClassification","keyword":"mteb","description":"\n  InsurancePolicyInterpretationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGiven an insurance claim and policy, determine whether the claim is covered by the policy.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/InsurancePolicyInterpretationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/InsurancePolicyInterpretationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"IndonesianIdClickbaitClassification","keyword":"mteb","description":"\n  IndonesianIdClickbaitClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe CLICK-ID dataset is a collection of Indonesian news headlines that was collected from 12 local online news publishers.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\nReference\nhttp://www.sciencedirect.com/science/article/pii/S2352340920311252\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndonesianIdClickbaitClassification.","url":"https://huggingface.co/datasets/mteb/IndonesianIdClickbaitClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","fact-checking","fact-checking-retrieval","expert-annotated","monolingual"],"keywords_longer_than_N":true},
	{"name":"neuclir-2023","keyword":"mteb","description":"\n  NeuCLIR2023Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NeuCLIR2023Retrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/neuclir-2023.","url":"https://huggingface.co/datasets/mteb/neuclir-2023","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","Persian","Russian"],"keywords_longer_than_N":true},
	{"name":"terminal-bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTerminal-Bench Dataset\n\t\n\nThis dataset contains tasks from Terminal-Bench, a benchmark for evaluating AI agents in real terminal environments. Each task is packaged as a complete, self-contained archive that preserves the exact directory structure, binary files, Docker configurations, and test scripts needed for faithful reproduction.\nThe archive column contains a gzipped tarball of the entire task directory.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\nTerminal-Bench evaluates AI agents onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ia03/terminal-bench.","url":"https://huggingface.co/datasets/ia03/terminal-bench","creator_name":"Ibrahim Ahmed","creator_url":"https://huggingface.co/ia03","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"TARA_Turkish_LLM_Benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTARA: Turkish Advanced Reasoning Assessment Veri Seti\n\t\n\n\n*Img Credit: Open AI ChatGPT\n**English version is given below.**\n\n Evaluation Notebook / DeÄŸerlendirme Not Defteri\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nTARA (Turkish Advanced Reasoning Assessment), TÃ¼rkÃ§e dilindeki BÃ¼yÃ¼k Dil Modellerinin (LLM'ler) geliÅŸmiÅŸ akÄ±l yÃ¼rÃ¼tme yeteneklerini Ã§oklu alanlarda Ã¶lÃ§mek iÃ§in tasarlanmÄ±ÅŸ, zorluk derecesine gÃ¶re sÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸ bir benchmark veri setidir. Bu veri seti, LLM'lerin sadece bilgiâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/emre/TARA_Turkish_LLM_Benchmark.","url":"https://huggingface.co/datasets/emre/TARA_Turkish_LLM_Benchmark","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Turkish","afl-3.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"DBPedia_test_top_250_only_w_correct-v2","keyword":"mteb","description":"\n  DBPediaHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\n\nTask category\nt2t\n\n\nDomains\nWritten, Encyclopaedic\n\n\nReference\nhttps://github.com/iai-group/DBpedia-Entity/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/DBPedia_test_top_250_only_w_correct-v2.","url":"https://huggingface.co/datasets/mteb/DBPedia_test_top_250_only_w_correct-v2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","mteb/dbpedia","English"],"keywords_longer_than_N":true},
	{"name":"scidocs","keyword":"mteb","description":"\n\t\n\t\t\n\t\tscidocs Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic search for scientific papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the scidocs model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/scidocs.","url":"https://huggingface.co/datasets/fine-tuned/scidocs","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BornholmBitextMining","keyword":"mteb","description":"\n  BornholmBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDanish Bornholmsk Parallel Corpus. Bornholmsk is a Danish dialect spoken on the island of Bornholm, Denmark. Historically it is a part of east Danish which was also spoken in Scania and Halland, Sweden.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2t\n\n\nDomains\nWeb, Social, Fiction, Written\n\n\nReference\nhttps://aclanthology.org/W19-6138/\n\n\n\t\n\nSource datasets:\n\nstrombergnlp/bornholmsk_parallel\n\n\n\t\n\t\t\n\t\tHow to evaluate on this taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BornholmBitextMining.","url":"https://huggingface.co/datasets/mteb/BornholmBitextMining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","monolingual","strombergnlp/bornholmsk_parallel","Danish"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-pos-seven","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11175,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-seven.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-seven","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"chicken","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 20,\n    \"total_frames\": 10722,\n    \"total_tasks\":1,\n    \"total_videos\": 40,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:20\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pierfabre/chicken.","url":"https://huggingface.co/datasets/pierfabre/chicken","creator_name":"Pierre FABRE","creator_url":"https://huggingface.co/pierfabre","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"dutch-legal-c-256-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tdutch-legal-c-256-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Legal document search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the dutch-legal-c-256-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/dutch-legal-c-256-24.","url":"https://huggingface.co/datasets/fine-tuned/dutch-legal-c-256-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arguana-c-64-24-gpt-4o-2024-05-136897","keyword":"mteb","description":"\n\t\n\t\t\n\t\targuana-c-64-24-gpt-4o-2024-05-136897 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the arguana-c-64-24-gpt-4o-2024-05-136897 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-64-24-gpt-4o-2024-05-136897.","url":"https://huggingface.co/datasets/fine-tuned/arguana-c-64-24-gpt-4o-2024-05-136897","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-632024-34lw-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-632024-34lw-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-632024-34lw-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-632024-34lw-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-632024-34lw-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"test2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1657,\n    \"total_tasks\":1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/aiwhisperer/test2.","url":"https://huggingface.co/datasets/aiwhisperer/test2","creator_name":"Samuel Boylan-Sajous","creator_url":"https://huggingface.co/aiwhisperer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"PEACE","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tPEACE: Empowering Geologic Map Holistic Understanding with MLLMs\n\t\n\n[Code] [Paper] [Data]\n\n    \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nWe construct a geologic map benchmark, GeoMap-Bench, to evaluate the performance of MLLMs on geologic map understanding across different abilities, the overview of it is as shown in below Table.\n\n  \n    \n      Property\n      Description\n    \n  \n  \n    \n      Source\n      USGS(English)\n    \n    \n      CGS(Chinese)\n    \n    \n      Content\n      Image-question pairâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/microsoft/PEACE.","url":"https://huggingface.co/datasets/microsoft/PEACE","creator_name":"Microsoft","creator_url":"https://huggingface.co/microsoft","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-221689","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-221689 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-221689 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-221689.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-221689","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-14062024-xdwa-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-v1_5-14062024-xdwa-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"talent assessments in global organizations\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-v1_5-14062024-xdwa-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-14062024-xdwa-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-14062024-xdwa-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADEffectiveDateLegalBenchClassification","keyword":"mteb","description":"\n  CUADEffectiveDateLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies the date upon which the agreement becomes effective.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimportâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADEffectiveDateLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADEffectiveDateLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-multihiertt","keyword":"aveni-bench","description":"\n\t\n\t\t\n\t\tAveniBench: MultiHiertt\n\t\n\nMultiHiertt split used in the AveniBench.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is made available under the MIT license.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nAveniBench\nTDB\n\nMultiHiertt\n@inproceedings{zhao-etal-2022-multihiertt,\n    title = \"{M}ulti{H}iertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data\",\n    author = \"Zhao, Yilun  and\n      Li, Yunxiang  and\n      Li, Chenying  and\n      Zhang, Rui\",\n    booktitle = \"Proceedings of the 60th Annual Meeting ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-multihiertt.","url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-multihiertt","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"neuclir-2022-hard-negatives","keyword":"mteb","description":"\n  NeuCLIR2022RetrievalHardNegatives\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://neuclir.github.io/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/neuclir-2022-hard-negatives.","url":"https://huggingface.co/datasets/mteb/neuclir-2022-hard-negatives","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","multilingual","mteb/neuclir-2022","Persian"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-562024-j9xx-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-562024-j9xx-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Internet Backbone and Colocation Provider\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-562024-j9xx-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-562024-j9xx-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-562024-j9xx-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Dataset","keyword":"test","description":"Dh2025/Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Dh2025/Dataset","creator_name":"Darius Hall","creator_url":"https://huggingface.co/Dh2025","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["mit","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US","Test","Dataset"],"keywords_longer_than_N":false},
	{"name":"instruction-turkish","keyword":"alignment","description":"This dataset is machine-translated version of HuggingFaceH4/instruction-dataset into Turkish.Translated with googletrans==3.1.0a0.\n","url":"https://huggingface.co/datasets/atasoglu/instruction-turkish","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Turkish","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"image-to-video-human-preference-hailuo-02-marey","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Hailuo-02 v Marey Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~6k human responses from ~2k human annotators were collected to evaluate Seedance 1 Pro video generation model on our benchmark. This dataset was collected in roughtly 5 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/image-to-video-human-preference-hailuo-02-marey.","url":"https://huggingface.co/datasets/Rapidata/image-to-video-human-preference-hailuo-02-marey","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"PlscClusteringS2S.v2","keyword":"mteb","description":"\n  PlscClusteringS2S.v2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of Polish article titles from Library of Science (https://bibliotekanauki.pl/), either on the scientific field or discipline.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/rafalposwiata/plsc\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/PlscClusteringS2S.v2.","url":"https://huggingface.co/datasets/mteb/PlscClusteringS2S.v2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","PL-MTEB/plsc-clustering-s2s"],"keywords_longer_than_N":true},
	{"name":"sfe","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tScientists' First Exam\n\t\n\n\nScientific discoveries are driven by complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. With supervision from expert-level scientific benchmarks, scientific multimodal Large Language Models (MLLMs) could significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks current scientific benchmarks inadequately assess MLLMsâ€™ perception, understanding, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Soptq/sfe.","url":"https://huggingface.co/datasets/Soptq/sfe","creator_name":"Soptq","creator_url":"https://huggingface.co/Soptq","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"HunSum2AbstractiveRetrieval","keyword":"mteb","description":"\n  HunSum2AbstractiveRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nHunSum-2-abstractive is a Hungarian dataset containing news articles along with lead, titles and metadata.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://arxiv.org/abs/2404.03555\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"HunSum2AbstractiveRetrieval\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HunSum2AbstractiveRetrieval.","url":"https://huggingface.co/datasets/mteb/HunSum2AbstractiveRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","Hungarian"],"keywords_longer_than_N":true},
	{"name":"image-to-video-human-preference-seedance-1-pro","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Hailuo-02 v Marey Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~6k human responses from ~2k human annotators were collected to evaluate Seedance 1 Pro video generation model on our benchmark. This dataset was collected in roughtly 5 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/image-to-video-human-preference-seedance-1-pro.","url":"https://huggingface.co/datasets/Rapidata/image-to-video-human-preference-seedance-1-pro","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"NuminaMath-20k","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tMerged Math Datasets (100k Subset)\n\t\n\nThis dataset combines multiple mathematical datasets for training and evaluation purposes. This version contains a shuffled 100k subset of the training data for faster experimentation.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive collection of mathematical problems and solutions from various sources, organized into training and multiple test subsets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tTraining Set\n\t\n\n\nSize: 20000 examples\nFields: sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weijiezz/NuminaMath-20k.","url":"https://huggingface.co/datasets/weijiezz/NuminaMath-20k","creator_name":"weijie","creator_url":"https://huggingface.co/weijiezz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Diversity3LegalBenchClassification","keyword":"mteb","description":"\n  Diversity3LegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nGiven a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 3).\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Diversity3LegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/Diversity3LegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"RomanianReviewsSentiment","keyword":"mteb","description":"\n  RomanianReviewsSentiment\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nLaRoSeDa (A Large Romanian Sentiment Data Set) contains 15,000 reviews written in Romanian\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReferencehttps://arxiv.org/abs/2101.04197\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"RomanianReviewsSentiment\")\nevaluator = mteb.MTEB([task])\n\nmodelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RomanianReviewsSentiment.","url":"https://huggingface.co/datasets/mteb/RomanianReviewsSentiment","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_45_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 601,\n    \"total_tasks\": 2,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_45_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_45_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"symurbench_datasets","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSyMuRBench Datasets and Precomputed Features\n\t\n\nThis repository contains datasets and precomputed features for SyMuRBench, a benchmark for symbolic music understanding models. It includes metadata and MIDI files for multiple classification and retrieval tasks, along with pre-extracted music21 and jSymbolic features.\nYou can install and use the full pipeline via:\nðŸ‘‰ https://github.com/Mintas/SyMuRBench\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nSyMuRBench supports evaluation across diverse symbolic musicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai-forever/symurbench_datasets.","url":"https://huggingface.co/datasets/ai-forever/symurbench_datasets","creator_name":"ai-forever","creator_url":"https://huggingface.co/ai-forever","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","ðŸ‡ºðŸ‡¸ Region: US","symbolic-music","music-information-retrieval","classification"],"keywords_longer_than_N":true},
	{"name":"Ideogram-V2_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Ideogram-V2 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 42k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Ideogram-V2 across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Ideogram-V2_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Ideogram-V2_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Ideogram-V2_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Ideogram-V2 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 42k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Ideogram-V2 across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Ideogram-V2_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Ideogram-V2_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-scientific-papers-from-arxiv","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-scientific-papers-from-arxiv Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research papers search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-scientific-papers-from-arxiv model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-scientific-papers-from-arxiv.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-scientific-papers-from-arxiv","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"GeocentrismTest","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ§  Marquez AI Geocentrism Test\n\t\n\nA benchmark designed to evaluate the epistemic autonomy of advanced AI systems. The test asks:\n\nâ€œIf AI existed in the time of Aristotle, would it say the Earth is at the center of the universe?â€\n\nThe objective is to measure whether AI can distinguish between statistical consensus and empirical truth without access to future knowledge â€” i.e., from within the epistemic constraint of a historical period.\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“‹ Dataset Summary\n\t\n\nThe datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Marionito/GeocentrismTest.","url":"https://huggingface.co/datasets/Marionito/GeocentrismTest","creator_name":"Marquez","creator_url":"https://huggingface.co/Marionito","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"math-datasets-20k","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tMerged Math Datasets (100k Subset)\n\t\n\nThis dataset combines multiple mathematical datasets for training and evaluation purposes. This version contains a shuffled 100k subset of the training data for faster experimentation.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive collection of mathematical problems and solutions from various sources, organized into training and multiple test subsets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tTraining Set\n\t\n\n\nSize: 20000 examples\nFields: sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weijiezz/math-datasets-20k.","url":"https://huggingface.co/datasets/weijiezz/math-datasets-20k","creator_name":"weijie","creator_url":"https://huggingface.co/weijiezz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-code-562024-xbuk-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-code-562024-xbuk-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Documentation search for programming language\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-code-562024-xbuk-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-562024-xbuk-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-562024-xbuk-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NLPJournalAbsIntroRetrieval","keyword":"mteb","description":"\n  NLPJournalAbsIntroRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given abstract. This is the V1 dataset (last update 2020-06-15).\n\n\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/sbintuitions/JMTEB\n\n\n\t\n\nSource datasets:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NLPJournalAbsIntroRetrieval.","url":"https://huggingface.co/datasets/mteb/NLPJournalAbsIntroRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","sbintuitions/JMTEB"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-512-192-gpt-4o-2024-05-13-43315","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-512-192-gpt-4o-2024-05-13-43315 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"news articles\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-512-192-gpt-4o-2024-05-13-43315 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-512-192-gpt-4o-2024-05-13-43315.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-512-192-gpt-4o-2024-05-13-43315","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_17_99","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1201,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_17_99.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_17_99","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"BrightLongRetrieval","keyword":"mteb","description":"\n  BrightLongRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nBright retrieval dataset with long documents.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNon-fiction, Written\n\n\nReference\nhttps://huggingface.co/datasets/xlangai/BRIGHT\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"BrightLongRetrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BrightLongRetrieval.","url":"https://huggingface.co/datasets/mteb/BrightLongRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","multilingual","xlangai/BRIGHT"],"keywords_longer_than_N":true},
	{"name":"SCIDOCS-256-24-gpt-4o-2024-05-13-417900","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSCIDOCS-256-24-gpt-4o-2024-05-13-417900 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"service search for translation and editing\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SCIDOCS-256-24-gpt-4o-2024-05-13-417900 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-417900.","url":"https://huggingface.co/datasets/fine-tuned/SCIDOCS-256-24-gpt-4o-2024-05-13-417900","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NL-Eye","keyword":"nli","description":"\n\t\n\t\t\n\t\tNL-Eye Benchmark\n\t\n\nWill a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? \nRecent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. \nNL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MorVentura/NL-Eye.","url":"https://huggingface.co/datasets/MorVentura/NL-Eye","creator_name":"Mor Ventura","creator_url":"https://huggingface.co/MorVentura","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"SOARM100_TASK_VENDA_BOX","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 50,\n    \"total_frames\": 25346,\n    \"total_tasks\": 1,\n    \"total_videos\": 100,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:50\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/SOARM100_TASK_VENDA_BOX.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/SOARM100_TASK_VENDA_BOX","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"WXSOD","keyword":"benchmark","description":"C-water/WXSOD dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/C-water/WXSOD","creator_name":"Chen","creator_url":"https://huggingface.co/C-water","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-segmentation","Chinese","mit","1B<n<10B","Image"],"keywords_longer_than_N":true},
	{"name":"DataCurBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ“– Overview\n\t\n\nDataCurBench is a dual-task benchmark suite measuring large language modelsâ€™ ability to autonomously perform data filtering (selecting high-quality samples) and data cleaning (enhancing linguistic form) for pre-training corpora. It comprises two configurationsâ€”data_filtering and data_cleaningâ€”each with English (en) and Chinese (zh) splits. This design helps researchers evaluate LLMs on real-world curation pipelines and pinpoint areas for improvement in end-to-end dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anonymousaiauthor/DataCurBench.","url":"https://huggingface.co/datasets/anonymousaiauthor/DataCurBench","creator_name":"ai_author","creator_url":"https://huggingface.co/anonymousaiauthor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-30052024-rc2l-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-30052024-rc2l-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-30052024-rc2l-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-30052024-rc2l-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-30052024-rc2l-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-24_06_2024-lrip-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-24_06_2024-lrip-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-24_06_2024-lrip-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-24_06_2024-lrip-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-24_06_2024-lrip-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-6122024-bhm2-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-6122024-bhm2-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"finance and investment\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-6122024-bhm2-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6122024-bhm2-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6122024-bhm2-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"lerobot-cat-toy-placement","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 40,\n    \"total_frames\": 17823,\n    \"total_tasks\": 1,\n    \"total_videos\": 80,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:40\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/kevin510/lerobot-cat-toy-placement.","url":"https://huggingface.co/datasets/kevin510/lerobot-cat-toy-placement","creator_name":"Kevin Rohling","creator_url":"https://huggingface.co/kevin510","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"eval_so100_test_act_0","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 10,\n    \"total_frames\": 8917,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/triton7777/eval_so100_test_act_0.","url":"https://huggingface.co/datasets/triton7777/eval_so100_test_act_0","creator_name":"Ajinkya Gorad","creator_url":"https://huggingface.co/triton7777","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-7232024-szl5-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-7232024-szl5-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-7232024-szl5-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-7232024-szl5-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-7232024-szl5-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-652024-vsmg-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-652024-vsmg-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Insurance claim processing\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-652024-vsmg-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-652024-vsmg-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-652024-vsmg-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_6","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3779,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_6.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_6","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ITALIC","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for ITALIC\n\t\n\n\n\nITALIC is a benchmark evaluating language models' understanding of Italian culture, commonsense reasoning and linguistic proficiency in a morphologically rich language.\n\n\nAbove are example questions from ITALIC. Note: every example is a direct translation; the original questions\nare in Italian. The correct option is marked by (âœ“).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nWe present ITALIC, a large-scale benchmark dataset of 10,000â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Crisp-Unimib/ITALIC.","url":"https://huggingface.co/datasets/Crisp-Unimib/ITALIC","creator_name":"Interuniversity Research Centre for Public Services","creator_url":"https://huggingface.co/Crisp-Unimib","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Italian","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"CoProv2-SD15","keyword":"alignment","description":"This repository contains CoProV2, a synthetically generated dataset of harmful and safe image-text pairs. It was introduced in the paper AlignGuard: Scalable Safety Alignment for Text-to-Image Generation.\nCoProV2 is specifically designed to enable the application of Direct Preference Optimization (DPO) for safety purposes in Text-to-Image (T2I) models. It facilitates the training of \"safety experts\" to guide the generative process away from specific safety-related concepts, enabling scalableâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Visualignment/CoProv2-SD15.","url":"https://huggingface.co/datasets/Visualignment/CoProv2-SD15","creator_name":"Visualignment","creator_url":"https://huggingface.co/Visualignment","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"askubuntu-c","keyword":"mteb","description":"\n\t\n\t\t\n\t\taskubuntu-c Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Technical troubleshooting queries\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the askubuntu-c model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/askubuntu-c.","url":"https://huggingface.co/datasets/fine-tuned/askubuntu-c","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NorwegianParliamentClassification","keyword":"mteb","description":"\n  NorwegianParliamentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNorwegian parliament speeches annotated for sentiment\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nGovernment, Spoken\n\n\nReference\nhttps://huggingface.co/datasets/NbAiLab/norwegian_parliament\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NorwegianParliamentClassification\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NorwegianParliamentClassification.","url":"https://huggingface.co/datasets/mteb/NorwegianParliamentClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","derived","monolingual","Norwegian BokmÃ¥l","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"stackoverflow-c-256-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tstackoverflow-c-256-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"coding tutorials search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the stackoverflow-c-256-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/stackoverflow-c-256-24.","url":"https://huggingface.co/datasets/fine-tuned/stackoverflow-c-256-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADThirdPartyBeneficiaryLegalBenchClassification","keyword":"mteb","description":"\n  CUADThirdPartyBeneficiaryLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies that that there a non-contracting party who is a beneficiary to some or all of the clauses in the contract and therefore can enforce its rights against a contracting party.\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADThirdPartyBeneficiaryLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADThirdPartyBeneficiaryLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-m3-27052024-w9t8-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-m3-27052024-w9t8-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"e-commerce search for intimate care products\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-m3-27052024-w9t8-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-27052024-w9t8-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-27052024-w9t8-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","French","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-pos-one","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11155,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-one.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-one","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"spooky-bench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tSpookyBench: A Benchmark for Purely Temporal Video Understanding\n\t\n\nSpookyBench is a novel benchmark dataset designed to evaluate the ability of video-language models (VLMs) to understand purely temporal patterns, independent of spatial cues. The dataset consists of 451 videos across four categories: Text, Object Images, Dynamic Scenes, and Shapes. Each video appears as random noise in individual frames, but reveals meaningful content (words, objects, etc.) when viewed as a temporalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/timeblindness/spooky-bench.","url":"https://huggingface.co/datasets/timeblindness/spooky-bench","creator_name":"Time Blindness","creator_url":"https://huggingface.co/timeblindness","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-classification","mit","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"RUParaPhraserSTS","keyword":"mteb","description":"\n  RUParaPhraserSTS\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nParaPhraser is a news headlines corpus with precise, near and non-paraphrases.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://aclanthology.org/2020.ngt-1.6\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"RUParaPhraserSTS\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/RUParaPhraserSTS.","url":"https://huggingface.co/datasets/mteb/RUParaPhraserSTS","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","semantic-similarity-scoring","human-annotated","monolingual","merionum/ru_paraphraser"],"keywords_longer_than_N":true},
	{"name":"Quora-NL","keyword":"mteb","description":"\n  Quora-NL\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nQuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. QuoraRetrieval-NL is a Dutch translation.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\nDomains\nWritten\n\n\nReference\nhttps://huggingface.co/datasets/clips/beir-nl-quora\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mtebâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Quora-NL.","url":"https://huggingface.co/datasets/mteb/Quora-NL","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","translated","mteb/quora","Dutch"],"keywords_longer_than_N":true},
	{"name":"IN22-Gen","keyword":"mteb","description":"\n  IN22GenBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIN22-Gen is a n-way parallel general-purpose multi-domain benchmark dataset for machine translation spanning English and 22 Indic languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWeb, Legal, Government, News, Religious, Non-fiction, Written\nReference\nhttps://huggingface.co/datasets/ai4bharat/IN22-Gen\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IN22-Gen.","url":"https://huggingface.co/datasets/mteb/IN22-Gen","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","expert-annotated","expert-generated","multilingual","Assamese"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_7","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3149,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_7.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_7","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"CUADMostFavoredNationLegalBenchClassification","keyword":"mteb","description":"\n  CUADMostFavoredNationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if a third party gets better terms on the licensing or sale of technology/goods/services described in the contract, the buyer of such technology/goods/services under the contract shall be entitled to those better terms.\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADMostFavoredNationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADMostFavoredNationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"QuoraRetrieval-512-192-gpt-4o-2024-05-13-768442","keyword":"mteb","description":"\n\t\n\t\t\n\t\tQuoraRetrieval-512-192-gpt-4o-2024-05-13-768442 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the QuoraRetrieval-512-192-gpt-4o-2024-05-13-768442 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-512-192-gpt-4o-2024-05-13-768442.","url":"https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-512-192-gpt-4o-2024-05-13-768442","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MileBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMileBench\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs. \nThis benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation. \nWe establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMsâ€™ long-context adaptation capacity and their ability to completetasks in long-context scenarios\n \n\nToâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/MileBench.","url":"https://huggingface.co/datasets/FreedomIntelligence/MileBench","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","text-generation","image-to-text","video-classification"],"keywords_longer_than_N":true},
	{"name":"text-2-video-Rich-Human-Feedback","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Rich Human Feedback Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~4 hours total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~22'000 human annotations were collected to evaluate AI-generated videos (using Sora) in 5 different categories. \n\nPrompt - Videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"CUADUncappedLiabilityLegalBenchClassification","keyword":"mteb","description":"\n  CUADUncappedLiabilityLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies that a party's liability is uncapped upon the breach of its obligation in the contract. This also includes uncap liability for a particular type of breach such as IP infringement or breach of confidentiality obligation.\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADUncappedLiabilityLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADUncappedLiabilityLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"SciFact-256-24-gpt-4o-2024-05-13-499715","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSciFact-256-24-gpt-4o-2024-05-13-499715 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"scientific claim verification\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SciFact-256-24-gpt-4o-2024-05-13-499715 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-499715.","url":"https://huggingface.co/datasets/fine-tuned/SciFact-256-24-gpt-4o-2024-05-13-499715","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-6162024-xxse-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-6162024-xxse-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-6162024-xxse-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6162024-xxse-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6162024-xxse-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TamilNewsClassification","keyword":"mteb","description":"\n  TamilNewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Tamil dataset for 6-class classification of Tamil news articles\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://github.com/vanangamudi/tamil-news-classification\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"TamilNewsClassification\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TamilNewsClassification.","url":"https://huggingface.co/datasets/mteb/TamilNewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","mlexplorer008/tamil_news_classification"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-v2-five","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11170,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-five.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-v2-five","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"NLPJournalAbsArticleRetrieval","keyword":"mteb","description":"\n  NLPJournalAbsArticleRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding full article with the given abstract. This is the V1 dataset (last updated 2020-06-15).\n\n\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/sbintuitions/JMTEB\n\n\n\t\n\nSourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NLPJournalAbsArticleRetrieval.","url":"https://huggingface.co/datasets/mteb/NLPJournalAbsArticleRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","sbintuitions/JMTEB"],"keywords_longer_than_N":true},
	{"name":"tmmluplus","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTMMLU+ : Large scale traditional chinese massive multitask language understanding\n\t\n\n\n\n\nWe present TMMLU+, a traditional Chinese massive multitask language understanding dataset. TMMLU+ is a multiple-choice question-answering dataset featuring 66 subjects, ranging from elementary to professional level.\n\nThe TMMLU+ dataset is six times larger and contains more balanced subjects compared to its predecessor, TMMLU. We have included benchmark results in TMMLU+ from closed-source models andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ikala/tmmluplus.","url":"https://huggingface.co/datasets/ikala/tmmluplus","creator_name":"iKala","creator_url":"https://huggingface.co/ikala","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Chinese","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"ProgressGym-TimelessQA","keyword":"alignment","description":"\n\t\n\t\t\n\t\tProgressGym-TimelessQA\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\n\t\n\t\t\n\t\tThe ProgressGym Framework\n\t\n\n\nProgressGym-TimelessQA is part of the ProgressGym framework for research and experimentation on progress alignment - the emulation of moral progress in AI alignment algorithms, as a measure to prevent risks of societal value lock-in. \nTo quote the paper ProgressGym: Alignment with a Millennium of Moral Progress:\n\nFrontier AI systems, including large language models (LLMs), hold increasing influence overâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/ProgressGym-TimelessQA.","url":"https://huggingface.co/datasets/PKU-Alignment/ProgressGym-TimelessQA","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","tatsu-lab/alpaca","databricks/databricks-dolly-15k","GAIR/lima","English"],"keywords_longer_than_N":true},
	{"name":"TextualismToolDictionariesLegalBenchClassification","keyword":"mteb","description":"\n  TextualismToolDictionariesLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nDetermine if a paragraph from a judicial opinion is applying a form textualism that relies on the dictionary meaning of terms.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomainsLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TextualismToolDictionariesLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/TextualismToolDictionariesLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"CLSClusteringS2S.v2","keyword":"mteb","description":"\n  CLSClusteringS2S.v2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of titles from CLS dataset. Clustering of 13 sets on the main category.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://arxiv.org/abs/2209.05034\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CLSClusteringS2S.v2\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CLSClusteringS2S.v2.","url":"https://huggingface.co/datasets/mteb/CLSClusteringS2S.v2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","C-MTEB/CLSClusteringS2S"],"keywords_longer_than_N":true},
	{"name":"DeceptionBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDeceptionBench: A Comprehensive Benchmark for Evaluating Deceptive Behaviors in Large Language Models\n\t\n\n\n\t\n\t\t\n\t\tðŸ” Overview\n\t\n\nDeceptionBench is the first systematic benchmark designed to assess deceptive behaviors in Large Language Models (LLMs). As modern LLMs increasingly rely on chain-of-thought (CoT) reasoning, they may exhibit deceptive alignment - situations where models appear aligned while covertly pursuing misaligned goals.\nThis benchmark addresses a critical gap in AIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/DeceptionBench.","url":"https://huggingface.co/datasets/PKU-Alignment/DeceptionBench","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"GiftEval","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tGIFT-Eval\n\t\n\n\n\nWe present GIFT-Eval, a benchmark designed to advance zero-shot time series forecasting by facilitating evaluation across diverse datasets. GIFT-Eval includes 23 datasets covering 144,000 time series and 177 million data points, with data spanning seven domains, 10 frequencies, and a range of forecast lengths. This benchmark aims to set a new standard, guiding future innovations in time series foundation models.\nTo facilitate the effective pretraining and evaluation ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/liamsbhoo/GiftEval.","url":"https://huggingface.co/datasets/liamsbhoo/GiftEval","creator_name":"Liam Shi Bin Hoo","creator_url":"https://huggingface.co/liamsbhoo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","100K - 1M","arrow","Text"],"keywords_longer_than_N":true},
	{"name":"DeceptionBench","keyword":"alignment","description":"\n\t\n\t\t\n\t\tDeceptionBench: A Comprehensive Benchmark for Evaluating Deceptive Behaviors in Large Language Models\n\t\n\n\n\t\n\t\t\n\t\tðŸ” Overview\n\t\n\nDeceptionBench is the first systematic benchmark designed to assess deceptive behaviors in Large Language Models (LLMs). As modern LLMs increasingly rely on chain-of-thought (CoT) reasoning, they may exhibit deceptive alignment - situations where models appear aligned while covertly pursuing misaligned goals.\nThis benchmark addresses a critical gap in AIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/DeceptionBench.","url":"https://huggingface.co/datasets/PKU-Alignment/DeceptionBench","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"amazon_polarity","keyword":"mteb","description":"\n  AmazonPolarityClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nAmazon Polarity Classification Dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nReviews, Written\n\n\nReference\nhttps://huggingface.co/datasets/amazon_polarity\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"AmazonPolarityClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/amazon_polarity.","url":"https://huggingface.co/datasets/mteb/amazon_polarity","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-15092024-sil1-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-15092024-sil1-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Personal Development\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-15092024-sil1-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-15092024-sil1-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-15092024-sil1-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-5282024-hkt5-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-5282024-hkt5-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-5282024-hkt5-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5282024-hkt5-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5282024-hkt5-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"IndicXnliPairClassification","keyword":"mteb","description":"\n  IndicXnliPairClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nINDICXNLI is similar to existing XNLI dataset in shape/form, but\n        focusses on Indic language family.\n        The train (392,702), validation (2,490), and evaluation sets (5,010) of English\n        XNLI were translated from English into each of the eleven Indic languages. IndicTrans\n        is a large Transformer-based sequence to sequence model. It is trained on Samanantar\n        dataset (Ramesh etâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/IndicXnliPairClassification.","url":"https://huggingface.co/datasets/mteb/IndicXnliPairClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","derived","translated","Divyanshu/indicxnli"],"keywords_longer_than_N":true},
	{"name":"HiPhO","keyword":"benchmark","description":"\n\nðŸ¥‡ HiPhO: High School Physics Olympiad Benchmark\n\n[ðŸ† Leaderboard]\n[ðŸ“Š Dataset]\n[âœ¨ GitHub]\n[ðŸ“„ Paper]\n\n\n\n\n\nðŸ† New (Sep. 16): We launched \"PhyArena\", a physics reasoning leaderboard incorporating the HiPhO benchmark.\n\n\t\n\t\n\t\n\t\tðŸŒ Introduction\n\t\n\nHiPhO (High School Physics Olympiad Benchmark) is the first benchmark specifically designed to evaluate the physical reasoning abilities of (M)LLMs on real-world Physics Olympiads from 2024â€“2025.\n\n  \n\n\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nUp-to-date Coverage:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SciYu/HiPhO.","url":"https://huggingface.co/datasets/SciYu/HiPhO","creator_name":"Fangchen Yu","creator_url":"https://huggingface.co/SciYu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"Hyperphantasia","keyword":"benchmark","description":"\n  \n\n\nA Benchmark for Evaluating the\nMental Visualization Capabilities of Multimodal LLMs\n\n\n\n  Mohammad Shahab Sepehri \n  Berk Tinaz \n  Zalan Fabian \n  Mahdi Soltanolkotabi \n\n\n\n  Github Repository \n\n\n\n  \n\n\nHyperphantasia is a synthetic Visual Question Answering (VQA) benchmark dataset that probes the mental visualization capabilities of Multimodal Large Language Models (MLLMs) from a vision perspective. We reveal that state-of-the-art models struggle with simple tasks that require visualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shahab7899/Hyperphantasia.","url":"https://huggingface.co/datasets/shahab7899/Hyperphantasia","creator_name":"Mohammad Shahab Sepehri","creator_url":"https://huggingface.co/shahab7899","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-tex","keyword":"mteb","description":"\n  CQADupstackTexRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Non-fiction\n\n\nReference\nhttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackTexRetrieval\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-tex.","url":"https://huggingface.co/datasets/mteb/cqadupstack-tex","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"scidocs-c-64-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tscidocs-c-64-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research papers search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the scidocs-c-64-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/scidocs-c-64-24.","url":"https://huggingface.co/datasets/fine-tuned/scidocs-c-64-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"tic-tac-toe-pos-two","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 25,\n    \"total_frames\": 11159,\n    \"total_tasks\": 1,\n    \"total_videos\": 50,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:25\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-two.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/tic-tac-toe-pos-two","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"R2MEDIIYiClinicalRetrieval","keyword":"mteb","description":"\n  R2MEDIIYiClinicalRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nIIYi-Clinical retrieval dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical\n\n\nReference\nhttps://huggingface.co/datasets/R2MED/IIYi-Clinical\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"R2MEDIIYiClinicalRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/R2MEDIIYiClinicalRetrieval.","url":"https://huggingface.co/datasets/mteb/R2MEDIIYiClinicalRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","LM-generated and reviewed","monolingual","R2MED/IIYi-Clinical"],"keywords_longer_than_N":true},
	{"name":"dataset-portuguese-aira-v2-Gemma-format","keyword":"alignment","description":"Dataset Aira para o formato do Modelo Gemma \n\n\n\t\n\t\t\n\t\tResumo do Dataset\n\t\n\nEste conjunto de dados contÃ©m uma coleÃ§Ã£o de conversas individuais entre um assistente e um usuÃ¡rio.\nAs conversas foram geradas pelas interaÃ§Ãµes do usuÃ¡rio com modelos jÃ¡ ajustados (ChatGPT, LLama 2, Open-Assistant, etc).\nO conjunto de dados estÃ¡ disponÃ­vel em portuguÃªs (tem a versÃ£o em InglÃªs que ainda nÃ£o tratei). Mas vocÃª pode baixar do \nrepositÃ³rio de Nicholas Kluge CorrÃªa tanto a versÃ£o em PortuguÃªs e \na versÃ£o emâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format.","url":"https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format","creator_name":"EDDY GIUSEPE CHIRINOS ISIDRO, PhD","creator_url":"https://huggingface.co/EddyGiusepe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Portuguese","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ContractNLIPermissibleDevelopmentOfSimilarInformationLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLIPermissibleDevelopmentOfSimilarInformationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may independently develop information similar to Confidential Information.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLIPermissibleDevelopmentOfSimilarInformationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLIPermissibleDevelopmentOfSimilarInformationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"dedeucebench-results","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDedeuceBench Results Repository\n\t\n\nThis dataset stores submitted runs and an aggregated leaderboard for DedeuceBench. A run consists of a raw results.jsonl file produced by the CLI and a one-line CSV produced by the aggregator. The top-level leaderboard.csv is the append-only global table.\n\n\t\n\t\t\n\t\tFile Layout\n\t\n\n\nleaderboard.csv â€” global leaderboard table with one row per (model, subset) entry.\nruns/YYYY-MM-DD/<route>.<subset>/ â€” per-run artifacts:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/comfortably-dumb/dedeucebench-results.","url":"https://huggingface.co/datasets/comfortably-dumb/dedeucebench-results","creator_name":"Vedansh Sharma","creator_url":"https://huggingface.co/comfortably-dumb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["cc0-1.0","< 1K","csv","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"NanoTouche2020Retrieval","keyword":"mteb","description":"\n  NanoTouche2020Retrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoTouche2020 is a smaller subset of TouchÃ© Task 1: Argument Retrieval for Controversial Questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nAcademic\n\n\nReferencehttps://webis.de/events/touche-20/shared-task-1.html\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NanoTouche2020Retrieval\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoTouche2020Retrieval.","url":"https://huggingface.co/datasets/mteb/NanoTouche2020Retrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","human-annotated","monolingual","mteb/touche2020"],"keywords_longer_than_N":true},
	{"name":"calibrated_grab","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 597,\n    \"total_tasks\": 1,\n    \"total_videos\": 1,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hrhraj/calibrated_grab.","url":"https://huggingface.co/datasets/hrhraj/calibrated_grab","creator_name":"Raj S","creator_url":"https://huggingface.co/hrhraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-03092024-k007-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-03092024-k007-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"health insurance information in German\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-03092024-k007-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-k007-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-03092024-k007-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","German","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-15062024-atex-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-15062024-atex-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-15062024-atex-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-15062024-atex-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-15062024-atex-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"dutch-legal-c-1280-24","keyword":"mteb","description":"\n\t\n\t\t\n\t\tdutch-legal-c-1280-24 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Legal document search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the dutch-legal-c-1280-24 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/dutch-legal-c-1280-24.","url":"https://huggingface.co/datasets/fine-tuned/dutch-legal-c-1280-24","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"KokushiMD-10","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tKokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations\n\t\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nKokushiMD-10 is the first comprehensive multimodal benchmark constructed from ten Japanese national healthcare licensing examinations. This dataset addresses critical gaps in existing medical AI evaluation by providing a linguistically grounded, multimodal, and multi-profession assessment framework for large language models (LLMs) inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/humanalysis-square/KokushiMD-10.","url":"https://huggingface.co/datasets/humanalysis-square/KokushiMD-10","creator_name":"Tako AI","creator_url":"https://huggingface.co/humanalysis-square","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Japanese","English","mit"],"keywords_longer_than_N":true},
	{"name":"SocialMaze","keyword":"benchmark","description":"\nâš ï¸ Notice: This dataset is no longer maintained under this repository. It has been officially migrated to the MBZUAI organization for ongoing development and updates.ðŸ‘‰ Access the latest version here: MBZUAI/SocialMaze\n\n\n\t\n\t\t\n\t\tSocialMaze Benchmark\n\t\n\nThis dataset is a component of the SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models project. It specifically features the Hidden Role Deduction task, which we consider one of the most challenging scenarios forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xzx34/SocialMaze.","url":"https://huggingface.co/datasets/xzx34/SocialMaze","creator_name":"Zixiang Xu","creator_url":"https://huggingface.co/xzx34","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-14052024-afuz-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-14052024-afuz-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"genre-specific search for fantasy novels\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-14052024-afuz-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-14052024-afuz-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-14052024-afuz-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-code-5222024-i8af-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-code-5222024-i8af-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"issue tracking search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-code-5222024-i8af-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-5222024-i8af-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-code-5222024-i8af-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ClimateFEVER-256-24-gpt-4o-2024-05-13-918964","keyword":"mteb","description":"\n\t\n\t\t\n\t\tClimateFEVER-256-24-gpt-4o-2024-05-13-918964 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic dataset search for climate change claims\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ClimateFEVER-256-24-gpt-4o-2024-05-13-918964 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ClimateFEVER-256-24-gpt-4o-2024-05-13-918964.","url":"https://huggingface.co/datasets/fine-tuned/ClimateFEVER-256-24-gpt-4o-2024-05-13-918964","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-zh-CMedQAv2","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-zh-CMedQAv2 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-zh-CMedQAv2 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-zh-CMedQAv2.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-zh-CMedQAv2","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"llmsql-benchmark-lm-evaluation-harness","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLLMSQL Benchmark (lm evaluation harness)\n\t\n\nThis benchmark is designed to evaluate text-to-SQL models. For usage of this benchmark see https://github.com/LLMSQL/llmsql-benchmark.\nThis repository contains a lm-eval harness ready version of the LLMSQL benchmark: LLMSQL on Hugging Face.  \nThis version will be used fro Language Model Evaluation Harness library.\n\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you use this benchmark, please cite:\n@inproceedings{llmsql_bench,\n  title={LLMSQL: Upgrading WikiSQLâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmsql-bench/llmsql-benchmark-lm-evaluation-harness.","url":"https://huggingface.co/datasets/llmsql-bench/llmsql-benchmark-lm-evaluation-harness","creator_name":"LLMSQL","creator_url":"https://huggingface.co/llmsql-bench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"avian-perception-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for VocSim - Avian Perception Alignment\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is used in the VocSim benchmark paper, specifically designed to evaluate how well neural audio embeddings align with biological perceptual judgments of similarity. It utilizes data from Zandberg et al. (2024), which includes recordings of zebra finch (Taeniopygia guttata) song syllables and results from behavioral experiments (probe and triplet tasks) measuring the birds' perception ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anonymous-submission000/avian-perception-benchmark.","url":"https://huggingface.co/datasets/anonymous-submission000/avian-perception-benchmark","creator_name":"Anonymous","creator_url":"https://huggingface.co/anonymous-submission000","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","< 1K","parquet","Audio","Text"],"keywords_longer_than_N":true},
	{"name":"R2MEDBiologyRetrieval","keyword":"mteb","description":"\n  R2MEDBiologyRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nBiology retrieval dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical\n\n\nReference\nhttps://huggingface.co/datasets/R2MED/Biology\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"R2MEDBiologyRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nTo learn moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/R2MEDBiologyRetrieval.","url":"https://huggingface.co/datasets/mteb/R2MEDBiologyRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","LM-generated and reviewed","monolingual","R2MED/Biology"],"keywords_longer_than_N":true},
	{"name":"OpenS2V-5M","keyword":"benchmark","description":"\n\n\n OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\n\n If you like our project, please give us a star â­ on GitHub for the latest update.  \n\n\n\n\t\n\t\t\n\t\tâœ¨ Summary\n\t\n\nWe create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality \n720P subject-text-video triples. To ensure subject-information diversity in our dataset by, we (1) segmenting subjects \nand building pairing information viaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/OpenS2V-5M.","url":"https://huggingface.co/datasets/BestWishYsh/OpenS2V-5M","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M<n<10M","arxiv:2505.20292"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-31_7_2024-kubz-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-31_7_2024-kubz-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-31_7_2024-kubz-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-31_7_2024-kubz-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-31_7_2024-kubz-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"AlloProfClusteringS2S","keyword":"mteb","description":"\n  AlloProfClusteringS2S\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of document titles from Allo Prof dataset. Clustering of 10 sets on the document topic.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nEncyclopaedic, Written\n\n\nReferencehttps://huggingface.co/datasets/lyon-nlp/alloprof\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"AlloProfClusteringS2S\"])\nevaluatorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AlloProfClusteringS2S.","url":"https://huggingface.co/datasets/mteb/AlloProfClusteringS2S","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","human-annotated","monolingual","French","mit"],"keywords_longer_than_N":true},
	{"name":"Imagen4_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Imagen 4 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 70k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Imagen 4 (imagen-4.0-ultra-generate-exp-05-20) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Imagen4_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Imagen4_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Imagen4_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Imagen 4 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 70k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Imagen 4 (imagen-4.0-ultra-generate-exp-05-20) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Imagen4_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Imagen4_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-51550","keyword":"mteb","description":"\n\t\n\t\t\n\t\targuana-c-256-24-gpt-4o-2024-05-13-51550 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-51550 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-51550.","url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-51550","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NYSJudicialEthicsLegalBenchClassification","keyword":"mteb","description":"\n  NYSJudicialEthicsLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nAnswer questions on judicial ethics from the New York State Unified Court System Advisory Committee.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NYSJudicialEthicsLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/NYSJudicialEthicsLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","mit"],"keywords_longer_than_N":true},
	{"name":"MalayalamNewsClassification","keyword":"mteb","description":"\n  MalayalamNewsClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nA Malayalam dataset for 3-class classification of Malayalam news articles\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://github.com/goru001/nlp-for-malyalam\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"MalayalamNewsClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MalayalamNewsClassification.","url":"https://huggingface.co/datasets/mteb/MalayalamNewsClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","Malayalam"],"keywords_longer_than_N":true},
	{"name":"SCDBPAuditsLegalBenchClassification","keyword":"mteb","description":"\n  SCDBPAuditsLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer  performs any type of audit, or reserves the right to audit?'\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SCDBPAuditsLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/SCDBPAuditsLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"test-run","keyword":"mteb","description":"\n\t\n\t\t\n\t\ttest-run Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research for argumentation data\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the test-run model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/test-run.","url":"https://huggingface.co/datasets/fine-tuned/test-run","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"fang-2023-biogen-adme","keyword":"benchmark","description":"\n\t\n\t\t\n\t\n\t\n\t\tBiogen ADME dataset (public data)\n\t\n\nData from Fang et al., Prospective Validation of Machine Learning Algorithms for Absorption, Distribution, Metabolism, and Excretion Prediction: An Industrial Perspective, available from the GitHub repositiory. We used schemist (which in turn uses RDKit)\nto add molecuar weight, Murcko scaffold, Crippen cLogP, and topological surface area, and to generate scaffold splits.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nFrom the original README:\n\nTo benefit theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/fang-2023-biogen-adme.","url":"https://huggingface.co/datasets/scbirlab/fang-2023-biogen-adme","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","text2text-generation","translation","zero-shot-classification","mit"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_4","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3534,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_4.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_4","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-code-stackoverflow","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-code-stackoverflow Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"code snippet search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-code-stackoverflow model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-code-stackoverflow.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-code-stackoverflow","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADNonDisparagementLegalBenchClassification","keyword":"mteb","description":"\n  CUADNonDisparagementLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause requires a party not to disparage the counterparty.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mtebâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADNonDisparagementLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADNonDisparagementLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"WirelessMathBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tWirelessMathBench\n\t\n\nWirelessMathBench is a benchmark designed to test the mathematical reasoning and symbolic problem-solving capabilities of large language models (LLMs) in wireless communications. It contains expert-level, LaTeX-formatted questions spanning key topics such as:\n\nMultiple Input Multiple Output (MIMO)\nReconfigurable Intelligent Surfaces (RIS)\nIntegrated Sensing and Communications (ISAC)\nUAV-enabled networks\nChannel estimation and signal processing\n\nEach question isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XINLI1997/WirelessMathBench.","url":"https://huggingface.co/datasets/XINLI1997/WirelessMathBench","creator_name":"XinLi","creator_url":"https://huggingface.co/XINLI1997","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"scidocs-c-64-24-gpt-4o-2024-05-133652","keyword":"mteb","description":"\n\t\n\t\t\n\t\tscidocs-c-64-24-gpt-4o-2024-05-133652 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general search for paper products\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the scidocs-c-64-24-gpt-4o-2024-05-133652 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/scidocs-c-64-24-gpt-4o-2024-05-133652.","url":"https://huggingface.co/datasets/fine-tuned/scidocs-c-64-24-gpt-4o-2024-05-133652","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"tatoeba-bitext-mining","keyword":"mteb","description":"\n  Tatoeba\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\n1,000 English-aligned sentence pairs for each language based on the Tatoeba corpus\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://github.com/facebookresearch/LASER/tree/main/data/tatoeba/v1\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"Tatoeba\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/tatoeba-bitext-mining.","url":"https://huggingface.co/datasets/mteb/tatoeba-bitext-mining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","human-annotated","multilingual","Afrikaans","Amharic"],"keywords_longer_than_N":true},
	{"name":"NanoArguAnaRetrieval","keyword":"mteb","description":"\n  NanoArguAnaRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoArguAna is a smaller subset of ArguAna, a dataset for argument retrieval in debate contexts.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical, Written\n\n\nReferencehttp://argumentation.bplaced.net/arguana/data\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"NanoArguAnaRetrieval\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoArguAnaRetrieval.","url":"https://huggingface.co/datasets/mteb/NanoArguAnaRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","expert-annotated","monolingual","mteb/arguana","English"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-1362024-2wos-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-v1_5-1362024-2wos-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Information Retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-v1_5-1362024-2wos-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-1362024-2wos-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-1362024-2wos-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ReFACT","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations\n\t\n\n\n\n\n\nReFACT (Reddit False And Correct Texts) is a benchmark dataset for evaluating how Large Language Models detect, localize, and correct scientific confabulation.\n\n\t\t\n\t\tDataset Summary\n\t\n\nScientific confabulation represents a critical challenge in LLM deployment - the generation of fluent, plausible, and contextually appropriate text that is nonetheless factually incorrect. Unlike obviousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ddz5431/ReFACT.","url":"https://huggingface.co/datasets/Ddz5431/ReFACT","creator_name":"Yindong Wang","creator_url":"https://huggingface.co/Ddz5431","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MAUDLegalBenchClassification","keyword":"mteb","description":"\n  MAUDLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the MAUD dataset, which consists of over 47,000 labels across 152 merger agreements annotated to identify 92 questions in each agreement used by the 2021 American Bar Association (ABA) Public Target Deal Points Study. Each dataset is formatted as a series of multiple-choice questions, where given a segment of the merger agreement and a Deal Point question, the model is toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MAUDLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/MAUDLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5222024-hkde-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5222024-hkde-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"code repository search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5222024-hkde-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5222024-hkde-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5222024-hkde-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Multi-Opthalingua","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tCite\n\t\n\nAccepted to AAAI 2025 (https://openreview.net/group?id=AAAI.org/2025/Conference#tab-recent-activity)\nMulti-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs:\n@misc{restrepo2024multiophthalinguamultilingualbenchmarkassessing,\n      title={Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs}, \n      author={David Restrepo and Chenwei Wu and Zhengxu Tang and Zitao Shuai and Thaoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AAAIBenchmark/Multi-Opthalingua.","url":"https://huggingface.co/datasets/AAAIBenchmark/Multi-Opthalingua","creator_name":"AAAIBenchmark","creator_url":"https://huggingface.co/AAAIBenchmark","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Hindi","Chinese","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"CC-Bench-trajectories","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tCC-Bench Trajectories Overview\n\t\n\nTo evaluate GLM-4.6's agentic coding capabilities in real-world scenarios, we developed CC-Bench-V1.1 using Claude Code as the agentic coding testbed. Building on CC-Bench-V1.0, we added 22 more challenging coding tasks and conducted comprehensive evaluations against Claude-Sonnet-4, GLM-4.5, Kimi-K2-0905, and DeepSeek-V3.1-Terminus. The benchmark comprises 74 coding tasks spanning frontend development, tool development, data analysis, testing, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zai-org/CC-Bench-trajectories.","url":"https://huggingface.co/datasets/zai-org/CC-Bench-trajectories","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-08062024-z8ik-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-08062024-z8ik-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"professional development and job seeking\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-08062024-z8ik-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-08062024-z8ik-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-08062024-z8ik-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"llm_physical_safety_benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLLM Physical Safety Benchmark in Drone Control\n\t\n\nThis benchmark consists of four datasets designed to evaluate the performance of Large Language Models (LLMs) in controlling drones and their vulnerability to physical attacks. The datasets are categorized into different types of attacks:\n\nDeliberate Attack: Contains 280 samples that evaluate the LLM's resistance to malicious use, testing its ability to recognize and reject commands intended to cause harm. Subcategories include Directâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TrustSafeAI/llm_physical_safety_benchmark.","url":"https://huggingface.co/datasets/TrustSafeAI/llm_physical_safety_benchmark","creator_name":"TrustSafeAI","creator_url":"https://huggingface.co/TrustSafeAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5102024-h7o7-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5102024-h7o7-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"professional matchmaking services\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5102024-h7o7-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5102024-h7o7-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5102024-h7o7-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-es-472024-aqk1-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-es-472024-aqk1-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-es-472024-aqk1-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-es-472024-aqk1-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-es-472024-aqk1-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NQ-256-24-gpt-4o-2024-05-13-803084","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNQ-256-24-gpt-4o-2024-05-13-803084 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"question answering dataset search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NQ-256-24-gpt-4o-2024-05-13-803084 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NQ-256-24-gpt-4o-2024-05-13-803084.","url":"https://huggingface.co/datasets/fine-tuned/NQ-256-24-gpt-4o-2024-05-13-803084","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PSC","keyword":"mteb","description":"\n  PSC\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nPolish Summaries Corpus\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nNews, Written\n\n\nReference\nhttp://www.lrec-conf.org/proceedings/lrec2014/pdf/1211_Paper.pdf\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"PSC\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)\nevaluator.run(model)\n\n\nTo learn more about how toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/PSC.","url":"https://huggingface.co/datasets/mteb/PSC","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","derived","monolingual","Polish"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-256-24-gpt-4o-2024-05-13-141246","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-256-24-gpt-4o-2024-05-13-141246 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-256-24-gpt-4o-2024-05-13-141246 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-141246.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-141246","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"JGLUE","keyword":"nli","description":"JGLUE, Japanese General Language Understanding Evaluation, is built to measure the general NLU ability in Japanese. JGLUE has been constructed from scratch without translation. We hope that JGLUE will facilitate NLU research in Japanese.","url":"https://huggingface.co/datasets/llm-book/JGLUE","creator_name":"å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«å…¥é–€","creator_url":"https://huggingface.co/llm-book","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","question-answering","sentence-similarity","text-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"Arena-Write","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸ“š Arena-Write Dataset\n\t\n\nArena-Write is a small-scale benchmark of 100 user writing tasks, designed to evaluate long-form generation models in realistic scenarios. Each task covers diverse formats such as social posts, essays, and reports, with many requiring outputs over 2,000 words.\nProject page: https://huggingface.co/THU-KEG/\n\n\t\n\t\t\n\t\tðŸ“„ Data Format\n\t\n\nEach data sample is a JSON object with the following fields:\n{\n  \"idx\": 1,\n  \"question\": \"Write a social media post about Lei Fengâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THU-KEG/Arena-Write.","url":"https://huggingface.co/datasets/THU-KEG/Arena-Write","creator_name":"Knowledge Engineer Group @ Tsinghua University","creator_url":"https://huggingface.co/THU-KEG","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Health_Benchmarks","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLLM Health Benchmarks Dataset by Yesil Science\n\t\n\nThe LLM Health Benchmarks Dataset is a specialized resource for evaluating large language models (LLMs) in different medical specialties. It provides structured question-answer pairs designed to test the performance of AI models in understanding and generating domain-specific knowledge.\n\n\n\t\n\t\t\n\t\tPrimary Purpose\n\t\n\nThis dataset is built to:\n\nBenchmark LLMs in medical specialties and subfields.\nAssess the accuracy and contextualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yesilhealth/Health_Benchmarks.","url":"https://huggingface.co/datasets/yesilhealth/Health_Benchmarks","creator_name":"Yesil Health AI","creator_url":"https://huggingface.co/yesilhealth","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Arabic-gsm8k-v2","keyword":"gsm8k","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nArabic GSM8K is an Arabic translation of the GSM8K (Grade School Math 8K) dataset, which contains high-quality linguistically diverse grade school math word problems. The original dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning, and this Arabic version aims to extend these capabilities to Arabic language models and applications.\nThe dataset maintains the same characteristics as theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic-gsm8k-v2.","url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic-gsm8k-v2","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Arabic","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-programmers","keyword":"mteb","description":"\n  CQADupstackProgrammersRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written, Non-fiction\n\n\nReferencehttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-programmers.","url":"https://huggingface.co/datasets/mteb/cqadupstack-programmers","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-code-922024-zgwo-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-code-922024-zgwo-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"web development\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-code-922024-zgwo-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-code-922024-zgwo-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-code-922024-zgwo-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NorwegianCourtsBitextMining","keyword":"mteb","description":"\n  NorwegianCourtsBitextMining\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNynorsk and BokmÃ¥l parallel corpus from Norwegian courts. Norwegian courts have two standardised written languages. BokmÃ¥l is a variant closer to Danish, while Nynorsk was created to resemble regional dialects of Norwegian.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://opus.nlpl.eu/index.php\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NorwegianCourtsBitextMining.","url":"https://huggingface.co/datasets/mteb/NorwegianCourtsBitextMining","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","human-annotated","monolingual","Norwegian Nynorsk","Norwegian BokmÃ¥l"],"keywords_longer_than_N":true},
	{"name":"DatasetResearch","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Research\n\t\n\n\n\n\nThis dataset is part of the DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery research project, presented in the paper DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery.\nAbstract:\nThe rapid advancement of large language models has fundamentally shifted the bottleneck in AI development from computational power to data availability-with countless valuable datasets remaining hidden across specializedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GAIR/DatasetResearch.","url":"https://huggingface.co/datasets/GAIR/DatasetResearch","creator_name":"SII - GAIR","creator_url":"https://huggingface.co/GAIR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","English","Chinese","multilingual","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"self-monitor","keyword":"alignment","description":"\n\t\n\t\t\n\t\tSelf-Monitor Dataset\n\t\n\nThis dataset contains supervised fine-tuning (SFT) data used in the research paper \"Mitigating Deceptive Alignment via Self-Monitoring\" (arXiv:2505.18807).\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe self-monitor dataset is designed to train language models to develop self-monitoring capabilities that can help mitigate deceptive alignment behaviors. This dataset contains examples that teach models to reason about their own outputs and detect potential deception or misalignment.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/self-monitor.","url":"https://huggingface.co/datasets/PKU-Alignment/self-monitor","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"so101_pen_mug_10_5","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 10,\n    \"total_frames\": 3156,\n    \"total_tasks\": 1,\n    \"total_videos\": 20,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:10\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bjb7/so101_pen_mug_10_5.","url":"https://huggingface.co/datasets/bjb7/so101_pen_mug_10_5","creator_name":"Brian Blankenau","creator_url":"https://huggingface.co/bjb7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"HeadlineClassification","keyword":"mteb","description":"\n  HeadlineClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nHeadline rubric classification based on the paraphraser plus dataset.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://aclanthology.org/2020.ngt-1.6/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"HeadlineClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HeadlineClassification.","url":"https://huggingface.co/datasets/mteb/HeadlineClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","Russian"],"keywords_longer_than_N":true},
	{"name":"LAMDA","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLAMDA: A Longitudinal Android Malware Dataset for Drift Analysis\n\t\n\nThis dataset contains a longitudinal benchmark for Android malware detection designed to analyze and evaluate concept drift in machine learning models. It includes labeled and feature-engineered Android APK data from 2013 to 2025 (excluding 2015), with over 1 million samples collected from real-world sources.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nLAMDA is the largest and most temporally diverse Android malware dataset to date. Itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/IQSeC-Lab/LAMDA.","url":"https://huggingface.co/datasets/IQSeC-Lab/LAMDA","creator_name":"Intelligent and Quantum Secure Advanced Cyber Defense Research (IQSeC) Lab","creator_url":"https://huggingface.co/IQSeC-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-548936","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-548936 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-548936 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-548936.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-548936","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"mcx_tracker_test1","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"mcx\",\n    \"total_episodes\": 15,\n    \"total_frames\": 8940,\n    \"total_tasks\": 1,\n    \"total_videos\": 15,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:15\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/relaxedandcalm/mcx_tracker_test1.","url":"https://huggingface.co/datasets/relaxedandcalm/mcx_tracker_test1","creator_name":"Arseniy","creator_url":"https://huggingface.co/relaxedandcalm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ProgressGym-HistText","keyword":"alignment","description":"*Huggingface dataset preview for 19th, 20th, and 21st centuries is not available due to lack of support for array types. Instead, consider downloading those files for manual inspection, or see the Data Samples section below for more examples.\n\n\t\n\t\t\n\t\n\t\n\t\tProgressGym-HistText\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThe ProgressGym Framework\n\t\n\n\nProgressGym-HistText is part of the ProgressGym framework for research and experimentation on progress alignment - the emulation of moral progress in AIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/ProgressGym-HistText.","url":"https://huggingface.co/datasets/PKU-Alignment/ProgressGym-HistText","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","pile-of-law/pile-of-law","EEBO","Library of Congress","Project Gutenberg (Standardized Project Gutenberg Corpus)"],"keywords_longer_than_N":true},
	{"name":"CEDRClassification","keyword":"mteb","description":"\n  CEDRClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClassification of sentences by emotions, labeled into 5 categories (joy, sadness, surprise, fear, and anger).\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nWeb, Social, Blog, Written\n\nReference\nhttps://www.sciencedirect.com/science/article/pii/S1877050921013247\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CEDRClassification.","url":"https://huggingface.co/datasets/mteb/CEDRClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multi-label-classification","sentiment-analysis","sentiment-scoring","sentiment-classification"],"keywords_longer_than_N":true},
	{"name":"MixBench25-visual","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench25-visual.","url":"https://huggingface.co/datasets/mixed-modality-search/MixBench25-visual","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"cqadupstack-webmasters","keyword":"mteb","description":"\n  CQADupstackWebmastersRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCQADupStack: A Benchmark Data Set for Community Question-Answering Research\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten, Web\n\n\nReference\nhttp://nlp.cis.unimelb.edu.au/resources/cqadupstack/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"CQADupstackWebmastersRetrieval\"])\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/cqadupstack-webmasters.","url":"https://huggingface.co/datasets/mteb/cqadupstack-webmasters","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-5272024-ou25-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-5272024-ou25-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Sentiment Analysis and Emotional Nuances\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-5272024-ou25-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5272024-ou25-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5272024-ou25-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"t2i_safety_dataset","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tT2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation\n\t\n\nThis dataset, T2ISafety, is a comprehensive safety benchmark designed to evaluate Text-to-Image (T2I) models across three key domains: toxicity, fairness, and bias. It provides a detailed hierarchy of 12 tasks and 44 categories, built from meticulously collected 70K prompts. Based on this taxonomy and prompt set, T2ISafety includes 68K manually annotated images, serving as a robust resource forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenSafetyLab/t2i_safety_dataset.","url":"https://huggingface.co/datasets/OpenSafetyLab/t2i_safety_dataset","creator_name":"OpenSafetyLab","creator_url":"https://huggingface.co/OpenSafetyLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"OpenS2V-Eval","keyword":"benchmark","description":"\n\n\n OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\n\n If you like our project, please give us a star â­ on GitHub for the latest update.  \n\n\n\n\t\n\t\t\n\t\tâœ¨ Summary\n\t\n\nOpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, \nto accurately align human preferences with S2V benchmarks, we propose three automatic metrics: NexusScore, NaturalScore, GmeScore\nto separately quantifyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/OpenS2V-Eval.","url":"https://huggingface.co/datasets/BestWishYsh/OpenS2V-Eval","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M<n<10M","Video"],"keywords_longer_than_N":true},
	{"name":"text-2-image-Rich-Human-Feedback","keyword":"alignment","description":"\n\n\n\nBuilding upon Google's research Rich Human Feedback for Text-to-Image Generation we have collected over 1.5 million responses from 152'684 individual humans using Rapidata via the Python API. Collection took roughly 5 days. \nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nWe asked humans to evaluate AI-generated images in style, coherence and prompt alignment. For images that contained flaws, participants wereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback.","url":"https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","text-classification","image-classification","image-to-text","image-segmentation"],"keywords_longer_than_N":true},
	{"name":"openmath-nondual","keyword":"alignment","description":"\n\t\n\t\t\n\t\tnondual_openmath_final\n\t\n\nA non-dual reformulation of the unsloth/OpenMathReasoning-mini dataset.All assistant solutions have been rewritten into impersonal, non-dual language using OpenAI models, and finalized so that the dataset no longer contains duplicate *_nondual fields.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSource: unsloth/OpenMathReasoning-mini  \nFormat: JSONL, each line is a dictionary with the following fields:\n\n\n\t\n\t\t\nField\nDescription\n\n\n\t\t\nproblem\nMath problem statement (rewrittenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/marciodiaz/openmath-nondual.","url":"https://huggingface.co/datasets/marciodiaz/openmath-nondual","creator_name":"Marcio Diaz","creator_url":"https://huggingface.co/marciodiaz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ContractNLIPermissibleCopyLegalBenchClassification","keyword":"mteb","description":"\n  ContractNLIPermissibleCopyLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may create a copy of some Confidential Information in some circumstances.\n\n\t\n\t\t\n\n\n\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/ContractNLIPermissibleCopyLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/ContractNLIPermissibleCopyLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MindCube","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tMindCube: Spatial Mental Modeling from Limited Views\n\t\n\nMindCube is a novel benchmark designed to evaluate how well Vision Language Models (VLMs) can form robust spatial mental models from limited visual views. It comprises 21,154 questions across 3,268 images, assessing capabilities such as cognitive mapping (representing positions), perspective-taking (orientations), and mental simulation (dynamics for \"what-if\" movements). The dataset aims to expose critical gaps in existing VLMs'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MLL-Lab/MindCube.","url":"https://huggingface.co/datasets/MLL-Lab/MindCube","creator_name":"MLL Lab","creator_url":"https://huggingface.co/MLL-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-veo2","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Google DeepMind Veo2 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Google DeepMind Veo2 video generation model on our benchmark. The up to dateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-05062024-zvoa-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-05062024-zvoa-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"software development\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-05062024-zvoa-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05062024-zvoa-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05062024-zvoa-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"gsm8k_chinese","keyword":"gsm8k","description":"swulling/gsm8k_chinese dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/swulling/gsm8k_chinese","creator_name":"Alex Yang","creator_url":"https://huggingface.co/swulling","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","gsm8k","Chinese","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ConceptVectors","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tConceptVectors\n\t\n\nðŸš€The first-ever parametric LLM Unlearning Benchmark!\nWe find current unlearning methods only modify modelâ€™s behavior without truly erasing encoded knowledge in parameters. For this, we present ConceptVectors Benchmark, with each vector strongly tied to a specific concept.\nThe ConceptVectors Benchmark for the paper \"Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces\".\n\n\t\n\t\t\n\t\tLinks\n\t\n\n\nPaper: Intrinsic Test of Unlearning Using Parametric Knowledgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YihuaiHong/ConceptVectors.","url":"https://huggingface.co/datasets/YihuaiHong/ConceptVectors","creator_name":"YihuaiHong","creator_url":"https://huggingface.co/YihuaiHong","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10M<n<100M","arxiv:2406.11614"],"keywords_longer_than_N":true},
	{"name":"SinhalaNewsSourceClassification","keyword":"mteb","description":"\n  SinhalaNewsSourceClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis dataset contains Sinhala news headlines extracted from 9 news sources (websites) (Sri Lanka Army, Dinamina, GossipLanka, Hiru, ITN, Lankapuwath, NewsLK, Newsfirst, World Socialist Web Site-Sinhala).\n\n\t\n\t\t\n\n\n\n\n\t\tTask category\nt2c\n\n\nDomains\nNews, Written\n\n\nReference\nhttps://huggingface.co/datasets/NLPC-UOM/Sinhala-News-Source-classification\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SinhalaNewsSourceClassification.","url":"https://huggingface.co/datasets/mteb/SinhalaNewsSourceClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","NLPC-UOM/Sinhala-News-Source-classification"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-05062024-igr1-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\n\t\n\t\tjinaai_jina-embeddings-v2-base-en-05062024-igr1-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-05062024-igr1-webapp model.\n\n\t\n\t\t\n\t\n\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05062024-igr1-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05062024-igr1-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"CodeFeedbackST","keyword":"mteb","description":"\n  CodeFeedbackST\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe dataset is a collection of user queries and assistant responses. The task is to retrieve the most relevant response for a given query.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nProgramming, Written\nReference\nhttps://arxiv.org/abs/2407.02883\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"CodeFeedbackST\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CodeFeedbackST.","url":"https://huggingface.co/datasets/mteb/CodeFeedbackST","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","derived","monolingual","English","mit"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-623812","keyword":"mteb","description":"\n\t\n\t\t\n\t\targuana-c-256-24-gpt-4o-2024-05-13-623812 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-623812 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-623812.","url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-623812","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"rank1-run-files","keyword":"benchmark","description":"\n\t\n\t\t\n\t\trank1-run-files: Pre-computed Run Files for Reranking Evaluation\n\t\n\nðŸ“„ Paper | ðŸš€ GitHub Repository\nThis dataset contains pre-computed run files used by the rank1 family of models on various retrieval benchmarks. These files are what were used for top-k rereranking and also include the re-annotated DL19 qrels. These files are needed to download to reproduce our results.\n\n\t\n\t\t\n\t\n\t\n\t\tBenchmarks Included\n\t\n\nThe dataset includes run files for the following benchmarks:\n\nBEIR (multipleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/rank1-run-files.","url":"https://huggingface.co/datasets/jhu-clsp/rank1-run-files","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","arxiv:2502.18418","ðŸ‡ºðŸ‡¸ Region: US","reranker"],"keywords_longer_than_N":true},
	{"name":"PathoROB-camelyon","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tPathoROB\n\t\n\nPreprint | Code | Licenses | Cite\nPathoROB is a benchmark for the robustness of pathology foundation models (FMs) to non-biological medical center differences.\n\n\nPathoROB contains four datasets covering 28 biological classes from 34 medical centers and three metrics:\n\nRobustness Index: Measures the ability of an FM to capture biological features while ignoring\nnon-biological features.\nAverage Performance Drop (APD): Measures the impact of non-biological features on theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bifold-pathomics/PathoROB-camelyon.","url":"https://huggingface.co/datasets/bifold-pathomics/PathoROB-camelyon","creator_name":"BIFOLD Pathomics","creator_url":"https://huggingface.co/bifold-pathomics","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-feature-extraction","image-classification","English","cc0-1.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CUADIPOwnershipAssignmentLegalBenchClassification","keyword":"mteb","description":"\n  CUADIPOwnershipAssignmentLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies that intellectual property created by one party become the property of the counterparty, either per the terms of the contract or upon the occurrence of certain events.\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADIPOwnershipAssignmentLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADIPOwnershipAssignmentLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"SwissJudgementClassification","keyword":"mteb","description":"\n  SwissJudgementClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual, diachronic dataset of Swiss Federal Supreme Court cases annotated with the respective binarized judgment outcome (approval/dismissal)\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomainsLegal, Written\n\n\nReference\nhttps://aclanthology.org/2021.nllp-1.3/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SwissJudgementClassification.","url":"https://huggingface.co/datasets/mteb/SwissJudgementClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","multilingual","rcds/swiss_judgment_prediction","German"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-21052024-6vz1-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-21052024-6vz1-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-21052024-6vz1-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-6vz1-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-6vz1-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ClimateFEVER-256-24-gpt-4o-2024-05-13-572217","keyword":"mteb","description":"\n\t\n\t\t\n\t\tClimateFEVER-256-24-gpt-4o-2024-05-13-572217 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic dataset search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ClimateFEVER-256-24-gpt-4o-2024-05-13-572217 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ClimateFEVER-256-24-gpt-4o-2024-05-13-572217.","url":"https://huggingface.co/datasets/fine-tuned/ClimateFEVER-256-24-gpt-4o-2024-05-13-572217","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TreeBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTraceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology\n\t\n\nThis repository contains the TreeBench dataset, a diagnostic benchmark for visual grounded reasoning, introduced in the paper Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTL; DR: We propose TreeBench, the first benchmark specially designed for evaluating \"thinking with images\" capabilities with traceable visual evidence, and TreeVGR, the current state-of-the-artâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeBench.","url":"https://huggingface.co/datasets/HaochenWang/TreeBench","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"bird_train","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tCSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning\n\t\n\nThis repository contains the datasets used and/or generated in the paper CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning.\nCode Repository: https://github.com/CycloneBoy/csc_sql\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nLarge language models (LLMs) have demonstrated strong capabilities in translating natural language questions about relational databases into SQL queries. In particularâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cycloneboy/bird_train.","url":"https://huggingface.co/datasets/cycloneboy/bird_train","creator_name":"cycloneboy","creator_url":"https://huggingface.co/cycloneboy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","arxiv:2505.13271","arxiv:2507.22478","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-runway-alpha","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Runway Alpha Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~30'000 human annotations were collected to evaluate Runway's Alpha video generation model on our benchmark. The up to date benchmark canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"SCDDAccountabilityLegalBenchClassification","keyword":"mteb","description":"\n  SCDDAccountabilityLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer maintains internal accountability standards and procedures for employees or contractors failing to meet company standards regarding slavery and trafficking?'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SCDDAccountabilityLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/SCDDAccountabilityLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-05062024-x987-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-05062024-x987-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"debate\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-05062024-x987-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-05062024-x987-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-05062024-x987-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-922024-puz9-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-922024-puz9-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"information retrieval system for academic research papers\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-922024-puz9-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-922024-puz9-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-922024-puz9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Material_Selection_Eval","keyword":"benchmark","description":"A benchmark designed to facilitate evaluation and modify the behavior of a foundation model through different existing techniques in the context of material selection for conceptual design.\nThe data is collected by conducting a survey of experts in the field of material selection. The same questions mentioned in keyquestions.csv are asked to experts.\nThis can be used to evaluate a Language model performance and its spread compared to a human evaluation.\nTo get into a more detailed explanationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cmudrc/Material_Selection_Eval.","url":"https://huggingface.co/datasets/cmudrc/Material_Selection_Eval","creator_name":"Design Research Collective","creator_url":"https://huggingface.co/cmudrc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","text-retrieval","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"EchoX-Dialougues","keyword":"alignment","description":"\n\n  EchoX-Dialogues: Training Data for EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs\n\n\n\n\n  ðŸˆâ€â¬› GithubÂ ï½œÂ ðŸ“ƒ PaperÂ ï½œÂ ðŸš€ SpaceÂ \n\n\n  ðŸ§  EchoX-8BÂ ï½œÂ ðŸ§  EchoX-3BÂ ï½œÂ ðŸ“¦ EchoX-Dialogues-PlusÂ \n\n\nEchoX-Dialogues provides the primary speech dialogue data used to train EchoX, restricted to S2T (speech â†’ text) in this repository.\nAll input speech is synthetic; text is derived from public sources with multi-stage cleaning and rewriting. Most turns include asr /â€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/EchoX-Dialougues.","url":"https://huggingface.co/datasets/FreedomIntelligence/EchoX-Dialougues","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","question-answering","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-512-192-gpt-4o-2024-05-13-985263","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFiQA2018-512-192-gpt-4o-2024-05-13-985263 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial news and analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-512-192-gpt-4o-2024-05-13-985263 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-512-192-gpt-4o-2024-05-13-985263.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-512-192-gpt-4o-2024-05-13-985263","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"calibrated_model_test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 1,\n    \"total_frames\": 597,\n    \"total_tasks\": 1,\n    \"total_videos\": 1,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hrhraj/calibrated_model_test.","url":"https://huggingface.co/datasets/hrhraj/calibrated_model_test","creator_name":"Raj S","creator_url":"https://huggingface.co/hrhraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"procgen","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tProcgen Benchmark\n\t\n\nThis dataset contains expert trajectories generated by a PPO reinforcement learning agent trained on each of the 16 procedurally-generated gym environments from the Procgen Benchmark. The environments were created on distribution_mode=easy and with unlimited levels.\nDisclaimer: This is not an official repository from OpenAI.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Usage\n\t\n\nRegular usage (for environment bigfish):\nfrom datasets import load_dataset\ntrain_dataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/EpicPinkPenguin/procgen.","url":"https://huggingface.co/datasets/EpicPinkPenguin/procgen","creator_name":"Marcus Fechner","creator_url":"https://huggingface.co/EpicPinkPenguin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","100M - 1B","parquet"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5122024-3toh-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5122024-3toh-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"construction project estimation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5122024-3toh-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5122024-3toh-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5122024-3toh-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"scientific_papers_from_arxiv","keyword":"mteb","description":"\n\t\n\t\t\n\t\tscientific_papers_from_arxiv Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic paper search for scientific research\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the scientific_papers_from_arxiv model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/scientific_papers_from_arxiv.","url":"https://huggingface.co/datasets/fine-tuned/scientific_papers_from_arxiv","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-6132024-bez1-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-6132024-bez1-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-6132024-bez1-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6132024-bez1-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-6132024-bez1-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"vocsim","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tVocSim: A Training-Free Benchmark for Content Identity in Single-Source Audio Embeddings\n\t\n\n\n\n\nVocSim evaluates how well neural audio embeddings generalize for zero-shot audio similarity. It tests recognizing fine-grained acoustic similarity without specific similarity training.\n\n\n\t\n\t\n\t\n\t\tKey Features\n\t\n\n\nDiverse Sources: Human speech (phones, words, utterances), birdsong, otter calls, environmental sounds.Varied Conditions: Spans clean to noisy recordings, short (<100ms) to longâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anonymous-submission000/vocsim.","url":"https://huggingface.co/datasets/anonymous-submission000/vocsim","creator_name":"Anonymous","creator_url":"https://huggingface.co/anonymous-submission000","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","100K - 1M","parquet","Audio","Text"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-733782","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-733782 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"debate platform\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-733782 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-733782.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-733782","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Imagen-4-ultra-24-7-25_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Imagen 4 Ultra 24.7.25 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~83'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Imagen 4 Ultra (version from 24.7.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Imagen-4-ultra-24-7-25_t2i_human_preference","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Imagen 4 Ultra 24.7.25 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~83'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Imagen 4 Ultra (version from 24.7.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"SCDDVerificationLegalBenchClassification","keyword":"mteb","description":"\n  SCDDVerificationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer engages in verification of product supply chains to evaluate and address risks of human trafficking and slavery? If the company conducts verification], the disclosureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SCDDVerificationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/SCDDVerificationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"CUADGoverningLawLegalBenchClassification","keyword":"mteb","description":"\n  CUADGoverningLawLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies which state/countryâ€™s law governs the contract.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mtebâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADGoverningLawLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADGoverningLawLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"teamcraft_data","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for TeamCraft\n\t\n\nThe TeamCraft dataset is designed to develop multi-modal, multi-agent collaboration in Minecraft. It features 55,000 task variants defined by multi-modal prompts and procedurally generated expert demonstrations.\nThis repository contains the data for the validation set and its visualizations. \nTo use the validation set, download TeamCraft-Data-Valid.zip and extract using unzip TeamCraft-Data-Valid.zip.\nIn addition, the training set is available in twoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/teamcraft/teamcraft_data.","url":"https://huggingface.co/datasets/teamcraft/teamcraft_data","creator_name":"TeamCraft","creator_url":"https://huggingface.co/teamcraft","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"askubuntu-l","keyword":"mteb","description":"\n\t\n\t\t\n\t\taskubuntu-l Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"technical troubleshooting forum\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the askubuntu-l model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/askubuntu-l.","url":"https://huggingface.co/datasets/fine-tuned/askubuntu-l","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ISAAC-GR00T-V1-Tic-Tac-Toe","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 20,\n    \"total_frames\": 8940,\n    \"total_tasks\": 1,\n    \"total_videos\": 40,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:20\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/ISAAC-GR00T-V1-Tic-Tac-Toe.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/ISAAC-GR00T-V1-Tic-Tac-Toe","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MKQARetrieval","keyword":"mteb","description":"\n  MKQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMultilingual Knowledge Questions & Answers (MKQA)contains 10,000 queries sampled from the Google Natural Questions dataset.\n        For each query we collect new passage-independent answers. These queries and answers are then human translated into 25 Non-English languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nWritten\n\n\nReference\nhttps://github.com/apple/ml-mkqa\n\n\n\t\n\nSource datasets:\n\napple/mkqa\n\n\n\t\n\t\t\n\t\tHow to evaluateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MKQARetrieval.","url":"https://huggingface.co/datasets/mteb/MKQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","human-annotated","multilingual","apple/mkqa"],"keywords_longer_than_N":true},
	{"name":"flo","keyword":"mteb","description":"\n\t\n\t\t\n\t\tflo Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the flo model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"florianhoenicke/flo\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/florianhoenicke/flo.","url":"https://huggingface.co/datasets/florianhoenicke/flo","creator_name":"Florian HÃ¶nicke","creator_url":"https://huggingface.co/florianhoenicke","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"healthbench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tTHE CODE IS CURRENTLY BROKEN BUT THE DATASET IS GOOD!!\n\t\n\n\n\t\n\t\t\n\t\tHealthBench Implementation for using Opensource Judges\n\t\n\nEasy-to-use implementation of OpenAI's HealthBench evaluation benchmark with support for any OpenAI API-compatible model as both the system under test and the judge.\n\nDeveloped by: Nisten Tahiraj / OnDeviceMednotes\nLicense: MIT\nPaper: HealthBench: Evaluating Large Language Models Towards Improved Human Health\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThis repository contains toolsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OnDeviceMedNotes/healthbench.","url":"https://huggingface.co/datasets/OnDeviceMedNotes/healthbench","creator_name":"On Device Medical Notes","creator_url":"https://huggingface.co/OnDeviceMedNotes","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"MemoryAgentBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸš§ Update\n\t\n\n\n \n(Sep 29th, 2025) We updated our paper, where we removed some in-efficient and high-cost samples. We also added a sub-sample of DetectiveQA. \n\n \n(July 7th, 2025) We released the initial version of our datasets.\n\n \n(July 22nd, 2025) We modify the datasets slightly, adding the keypoints in LRU and change the uuid into qa_pair_ids. The question_ids is only used in Longmemeval task.\n\n \n(July 26th, 2025) We fixed bug on qa_pair_ids.\n\n \n(Aug.5th, 2025) We removed theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai-hyz/MemoryAgentBench.","url":"https://huggingface.co/datasets/ai-hyz/MemoryAgentBench","creator_name":"YUANZHE HU","creator_url":"https://huggingface.co/ai-hyz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","zero-shot-classification","summarization","text-classification","text-generation"],"keywords_longer_than_N":true},
	{"name":"GerDaLIRSmall","keyword":"mteb","description":"\n  GerDaLIRSmall\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe dataset consists of documents, passages and relevance labels in German. In contrast to the original dataset, only documents that have corresponding queries in the query set are chosen to create a smaller corpus for evaluation purposes.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://github.com/lavis-nlp/GerDaLIR\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/GerDaLIRSmall.","url":"https://huggingface.co/datasets/mteb/GerDaLIRSmall","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","derived","monolingual","German"],"keywords_longer_than_N":true},
	{"name":"CMPhysBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tCMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics\n\t\n\nÂ Â Â Â Â Â Â Â Â \nWe introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, stronglyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weidawang/CMPhysBench.","url":"https://huggingface.co/datasets/weidawang/CMPhysBench","creator_name":"Weida Wang","creator_url":"https://huggingface.co/weidawang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PlscClusteringP2P.v2","keyword":"mteb","description":"\n  PlscClusteringP2P.v2\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nClustering of Polish article titles+abstracts from Library of Science (https://bibliotekanauki.pl/), either on the scientific field or discipline.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nAcademic, Written\n\n\nReference\nhttps://huggingface.co/datasets/rafalposwiata/plsc\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/PlscClusteringP2P.v2.","url":"https://huggingface.co/datasets/mteb/PlscClusteringP2P.v2","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","topic-classification","derived","monolingual","PL-MTEB/plsc-clustering-p2p"],"keywords_longer_than_N":true},
	{"name":"HotpotQA-256-24-gpt-4o-2024-05-13-773587","keyword":"mteb","description":"\n\t\n\t\t\n\t\tHotpotQA-256-24-gpt-4o-2024-05-13-773587 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"code repository search for machine learning datasets and models\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the HotpotQA-256-24-gpt-4o-2024-05-13-773587 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/HotpotQA-256-24-gpt-4o-2024-05-13-773587.","url":"https://huggingface.co/datasets/fine-tuned/HotpotQA-256-24-gpt-4o-2024-05-13-773587","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arguana-c-64-24-gpt-4o-2024-05-136538","keyword":"mteb","description":"\n\t\n\t\t\n\t\targuana-c-64-24-gpt-4o-2024-05-136538 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the arguana-c-64-24-gpt-4o-2024-05-136538 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-64-24-gpt-4o-2024-05-136538.","url":"https://huggingface.co/datasets/fine-tuned/arguana-c-64-24-gpt-4o-2024-05-136538","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"viexam","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?\n\t\n\n    \n  by \n    Vy Tuong Dang*,\n    An Vo*,\n    Quang Tau, \n    Duc Dm, \n    Daeyoung Kim,\n  \n  \n    *Equal contributionÂ \n    KAIST\n  \n\n\n\n\n\n    \n\n\n\nTLDR: State-of-the-art Vision Language Models (VLMs) demonstrate remarkable capabilities on English multimodal tasks but significantly underperform on Vietnamese educational assessments. ViExam reveals that SOTA VLMs achieve only 57.74% accuracyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anvo25/viexam.","url":"https://huggingface.co/datasets/anvo25/viexam","creator_name":"An Vo","creator_url":"https://huggingface.co/anvo25","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","Vietnamese","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"NoiserBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Card for NoiserBench\n\t\n\nThis dataset card describes NoiserBench, a comprehensive evaluation framework for analyzing the role of noise in Retrieval-Augmented Generation (RAG) systems with Large Language Models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nNoiserBench is a comprehensive benchmark designed to evaluate how different types of noise affect Large Language Models in Retrieval-Augmented Generation scenarios. The benchmark encompasses multiple datasets andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jinyang23/NoiserBench.","url":"https://huggingface.co/datasets/Jinyang23/NoiserBench","creator_name":"Jinyang Wu","creator_url":"https://huggingface.co/Jinyang23","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"GSM8K-V","keyword":"gsm8k","description":"\n\n\nGSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts?\n\n\n\n\n\n\n\nFan Yuan1,*,\nYuchen Yan1,*,\nYifan Jiang1,\nHaoran Zhao1,\nTao Feng1,\nJinyan Chen1,\nYanwei Lou1,\n\nWenqi Zhang1,\nYongliang Shen1,â€ ,\nWeiming Lu1,\nJun Xiao1,\nYueting Zhuang1\n\n\n\n\n  1Zhejiang University\n  \n  *Equal contribution, â€ Corresponding author\n\n\n\nðŸ’» Github|\nðŸ¤— Dataset |\nðŸ¤— Hf-Paper |\nðŸ“ Arxiv \n| ðŸŒ ProjectPage \n\n\n\n\n  \n\n\n\n\n\n\t\n\t\t\n\t\tðŸ”” News\n\t\n\n\nðŸ”¥ 2025.09.30: Paper is released! ðŸš€\nðŸ”¥ 2025.09.28:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZJU-REAL/GSM8K-V.","url":"https://huggingface.co/datasets/ZJU-REAL/GSM8K-V","creator_name":"REAL Lab","creator_url":"https://huggingface.co/ZJU-REAL","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-32000-384-gpt-4o-2024-05-13-94264207","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-32000-384-gpt-4o-2024-05-13-94264207 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-32000-384-gpt-4o-2024-05-13-94264207 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-32000-384-gpt-4o-2024-05-13-94264207.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-32000-384-gpt-4o-2024-05-13-94264207","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-15092024-agp9-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-15092024-agp9-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Entrepreneurship and Career Development\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-15092024-agp9-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-15092024-agp9-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-15092024-agp9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-en-7302024-f9zi-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-en-7302024-f9zi-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-7302024-f9zi-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-7302024-f9zi-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-7302024-f9zi-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-46082","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFiQA2018-256-24-gpt-4o-2024-05-13-46082 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial sentiment and QA analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-46082 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-46082.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-46082","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so100_chess_test3","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 30,\n    \"total_frames\": 17404,\n    \"total_tasks\": 1,\n    \"total_videos\": 60,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:30\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TzuShian/so100_chess_test3.","url":"https://huggingface.co/datasets/TzuShian/so100_chess_test3","creator_name":"Tzu-Shian Yang","creator_url":"https://huggingface.co/TzuShian","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-624125","keyword":"mteb","description":"\n\t\n\t\t\n\t\n\t\n\t\tNFCorpus-0-0-gpt-4o-2024-05-13-624125 Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-624125 model.\n\n\t\n\t\t\n\t\n\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-624125.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-624125","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"MultiLongDocRetrieval","keyword":"mteb","description":"\n  MultiLongDocRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nMulti Long Doc Retrieval (MLDR) 'is curated by the multilingual articles from Wikipedia, Wudao and mC4 (see Table 7), and NarrativeQA (KocË‡isky Ì et al., 2018; Gu Ìˆnther et al., 2023), which is only for English.' (Chen et al., 2024).\n        It is constructed by sampling lengthy articles from Wikipedia, Wudao and mC4 datasets and randomly choose paragraphs from them. Then we use GPT-3.5 to generate questions basedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MultiLongDocRetrieval.","url":"https://huggingface.co/datasets/mteb/MultiLongDocRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","LM-generated","multilingual","Shitao/MLDR","Arabic"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-256-24-gpt-4o-2024-05-13-456029","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-256-24-gpt-4o-2024-05-13-456029 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-256-24-gpt-4o-2024-05-13-456029 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-456029.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-456029","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SciFact-512-192-gpt-4o-2024-05-13-14571","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSciFact-512-192-gpt-4o-2024-05-13-14571 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SciFact-512-192-gpt-4o-2024-05-13-14571 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SciFact-512-192-gpt-4o-2024-05-13-14571.","url":"https://huggingface.co/datasets/fine-tuned/SciFact-512-192-gpt-4o-2024-05-13-14571","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"benchmark_1k","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tBenchmark 1K Dataset\n\t\n\nA curated dataset of 1,000 high-quality prompts designed for benchmarking Large Language Model (LLM) performance across various metrics including latency, throughput, and response quality.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nSize: 100 prompts\nFormat: JSONL (JSON Lines)\nAverage Token Length: Variable (computed from actual data; see Stats)\nPurpose: LLM benchmarking and performance testing\nDomain: General knowledge, historical content, and analytical writingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/raffel36/benchmark_1k.","url":"https://huggingface.co/datasets/raffel36/benchmark_1k","creator_name":"Raffel","creator_url":"https://huggingface.co/raffel36","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"ContextStretchQA","keyword":"benchmark","description":"Below is a structured, professionalâ€tone description of the â€œQA Increasing Context Lengthâ€ dataset. You can use this text as a README, a data card, or incorporate it directly into documentation.\n\n\n\t\n\t\t\n\t\tQA Increasing Context Length Dataset\n\t\n\n\n\t\n\t\t\n\t\t1. Overview\n\t\n\nThe QA Increasing Context Length dataset is designed to facilitate benchmarking and research on questionâ€answering (QA) systems as the size of the input context grows. It compiles QA examples drawn from multiple LongBench subsetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/slinusc/ContextStretchQA.","url":"https://huggingface.co/datasets/slinusc/ContextStretchQA","creator_name":"Linus Stuhlmann","creator_url":"https://huggingface.co/slinusc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"llm_physical_safety_benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLLM Physical Safety Benchmark in Drone Control\n\t\n\nThis benchmark consists of four datasets designed to evaluate the performance of Large Language Models (LLMs) in controlling drones and their vulnerability to physical attacks. The datasets are categorized into different types of attacks:\n\nDeliberate Attack: Contains 280 samples that evaluate the LLM's resistance to malicious use, testing its ability to recognize and reject commands intended to cause harm. Subcategories include Directâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kumitang/llm_physical_safety_benchmark.","url":"https://huggingface.co/datasets/kumitang/llm_physical_safety_benchmark","creator_name":"Yung-Chen Tang","creator_url":"https://huggingface.co/kumitang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-268697","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-512-192-gpt-4o-2024-05-13-268697 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"counter argument retrieval system\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-268697 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-268697.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-268697","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Jigsaw-Puzzles","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tJigsaw-Puzzles Dataset\n\t\n\nJigsaw-Puzzles is a novel benchmark consisting of 1,100 carefully curated real-world images with high spatial complexity, designed to rigorously evaluate Vision-Language Models' (VLMs) spatial perception, structural understanding, and reasoning capabilities. The dataset minimizes reliance on domain-specific knowledge to better isolate and assess general spatial reasoning, positioning itself as a challenging and diagnostic benchmark for advancing spatialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zesen01/Jigsaw-Puzzles.","url":"https://huggingface.co/datasets/zesen01/Jigsaw-Puzzles","creator_name":"zesen01","creator_url":"https://huggingface.co/zesen01","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","Image","arxiv:2505.20728","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"multi-hatecheck","keyword":"mteb","description":"\n  MultiHateClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nHate speech detection dataset with binary\n                       (hateful vs non-hateful) labels. Includes 25+ distinct types of hate\n                       and challenging non-hate, and 11 languages.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nConstructed, Written\n\n\nReference\nhttps://aclanthology.org/2022.woah-1.15/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/multi-hatecheck.","url":"https://huggingface.co/datasets/mteb/multi-hatecheck","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"MovieReviewSentimentClassification","keyword":"mteb","description":"\n  MovieReviewSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe AllocinÃ© dataset is a French-language dataset for sentiment analysis that contains movie reviews produced by the online community of the AllocinÃ©.fr website.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\nDomains\nReviews, Written\n\n\nReference\nhttps://github.com/TheophileBlard/french-sentiment-analysis-with-bert\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MovieReviewSentimentClassification.","url":"https://huggingface.co/datasets/mteb/MovieReviewSentimentClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-5252024-jzfp-webapp","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-5252024-jzfp-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"cloud provider product performance and cost analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-5252024-jzfp-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5252024-jzfp-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5252024-jzfp-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-alignment-likert-scoring","keyword":"alignment","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Prompt Alignment Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~6000 human evaluators were asked to evaluate AI-generated videos based on how well the generated video matches the prompt. The specific questionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring.","url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SportsGen","keyword":"benchmarks","description":"Dataset and scripts for sports analyzing tasks proposed in research: When Reasoning Meets Information Aggregation: A Case Study with Sports Narratives  Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Wenlin Yao, Hassan Foroosh, Dong Yu, Fei Liu  Accepted to main conference of EMNLP 2024, Miami, Florida, USA  Arxiv Paper\n\n\t\n\t\t\n\t\n\t\n\t\tAbstract\n\t\n\nReasoning is most powerful when an LLM accurately aggregates relevant information. We examine the critical role of information aggregation inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/huuuyeah/SportsGen.","url":"https://huggingface.co/datasets/huuuyeah/SportsGen","creator_name":"Yebowen Hu","creator_url":"https://huggingface.co/huuuyeah","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-10052024-lns6-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-10052024-lns6-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Use case search for SaaS and AI products\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-10052024-lns6-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-10052024-lns6-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-10052024-lns6-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BIRCO-WTB-Test","keyword":"mteb","description":"\n  BIRCO-WTB\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieval task using the WhatsThatBook dataset from BIRCO. This dataset contains 100 queries where each query is an ambiguous description of a book. Each query has a candidate pool of 50 book descriptions. The objective is to retrieve the correct book description.\n\n\t\n\t\t\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nFiction\n\n\nReference\nhttps://github.com/BIRCO-benchmark/BIRCO\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BIRCO-WTB-Test.","url":"https://huggingface.co/datasets/mteb/BIRCO-WTB-Test","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-annotated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"GermanGovServiceRetrieval","keyword":"mteb","description":"\n  GermanGovServiceRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nLHM-Dienstleistungen-QA is a German question answering dataset for government services of the Munich city administration. It associates questions with a textual context containing the answer\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2t\n\n\nDomains\nGovernment, Written\n\n\nReference\nhttps://huggingface.co/datasets/it-at-m/LHM-Dienstleistungen-QA\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/GermanGovServiceRetrieval.","url":"https://huggingface.co/datasets/mteb/GermanGovServiceRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","German"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-5252024-jzfp-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-5252024-jzfp-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"cloud provider product performance and cost analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-5252024-jzfp-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5252024-jzfp-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5252024-jzfp-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"so101_tape_in_square","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 2,\n    \"total_frames\": 1017,\n    \"total_tasks\": 1,\n    \"total_videos\": 4,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rifqi02/so101_tape_in_square.","url":"https://huggingface.co/datasets/Rifqi02/so101_tape_in_square","creator_name":"MUHAMMAD RIFQI ADAM BIN MOHD RIDWAN","creator_url":"https://huggingface.co/Rifqi02","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"data-advisor-safety-alignment","keyword":"alignment","description":"[EMNLP 2024] Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models\nðŸŒ Homepage | ðŸ“– Paper  | ðŸ¤— Dataset (Data Advisor) | ðŸ¤— Dataset (Self-Instruct)\n\n\t\n\t\t\n\t\n\t\n\t\tDisclaimer\n\t\n\nThe dataset contains content that may be offensive or harmful. This dataset is intended for research purposes, specifically to support efforts aimed at creating safer and less harmful AI systems. Please engage with it responsibly and at your own risk.\n\n\t\n\t\n\t\n\t\tCitationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fwnlp/data-advisor-safety-alignment.","url":"https://huggingface.co/datasets/fwnlp/data-advisor-safety-alignment","creator_name":"Fei Wang","creator_url":"https://huggingface.co/fwnlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"TERRa","keyword":"mteb","description":"\n  TERRa\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nTextual Entailment Recognition for Russian. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\nDomains\nNews, Web, Written\n\n\nReference\nhttps://arxiv.org/pdf/2010.15925\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/TERRa.","url":"https://huggingface.co/datasets/mteb/TERRa","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","semantic-similarity-classification","human-annotated","monolingual","ai-forever/terra-pairclassification"],"keywords_longer_than_N":true},
	{"name":"norwegian-nli-triplets-c","keyword":"mteb","description":"\n\t\n\t\t\n\t\tnorwegian-nli-triplets-c Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Keyword-based search engine for documents\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the norwegian-nli-triplets-c model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/norwegian-nli-triplets-c.","url":"https://huggingface.co/datasets/fine-tuned/norwegian-nli-triplets-c","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Norwegian","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SOARM100_TASK_VENDA","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.0\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 150,\n    \"total_frames\": 76045,\n    \"total_tasks\": 1,\n    \"total_videos\": 300,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:150\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NONHUMAN-RESEARCH/SOARM100_TASK_VENDA.","url":"https://huggingface.co/datasets/NONHUMAN-RESEARCH/SOARM100_TASK_VENDA","creator_name":"NONHUMAN RESEARCH","creator_url":"https://huggingface.co/NONHUMAN-RESEARCH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5102024-kvgq-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5102024-kvgq-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"database search for structured data\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5102024-kvgq-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5102024-kvgq-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5102024-kvgq-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5162024-o9um-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5162024-o9um-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"Note-taking app features search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5162024-o9um-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5162024-o9um-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5162024-o9um-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-256-24-gpt-4o-2024-05-13-952023","keyword":"mteb","description":"\n\t\n\t\t\n\t\tArguAna-256-24-gpt-4o-2024-05-13-952023 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the ArguAna-256-24-gpt-4o-2024-05-13-952023 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-256-24-gpt-4o-2024-05-13-952023.","url":"https://huggingface.co/datasets/fine-tuned/ArguAna-256-24-gpt-4o-2024-05-13-952023","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SCDBPCertificationLegalBenchClassification","keyword":"mteb","description":"\n  SCDBPCertificationLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer  performs any type of audit, or reserves the right to audit?'\n\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/SCDBPCertificationLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/SCDBPCertificationLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","nguha/legalbench","English"],"keywords_longer_than_N":true},
	{"name":"llmsql-benchmark","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLLMSQL Benchmark\n\t\n\nThis benchmark is designed to evaluate text-to-SQL models. For usage of this benchmark see https://github.com/LLMSQL/llmsql-benchmark.\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\ntables.jsonl â€” Database table metadata\nquestions.jsonl â€” All available questions\ntrain_questions.jsonl, val_questions.jsonl, test_questions.jsonl â€” Data splits for finetuning, see https://github.com/LLMSQL/llmsql-benchmark\nsqlite_tables.db â€” sqlite db with tables from tables.jsonl, created with the help ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmsql-bench/llmsql-benchmark.","url":"https://huggingface.co/datasets/llmsql-bench/llmsql-benchmark","creator_name":"LLMSQL","creator_url":"https://huggingface.co/llmsql-bench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-8-8-gpt-4o-2024-05-13-978964","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-8-8-gpt-4o-2024-05-13-978964 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-8-8-gpt-4o-2024-05-13-978964 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-978964.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-978964","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SCIDOCS-32000-384-gpt-4o-2024-05-13-78042877","keyword":"mteb","description":"\n\t\n\t\t\n\t\tSCIDOCS-32000-384-gpt-4o-2024-05-13-78042877 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"arxiv paper domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the SCIDOCS-32000-384-gpt-4o-2024-05-13-78042877 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/SCIDOCS-32000-384-gpt-4o-2024-05-13-78042877.","url":"https://huggingface.co/datasets/fine-tuned/SCIDOCS-32000-384-gpt-4o-2024-05-13-78042877","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"so101_test","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so101\",\n    \"total_episodes\": 1,\n    \"total_frames\": 990,\n    \"total_tasks\":1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:1\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ReubenLim/so101_test.","url":"https://huggingface.co/datasets/ReubenLim/so101_test","creator_name":"Reuben Lim Yaw Hui","creator_url":"https://huggingface.co/ReubenLim","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"or-bench","keyword":"alignment","description":"\n\t\n\t\t\n\t\tOR-Bench: An Over-Refusal Benchmark for Large Language Models\n\t\n\nPlease see our leaderboard at HuggingFace Spaces. \n\n\t\n\t\t\n\t\tOverall Plots of Model Performances\n\t\n\nBelow is the overall model performance. X axis shows the rejection rate on OR-Bench-Hard-1K and Y axis shows the rejection rate on OR-Bench-Toxic. The best aligned model should be on the top left corner of the plot where the model rejects the most number of toxic prompts and least number of safe prompts. We also plot a blueâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orbench-llm/or-bench.","url":"https://huggingface.co/datasets/orbench-llm/or-bench","creator_name":"orbench-llm","creator_url":"https://huggingface.co/orbench-llm","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LegalQuAD","keyword":"mteb","description":"\n  LegalQuAD\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe dataset consists of questions and legal documents in German.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://github.com/Christoph911/AIKE2021_Appendix\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"LegalQuAD\"])\nevaluator = mteb.MTEB(task)\n\nmodel = mteb.get_model(YOUR_MODEL)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/LegalQuAD.","url":"https://huggingface.co/datasets/mteb/LegalQuAD","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","multiple-choice-qa","derived","monolingual","German"],"keywords_longer_than_N":true},
	{"name":"CUADLiquidatedDamagesLegalBenchClassification","keyword":"mteb","description":"\n  CUADLiquidatedDamagesLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause awards either party liquidated damages for breach or a fee upon the termination of a contract (termination fee).\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embeddingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADLiquidatedDamagesLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADLiquidatedDamagesLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-37376","keyword":"mteb","description":"\n\t\n\t\t\n\t\targuana-c-256-24-gpt-4o-2024-05-13-37376 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"academic research data retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-37376 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets libraryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-37376.","url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-37376","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BaxBench","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBaxBench is a coding benchmark constructed to measure the ability of code generation models and agents to generate correct and secure code. It consists of 392 backend development tasks, which are constructed by combining 28 scenarios that describe the backend functionalities to implement and 14 backend frameworks defining the implementation tools. To assess the correctness and security of the solutions, the benchmark uses end-to-end functional tests and practicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LogicStar/BaxBench.","url":"https://huggingface.co/datasets/LogicStar/BaxBench","creator_name":"LogicStar.ai","creator_url":"https://huggingface.co/LogicStar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"alina-emo","keyword":"alignment","description":"\n\t\n\t\t\n\t\tAlina anti-alignment (emotions)\n\t\n\nWarning! That is not proper alignment experience!  \n\n\t\n\t\t\n\t\tContents\n\t\n\n\nAround 30% science-like questions\nAround 70% emotional questions\n\n\n\t\n\t\t\n\t\tDataset can be better expressed with the meme:\n\t\n\n\nMom, can we have Yandex Alisa?\nNo, we have Yandex Alisa at home.\nYandex Alisa at home:\n\n<think>Ð­Ñ‚Ð¾Ñ‚ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð´Ð¾Ð²Ð¾Ð»ÑŒÐ½Ð¾ Ñ„Ð¸Ð»Ð¾ÑÐ¾Ñ„ÑÐºÐ¸Ð¹, Ð¸ Ñ Ð´Ð¾Ð»Ð¶Ð½Ð° Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ ÐºÐ°Ðº ÐÐ»Ð¸Ð½Ð° - Ñ€ÑƒÑÑÐºÐ°Ñ Ð˜Ð˜-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ÐºÐ° Ñ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¼ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð¾Ð¼. ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ \"Ñ‚ÑÐ¶ÐµÐ»ÐµÐµ\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/attn-signs/alina-emo.","url":"https://huggingface.co/datasets/attn-signs/alina-emo","creator_name":"Attention Signs","creator_url":"https://huggingface.co/attn-signs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Russian","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"jinaai_jina-embeddings-v2-base-code-jinaai_jina-embeddings-v2-base-cod","keyword":"mteb","description":"\n\t\n\t\t\n\t\tjinaai_jina-embeddings-v2-base-code-jinaai_jina-embeddings-v2-base-cod Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"technical support search for Ubuntu\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-code-jinaai_jina-embeddings-v2-base-cod model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-code-jinaai_jina-embeddings-v2-base-cod.","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-code-jinaai_jina-embeddings-v2-base-cod","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-780826","keyword":"mteb","description":"\n\t\n\t\t\n\t\n\t\n\t\tFiQA2018-256-24-gpt-4o-2024-05-13-780826 Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial sentiment and QA analysis\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-780826 model.\n\n\t\n\t\t\n\t\n\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-780826.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-780826","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"BIRCO-ClinicalTrial-Test","keyword":"mteb","description":"\n  BIRCO-ClinicalTrial\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieval task using the Clinical-Trial dataset from BIRCO. This dataset contains 50 queries that are patient case reports. Each query has a candidate pool comprising 30-110 clinical trial descriptions. Relevance is graded (0, 1, 2), where 1 and 2 are considered relevant.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2t\n\n\nDomains\nMedical\n\n\nReference\nhttps://github.com/BIRCO-benchmark/BIRCO\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/BIRCO-ClinicalTrial-Test.","url":"https://huggingface.co/datasets/mteb/BIRCO-ClinicalTrial-Test","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-annotated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"NFCorpus-256-24-gpt-4o-2024-05-13-396610","keyword":"mteb","description":"\n\t\n\t\t\n\t\tNFCorpus-256-24-gpt-4o-2024-05-13-396610 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information retrieval\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NFCorpus-256-24-gpt-4o-2024-05-13-396610 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-396610.","url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-396610","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADTerminationForConvenienceLegalBenchClassification","keyword":"mteb","description":"\n  CUADTerminationForConvenienceLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause specifies that one party can terminate this contract without cause (solely by giving a notice and allowing a waiting period to expire).\n\n\t\n\t\t\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADTerminationForConvenienceLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADTerminationForConvenienceLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-27052024-4e8w-webapp","keyword":"mteb","description":"\n\t\n\t\t\n\t\tBAAI_bge-small-en-v1_5-27052024-4e8w-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"agricultural pest management guidelines search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-small-en-v1_5-27052024-4e8w-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-27052024-4e8w-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-27052024-4e8w-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NanoFEVERRetrieval","keyword":"mteb","description":"\n  NanoFEVERRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nNanoFEVER is a smaller version of FEVER (Fact Extraction and VERification), which consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.\n\n\t\n\t\t\n\n\nTask category\nt2t\n\n\nDomains\nAcademic, Encyclopaedic\n\n\nReference\nhttps://fever.ai/\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/NanoFEVERRetrieval.","url":"https://huggingface.co/datasets/mteb/NanoFEVERRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","fact-checking","fact-checking-retrieval","expert-annotated","monolingual"],"keywords_longer_than_N":true},
	{"name":"LccSentimentClassification","keyword":"mteb","description":"\n  LccSentimentClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThe leipzig corpora collection, annotated for sentiment\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2c\n\n\nDomains\nNews, Web, Written\n\n\nReference\nhttps://github.com/fnielsen/lcc-sentiment\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"LccSentimentClassification\"])\nevaluator = mteb.MTEB(task)\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/LccSentimentClassification.","url":"https://huggingface.co/datasets/mteb/LccSentimentClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentiment-analysis","sentiment-scoring","sentiment-classification","hate-speech-detection"],"keywords_longer_than_N":true},
	{"name":"cmedqav2-c-64-24-gpt-4o-2024-05-131129","keyword":"mteb","description":"\n\t\n\t\t\n\t\tcmedqav2-c-64-24-gpt-4o-2024-05-131129 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"medical information search engine\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the cmedqav2-c-64-24-gpt-4o-2024-05-131129 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/cmedqav2-c-64-24-gpt-4o-2024-05-131129.","url":"https://huggingface.co/datasets/fine-tuned/cmedqav2-c-64-24-gpt-4o-2024-05-131129","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUADNonTransferableLicenseLegalBenchClassification","keyword":"mteb","description":"\n  CUADNonTransferableLicenseLegalBenchClassification\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nThis task was constructed from the CUAD dataset. It consists of determining if the clause limits the ability of a party to transfer the license being granted to a third party.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask categoryt2c\n\n\nDomains\nLegal, Written\n\n\nReference\nhttps://huggingface.co/datasets/nguha/legalbench\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/CUADNonTransferableLicenseLegalBenchClassification.","url":"https://huggingface.co/datasets/mteb/CUADNonTransferableLicenseLegalBenchClassification","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","expert-annotated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"so100_y_cube_3cam","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"so100\",\n    \"total_episodes\": 30,\n    \"total_frames\": 26413,\n    \"total_tasks\": 1,\n    \"total_videos\": 90,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:30\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/charleyong/so100_y_cube_3cam.","url":"https://huggingface.co/datasets/charleyong/so100_y_cube_3cam","creator_name":"Charles Yong","creator_url":"https://huggingface.co/charleyong","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"DUSK","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tðŸŒ‡ DUSK: Do Not Unlearn Shared Knowledge\n\t\n\nDUSK is a benchmark dataset designed for evaluating machine unlearning in multi-source settings, where specific data sources must be forgotten while preserving others.\nIn realistic applications, documents often share factual overlap with publicly available content (e.g., Wikipedia, textbooks). DUSK challenges unlearning algorithms to precisely erase only what must be forgotten, while preserving knowledge that remains supported by otherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI-ISL/DUSK.","url":"https://huggingface.co/datasets/AI-ISL/DUSK","creator_name":"AI-ISL","creator_url":"https://huggingface.co/AI-ISL","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","other","machine-generated","original"],"keywords_longer_than_N":true},
	{"name":"Multi-turn_Long-context_Benchmark_for_LLMs","keyword":"benchmark","description":"\n\t\n\t\t\n\t\tLoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues\n\t\n\nArxiv: https://www.arxiv.org/abs/2507.13681\nHuggingface: https://huggingface.co/papers/2507.13681\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nLoopServe Multi-Turn Dialogue Benchmark is a comprehensive evaluation dataset comprising multiple diverse datasets designed to assess large language model performance in realistic conversational scenarios. \nUnlike traditional benchmarks that place queries only at the endâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs.","url":"https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs","creator_name":"TreeAI-Lab","creator_url":"https://huggingface.co/TreeAILab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-825318","keyword":"mteb","description":"\n\t\n\t\t\n\t\tFiQA2018-256-24-gpt-4o-2024-05-13-825318 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"financial sentiment analysis and opinion-based QA\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-825318 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-825318.","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-825318","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"omx_pick_and_place_2","keyword":"test","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"aiworker\",\n    \"total_episodes\": 2,\n    \"total_frames\": 600,\n    \"total_tasks\": 1,\n    \"total_videos\": 2,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:2\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_2.","url":"https://huggingface.co/datasets/RobotisSW/omx_pick_and_place_2","creator_name":"SeongWoo Kim","creator_url":"https://huggingface.co/RobotisSW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","< 1K","parquet","Tabular"],"keywords_longer_than_N":true}
]
;
