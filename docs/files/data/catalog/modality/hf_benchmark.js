var data_for_benchmark = 
[
	{"name":"LOGIC-701","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLOGIC-701 Benchmark\\n\\t\\n\\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThis benchmark is also part of an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701."},
	{"name":"OpsEval","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Junetheriver/OpsEval","creator_name":"Liu Yuhe","creator_url":"https://huggingface.co/Junetheriver","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpsEval Dataset\\n\\t\\n\\nWebsite | Reporting Issues\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThe OpsEval dataset represents a pioneering effort in the evaluation of Artificial Intelligence for IT Operations (AIOps), focusing on the application of Large Language Models (LLMs) within this domain. In an era where IT operations are increasingly reliant on AI technologies for automation and efficiency, understanding the performance of LLMs in operational tasks becomes crucial. OpsEval offers a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Junetheriver/OpsEval."},
	{"name":"MAGB","keyword":"benchmark","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sherirto/MAGB","creator_name":"Sherirto","creator_url":"https://huggingface.co/Sherirto","description":"\\n\\t\\n\\t\\t\\n\\t\\tMAGB\\n\\t\\n\\nThis repository contains the Multimodal Attribute Graph Benchmark (MAGB) datasets described in the paper When Graph meets Multimodal: Benchmarking on Multimodal Attributed Graphs Learning.\\nGithub repository\\nMAGB provides 5 datasets from E-Commerce and Social Networks, and evaluates two major learning paradigms: GNN-as-Predictor and VLM-as-Predictor.  The datasets are publicly available on Hugging Face: https://huggingface.co/datasets/Sherirto/MAGB.\\nEach dataset consists of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sherirto/MAGB."},
	{"name":"MMBench-DEV-RU","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vikhrmodels/MMBench-DEV-RU","creator_name":"Vikhr models","creator_url":"https://huggingface.co/Vikhrmodels","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMBench-DEV-RU\\n\\t\\n\\n–≠—Ç–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π Dev —Å–ø–ª–∏—Ç mmbench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM.\\n–ü–µ—Ä–µ–≤–æ–¥ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª –ø—Ä–∏ –ø–æ–º–æ—â–∏ gpt-4, —á–∞—Å—Ç—å –≤–æ–ø—Ä–æ—Å–æ–≤ –±—ã–ª–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞ –∞—Å—Å–µ—Å–æ—Ä–∞–º–∏.\\n–í –¥–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –º–∞–ª–∞—è —á–∞—Å—Ç—å –≤–æ–ø—Ä–æ—Å–æ–≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞. \\n–°—Å—ã–ª–∫–∞ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫: https://huggingface.co/spaces/opencompass/MMBench\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞\\n\\t\\n\\nhttps://github.com/Natyren/mmbench-ru-eval\\n–§–∞–π–ª, –∫–æ—Ç–æ—Ä—ã–π –≤—ã —Å–æ–±–∏—Ä–∞–µ—Ç–µ—Å—å –ø—Ä–æ–≥–Ω–∞—Ç—å –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É gt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vikhrmodels/MMBench-DEV-RU."},
	{"name":"STEM","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/stemdataset/STEM","creator_name":"stem","creator_url":"https://huggingface.co/stemdataset","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSTEM Dataset\\n\\t\\n\\n\\n  üìÉ [Paper] ‚Ä¢ üíª [Github] ‚Ä¢ ü§ó [Dataset] ‚Ä¢ üèÜ [Leaderboard] ‚Ä¢ üìΩ [Slides] ‚Ä¢ üìã [Poster]\\n\\n\\nThis dataset is proposed in the ICLR 2024 paper: Measuring Vision-Language STEM Skills of Neural Models. We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stemdataset/STEM."},
	{"name":"ISHate","keyword":"benchmark","license":"Boost Software License 1.0","license_url":"https://choosealicense.com/licenses/bsl-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BenjaminOcampo/ISHate","creator_name":"Nicol√°s Benjam√≠n Ocampo","creator_url":"https://huggingface.co/BenjaminOcampo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tImplicit and Subtle Hate\\n\\t\\n\\nImplicit and Subtle Hate (ISHate) is a Hate Speech benchmark for implicit and subtle HS detection on social media messages. The dataset has been presented in the paper \\\"An In-depth Analysis of Implicit and Subtle Hate Speech Messages\\\" accepted at EACL 2023.\\n[Read the Paper]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhat is Implicit and Subtle Hate?\\n\\t\\n\\nImplicit Hate Speech does not immediately denote abuse/hate. Implicitness goes beyond word-related meaning, implying figurative‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BenjaminOcampo/ISHate."},
	{"name":"social","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/socialnormdataset/social","creator_name":"socialnormdataset","creator_url":"https://huggingface.co/socialnormdataset","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSocial Dataset\\n\\t\\n\\n\\n  üìÉ [Paper] ‚Ä¢ üíª [Github] ‚Ä¢ ü§ó [Dataset] ‚Ä¢ üìΩ [Slides] ‚Ä¢ üìã [Poster]\\n\\n\\nThis dataset is proposed in the NAACL 2024 paper: Measuring Social Norms of Large Language Models.\\nWe present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/socialnormdataset/social."},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-799305","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-799305","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\targuana-c-256-24-gpt-4o-2024-05-13-799305 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research on argumentation\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-799305 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-799305."},
	{"name":"Material_Selection_Eval","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cmudrc/Material_Selection_Eval","creator_name":"Design Research Collective","creator_url":"https://huggingface.co/cmudrc","description":"A benchmark designed to facilitate evaluation and modify the behavior of a foundation model through different existing techniques in the context of material selection for conceptual design.\\nThe data is collected by conducting a survey of experts in the field of material selection. The same questions mentioned in keyquestions.csv are asked to experts.\\nThis can be used to evaluate a Language model performance and its spread compared to a human evaluation.\\nTo get into a more detailed explanation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cmudrc/Material_Selection_Eval."},
	{"name":"Touche2020-256-24-gpt-4o-2024-05-13-27907","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/Touche2020-256-24-gpt-4o-2024-05-13-27907","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTouche2020-256-24-gpt-4o-2024-05-13-27907 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"information retrieval benchmark search\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the Touche2020-256-24-gpt-4o-2024-05-13-27907 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/Touche2020-256-24-gpt-4o-2024-05-13-27907."},
	{"name":"QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuoraRetrieval-256-24-gpt-4o-2024-05-13-635320 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"information retrieval benchmark\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320."},
	{"name":"BAAI_bge-small-en-v1_5-5252024-jzfp-webapp","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5252024-jzfp-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-small-en-v1_5-5252024-jzfp-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"cloud provider product performance and cost analysis\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-small-en-v1_5-5252024-jzfp-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-5252024-jzfp-webapp."},
	{"name":"QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuoraRetrieval-256-24-gpt-4o-2024-05-13-80208 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"information retrieval benchmark\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208."},
	{"name":"InfiBench","keyword":"benchmark","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llylly001/InfiBench","creator_name":"Linyi Li","creator_url":"https://huggingface.co/llylly001","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInfiBench (Data Part)\\n\\t\\n\\nNote: For full description, please visit our main website https://infi-coder.github.io/infibench.\\nThis repo contains all data of our code LLM evaluation dataset InfiBench. suite_v2.1.yaml lists the case list and suite_v2.1_data.csv records all data (prompt, reference answer, evaluation metric). The data can be directly consumed by our automatic evaluation tool to evaluate any model's response.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\n\\nName: InfiBench\\nDescription:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llylly001/InfiBench."},
	{"name":"TextOCR-GPT4o","keyword":"benchmarks","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/TextOCR-GPT4o","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for TextOCR-GPT4o\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTextOCR-GPT4o is Meta's TextOCR dataset dataset captioned with emphasis on text OCR using GPT4o. To get the image, you will need to agree to their terms of service.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\nThe TextOCR-GPT4o dataset is intended for generating benchmarks for comparison of an VLM to GPT4o.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe caption languages are in English, while various texts in images are in many languages such as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/TextOCR-GPT4o."},
	{"name":"Benchmark-Testing","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shounakpaul95/Benchmark-Testing","creator_name":"Shounak Paul","creator_url":"https://huggingface.co/shounakpaul95","description":"shounakpaul95/Benchmark-Testing dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"DefAn","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/iamasQ/DefAn","creator_name":"A B M Ashikur Rahman","creator_url":"https://huggingface.co/iamasQ","description":"DefAn: Definitive-Answer-Dataset-for-LLMs-Hallucination-Evaluation\\n\\n  A.B.M. Ashikur Rahman1, Saeed Anwar1,2, Muhammad Usman1,2, Ajmal Mian3, \\n\\n\\n1 King Fahd University of Petroleum and Minerals, Dhahran, KSA\\n\\n\\n2JRCAI, SDAIA-KFUPM \\n\\n\\n3The University of Western Australia, Crawley, Western Australia\\n\\n\\n    Arxiv Paper,  GitHub Repository\\n\\n\\n\\n\\\"DefAn\\\" is a comprehensive evaluation benchmark dataset, with more than 75000 samples, designed to assess the hallucination tendencies of large language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iamasQ/DefAn."},
	{"name":"BeHonest","keyword":"benchmark","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GAIR/BeHonest","creator_name":"SII - GAIR","creator_url":"https://huggingface.co/GAIR","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBeHonest: Benchmarking Honesty in Large Language Models\\n\\t\\n\\nBeHonest is a pioneering benchmark specifically designed to assess honesty in LLMs comprehensively. BeHonest evaluates three essential aspects of honesty: awareness of knowledge boundaries (self-knowledge), avoidance of deceit (non-deceptiveness), and consistency in responses (consistency).\\nBeHonest supports the following 10 scenarios:\\n\\nAdmitting Unknowns: LLMs should appropriately refuse to answer questions that are beyond‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GAIR/BeHonest."},
	{"name":"rublimp","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RussianNLP/rublimp","creator_name":"Natural Language Processing in Russian","creator_url":"https://huggingface.co/RussianNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRuBLiMP\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nRuBLiMP, or Russian Benchmark of Linguistic Minimal Pairs, is the first diverse and large-scale benchmark of minimal pairs in Russian.\\nRuBLiMP includes 45k minimal pairs of sentences that differ in grammaticality and isolate morphological, syntactic, or semantic phenomena. In contrast to existing benchmarks of linguistic minimal pairs, RuBLiMP is created by applying linguistic perturbations to automatically annotated sentences from open‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RussianNLP/rublimp."},
	{"name":"Set_Eval","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AarushSah/Set_Eval","creator_name":"Aarush Sah","creator_url":"https://huggingface.co/AarushSah","description":"AarushSah/Set_Eval dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"LIBRA","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ai-forever/LIBRA","creator_name":"ai-forever","creator_url":"https://huggingface.co/ai-forever","description":"\\n\\t\\n\\t\\t\\n\\t\\tLIBRA: Long Input Benchmark for Russian Analysis\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nLIBRA (Long Input Benchmark for Russian Analysis) is designed to evaluate the capabilities of large language models (LLMs) in understanding and processing long texts in Russian. This benchmark includes 21 datasets adapted for different tasks and complexities. The tasks are divided into four complexity groups and allow evaluation across various context lengths ranging from 4k up to 128k tokens.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai-forever/LIBRA."},
	{"name":"mozzarella","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/feedback-to-code/mozzarella","creator_name":"Feedback-2-Code","creator_url":"https://huggingface.co/feedback-to-code","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMozzarella-0.3.1 \\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMotivation\\n\\t\\n\\n\\nMozzarella is a dataset matching issues (= problem statements) and corresponding pull requests (PRs = problem solutions) of a selection of well maintained Java GitHub repositories. The original purpose was to serve as training and evaluation data for ML models concerned with fault localization and automated program repair of complex code bases. However, there might be more use cases that could benefit from this data. \\nInspired by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/feedback-to-code/mozzarella."},
	{"name":"llmfao","keyword":"benchmark","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dustalov/llmfao","creator_name":"Dmitry Ustalov","creator_url":"https://huggingface.co/dustalov","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLarge Language Model Feedback Analysis and Optimization (LLMFAO)\\n\\t\\n\\nThe original Crowdsourced LLM Benchmark dataset in files prompts.parqet and outputs.parquet was kindly provided by the team at llmonitor.com under a CC¬†BY 4.0 license. This dataset can be conveniently processed with Evalica (arXiv).\\n"},
	{"name":"llm_physical_safety_benchmark","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kumitang/llm_physical_safety_benchmark","creator_name":"Yung-Chen Tang","creator_url":"https://huggingface.co/kumitang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLLM Physical Safety Benchmark in Drone Control\\n\\t\\n\\nThis benchmark consists of four datasets designed to evaluate the performance of Large Language Models (LLMs) in controlling drones and their vulnerability to physical attacks. The datasets are categorized into different types of attacks:\\n\\nDeliberate Attack: Contains 280 samples that evaluate the LLM's resistance to malicious use, testing its ability to recognize and reject commands intended to cause harm. Subcategories include‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kumitang/llm_physical_safety_benchmark."},
	{"name":"llm_physical_safety_benchmark","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TrustSafeAI/llm_physical_safety_benchmark","creator_name":"TrustSafeAI","creator_url":"https://huggingface.co/TrustSafeAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLLM Physical Safety Benchmark in Drone Control\\n\\t\\n\\nThis benchmark consists of four datasets designed to evaluate the performance of Large Language Models (LLMs) in controlling drones and their vulnerability to physical attacks. The datasets are categorized into different types of attacks:\\n\\nDeliberate Attack: Contains 280 samples that evaluate the LLM's resistance to malicious use, testing its ability to recognize and reject commands intended to cause harm. Subcategories include‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TrustSafeAI/llm_physical_safety_benchmark."},
	{"name":"LOGIC-701-instruct","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLOGIC-701 (instruct)\\n\\t\\n\\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\\n"},
	{"name":"SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","keyword":"benchmarks","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSWE-Bench Verified O1 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExecutive Summary\\n\\t\\n\\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities using their native tool calling capabilities on the SWE-Bench Verified dataset, achieving a 45.8% success rate across 500 test instances.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset was generated using the CodeAct framework, which aims to improve‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results."},
	{"name":"Bills","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zli12321/Bills","creator_name":"LZX","creator_url":"https://huggingface.co/zli12321","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThis repository contains benchmark datasets for evaluating Large Language Model (LLM)-based topic discovery methods and comparing them against traditional topic models.  These datasets provide a valuable resource for researchers studying topic modeling and LLM capabilities in this domain.  The work is described in the following paper: Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs.  Original data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zli12321/Bills."},
	{"name":"Wiki","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zli12321/Wiki","creator_name":"LZX","creator_url":"https://huggingface.co/zli12321","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThis repository contains benchmark datasets for LLM-based topic discovery and traditional topic models.  These datasets allow for comparison of different topic modeling approaches, including LLMs.  Original data source: GitHub\\nPaper: LLM-based Topic Discovery\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBills Dataset\\n\\t\\n\\nThe Bills Dataset is a collection of legislative documents with 32,661 bill summaries (train) from the 110th‚Äì114th U.S. Congresses, categorized into 21 top-level and 112‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zli12321/Wiki."},
	{"name":"teamcraft_data","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/teamcraft/teamcraft_data","creator_name":"TeamCraft","creator_url":"https://huggingface.co/teamcraft","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for TeamCraft\\n\\t\\n\\nThe TeamCraft dataset is designed to develop multi-modal, multi-agent collaboration in Minecraft. It features 55,000 task variants defined by multi-modal prompts and procedurally generated expert demonstrations.\\nThis repository contains the data for the validation set and its visualizations. \\nTo use the validation set, download TeamCraft-Data-Valid.zip and extract using unzip TeamCraft-Data-Valid.zip.\\nIn addition, the training set is available in two‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/teamcraft/teamcraft_data."},
	{"name":"SportsGen","keyword":"benchmarks","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/huuuyeah/SportsGen","creator_name":"Yebowen Hu","creator_url":"https://huggingface.co/huuuyeah","description":"Dataset and scripts for sports analyzing tasks proposed in research: When Reasoning Meets Information Aggregation: A Case Study with Sports Narratives  Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Wenlin Yao, Hassan Foroosh, Dong Yu, Fei Liu  Accepted to main conference of EMNLP 2024, Miami, Florida, USA  Arxiv Paper\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbstract\\n\\t\\n\\nReasoning is most powerful when an LLM accurately aggregates relevant information. We examine the critical role of information aggregation in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huuuyeah/SportsGen."},
	{"name":"Arabic_Openai_MMMLU","keyword":"benchmarks","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic_Openai_MMMLU","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArabic Multilingual Massive Multitask Language Understanding (MMMLU)\\n\\t\\n\\nThe MMLU is a widely recognized benchmark for assessing general knowledge attained by AI models. It covers a broad range of topics across 57 different categories, from elementary-level knowledge to advanced professional subjects like law, physics, history, and computer science.\\nWe have extracted the Arabic subset from the MMMLU test set, which was translated by professional human translators. This dataset, now‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic_Openai_MMMLU."},
	{"name":"steshin-2023-lohi","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/scbirlab/steshin-2023-lohi","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLo-Hi Benchmark\\n\\t\\n\\nData from Simon Steshin, Lo-Hi: Practical ML Drug Discovery Benchmark, available from the GitHub repositiory. We used schemist (which in turn uses RDKit)\\nto add molecuar weight, Murcko scaffold, Crippen cLogP, and topological surface area.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nFrom the original README:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHit Identification\\n\\t\\n\\nThe goal of the Hit Identification task is to find novel molecules that have desirable property, but are dissimilar from the molecules‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/steshin-2023-lohi."},
	{"name":"fang-2023-biogen-adme","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/scbirlab/fang-2023-biogen-adme","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBiogen ADME dataset (public data)\\n\\t\\n\\nData from Fang et al., Prospective Validation of Machine Learning Algorithms for Absorption, Distribution, Metabolism, and Excretion Prediction: An Industrial Perspective, available from the GitHub repositiory. We used schemist (which in turn uses RDKit)\\nto add molecuar weight, Murcko scaffold, Crippen cLogP, and topological surface area, and to generate scaffold splits.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nFrom the original README:\\n\\nTo benefit the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/fang-2023-biogen-adme."},
	{"name":"MixEval-X","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","description":"\\n\\n\\nüöÄ Project Page | üìú arXiv | üë®‚Äçüíª Github | üèÜ Leaderboard | üìù blog | ü§ó HF Paper | ùïè Twitter\\n\\n\\n\\n\\n\\n\\n\\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations‚Äô flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X."},
	{"name":"scips_qa","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zorpsoon/scips_qa","creator_name":"Prasoon Bajpai","creator_url":"https://huggingface.co/zorpsoon","description":"zorpsoon/scips_qa dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"jnli","keyword":"benchmark","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zenless-lab/jnli","creator_name":"Zenless Lab","creator_url":"https://huggingface.co/zenless-lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJGLUE[JNLI]: Japanese General Language Understanding Evaluation\\n\\t\\n\\nJNLI(yahoojapan/JGLUE) is a Japanese version of the NLI (Natural Language Inference) dataset. \\nNLI is a task to recognize the inference relation that a premise sentence has to a hypothesis sentence. \\nThe inference relations are entailment, contradiction, and neutral.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\nRepository: yahoojapan/JGLUE\\nPaper: [More Information Needed]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zenless-lab/jnli."},
	{"name":"SWE-Bench-Verified-O1-reasoning-high-results","keyword":"benchmarks","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results","creator_name":"Alejandro Cuadron Lafuente","creator_url":"https://huggingface.co/AlexCuadron","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSWE-Bench Verified O1 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExecutive Summary\\n\\t\\n\\nThis repository contains verified reasoning traces from the O1 model evaluating software engineering tasks. Using OpenHands + CodeAct v2.2, we tested O1's bug-fixing capabilities on the SWE-Bench Verified dataset, achieving a 28.8% success rate across 500 test instances.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset was generated using the CodeAct framework, which aims to improve code generation through enhanced action-based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlexCuadron/SWE-Bench-Verified-O1-reasoning-high-results."},
	{"name":"oab_bench","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/felipeoes/oab_bench","creator_name":"Felipe Oliveira","creator_url":"https://huggingface.co/felipeoes","description":"\\n\\t\\n\\t\\t\\n\\t\\tOABench: Brazilian Bar Exams Benchmark Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nOABench is a benchmark dataset designed to evaluate the performance of Large Language Models (LLMs) on Brazilian legal exams. It is based on the Unified Bar Exam of the Brazilian Bar Association (OAB), a comprehensive and challenging exam required for law graduates to practice law in Brazil. This dataset provides a rigorous and realistic testbed for LLMs in the legal domain, covering a wide range of legal topics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/felipeoes/oab_bench."},
	{"name":"superglue","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Hyukkyu/superglue","creator_name":"Hyukkyu Kang","creator_url":"https://huggingface.co/Hyukkyu","description":"\\n\\t\\n\\t\\t\\n\\t\\tSuperGLUE Benchmark Datasets\\n\\t\\n\\nThis repository contains the SuperGLUE benchmark datasets. Each dataset is available as a separate configuration, making it easy to load individual datasets using the datasets library.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Descriptions\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDatasets Included\\n\\t\\n\\n\\nBoolQ: A question-answering task where each example consists of a short passage and a yes/no question about the passage. The questions are provided anonymously and unsolicited by users of the Google search‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Hyukkyu/superglue."},
	{"name":"jamp","keyword":"benchmark","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zenless-lab/jamp","creator_name":"Zenless Lab","creator_url":"https://huggingface.co/zenless-lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJamp: Controlled Japanese Temporal Inference Dataset for Evaluating Generalization Capacity of Language Models\\n\\t\\n\\nJamp(tomo-vv/temporalNLI_dataset) is the Japanese temporal inference benchmark. \\nThis dataset consists of templates, test data, and training data. \\nTemplate subset containing template, time format, or time span in their names are split based on tense fragment, time format, \\nor time span, respectively.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zenless-lab/jamp."},
	{"name":"jsem","keyword":"benchmark","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/zenless-lab/jsem","creator_name":"Zenless Lab","creator_url":"https://huggingface.co/zenless-lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJSeM: Japanese semantic test suite (Japanese FraCaS and extensions)\\n\\t\\n\\nÂèôËø∞ÊñáÈñì„ÅÆÂê´ÊÑèÈñ¢‰øÇ„ÅØ„ÄÅË®ÄË™ûÂ≠¶„Å´„Åä„ÅÑ„Å¶„ÅØÊÑèÂë≥Ë´ñ„ÅÆ‰∏≠ÂøÉÁöÑ„Å™Ë™¨ÊòéÂØæË±°„ÅÆ‰∏Ä„Å§„Åß„ÅÇ„Çã„Å®„Å®„ÇÇ„Å´„ÄÅÁêÜË´ñ„ÇíÊ§úË®º„Åô„Çã„Åü„ÇÅ„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Å®„Åó„Å¶Áî®„ÅÑ„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\\n„Åæ„ÅüËøëÂπ¥„ÅÆËá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„Å´„Åä„ÅÑ„Å¶„ÅØ„ÄÅÂê´ÊÑèÈñ¢‰øÇË™çË≠ò(Recognizing Textual Entailment: RTE)„ÅåÊÑèÂë≥Âá¶ÁêÜ„Çø„Çπ„ÇØ„ÅÆ‰∏≠Ê†∏„Å®„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ\\n\\nÂâçÊèê(premise)Ôºö‰∏Ä„Å§„ÅÆÊñá\\n‰ªÆË™¨(hypothesis)Ôºö‰∏Ä„Å§„ÅÆÊñá\\nÂà§ÂÆö(answer)Ôºö1.„Å®2.„ÅÆÈñì„Å´Âê´ÊÑèÈñ¢‰øÇ„Åå„ÅÇ„Çã„Åã„Å©„ÅÜ„Åã„Å´„Å§„ÅÑ„Å¶„ÅÆÊØçË™ûË©±ËÄÖ„ÅÆÂà§Êñ≠Ôºàentailment, neutral„ÅÇ„Çã„ÅÑ„ÅØcontradiction)\\n\\n„Åì„ÅÆ„ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß„ÅØ„ÄÅFraCaS test suiteÔºàCooper et al.(1996)„ÅßÂÖ¨Èñã„Åï„Çå„Åü„Ç™„É™„Ç∏„Éä„É´„ÅÆ„ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà, \\n„Åä„Çà„Å≥Bill MacCartneyÊ∞è„Å´„Çà„ÇãÂêå„Çª„ÉÉ„Éà„ÅÆXMLÁâàÔºâ„ÅÆÊñπÈáù„Å´„Å™„Çâ„ÅÑ„ÄÅË®ÄË™ûÁèæË±°„Åî„Å®„Å´Âê´ÊÑèÈñ¢‰øÇ„ÅÆ„ÉÜ„Çπ„Éà„Çí„Åæ„Å®„ÇÅ„Å¶„ÅÑ„Åæ„Åô„ÄÇ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zenless-lab/jsem."},
	{"name":"spider-test-portuguese","keyword":"benchmark","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Boakpe/spider-test-portuguese","creator_name":"Breno","creator_url":"https://huggingface.co/Boakpe","description":"\\n\\t\\n\\t\\t\\n\\t\\tSpider Dataset - Vers√£o em Portugu√™s\\n\\t\\n\\nEste reposit√≥rio cont√©m a tradu√ß√£o para portugu√™s da parti√ß√£o de teste do dataset Spider, um benchmark para a tarefa de Text-to-SQL.\\n\\n\\t\\n\\t\\t\\n\\t\\tSobre esta tradu√ß√£o\\n\\t\\n\\nA tradu√ß√£o da parti√ß√£o \\\"test\\\" do Spider (contendo 2.147 inst√¢ncias) foi realizada seguindo um processo rigoroso:\\n\\nTradu√ß√£o inicial: Utilizando a API do GPT-4o mini da OpenAI\\nRevis√£o manual: Todas as 2.147 quest√µes foram revisadas e validadas manualmente\\nCrit√©rios de tradu√ß√£o:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Boakpe/spider-test-portuguese."},
	{"name":"tmmluplus","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ikala/tmmluplus","creator_name":"iKala","creator_url":"https://huggingface.co/ikala","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTMMLU+ : Large scale traditional chinese massive multitask language understanding\\n\\t\\n\\n\\n\\n\\nWe present TMMLU+, a traditional Chinese massive multitask language understanding dataset. TMMLU+ is a multiple-choice question-answering dataset featuring 66 subjects, ranging from elementary to professional level.\\n\\nThe TMMLU+ dataset is six times larger and contains more balanced subjects compared to its predecessor, TMMLU. We have included benchmark results in TMMLU+ from closed-source models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ikala/tmmluplus."},
	{"name":"blockchain-benchmark","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/revflask/blockchain-benchmark","creator_name":"Mayank Panjiyara","creator_url":"https://huggingface.co/revflask","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for LLM Blockchain Benchmark\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Blockchain Benchmark Dataset is a comprehensive collection of data specifically curated for benchmarking Language Models (LMs) in the domain of blockchain technology. This dataset is designed to facilitate research and development in natural language understanding within the blockchain domain.\\nA complete list of tasks: ['general-reasoning', 'code', 'math']\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/revflask/blockchain-benchmark."},
	{"name":"blockchain-benchmark-formatted","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/revflask/blockchain-benchmark-formatted","creator_name":"Mayank Panjiyara","creator_url":"https://huggingface.co/revflask","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for LLM Blockchain Benchmark\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Blockchain Benchmark Dataset is a comprehensive collection of data specifically curated for benchmarking Language Models (LMs) in the domain of blockchain technology. This dataset is designed to facilitate research and development in natural language understanding within the blockchain domain.\\nA complete list of tasks: ['general-reasoning', 'code', 'math']\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/revflask/blockchain-benchmark-formatted."},
	{"name":"MixEval","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MixEval/MixEval","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","description":"\\n\\n\\nüè† Homepage | üë®‚Äçüíª Github | üèÜ Leaderboard | üìú arXiv | üìù blog | ü§ó HF Paper | ùïè Twitter\\n\\n\\n\\n\\n\\n\\n\\nBenchmark correlations (%) with Chatbot Arena Elo, against the total costs of evaluating a single GPT-3.5-Turbo-0125 model. MixEval and MixEval-Hard show the highest correlations with Arena Elo and Arena Elo (En) among leading benchmarks. We reference the crowdsourcing price for Amazon Mechanical Turk ($0.05 per vote) when estimating the cost of evaluating a single model on Chatbot Arena‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval."},
	{"name":"procgen","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EpicPinkPenguin/procgen","creator_name":"Marcus Fechner","creator_url":"https://huggingface.co/EpicPinkPenguin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProcgen Benchmark\\n\\t\\n\\nThis dataset contains expert trajectories generated by a PPO reinforcement learning agent trained on each of the 16 procedurally-generated gym environments from the Procgen Benchmark. The environments were created on distribution_mode=easy and with unlimited levels.\\nDisclaimer: This is not an official repository from OpenAI.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Usage\\n\\t\\n\\nRegular usage (for environment bigfish):\\nfrom datasets import load_dataset\\ntrain_dataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/EpicPinkPenguin/procgen."},
	{"name":"OllaBench","keyword":"benchmark","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theResearchNinja/OllaBench","creator_name":"Tam Nguyen","creator_url":"https://huggingface.co/theResearchNinja","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nLarge Language Models (LLMs) have the potential to enhance Agent-Based Modeling by better representing complex interdependent cybersecurity systems, improving cybersecurity threat modeling and risk management. Evaluating LLMs in this context is crucial for legal compliance and effective application development. Existing LLM evaluation frameworks often overlook the human factor and cognitive computing capabilities essential for interdependent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/theResearchNinja/OllaBench."},
	{"name":"hwtcm","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Monor/hwtcm","creator_name":"Monor Huang","creator_url":"https://huggingface.co/Monor","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis dataset can be used to evaluate the capabilities of large language models in traditional Chinese medicine and contains multiple-choice, multiple-answer, and true/false questions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChangelog\\n\\t\\n\\n\\n2024-08-28: Added 7226 questions.\\n2024-08-09: The benchmark code is available at https://github.com/huangxinping/HWTCMBench.\\n2024-08-02: System prompts are removed to ensure the purity of the evaluation results.\\n2024-07-20: Debut.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExamples‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Monor/hwtcm."},
	{"name":"comic-eval-benchmark","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gctian/comic-eval-benchmark","creator_name":"TianGuicheng","creator_url":"https://huggingface.co/gctian","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for comic-eval-benchmark\\n\\t\\n\\n\\n\\n‰∏≠Êñá‰∫åÊ¨°ÂÖÉÊº´ÁîªÈ¢ÜÂüüÁöÑÂü∫ÂáÜËØÑ‰º∞Êï∞ÊçÆÈõÜÔºåÂåÖÂê´‰∏äÂçÉÈÉ®Êº´Áîª‰ΩúÂìÅÁöÑ‰ΩúËÄÖ‰ø°ÊÅØ„ÄÅÁîªÈ£é„ÄÅÂú∫ÊôØ„ÄÅÁ±ªÂûã„ÄÅÂâßÊÉÖÁ≠âÁª¥Â∫¶ÁöÑÈÄâÊã©È¢òËØÑ‰º∞ÔºåÂÖ± 41175 ‰∏™ÂçïÈÄâÈ¢ò„ÄÇ\\nÂèØ‰Ωú‰∏∫‰∫åÊ¨°ÂÖÉÂûÇÁõ¥È¢ÜÂüüÂ§ßÊ®°ÂûãÁöÑËØÑ‰º∞Âü∫ÂáÜ„ÄÇ\\n‰ª•‰∏ãÊòØ‰ΩúËÄÖÂü∫‰∫éBaichuan2-13BÂæÆË∞ÉÁöÑ‰∫åÊ¨°ÂÖÉÈ¢ÜÂüüÂûÇÁõ¥Â§ßÊ®°ÂûãÔºåÂú®Ê≠§Êï∞ÊçÆÈõÜ‰∏äÁöÑËØÑ‰º∞ÁªìÊûúÔºö\\n\\n\\t\\n\\t\\t\\nÊ®°Âûã\\nzero-shot\\n3-shot\\n\\n\\n\\t\\t\\nQwen-7b\\n33.647\\n36.439\\n\\n\\nChatGLM3-6b\\n34.373\\n37.015\\n\\n\\nBaiChuan2-13b\\n37.416\\n39.08\\n\\n\\nBaiChuan2-13b-ÂæÆË∞É\\n41.035\\n41.086\\n\\n\\nYi-34b\\n50.103\\n45.606\\n\\n\\n\\t\\n\\nÊ¨¢ËøéË¥°ÁåÆÊõ¥Â§ö‰∫åÊ¨°ÂÖÉÈ¢ÜÂüüËØ≠ÊñôÂèä‰∫åÊ¨°ÂÖÉÂ§ßÊ®°ÂûãÔºåÂ¶ÇÈúÄËØÑÊµãËØ∑ËÅîÁ≥ª‰ΩúËÄÖËé∑ÂèñËØÑÊµãËÑöÊú¨„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n‰∏≠Êñá‰∫åÊ¨°ÂÖÉÈ¢ÜÂüüÊº´ÁîªÂü∫ÂáÜËØÑ‰º∞Êï∞ÊçÆÈõÜ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gctian/comic-eval-benchmark."},
	{"name":"PEACE","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/microsoft/PEACE","creator_name":"Microsoft","creator_url":"https://huggingface.co/microsoft","description":"\\n\\t\\n\\t\\t\\n\\t\\tPEACE: Empowering Geologic Map Holistic Understanding with MLLMs\\n\\t\\n\\n[Code] [Paper] [Data]\\n\\n    \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nWe construct a geologic map benchmark, GeoMap-Bench, to evaluate the performance of MLLMs on geologic map understanding across different abilities, the overview of it is as shown in below Table.\\n\\n  \\n    \\n      Property\\n      Description\\n    \\n  \\n  \\n    \\n      Source\\n      USGS(English)\\n    \\n    \\n      CGS(Chinese)\\n    \\n    \\n      Content\\n      Image-question pair‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/microsoft/PEACE."},
	{"name":"GiftEval","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/GiftEval","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\\n\\t\\n\\t\\t\\n\\t\\tGIFT-Eval\\n\\t\\n\\n\\n\\nWe present GIFT-Eval, a benchmark designed to advance zero-shot time series forecasting by facilitating evaluation across diverse datasets. GIFT-Eval includes 23 datasets covering 144,000 time series and 177 million data points, with data spanning seven domains, 10 frequencies, and a range of forecast lengths. This benchmark aims to set a new standard, guiding future innovations in time series foundation models.\\nTo facilitate the effective pretraining and evaluation of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/GiftEval."},
	{"name":"GiftEvalPretrain","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/GiftEvalPretrain","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\\n\\t\\n\\t\\t\\n\\t\\tGIFT-Eval Pre-training Datasets\\n\\t\\n\\nPretraining dataset aligned with GIFT-Eval that has 71 univariate and 17 multivariate datasets, spanning seven domains and 13 frequencies, totaling 4.5 million time series and 230 billion data points. Notably this collection of data has no leakage issue with the train/test split and can be used to pretrain foundation models that can be fairly evaluated on GIFT-Eval.\\nüìÑ Paper\\nüñ•Ô∏è Code\\nüìî Blog Post\\nüèéÔ∏è Leader Board\\n\\n\\t\\n\\t\\n\\t\\n\\t\\tEthical Considerations‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/GiftEvalPretrain."},
	{"name":"MM-TelecoBench","keyword":"benchmark","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Exploration-Lab/MM-TelecoBench","creator_name":"Exploration Lab","creator_url":"https://huggingface.co/Exploration-Lab","description":"Exploration-Lab/MM-TelecoBench dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"UWB_IMU_GT_QDrone_Benchmark_Dataset","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/QDrone/UWB_IMU_GT_QDrone_Benchmark_Dataset","creator_name":"Zara Arj","creator_url":"https://huggingface.co/QDrone","description":"For additional details, please visit our website: https://benchmark.qdrone.ausmlab.com.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQ-Drone UWB Benchmark Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nWe present the Q-Drone UWB Benchmark, a unique dataset derived from experiments conducted using the Q-Drone system‚Äîa UAV equipped with a UWB network at York University. This dataset encompasses data from five different sites, including an indoor environment, an open sports field, an area near a glass building, a semi-open tunnel, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QDrone/UWB_IMU_GT_QDrone_Benchmark_Dataset."},
	{"name":"FACTS-grounding-public","keyword":"benchmark","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/FACTS-grounding-public","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFACTS Grounding 1.0 Public Examples\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t860 public FACTS Grounding examples from Google DeepMind and Google Research\\n\\t\\n\\nFACTS Grounding is a benchmark from Google DeepMind and Google Research designed to measure the performance of AI Models on factuality and grounding. \\n‚ñ∂ FACTS Grounding Leaderboard on Kaggle‚ñ∂ Technical Report‚ñ∂ Evaluation Starter Code‚ñ∂ Google DeepMind Blog Post\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nThe FACTS Grounding benchmark evaluates the ability of Large Language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/FACTS-grounding-public."},
	{"name":"Health_Benchmarks","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yesilhealth/Health_Benchmarks","creator_name":"Yesil Health AI","creator_url":"https://huggingface.co/yesilhealth","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHealth Benchmarks Dataset\\n\\t\\n\\nThe Health Benchmarks Dataset is a specialized resource for evaluating large language models (LLMs) in different medical specialties. It provides structured question-answer pairs designed to test the performance of AI models in understanding and generating domain-specific knowledge.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPrimary Purpose\\n\\t\\n\\nThis dataset is built to:\\n\\nBenchmark LLMs in medical specialties and subfields.\\nAssess the accuracy and contextual understanding of AI in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yesilhealth/Health_Benchmarks."},
	{"name":"GPQA-diamond-ClaudeR1","keyword":"benchmarks","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/spawn99/GPQA-diamond-ClaudeR1","creator_name":"Cavit Erginsoy","creator_url":"https://huggingface.co/spawn99","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for GPQA Diamond Reasoning Benchmark\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nA benchmark dataset for evaluating hybrid AI architectures, comparing reasoning-augmented LLMs (DeepSeek R1) against standalone models (Claude Sonnet 3.5). Contains 198 physics questions with:\\n\\nGround truth answers and explanations\\nModel responses from multiple architectures\\nGranular token usage and cost metrics\\nDifficulty metadata and domain categorization\\n\\nCurated by: LLM‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/spawn99/GPQA-diamond-ClaudeR1."},
	{"name":"optillmbench","keyword":"benchmark","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/codelion/optillmbench","creator_name":"Asankhaya Sharma","creator_url":"https://huggingface.co/codelion","description":"\\n\\t\\n\\t\\t\\n\\t\\tOptiLLMBench Dataset\\n\\t\\n\\nA benchmark dataset for evaluating test-time optimization and scaling capabilities of language models.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nOptiLLMBench contains 500 carefully selected challenging problems across multiple domains:\\n\\nMathematical reasoning (from competition_math)\\nCode generation (from HumanEval)\\nWord problems (from GSM8K)\\nMultiple choice reasoning (from MMLU)\\nLogical deduction (from BBH)\\n\\nEach example is chosen to benefit from test-time optimization‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/codelion/optillmbench."},
	{"name":"Fraud-R1-LLM-Defense-Fraud-Benchmark","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Chouoftears/Fraud-R1-LLM-Defense-Fraud-Benchmark","creator_name":"Shenzhe Zhu","creator_url":"https://huggingface.co/Chouoftears","description":"\\n\\t\\n\\t\\t\\n\\t\\t Fraud-R1 : A Comprehensive Benchmark for Assessing LLM Robustness Against Fraud and Phishing Inducement\\n\\t\\n\\nShu Yang*, Shenzhe Zhu*, Zeyu Wu, Keyu Wang, Junchi Yao, Junchao Wu, Lijie Hu, Mengdi Li, Derek F. Wong, Di Wang‚Ä†\\n(*Contribute equally, ‚Ä†Corresponding author)\\nüòÉ Github | üìú Project Page | üìù arxiv\\n‚ùóÔ∏èContent Warning: This repo contains examples of harmful language.\\n\\n\\t\\n\\t\\n\\t\\n\\t\\tüì∞ News\\n\\t\\n\\n\\n2025/02/16: ‚ùóÔ∏èWe have released our evaluation code.\\n2025/02/16: ‚ùóÔ∏èWe have released our dataset.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Chouoftears/Fraud-R1-LLM-Defense-Fraud-Benchmark."},
	{"name":"BaxBench","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LogicStar/BaxBench","creator_name":"LogicStar.ai","creator_url":"https://huggingface.co/LogicStar","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nBaxBench is a coding benchmark constructed to measure the ability of code generation models and agents to generate correct and secure code. It consists of 392 backend development tasks, which are constructed by combining 28 scenarios that describe the backend functionalities to implement and 14 backend frameworks defining the implementation tools. To assess the correctness and security of the solutions, the benchmark uses end-to-end functional tests and practical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LogicStar/BaxBench."},
	{"name":"HashtagPrediction","keyword":"benchmark","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Twitter/HashtagPrediction","creator_name":"Twitter","creator_url":"https://huggingface.co/Twitter","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHashtag Prediction Dataset from paper TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations\\n\\t\\n\\n  \\nThis repo contains the Hashtag prediction dataset from our paper TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations. \\n[arXiv]\\n[HuggingFace Models]\\n[Github repo]\\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDownload\\n\\t\\n\\nUse the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Twitter/HashtagPrediction."},
	{"name":"protein_chain_conformational_states","keyword":"benchmark","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PDBEurope/protein_chain_conformational_states","creator_name":"Protein Data Bank in Europe","creator_url":"https://huggingface.co/PDBEurope","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSchema description:\\n\\t\\n\\nThe manually curated dataset of open-closed monomers is included here as benchmarking_monomeric_open_closed_conformers.csv.  \\nColumn descriptions:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSchema description:\\n\\t\\n\\nThe manually curated dataset of open-closed monomers is included here as benchmarking_monomeric_open_closed_conformers.csv.  \\nColumn descriptions:\\n\\nUNP_ACC | UniProt accession code\\nUNP_START | Start of UniProt sequence for given PDBe entries\\nUNP_END | End of UniProt sequence for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PDBEurope/protein_chain_conformational_states."},
	{"name":"NeoEvalPlusN_benchmark","keyword":"benchmark","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/ChuckMcSneed/NeoEvalPlusN_benchmark","creator_name":"Charles McSneed","creator_url":"https://huggingface.co/ChuckMcSneed","description":"Since automatic open source benchmark leaderboard got flooded with incoherent overtrained cheater meme models, I decided to take the matters in my own hands and create my own set of proprietary tests. The aim of these tests is not to see how smart the model is, but to see how good it is at execution of commands and creative writing in a reasonably quantifiable way. All tests are executed with temperature and top P‚âà0 and rep. penalty=1 in koboldcpp. Model-appropriate format is used, unless it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChuckMcSneed/NeoEvalPlusN_benchmark."},
	{"name":"VLM4Bio","keyword":"benchmarks","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imageomics/VLM4Bio","creator_name":"HDR Imageomics Institute","creator_url":"https://huggingface.co/imageomics","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for VLM4Bio\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstructions for downloading the dataset\\n\\t\\n\\n\\nInstall Git LFS\\nGit clone the VLM4Bio repository to download all metadata and associated files\\nRun the following commands in a terminal:\\n\\n\\n\\ngit clone https://huggingface.co/datasets/imageomics/VLM4Bio\\ncd VLM4Bio\\n\\nDownloading and processing bird images\\n\\nTo download the bird images, run the following command:\\n\\nbash download_bird_images.sh\\n\\n\\nThis should download the bird images inside‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imageomics/VLM4Bio."},
	{"name":"r1-aime-figures","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/benxh/r1-aime-figures","creator_name":"Benjamim Shehu","creator_url":"https://huggingface.co/benxh","description":"Ran the simplescaling/aime25_figures dataset through R1-671B to get 64 runs per each entry. Results are as follows:\\nR1 - 671B\\n\\nCONSENSUS @ 64: 53.33% (16/30)\\n\\nPASS @ 64: 76.67% (23/30)\\n\\nCompleted: 30, Partial: 0\\n\\n"},
	{"name":"rank1-run-files","keyword":"benchmark","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jhu-clsp/rank1-run-files","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","description":"\\n\\t\\n\\t\\t\\n\\t\\trank1-run-files: Pre-computed Run Files for Reranking Evaluation\\n\\t\\n\\nüìÑ Paper | üöÄ GitHub Repository\\nThis dataset contains pre-computed run files used by the rank1 family of models on various retrieval benchmarks. These files are what were used for top-k rereranking and also include the re-annotated DL19 qrels. These files are needed to download to reproduce our results.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBenchmarks Included\\n\\t\\n\\nThe dataset includes run files for the following benchmarks:\\n\\nBEIR (multiple‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/rank1-run-files."}
]
;
