const data_for_modality_embeddings = 
[
	{"name":"sa-data","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/paolodegasperis/sa-data","creator_name":"Paolo De Gasperis","creator_url":"https://huggingface.co/paolodegasperis","description":"\n\t\n\t\t\n\t\tStoria dell'Arte Dataset (SA-Data)\n\t\n\n \n\n\t\n\t\t\n\t\tğŸ“Œ Descrizione del Dataset\n\t\n\nIl dataset SA-Data Ã¨ una raccolta strutturata di articoli della rivista Storia dell'Arte (https://www.storiadellarterivista.it/) digitalizzati e arricchiti con metadati dettagliati e rappresentazioni semantiche. Ãˆ stato creato per supportare la ricerca accademica e le applicazioni di elaborazione del linguaggio naturale.\n\n\t\n\t\t\n\t\n\t\n\t\tğŸ” Contenuto\n\t\n\nIl dataset include:\n\n1050 articoli pubblicati tra il 1969 eâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/paolodegasperis/sa-data.","first_N":5,"first_N_keywords":["token-classification","text-retrieval","Italian","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"swissprot_protein_embeddings-ankh","keyword":"embedding","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pitagoras-alves/swissprot_protein_embeddings-ankh","creator_name":"PitÃ¡goras Alves","creator_url":"https://huggingface.co/pitagoras-alves","description":"\n\t\n\t\t\n\t\tProtein Language Model Embeddings ğŸ”¢\n\t\n\nProtein embeddings for Uniprot/Swiss-Prot sequences.\n\n\t\n\t\t\nName\nModel ğŸ¤–\nVector Length ğŸ“\nFile Size\n\n\n\t\t\nemb.ankh_large.parquet\nankh-large\n1536\n3.4G\n\n\nemb.ankh_base.parquet\nankh-base\n768\n1.7G\n\n\n\t\n\n","first_N":5,"first_N_keywords":["mit","100K<n<1M","doi:10.57967/hf/5180","ğŸ‡ºğŸ‡¸ Region: US","biology"],"keywords_longer_than_N":true},
	{"name":"5000-podcast-conversations-with-metadata-and-embedding-dataset","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ReadyAi/5000-podcast-conversations-with-metadata-and-embedding-dataset","creator_name":"ReadyAI","creator_url":"https://huggingface.co/ReadyAi","description":"\n\t\n\t\t\n\t\tğŸ—‚ï¸ ReadyAI - 5,000 Podcast Conversations with Metadata and Embedding Dataset\n\t\n\nReadyAI, operating subnet 33 on the Bittensor Network is an open-source initiative focused on low-cost, resource-minimal pipelines for structuring raw data for AI applications.\nThis dataset is part of the ReadyAI Conversational Genome Project, leveraging the Bittensor decentralized network.\nAI runs on structured data â€” and this dataset bridges the gap between raw conversation transcripts and structuredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ReadyAi/5000-podcast-conversations-with-metadata-and-embedding-dataset.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MMEB-train","keyword":"embedding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Y-J-Ju/MMEB-train","creator_name":"Yeong-Joon Ju","creator_url":"https://huggingface.co/Y-J-Ju","description":"\n\t\n\t\t\n\t\tMassive Multimodal Embedding Benchmark\n\t\n\nThe training data split used for training VLM2Vec models in the paper VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks (ICLR 2025).\nMMEB benchmark covers 4 meta tasks and 36 datasets meticulously selected for evaluating capabilities of multimodal embedding models.\nDuring training, we utilize 20 out of the 36 datasets.\nFor evaluation, we assess performance on the 20 in-domain (IND) datasets and the remaining 16â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Y-J-Ju/MMEB-train.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"gtl-hids-embeddings","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Hmehdi515/gtl-hids-embeddings","creator_name":"Hasan Mehdi","creator_url":"https://huggingface.co/Hmehdi515","description":"\n\t\n\t\t\n\t\tNetwork Traffic Embeddings Dataset\n\t\n\n\n\t\n\t\t\n\t\tModel Description\n\t\n\nThis dataset contains embeddings generated from the CICIDS2017 network traffic dataset using a fine-tuned Meta-Llama-3.1-70B-Instruct model. The embeddings represent network traffic flows formatted in a structured way to capture key network traffic characteristics.\n\n\t\n\t\t\n\t\tStructure of Embeddings Files\n\t\n\n\n\t\n\t\t\n\t\tcombined.npy\n\t\n\nThe combined.npy file contains a NumPy array of shape (N, D) where:\n\nN is the total numberâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Hmehdi515/gtl-hids-embeddings.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"maltese_embeddings","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DGurgurov/maltese_embeddings","creator_name":"Daniil Gurgurov","creator_url":"https://huggingface.co/DGurgurov","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository contains three distinct datasets focusing on Maltese word embeddings:\n\nGloVe Maltese Word Embeddings\nEmbeddings generated using GloVe on the \"korpus_malti\" dataset, the largest Maltese corpus available.\n\nWord2Vec Maltese Word Embeddings\nWord embeddings for Maltese obtained using Word2Vec trained on the \"korpus_malti\" dataset.\n\nPPMI Maltese Word Embeddings\nPointwise Mutual Information (PPMI) based word embeddings generated from ConceptNet data via SVDâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DGurgurov/maltese_embeddings.","first_N":5,"first_N_keywords":["Maltese","mit","1M - 10M","text","Text"],"keywords_longer_than_N":true},
	{"name":"openalex-multilingual-embeddings","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GlobalCampus/openalex-multilingual-embeddings","creator_name":"Global Campus","creator_url":"https://huggingface.co/GlobalCampus","description":"\n\t\n\t\t\n\t\tOpenAlex Multilingual Embeddings\n\t\n\nThis dataset contains multilingual text embeddings of all records in OpenAlex with a title or an abstract from the snapshot of 2023-10-20.\nThe dataset was created for the FORAS project to investigate the efficacy of \ndifferent methods of searching in databases of academic publications. All scripts will be available in a GitHub repository. \nThe project is supported by a grant from the Dutch Research Council (grant no. 406.22.GO.048)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/GlobalCampus/openalex-multilingual-embeddings.","first_N":5,"first_N_keywords":["cc0-1.0","100M - 1B","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"movie_descriptors_small","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mt0rm0/movie_descriptors_small","creator_name":"Mario Tormo Romero","creator_url":"https://huggingface.co/mt0rm0","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis dataset is a subset from Kaggle's The Movie Dataset that contains only name, release year and overview for some movies from the original dataset.\nIt is intended as a toy dataset for learning about embeddings in a workshop from the AI Service Center Berlin-Brandenburg at the Hasso Plattner Institute.\nThis dataset has a bigger version here.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset has 28655 rows and 3 columns:\n\n'name': includesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mt0rm0/movie_descriptors_small.","first_N":5,"first_N_keywords":["sentence-similarity","English","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"movie_descriptors","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mt0rm0/movie_descriptors","creator_name":"Mario Tormo Romero","creator_url":"https://huggingface.co/mt0rm0","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis dataset is a subset from Kaggle's The Movie Dataset that contains only name, release year and overview for every film in the original dataset that has that information complete.\nIt is intended as a toy dataset for learning about embeddings in a workshop from the AI Service Center Berlin-Brandenburg at the Hasso Plattner Institute.\nThis dataset has a smaller version here.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset has 44435 rowsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mt0rm0/movie_descriptors.","first_N":5,"first_N_keywords":["sentence-similarity","English","cc0-1.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"oscar-en-minilm-2m","keyword":"embeddings","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jamescalam/oscar-en-minilm-2m","creator_name":"James Briggs","creator_url":"https://huggingface.co/jamescalam","description":"\n\t\n\t\t\n\t\n\t\n\t\tOscar EN 2M Embeddings\n\t\n\nThis dataset contains 2M sentences extracted from the English subset of the OSCAR dataset, and encoded into sentence embeddings using the sentence-transformers/all-MiniLM-L6-v2 model.\n","first_N":5,"first_N_keywords":["sentence-similarity","no-annotation","other","extended|oscar","English"],"keywords_longer_than_N":true},
	{"name":"pwesuite-eval","keyword":"embedding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zouharvi/pwesuite-eval","creator_name":"VilÃ©m Zouhar","creator_url":"https://huggingface.co/zouharvi","description":"\n\n\t\n\t\t\n\t\tPWESuite-Eval\n\t\n\nDataset composed of multiple smaller datasets used for the evaluation of phonetic word embeddings.\nSee code for evaluation here.\nIf you use this dataset/evaluation, please cite the paper at LREC-COLING 2024:\n@inproceedings{zouhar-etal-2024-pwesuite,\n    title = \"{PWES}uite: Phonetic Word Embeddings and Tasks They Facilitate\",\n    author = \"Zouhar, Vil{\\'e}m  and\n      Chang, Kalvin  and\n      Cui, Chenxuan  and\n      Carlson, Nate B.  and\n      Robinson, Nathanielâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zouharvi/pwesuite-eval.","first_N":5,"first_N_keywords":["multilingual","English","Amharic","Bengali","Swahili"],"keywords_longer_than_N":true},
	{"name":"cifar100-enriched","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/renumics/cifar100-enriched","creator_name":"Renumics","creator_url":"https://huggingface.co/renumics","description":"The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images\nper class. There are 500 training images and 100 testing images per class. There are 50000 training images and 10000 test images. The 100 classes are grouped into 20 superclasses.\nThere are two labels per image - fine label (actual class) and coarse label (superclass).","first_N":5,"first_N_keywords":["image-classification","crowdsourced","found","monolingual","extended|other-80-Million-Tiny-Images"],"keywords_longer_than_N":true},
	{"name":"blbooks-parquet-embedded","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/blbooks-parquet-embedded","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\n\t\n\t\t\n\t\tDataset Card for \"blbooks-parquet-embedded\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["text-generation","fill-mask","other","language-modeling","masked-language-modeling"],"keywords_longer_than_N":true},
	{"name":"local-emoji-search-gte","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pszemraj/local-emoji-search-gte","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","description":"\n\t\n\t\t\n\t\tlocal emoji semantic search\n\t\n\nEmoji, their text descriptions and precomputed text embeddings with Alibaba-NLP/gte-large-en-v1.5 for use in emoji semantic search. \nThis work is largely inspired by the original emoji-semantic-search repo and aims to provide the data for fully local use, as the demo is not working as of a few days ago.\n\nThis repo only contains a precomputed embedding \"database\", equivalent to server/emoji-embeddings.jsonl.gz in the original repo, to be used as theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/local-emoji-search-gte.","first_N":5,"first_N_keywords":["feature-extraction","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Indian-Supreme-Court-Judgements-Chunked","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vihaannnn/Indian-Supreme-Court-Judgements-Chunked","creator_name":"Vihaan Nama","creator_url":"https://huggingface.co/vihaannnn","description":"\n\t\n\t\t\n\t\tIndian Supreme Court Judgements Chunked\n\t\n\n\n\t\n\t\t\n\t\tExecutive Summary\n\t\n\nThe dataset aims to address the chronic backlog in the Indian judiciary system, particularly in the Supreme Court, by creating a dataset optimized for legal language models (LLMs). The dataset will consist of pre-processed, chunked, and embedded textual data derived from the Supreme Court's judgment PDFs.\n\n\t\n\t\t\n\t\tProblem and Importance - Motivation\n\t\n\nIndian courts are overwhelmed with pending cases, with theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vihaannnn/Indian-Supreme-Court-Judgements-Chunked.","first_N":5,"first_N_keywords":["feature-extraction","text-classification","sentence-similarity","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"Core-S2L1C-DeCUR","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S2L1C-DeCUR","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\n\n\t\n\t\t\n\t\tCore-S2L1C-DeCUR  ğŸŸ¥ğŸŸ©ğŸŸ¦ğŸŸ§ğŸŸ¨ğŸŸª ğŸ›°ï¸\n\t\n\n\n\t\n\t\t\nDataset\nModality\nNumber of Embeddings\nSensing Type\nTotal Comments\nSource Dataset\nSource Model\nSize\n\n\n\t\t\nCore-S2L1C-DeCUR\nSentinel-2 (Level 1C)\n56,147,150\nMulti-Spectral\nGeneral-Purpose Global\nCore-S2L1C\nDeCUR\n342.7 GB\n\n\n\t\n\n\n\t\n\t\n\t\n\t\tContent\n\t\n\n\n\t\n\t\t\nField\nTypeDescription\n\n\n\t\t\nunique_id\nstring\nhash generated from geometry, time, product_id, and embedding model\n\n\nembedding\narray\nraw embedding array\n\n\ngrid_cell\nstring\nMajor TOM cell\n\n\ngrid_row_uâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S2L1C-DeCUR.","first_N":5,"first_N_keywords":["cc-by-sa-4.0","10M - 100M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"commonvoice-12.0-arabic-voice-converted-speakers","keyword":"embedding","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmodar/commonvoice-12.0-arabic-voice-converted-speakers","creator_name":"Modar M. Alfadly","creator_url":"https://huggingface.co/xmodar","description":"These are the speakers' embeddings and information for the Voice Converted Arabic Common Voice 12.0 dataset.\n","first_N":5,"first_N_keywords":["cc0-1.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Core-S1RTC-DeCUR","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S1RTC-DeCUR","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\n\n\t\n\t\t\n\t\tCore-S1RTC-DeCUR ğŸ“¡âš¡ğŸ›°ï¸\n\t\n\n\n\t\n\t\t\nDataset\nModality\nNumber of Embeddings\nSensing Type\nTotal Comments\nSource Dataset\nSource Model\nSize\n\n\n\t\t\nCore-S1RTC-SSL4EO\nSentinel-1 RTC\n36,748,875\nSAR\nGeneral-Purpose Global\nCore-S1RTC\nDeCUR\nGB\n\n\n\t\n\n\n\t\n\t\n\t\n\t\tContent\n\t\n\n\n\t\n\t\t\nField\nTypeDescription\n\n\n\t\t\nunique_id\nstring\nhash generated from geometry, time, product_id, and embedding model\n\n\nembedding\narray\nraw embedding array\n\n\ngrid_cell\nstring\nMajor TOM cell\n\n\ngrid_row_u\nint\nMajor TOM cell rowâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S1RTC-DeCUR.","first_N":5,"first_N_keywords":["cc-by-sa-4.0","10M<n<100M","Geospatial","arxiv:2412.05600","doi:10.57967/hf/5239"],"keywords_longer_than_N":true},
	{"name":"Core-S1RTC-SSL4EO","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S1RTC-SSL4EO","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\n\n\t\n\t\t\n\t\tCore-S1RTC-SSL4EO ğŸ“¡âš¡ğŸ›°ï¸\n\t\n\n\n\t\n\t\t\nDataset\nModality\nNumber of Embeddings\nSensing Type\nTotal Comments\nSource Dataset\nSource Model\nSize\n\n\n\t\t\nCore-S1RTC-SSL4EO\nSentinel-1 RTC\n36,748,875\nSAR\nGeneral-Purpose Global\nCore-S1RTC\nSSL4EO-ResNet50-MOCO\n332.5 GB\n\n\n\t\n\n\n\t\n\t\n\t\n\t\tContent\n\t\n\n\n\t\n\t\t\nField\nTypeDescription\n\n\n\t\t\nunique_id\nstring\nhash generated from geometry, time, product_id, and embedding model\n\n\nembedding\narray\nraw embedding array\n\n\ngrid_cell\nstring\nMajor TOM cell\n\n\ngrid_row_u\nint\nMajorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S1RTC-SSL4EO.","first_N":5,"first_N_keywords":["cc-by-sa-4.0","10M - 100M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"Core-S2L1C-SSL4EO","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S2L1C-SSL4EO","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\n\n\t\n\t\t\n\t\tCore-S2L1C-SSL4EO  ğŸŸ¥ğŸŸ©ğŸŸ¦ğŸŸ§ğŸŸ¨ğŸŸª ğŸ›°ï¸\n\t\n\n\n\t\n\t\t\nDataset\nModality\nNumber of Embeddings\nSensing Type\nTotal Comments\nSource Dataset\nSource Model\nSize\n\n\n\t\t\nCore-S2L1C-SSL4EO\nSentinel-2 (Level 1C)\n56,147,150\nMulti-Spectral\nGeneral-Purpose Global\nCore-S2L1C\nSSL4EO-ResNet50-DINO\n252.9 GB\n\n\n\t\n\n\n\t\t\n\t\n\t\tContent\n\t\n\n\n\t\n\t\t\nField\nType\nDescription\n\n\n\t\t\nunique_id\nstring\nhash generated from geometry, time, product_id, and embedding model\n\n\nembedding\narray\nraw embedding array\n\n\ngrid_cell\nstring\nMajor TOMâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S2L1C-SSL4EO.","first_N":5,"first_N_keywords":["cc-by-sa-4.0","10M - 100M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"USCode-QAPairs-Finetuning","keyword":"embeddings","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning","creator_name":"Archit Rastogi ","creator_url":"https://huggingface.co/ArchitRastogi","description":"\n\t\n\t\t\n\t\tUSCode-QueryPairs Dataset\n\t\n\nThis dataset contains query-answer pairs curated from the United States Code, suitable for fine-tuning any embedding model. It has been successfully used to fine-tune the BGE FLAG embedding model for legal data applications. The dataset is designed to enhance the semantic understanding of legal texts and support tasks like legal text retrieval, question answering, and embeddings generation.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nSource: United States Codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning.","first_N":5,"first_N_keywords":["text-retrieval","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5192024-xqq9-webapp","keyword":"embeddings","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-xqq9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5192024-xqq9-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"machine learning data generation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5192024-xqq9-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-xqq9-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"supreme-court-of-pak-judgments","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Ibtehaj10/supreme-court-of-pak-judgments","creator_name":"Ibtehaj Khan","creator_url":"https://huggingface.co/Ibtehaj10","description":"\n\t\n\t\t\n\t\tDataset Card for Pak-Law Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset contains legal documents related to Pakistani law. It includes text data, case details, embeddings generated using the mixedbread-ai/mxbai-embed-large-v1 model, and citation numbers.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset has the following features:\n\ntext: The text of the legal documents.\ncase_details: Details about the legal cases.\nembeddings: Embeddings of the text, generated using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ibtehaj10/supreme-court-of-pak-judgments.","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Arabic-finanical-rag-embedding-dataset","keyword":"embeddings","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic-finanical-rag-embedding-dataset","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","description":"\n\t\n\t\t\n\t\tArabic Version of The Finanical Rag Embedding Dataset\n\t\n\n\nThis dataset is tailored for fine-tuning embedding models in Retrieval-Augmented Generation (RAG) setups. It consists of 7,000 question-context pairs translated into Arabic, sourced from NVIDIA's 2023 SEC Filing Report. \nThe dataset is designed to improve the performance of embedding models by providing positive samples for financial question-answering tasks in Arabic.\nThis dataset is the Arabic version of the originalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic-finanical-rag-embedding-dataset.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Core-S2RGB-SigLIP","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S2RGB-SigLIP","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\n\n\t\n\t\t\n\t\tCore-S2RGB-SigLIP ğŸ”´ğŸŸ¢ğŸ”µ\n\t\n\n\n\t\n\t\t\nModality\nNumber of Embeddings\nSensing Type\nComments\nSource Dataset\nSource Model\nSize\n\n\n\t\t\nSentinel-2 Level 2A (RGB)\n20,212,974\nTrue Colour\nVision-Language Global\nCore-S2L2A\nSigLIP-SO400M-384\n41.3 GB\n\n\n\t\n\n\n\t\n\t\n\t\n\t\tContent\n\t\n\n\n\t\n\t\t\nField\nTypeDescription\n\n\n\t\t\nunique_id\nstring\nhash generated from geometry, time, product_id, and embedding model\n\n\nembedding\narray\nraw embedding array\n\n\ngrid_cell\nstring\nMajor TOM cell\n\n\ngrid_row_u\nint\nMajor TOM cell rowâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S2RGB-SigLIP.","first_N":5,"first_N_keywords":["cc-by-sa-4.0","10M - 100M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"Core-S2RGB-DINOv2","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S2RGB-DINOv2","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\n\n\t\n\t\t\n\t\tCore-S2RGB-DINOv2 ğŸ”´ğŸŸ¢ğŸ”µ\n\t\n\n\n\t\n\t\t\nDataset\nModality\nNumber of Embeddings\nSensing Type\nTotal Comments\nSource Dataset\nSource Model\nSize\n\n\n\t\t\nCore-S2RGB-SigLIP\nSentinel-2 Level 2A (RGB)\n56,147,150\nTrue Colour (RGB)\nGeneral-Purpose Global\nCore-S2L2A\nDINOv2\n223.1 GB\n\n\n\t\n\n\n\t\t\n\t\n\t\tContent\n\t\n\n\n\t\n\t\t\nField\nType\nDescription\n\n\n\t\t\nunique_id\nstring\nhash generated from geometry, time, product_id, and embedding model\n\n\nembedding\narray\nraw embedding array\n\n\ngrid_cell\nstring\nMajor TOM cell\n\n\ngrid_row_uâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S2RGB-DINOv2.","first_N":5,"first_N_keywords":["cc-by-sa-4.0","10M - 100M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"MMEB-train","keyword":"embedding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/MMEB-train","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tMassive Multimodal Embedding Benchmark\n\t\n\nThe training data split used for training VLM2Vec models in the paper VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks (ICLR 2025).\nMMEB benchmark covers 4 meta tasks and 36 datasets meticulously selected for evaluating capabilities of multimodal embedding models.\nDuring training, we utilize 20 out of the 36 datasets.\nFor evaluation, we assess performance on the 20 in-domain (IND) datasets and the remaining 16â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/MMEB-train.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Glove-Embedding","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liandarizkia/Glove-Embedding","creator_name":"annisa rizki liliandari","creator_url":"https://huggingface.co/liandarizkia","description":"liandarizkia/Glove-Embedding dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["sentence-similarity","English","cc0-1.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Chunked-Indian-Supreme-Court-Judgements","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vihaannnn/Chunked-Indian-Supreme-Court-Judgements","creator_name":"Vihaan Nama","creator_url":"https://huggingface.co/vihaannnn","description":"\n\t\n\t\t\n\t\tIndian Supreme Court Judgements Chunked\n\t\n\n","first_N":5,"first_N_keywords":["token-classification","feature-extraction","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"italian-embedding-finetune-dataset","keyword":"embeddings","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ArchitRastogi/italian-embedding-finetune-dataset","creator_name":"Archit Rastogi ","creator_url":"https://huggingface.co/ArchitRastogi","description":"\n\t\n\t\t\n\t\tItalian-BERT-FineTuning-Embeddings\n\t\n\nThis repository contains a comprehensive dataset designed for fine-tuning BERT-based Italian embedding models. The dataset aims to enhance performance on tasks such as information retrieval, semantic search, and embeddings generation.\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset leverages the C4 dataset (Italian subset) and employs advanced techniques like sliding window segmentation and in-document sampling to create high-quality, diverse examplesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ArchitRastogi/italian-embedding-finetune-dataset.","first_N":5,"first_N_keywords":["text-classification","question-answering","Italian","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"DS569k","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/donnyb/DS569k","creator_name":"Donny Bertucci","creator_url":"https://huggingface.co/donnyb","description":"Want to analyze some proteins, but lack embeddings? Want to perform vector similarity search? Want a context of known proteins embeddings? Look no further!\nThis repository is a dataset of Reviewed Swiss-Prot Proteins. Each protein I compute the embeddings for ESM2 (6 layer model) and ProteinCLIP. \n\n\t\n\t\t\n\t\n\t\n\t\tSpecs\n\t\n\nSee the data viewer for all information. Most of the metadata on each protein and the sequences themself come from Reviewed Swiss-Prot Proteins.\nImportant columns\n\naccession: theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/donnyb/DS569k.","first_N":5,"first_N_keywords":["feature-extraction","mit","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"mmE5-Synthetic","keyword":"embedding","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mmE5/mmE5-Synthetic","creator_name":"mmE5","creator_url":"https://huggingface.co/mmE5","description":"\n\t\n\t\t\n\t\tmmE5 Synthetic Data\n\t\n\n","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"embedded_movies_small","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/acloudfan/embedded_movies_small","creator_name":"raj","creator_url":"https://huggingface.co/acloudfan","description":"This dataset was created from the HuggingFace dataset AIatMongoDB/embedded_movies\nWhy was it needed?\n\nThe original dataset is close to 25 GB, for learning and experiments it is an overkill\nData in the dataset needs to be cleaned up e.g., some features are Null that requires extra care\nSome of the embeddings are missing\n\nHow to use?\n\nUse for sentiment analysis\nText similarity (plot)\nEmbeddings : ready to use with vector DB & search libraries\n\n\n\n\t\n\t\n\t\n\t\tdataset_info:\n  features:\n  - name: ratedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acloudfan/embedded_movies_small.","first_N":5,"first_N_keywords":["text-classification","table-question-answering","fill-mask","sentence-similarity","English"],"keywords_longer_than_N":true},
	{"name":"datacomp-small-clip","keyword":"embeddings","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fondant-ai/datacomp-small-clip","creator_name":"Fondant","creator_url":"https://huggingface.co/fondant-ai","description":"\n    \n        \n    \n\n\n\n    \n        Production-ready \n        data processing made \n        easy \n        and \n        shareable\n    \n    \n    Explore the Fondant docs Â»\n    \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for fondant-ai/datacomp-small-clip\n\t\n\n\n\nThis is a dataset containing image urls and their CLIP embeddings, based on the datacomp_small dataset, and processed with fondant.\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nLarge (image) datasets are often unwieldy to use due to theirâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fondant-ai/datacomp-small-clip.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","cc-by-4.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"glove.6B.50d.umap.2d","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mt0rm0/glove.6B.50d.umap.2d","creator_name":"Mario Tormo Romero","creator_url":"https://huggingface.co/mt0rm0","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis dataset is a UMAP 2D-projection of the glove.6B.50d embeddings from Stanford. It is intended as a fast reference for visualizing embeddings in a workshop from the AI Service Center Berlin-Brandenburg at the Hasso Plattner Institute.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe embeddings have a vocabulary of 400k tokens with 2 dimensions each token.\nCurated by: Mario Tormo Romero\nLicense: cc0-1.0\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\nThis Dataset has beenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mt0rm0/glove.6B.50d.umap.2d.","first_N":5,"first_N_keywords":["sentence-similarity","English","cc0-1.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"korean_parallel_sentences_v1.1","keyword":"embedding","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lemon-mint/korean_parallel_sentences_v1.1","creator_name":"Lemon Mint","creator_url":"https://huggingface.co/lemon-mint","description":"\n\t\n\t\t\n\t\tDataset Card for Korean Parallel Sentences Ver 1.1\n\t\n\nThis dataset card provides information about the Korean Parallel Sentences Ver 1.1 dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Korean Parallel Sentences Ver 1.1 dataset is a collection of parallel sentences in Korean and English.\nAlthough the factual accuracy of the data is not guaranteed, it has been designed to ensure accurate and consistent translation style between English and Korean.\n\nCurated by:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lemon-mint/korean_parallel_sentences_v1.1.","first_N":5,"first_N_keywords":["translation","text2text-generation","Korean","English","mit"],"keywords_longer_than_N":true},
	{"name":"Arab3M-Triplets","keyword":"embeddings","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arab3M-Triplets","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","description":"\n\t\n\t\t\n\t\tArab3M-Triplets\n\t\n\nThis dataset is designed for training and evaluating models using contrastive learning techniques, particularly in the context of natural language understanding. The dataset consists of triplets: an anchor sentence, a positive sentence, and a negative sentence. The goal is to encourage models to learn meaningful representations by distinguishing between semantically similar and dissimilar sentences.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nFormat: Parquet\nNumber of rows: 3.03â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arab3M-Triplets.","first_N":5,"first_N_keywords":["sentence-similarity","Arabic","apache-2.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"legal-rag-embedding-dataset","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/axondendriteplus/legal-rag-embedding-dataset","creator_name":"amand","creator_url":"https://huggingface.co/axondendriteplus","description":"\n\t\n\t\t\n\t\tLegal Embedding Dataset\n\t\n\nThis dataset was created to finetune embedding models for generating domain-specific embeddings on Indian legal texts, specifically SEBI (Securities and Exchange Board of India) documents.\nData SourcePublicly available SEBI PDF documents were parsed and processed.\nData Preparation  \n\nPDFs were parsed to extract raw text, Text was chunked into manageable segments.  \nFor each chunk, a question was generated using gpt-4o-mini.  \nEach question is directlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/axondendriteplus/legal-rag-embedding-dataset.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","sentence-similarity","English","mit"],"keywords_longer_than_N":true},
	{"name":"sed-ua-small-sts-v1","keyword":"embeddings","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/suntez13/sed-ua-small-sts-v1","creator_name":"Oleksandr Mediakov","creator_url":"https://huggingface.co/suntez13","description":"\n\t\n\t\t\n\t\tSyntheticEmbeddingDataset-UA: small-v1\n\t\n\nSmall (100k+) sythetic dataset for fine-tuning text embedding models for Ukraininan language (STS task)\n","first_N":5,"first_N_keywords":["sentence-similarity","Ukrainian","bsd-3-clause","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"XTD-10","keyword":"embedding","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Haon-Chen/XTD-10","creator_name":"Haonan Chen","creator_url":"https://huggingface.co/Haon-Chen","description":"\n\t\n\t\t\n\t\tXTD Multimodal Multilingual Data With Instruction\n\t\n\nThis dataset contains datasets (with English instruction) used for evaluating the multilingual capability of a multimodal embedding model, including seven languages:\n\nit, es, ru, zh, pl, tr, ko\n\n\n\t\n\t\t\n\t\tDataset Usage\n\t\n\n\nThe instruction on the query side is: \"Retrieve an image of this caption.\"\nThe instruction on the document side is: \"Represent the given image.\"\nEach example contains a query and a set of targets. The first one inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Haon-Chen/XTD-10.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"mmE5-MMEB-hardneg","keyword":"embedding","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","description":"\n\t\n\t\t\n\t\tmmE5 Labeled Data\n\t\n\nThis dataset contains datasets used for the supervised finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\n\nMMEB (with hard negative)\nInfoSeek (from M-BEIR)\nTAT-DQA\nArxivQA\n\nGithub\n\n\t\n\t\t\n\t\n\t\n\t\tImage Preparation\n\t\n\nFirst, you should prepare the images used for training:\n\n\t\n\t\t\n\t\n\t\n\t\tImage Downloads\n\t\n\n\nDownload All Images Used in mmE5:\n\nYou can use the script provided in our source code to download all images usedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg.","first_N":5,"first_N_keywords":["English","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"mmE5-synthetic","keyword":"embedding","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/intfloat/mmE5-synthetic","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","description":"\n\t\n\t\t\n\t\tmmE5 Synthetic Data\n\t\n\nThis dataset contains synthetic datasets used for the finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\n\nClassification\nRetrieval\nVQA\n\nGithub\n\n\t\n\t\t\n\t\n\t\n\t\tImage Preparation\n\t\n\nFirst, you should prepare the images used for training:\n\n\t\n\t\t\n\t\n\t\n\t\tImage Downloads\n\t\n\n\nDownload mmE5-synthetic Images:\n\nRun the following command to download and extract the images only in this dataset.\nmkdir -p images && cd images\nwgetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-synthetic.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Embeddings__Ultimate_1Million_Movies_Dataset","keyword":"embeddings","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Remsky/Embeddings__Ultimate_1Million_Movies_Dataset","creator_name":"Jeremy Braun","creator_url":"https://huggingface.co/Remsky","description":"\n\t\n\t\t\n\t\tDataset Card for Embedding Enriched The Ultimate 1Million Movies Dataset (TMDB + IMDb)\n\t\n\nThis dataset contains movie metadata from TMDB sourced from Kaggle, with an added layer of embeddings and token counts for semantic search and ML applications.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nA daily-updated snapshot taken in early 2025 of an existing movie metadata collection, sourced from TMDB Movies Dataset 2025 on Kaggle, enriched with 768-dimensionalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Remsky/Embeddings__Ultimate_1Million_Movies_Dataset.","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"saudi-dialect-test-samples","keyword":"embeddings","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/saudi-dialect-test-samples","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","description":"\n\t\n\t\t\n\t\tSaudi Dialect Test Samples\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 1280 Saudi dialect utterances across 44 categories, used for testing and evaluating the Omartificial-Intelligence-Space/SA-BERT-V1 model. The sentences represent a wide range of topics, from daily conversations to specialized domains.\n\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Fields\n\t\n\n\ncategory: The topic category of the utterance (one of 44 categories)\ntext: The Saudi dialect textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/saudi-dialect-test-samples.","first_N":5,"first_N_keywords":["text-classification","Arabic","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true}
]
;
