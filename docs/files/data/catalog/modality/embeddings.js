const data_for_modality_embeddings = 
[
	{"name":"local-embeddings-2022","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tLocal Embeddings Dataset\n\t\n\nMulti-temporal satellite imagery dataset for phenology embedding training.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains multi-spectral satellite tiles across 6 months (April-September 2022) with 16 bands per tile.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nlocal_embeddings/\nâ”œâ”€â”€ alphaearth_embeddings_tiles_202204/  (263 tiles)\nâ”œâ”€â”€ alphaearth_embeddings_tiles_202205/  (263 tiles)\nâ”œâ”€â”€ alphaearth_embeddings_tiles_202206/  (263 tiles)\nâ”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/gabrielireland/local-embeddings-2022.","url":"https://huggingface.co/datasets/gabrielireland/local-embeddings-2022","creator_name":"Gabriel Diaz Ireland","creator_url":"https://huggingface.co/gabrielireland","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-segmentation","cc-by-4.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"book-recommender-dataset","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tBook Recommender Dataset\n\t\n\nCSV exports from my Book Recommender pipeline. Includes cleaned metadata, category labels, emotion tags, and a tagged description file.\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\nbooks_cleaned.csv: Core cleaned book metadata.\nbooks_with_categories.csv: Adds multi-label categories column.\nbooks_with_emotions.csv: Adds emotion_* columns (one-hot or scores).\ntagged_description.txt: Preprocessed descriptions (one per line, or TSV).\n\n\n\t\n\t\t\n\t\tColumn Schema (example)\n\t\n\n\nbook_id (str)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/swayista/book-recommender-dataset.","url":"https://huggingface.co/datasets/swayista/book-recommender-dataset","creator_name":"ahmed","creator_url":"https://huggingface.co/swayista","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["tabular-classification","mit","10K<n<100K","Text","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"anti-echo-chamber-data","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tAnti-Echo Chamber Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains processed news articles with metadata and embeddings designed to break echo chambers by identifying opposing viewpoints across political spectrums.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Anti-Echo Chamber dataset consists of news articles scraped from diverse sources across the political spectrum, processed through a sophisticated pipeline that extracts:\n\nTopic embeddings using sentence transformers\nPoliticalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zanimal/anti-echo-chamber-data.","url":"https://huggingface.co/datasets/zanimal/anti-echo-chamber-data","creator_name":"Alexander","creator_url":"https://huggingface.co/zanimal","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-retrieval","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"movie_descriptors_small","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis dataset is a subset from Kaggle's The Movie Dataset that contains only name, release year and overview for some movies from the original dataset.\nIt is intended as a toy dataset for learning about embeddings in a workshop from the AI Service Center Berlin-Brandenburg at the Hasso Plattner Institute.\nThis dataset has a bigger version here.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset has 28655 rows and 3 columns:\n\n'name': includesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mt0rm0/movie_descriptors_small.","url":"https://huggingface.co/datasets/mt0rm0/movie_descriptors_small","creator_name":"Mario Tormo Romero","creator_url":"https://huggingface.co/mt0rm0","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","English","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"movie_descriptors","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis dataset is a subset from Kaggle's The Movie Dataset that contains only name, release year and overview for every film in the original dataset that has that information complete.\nIt is intended as a toy dataset for learning about embeddings in a workshop from the AI Service Center Berlin-Brandenburg at the Hasso Plattner Institute.\nThis dataset has a smaller version here.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset has 44435 rowsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mt0rm0/movie_descriptors.","url":"https://huggingface.co/datasets/mt0rm0/movie_descriptors","creator_name":"Mario Tormo Romero","creator_url":"https://huggingface.co/mt0rm0","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","English","cc0-1.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Embeddings__Ultimate_1Million_Movies_Dataset","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tDataset Card for Embedding Enriched The Ultimate 1Million Movies Dataset (TMDB + IMDb)\n\t\n\nThis dataset contains movie metadata from TMDB sourced from Kaggle, with an added layer of embeddings and token counts for semantic search and ML applications.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nA daily-updated snapshot taken in early 2025 of an existing movie metadata collection, sourced from TMDB Movies Dataset 2025 on Kaggle, enriched with 768-dimensionalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Remsky/Embeddings__Ultimate_1Million_Movies_Dataset.","url":"https://huggingface.co/datasets/Remsky/Embeddings__Ultimate_1Million_Movies_Dataset","creator_name":"Jeremy Braun","creator_url":"https://huggingface.co/Remsky","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"service-public","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ‡«ðŸ‡· Service-Public.fr practical sheets dataset (Administrative Procedures)\n\t\n\nThis dataset is derived from the official Service-Public.fr platform and contains practical information sheets and resources targeting both individuals (Particuliers) and entrepreneurs (Entreprendre). \nThe purpose of these sheets is to provide information on administrative procedures relating to a number of themes.\nThe data is publicly available on data.gouv.fr and has been processed and chunked forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AgentPublic/service-public.","url":"https://huggingface.co/datasets/AgentPublic/service-public","creator_name":"AgentPublic","creator_url":"https://huggingface.co/AgentPublic","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["French","etalab-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"CapRetrievalEn","keyword":"embeddings","description":"The english version of CapRetrieval introduced in the EMNLP 2025 Finding paper: [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings].\nCode: https://github.com/lxucs/CapRetrieval\nQueries and passages are translated automatically by GPT-4.1; all IDs and labels are kept the same as CapRetrieval. A few labels thus are not entirely accurate due to different language traits and expressions, but most labels should remain consistent.\nCapRetrieval evaluates theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lxucs/CapRetrievalEn.","url":"https://huggingface.co/datasets/lxucs/CapRetrievalEn","creator_name":"Liyan Xu","creator_url":"https://huggingface.co/lxucs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-ranking","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"supreme-court-of-pak-judgments","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tDataset Card for Pak-Law Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset contains legal documents related to Pakistani law. It includes text data, case details, embeddings generated using the mixedbread-ai/mxbai-embed-large-v1 model, and citation numbers.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset has the following features:\n\ntext: The text of the legal documents.\ncase_details: Details about the legal cases.\nembeddings: Embeddings of the text, generated using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ibtehaj10/supreme-court-of-pak-judgments.","url":"https://huggingface.co/datasets/Ibtehaj10/supreme-court-of-pak-judgments","creator_name":"Ibtehaj Khan","creator_url":"https://huggingface.co/Ibtehaj10","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"domain-translations","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tMultilingual Domain Name Translations Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 155,004 domain names with their multilingual translations across 20 languages. Each domain has been segmented into constituent words and translated while preserving semantic meaning and commercial appeal. The dataset is particularly valuable for domain name research, multilingual NLP tasks, and understanding how brand names and concepts translate across languages.\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/humbleworth/domain-translations.","url":"https://huggingface.co/datasets/humbleworth/domain-translations","creator_name":"HumbleWorth","creator_url":"https://huggingface.co/humbleworth","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","feature-extraction","multilingual","Arabic"],"keywords_longer_than_N":true},
	{"name":"Glove-Embedding","keyword":"embeddings","description":"liandarizkia/Glove-Embedding dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/liandarizkia/Glove-Embedding","creator_name":"annisa rizki liliandari","creator_url":"https://huggingface.co/liandarizkia","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","English","cc0-1.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"transformers_code_embeddings","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tTransformers Code Embeddings\n\t\n\nCompact index of function/class definitions from src/transformers/models/**/modeling_*.py for cross-model similarity. Built to help surface reusable code when modularizing models.\n\n\t\n\t\t\n\t\tContents\n\t\n\n\nembeddings.safetensors â€” float32, L2-normalized embeddings shaped [N, D].\ncode_index_map.json â€” {int_id: \"relative/path/to/modeling_*.py:SymbolName\"}.\ncode_index_tokens.json â€” {identifier: [sorted_unique_tokens]} for Jaccard.\n\n\n\t\n\t\t\n\t\tHow these were builtâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hf-internal-testing/transformers_code_embeddings.","url":"https://huggingface.co/datasets/hf-internal-testing/transformers_code_embeddings","creator_name":"Hugging Face Internal Testing Organization","creator_url":"https://huggingface.co/hf-internal-testing","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["code","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","embeddings","transformers-internal"],"keywords_longer_than_N":true},
	{"name":"drone-lsr","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tLight Stable Representations Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains aerial orthomosaic tiles captured at three different times of day (10:00, 12:00, and 15:00). The dataset is organized into three configurations: default (raw images + canopy height), dinov2_base (DINOv2 embeddings), and dinov3_sat (DINOv3 embeddings). All configurations share consistent train/test splits with matching tile identifiers for cross-referencing. The dataset is designed for trainingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mpg-ranch/drone-lsr.","url":"https://huggingface.co/datasets/mpg-ranch/drone-lsr","creator_name":"MPG Ranch","creator_url":"https://huggingface.co/mpg-ranch","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","image-to-image","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"usc-chroma-vecs-v1-chunks-v1-s4096-o512-sentence-transformers-static-retrieval-mrl-en-v1","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tusc-chroma-vecs-v1-chunks-v1-s4096-o512-sentence-transformers-static-retrieval-mrl-en-v1\n\t\n\nThis dataset contains a pre-built ChromaDB database with US Congressional legislation embeddings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains the ChromaDB files for legislation chunks with embeddings. The database can be loaded directly using ChromaDB's PersistentClient.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nimport chromadb\nfrom huggingface_hub import snapshot_download\n\n# Download the dataset\nlocal_dir =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hyperdemocracy/usc-chroma-vecs-v1-chunks-v1-s4096-o512-sentence-transformers-static-retrieval-mrl-en-v1.","url":"https://huggingface.co/datasets/hyperdemocracy/usc-chroma-vecs-v1-chunks-v1-s4096-o512-sentence-transformers-static-retrieval-mrl-en-v1","creator_name":"hyperdemocracy","creator_url":"https://huggingface.co/hyperdemocracy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["cc0-1.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US","chromadb","legal"],"keywords_longer_than_N":true},
	{"name":"cnil","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ‡«ðŸ‡· CNIL Deliberations Dataset\n\t\n\nThis dataset is a processed and embedded version of the official deliberations and decisions published by the CNIL (Commission Nationale de lâ€™Informatique et des LibertÃ©s), the French data protection authority.It includes a variety of legal documents such as opinions, recommendations, simplified norms, general authorizations, and formal decisions.\nThe original data is downloaded from the dedicated DILA open data repository and the dataset is alsoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AgentPublic/cnil.","url":"https://huggingface.co/datasets/AgentPublic/cnil","creator_name":"AgentPublic","creator_url":"https://huggingface.co/AgentPublic","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["French","etalab-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"legi","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ‡«ðŸ‡· French Consolidated Legislation Dataset (LEGI)\n\t\n\nThis dataset contains a semantic-ready and embedded version of the French full consolidated text of national legislation and regulations as published in the official LEGI database by LÃ©gifrance.\nThe original data is downloaded from the dedicated DILA open data repository and is also published on data.gouv.fr.\nThe original and full LEGI dataset includes:\n\nAll laws, codes, decrees, circulars, deliberations, decree-laws, ordinancesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AgentPublic/legi.","url":"https://huggingface.co/datasets/AgentPublic/legi","creator_name":"AgentPublic","creator_url":"https://huggingface.co/AgentPublic","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["French","etalab-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"FIGNEWS_generated_queries","keyword":"embedding","description":"This repository contains the FIGNEWS dataset with predicted queries, a core component used in the paper QAEncoder: Towards Aligned Representation Learning in Question Answering Systems.\nThe official implementation and related code are available on GitHub: https://github.com/IAAR-Shanghai/QAEncoder\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nModern QA systems entail retrieval-augmented generation (RAG) for accurate and trustworthy responses. However, the inherent gap between user queries and relevant documentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zr-wang/FIGNEWS_generated_queries.","url":"https://huggingface.co/datasets/zr-wang/FIGNEWS_generated_queries","creator_name":"Wang","creator_url":"https://huggingface.co/zr-wang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","arxiv:2409.20434","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"ESM2_embeddings_Human_Mouse","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tESM2-15B Human and Mouse protein embeddings\n\t\n\nThis dataset contains protein embeddings obtained through the ESM2-15B model for the Human and Mouse species.\nThe model used can be found here: https://huggingface.co/facebook/esm2_t48_15B_UR50D\n\n\t\n\t\t\n\t\tInput sequences\n\t\n\nProtein sequences were obtained from Swiss-Prot/Uniprot, meaning they were curated beforehand. The sequences were obtained from the following link in the month of May, 2025.\nLink:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Darkadin/ESM2_embeddings_Human_Mouse.","url":"https://huggingface.co/datasets/Darkadin/ESM2_embeddings_Human_Mouse","creator_name":"DarkAdin","creator_url":"https://huggingface.co/Darkadin","license_name":"Public Domain Dedication & License","license_url":"https://scancode-licensedb.aboutcode.org/pddl-1.0.html","language":"en","first_N":5,"first_N_keywords":["pddl","ðŸ‡ºðŸ‡¸ Region: US","biology","bioinformatics","llm"],"keywords_longer_than_N":true},
	{"name":"MMEB-train","keyword":"embedding","description":"\n\t\n\t\t\n\t\tMassive Multimodal Embedding Benchmark\n\t\n\nThe training data split used for training VLM2Vec models in the paper VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks (ICLR 2025).\nMMEB benchmark covers 4 meta tasks and 36 datasets meticulously selected for evaluating capabilities of multimodal embedding models.\nDuring training, we utilize 20 out of the 36 datasets.\nFor evaluation, we assess performance on the 20 in-domain (IND) datasets and the remaining 16â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/MMEB-train.","url":"https://huggingface.co/datasets/TIGER-Lab/MMEB-train","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"amzn_sec_db","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tAMZN SEC Filings â€“ Chunk-level Corpus (10-K, 10-Q, 8-K)\n\t\n\n\n  \n\n\nA ready-to-use, chunk-level corpus of Amazon's (AMZN) recent SEC filings\n(10-K, 10-Q, and 8-K).Each paragraph and sentence is stored together with rich metadata,\nmaking the dataset ideal for:\n\nsemantic search / RAG pipelines (ChromaDB, FAISS, Weaviate, â€¦)\nquestion-answering over financial filings\nexperimenting with financial-domain embeddings\n\n\nTime spanâ€ƒ: last 5 fiscal years (rolling window, as of 2025-05-11)Collection :â€¦ See the full description on the dataset page: https://huggingface.co/datasets/kurry/amzn_sec_db.","url":"https://huggingface.co/datasets/kurry/amzn_sec_db","creator_name":"Kurry Tran","creator_url":"https://huggingface.co/kurry","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-retrieval","question-answering","external:sec-edgar","English","mit"],"keywords_longer_than_N":true},
	{"name":"stamp-dataset","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tDataset êµ¬ì¡° ì„¤ëª…\n\t\n\nì´ ë””ë ‰í† ë¦¬ëŠ” STAMP í”„ë¡œì íŠ¸ì˜ ë°ì´í„°ì…‹ì„ í¬í•¨í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ êµ¬ì„±ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.\n\n\t\n\t\t\n\t\të””ë ‰í† ë¦¬ êµ¬ì¡°\n\t\n\ndataset/\nâ”œâ”€â”€ emb128/                        # 128ì°¨ì› ìž„ë² ë”© ë°ì´í„°\nâ”‚   â”œâ”€â”€ trace_embed/               # ì›ë³¸ ìž„ë² ë”© ë°ì´í„°\nâ”‚   â”œâ”€â”€ autoencoder_stage1/        # ì˜¤í† ì¸ì½”ë” í•™ìŠµ/ì¶”ë¡  ê²°ê³¼\nâ”‚   â”‚   â”œâ”€â”€ weight/                # í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸\nâ”‚   â”‚   â””â”€â”€ infer_result/          # ì¶”ë¡  ê²°ê³¼ (ìž ìž¬ ê³µê°„ ë°ì´í„°)\nâ”‚   â”œâ”€â”€ trace_latent/              # â†’ autoencoder_stage1/infer_result (ì‹¬ë³¼ë¦­ ë§í¬)\nâ”‚   â””â”€â”€ setting/                   # ì„¤ì • íŒŒì¼\nâ”œâ”€â”€ emb256/â€¦ See the full description on the dataset page: https://huggingface.co/datasets/selen-kim/stamp-dataset.","url":"https://huggingface.co/datasets/selen-kim/stamp-dataset","creator_name":"kim","creator_url":"https://huggingface.co/selen-kim","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["feature-extraction","English","apache-2.0","10M<n<100M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"local-administrations-directory","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ‡«ðŸ‡· French Local Administrations Directory Dataset\n\t\n\nThis dataset is a processed and embedded version of the public data Annuaire de lâ€™administration - Base de donnÃ©es locales (French Local Administrations Directory), published on data.gouv.fr.This information is also available on the official directory website of Service-Public.fr: https://lannuaire.service-public.fr/\nThe dataset provides semantic-ready, structured and chunked data of French local public entities, includingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AgentPublic/local-administrations-directory.","url":"https://huggingface.co/datasets/AgentPublic/local-administrations-directory","creator_name":"AgentPublic","creator_url":"https://huggingface.co/AgentPublic","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["French","etalab-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"XTD-10","keyword":"embedding","description":"\n\t\n\t\t\n\t\tXTD Multimodal Multilingual Data With Instruction\n\t\n\nThis dataset contains datasets (with English instruction) used for evaluating the multilingual capability of a multimodal embedding model, including seven languages:\n\nit, es, ru, zh, pl, tr, ko\n\n\n\t\n\t\t\n\t\tDataset Usage\n\t\n\n\nThe instruction on the query side is: \"Retrieve an image of this caption.\"\nThe instruction on the document side is: \"Represent the given image.\"\nEach example contains a query and a set of targets. The first one inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Haon-Chen/XTD-10.","url":"https://huggingface.co/datasets/Haon-Chen/XTD-10","creator_name":"Haonan Chen","creator_url":"https://huggingface.co/Haon-Chen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"lmd-dedup-supplements","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tLMD Deduplication Supplements\n\t\n\nThis repository provides pre-computed embedding files extracted from the Lakh MIDI Dataset (LMD-clean and LMD-full) using the CAugBERT and CLaMP-1024 models.These embeddings were used in our paper:\"On the De-duplication of the Lakh MIDI Dataset\" (ISMIR 2025)[Paper] | [GitHub Code]\n\n\n\t\n\t\t\n\t\n\t\n\t\tContents\n\t\n\nEach folder includes:\n\nembeddings.pt: Torch tensor of embeddings (shape: N Ã— D)  \nrefs.txt: List of MIDI filenames corresponding to each embedding rowâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jech2/lmd-dedup-supplements.","url":"https://huggingface.co/datasets/jech2/lmd-dedup-supplements","creator_name":"Eunjin Choi","creator_url":"https://huggingface.co/jech2","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","text","Text"],"keywords_longer_than_N":true},
	{"name":"imageomics-2025","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tAnonymized Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains aerial orthomosaic tiles captured at three different times of day (10:00, 12:00, and 15:00). The dataset is organized into three configurations: default (raw images + canopy height), dinov2_base (DINOv2 embeddings), and dinov3_sat (DINOv3 embeddings). All configurations share consistent train/test splits with matching tile identifiers for cross-referencing. The dataset is designed for training vision encodersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anondatasets/imageomics-2025.","url":"https://huggingface.co/datasets/anondatasets/imageomics-2025","creator_name":"anonymous","creator_url":"https://huggingface.co/anondatasets","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","image-to-image","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"feverous-gemma-300m-simple","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tFEVEROUS Claims with Embeddings\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains the claims from the FEVEROUS (Fact Extraction and VERification Over Unstructured and Structured information) dataset, with pre-computed embeddings for efficient claim-only classification.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nOriginal Dataset: FEVEROUS (Aly et al., 2021)\nTotal Claims: 87,026\nEmbedding Model: google/embeddinggemma-300m\nEmbedding Dimension: 768\nProcessing Date: October 17, 2025\nLabels:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/KingTechnician/feverous-gemma-300m-simple.","url":"https://huggingface.co/datasets/KingTechnician/feverous-gemma-300m-simple","creator_name":"Isaiah Freeman","creator_url":"https://huggingface.co/KingTechnician","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","English","cc-by-3.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"cifar10-enriched","keyword":"embeddings","description":"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images\nper class. There are 50000 training images and 10000 test images.\nThis version if CIFAR-10 is enriched with several metadata such as embeddings, baseline results and label error scores.","url":"https://huggingface.co/datasets/renumics/cifar10-enriched","creator_name":"Renumics","creator_url":"https://huggingface.co/renumics","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-classification","multi-class-image-classification","extended|cifar10","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"embedded_movies_small","keyword":"embeddings","description":"This dataset was created from the HuggingFace dataset AIatMongoDB/embedded_movies\nWhy was it needed?\n\nThe original dataset is close to 25 GB, for learning and experiments it is an overkill\nData in the dataset needs to be cleaned up e.g., some features are Null that requires extra care\nSome of the embeddings are missing\n\nHow to use?\n\nUse for sentiment analysis\nText similarity (plot)\nEmbeddings : ready to use with vector DB & search libraries\n\n\n\n\t\n\t\n\t\n\t\tdataset_info:\n  features:\n  - name: ratedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acloudfan/embedded_movies_small.","url":"https://huggingface.co/datasets/acloudfan/embedded_movies_small","creator_name":"raj","creator_url":"https://huggingface.co/acloudfan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","table-question-answering","fill-mask","sentence-similarity","English"],"keywords_longer_than_N":true},
	{"name":"philippine-budget-2025-embeddings-mpnet","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tPhilippine Budget 2025 - Vector Embeddings (all-mpnet-base-v2)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains vector embeddings of the 2025 People's Budget of the Philippines, a citizen-friendly overview of the PHP 6.326 trillion national budget published by the Department of Budget and Management (DBM).\n\n\t\n\t\t\n\t\tSource Document\n\t\n\nThese embeddings are based on the 2025 People's Enacted Budget (English version, revised as of April 22, 2025).\nDirect Download Link: 2025 People'sâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pageman/philippine-budget-2025-embeddings-mpnet.","url":"https://huggingface.co/datasets/pageman/philippine-budget-2025-embeddings-mpnet","creator_name":"The Pageman","creator_url":"https://huggingface.co/pageman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","text-retrieval","feature-extraction","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"infovqa_colqwen2_embeddings","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tInfoVQA ColQwen2.5 Embeddings\n\t\n\nThis dataset contains pre-computed embeddings for the InfoVQA dataset using the ColQwen2.5 model.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of three configurations:\n\n\t\n\t\t\n\t\tCorpus Configuration\n\t\n\nContains document images with their embeddings.\nfrom datasets import load_dataset\ncorpus = load_dataset(\"WenxingZhu/infovqa_colqwen2_embeddings\", \"corpus\", split=\"test\")\n\nFields:\n\ncorpus-id (int): Document identifier\nimage (Image): Original documentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WenxingZhu/infovqa_colqwen2_embeddings.","url":"https://huggingface.co/datasets/WenxingZhu/infovqa_colqwen2_embeddings","creator_name":"WenxingZhu","creator_url":"https://huggingface.co/WenxingZhu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","feature-extraction","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"TSMPD-US-Public-v1_1","keyword":"embedding","description":"\n\t\n\t\t\n\t\t[Updated with SBERT Embeddings + Search Notebook]\n\t\n\n\n\t\n\t\t\n\t\tTSMPDâ€‘US: U.S. Small Merchant Product Dataset + SBERT Embeddings + Search Notebook\n\t\n\nâš¡ New in this release (April 2025):\nSBERT vector embeddings for all products (MiniLMâ€‘L6)\nChunked Parquet format for scalable vector search\nJupyter notebook demo for live semantic queries\nThese additions make it easier to integrate small merchant data into RAG pipelines, grounding tasks, and real-time AI agents.\n\n\t\n\t\t\n\t\n\t\n\t\tAn open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Tokuhn/TSMPD-US-Public-v1_1.","url":"https://huggingface.co/datasets/Tokuhn/TSMPD-US-Public-v1_1","creator_name":"Tokuhn","creator_url":"https://huggingface.co/Tokuhn","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-retrieval","sentence-similarity","document-retrieval","semantic-similarity-classification","English"],"keywords_longer_than_N":true},
	{"name":"virginia-woolf-monologue-chunks","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tVirginia Woolf Monologue Chunks Dataset\n\t\n\nThis dataset contains 6 semantically chunked text segments derived from a contemporary monologue based on Virginia Woolf's seminal essay \"A Room of One's Own\" (1929). It comes pre-loaded with vector embeddings from three different models, making it a ready-to-use resource for a variety of NLP tasks.\nIn addition to the dataset itself, this repository includes a comprehensive embedding analysis, detailed statistics, and 7 visualizations to helpâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pageman/virginia-woolf-monologue-chunks.","url":"https://huggingface.co/datasets/pageman/virginia-woolf-monologue-chunks","creator_name":"The Pageman at Bettergov.ph","creator_url":"https://huggingface.co/pageman","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","text-retrieval","feature-extraction","English"],"keywords_longer_than_N":true},
	{"name":"CleanSinhalaTextCorpus","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ“š Cleaned Sinhala Text Corpus\n\t\n\nThe Cleaned Sinhala Text Corpus is a large-scale, high-quality Sinhala text dataset prepared and released by Remeinium.\n\n\t\n\t\t\n\t\tðŸ“„ Sample Data (randomly copied from 4 different files)\n\t\n\nà·€à·’à·€à·˜à¶­ à¶´à·”à¶©à·” à¶…à·€à·ƒà·Šà¶®à·à·€à·š à·€à·’à·€à·˜à¶­ à¶´à·”à¶©à·” à¶½à·à¶·à¶º à¶šà·à¶»à¶šà·à¶­à·Šà¶¸à¶š à·€à¶»à·Šà¶°à¶šà¶ºà¶š à¶‰à¶­à· à·€à·’à·à·à¶½à¶º à¶‘à¶º à¶´à¶»à·à·ƒà¶ºà·š à·€à·š à¶‘à¶±à·’à·ƒà· à¶šà·”à¶©à· à¶´à·Šà¶»à¶¯à·à¶± à·€à·à¶½à·Šà¶§à·“à¶ºà¶­à·à·€à¶šà¶§ à·€à·”à·€à¶¯ à·€à·’à·à·à¶½ à¶´à·Šà¶»à¶­à·’à¶¯à·à¶± à·€à·à¶½à·Šà¶§à·“à¶ºà¶­à·à·€à¶šà·Š à¶½à¶¶à· à¶¯à·“à¶¸à·š à·„à·à¶šà·’à¶ºà·à·€ à¶‡à¶­ à¶‘à¶±à¶¸à·Š à¶´à·Šà¶»à¶¯à·à¶± à·€à·à¶½à·Šà¶§à·“à¶ºà¶­à· à¶šà·”à¶©à· à¶´à¶»à·à·ƒà¶ºà¶šà·Š à¶­à·”à·… à·€à·à¶©à·’ à¶šà·’à¶»à·“à¶¸à·šà¶¯à·“ à¶´à·Šà¶»à¶­à·’à¶¯à·à¶± à·€à·à¶½à·’à¶§à·“à¶ºà¶­à·à·€ à·€à·’à·à·à¶½ à¶´à¶»à·à·ƒà¶ºà¶šà·Š à¶­à·”à·… à·€à·à¶©à·’ à·€à·š\nà¶¸à·™à·ƒà·šâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Remeinium/CleanSinhalaTextCorpus.","url":"https://huggingface.co/datasets/Remeinium/CleanSinhalaTextCorpus","creator_name":"Remeinium AI","creator_url":"https://huggingface.co/Remeinium","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","token-classification","Sinhala","cc-by-4.0","10M<n<100M"],"keywords_longer_than_N":true},
	{"name":"korean_parallel_sentences_v1.1","keyword":"embedding","description":"\n\t\n\t\t\n\t\tDataset Card for Korean Parallel Sentences Ver 1.1\n\t\n\nThis dataset card provides information about the Korean Parallel Sentences Ver 1.1 dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Korean Parallel Sentences Ver 1.1 dataset is a collection of parallel sentences in Korean and English.\nAlthough the factual accuracy of the data is not guaranteed, it has been designed to ensure accurate and consistent translation style between English and Korean.\n\nCurated by:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lemon-mint/korean_parallel_sentences_v1.1.","url":"https://huggingface.co/datasets/lemon-mint/korean_parallel_sentences_v1.1","creator_name":"Lemon Mint","creator_url":"https://huggingface.co/lemon-mint","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","text2text-generation","Korean","English","mit"],"keywords_longer_than_N":true},
	{"name":"MoodPulse","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ“Š MoodPulse: Processed Data and Embeddings for Emotion Analysis\n\t\n\nMoodPulse provides a self-contained dataset repository for use with the AffectiveLens projectâ€”an end-to-end NLP pipeline for emotion detection in text. It includes the full processing stack from raw text to final DistilBERT-based sentence embeddings, allowing researchers to bypass time-consuming preprocessing and directly train or benchmark models.\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ§¾ Dataset Description\n\t\n\nThis dataset builds upon theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/psyrishi/MoodPulse.","url":"https://huggingface.co/datasets/psyrishi/MoodPulse","creator_name":"Sanyam Sanjay Sharma","creator_url":"https://huggingface.co/psyrishi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","ðŸ‡ºðŸ‡¸ Region: US","emotion-classification"],"keywords_longer_than_N":true},
	{"name":"travail-emploi","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ‡«ðŸ‡· Travail Emploi website Dataset (French Minister of Labor and Employment)\n\t\n\nThis dataset is a processed and embedded version of public practical information sheets extracted from the official website of MinistÃ¨re du Travail et de lâ€™Emploi (Minister of Labor and Employment): travail-emploi.gouv.fr.\nThese datas are downloaded from the government Social Gouv GitHub repository.\nThe dataset provides semantic-ready, structured and chunked data of official content related to employmentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AgentPublic/travail-emploi.","url":"https://huggingface.co/datasets/AgentPublic/travail-emploi","creator_name":"AgentPublic","creator_url":"https://huggingface.co/AgentPublic","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["French","etalab-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"legal-rag-embedding-dataset","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tLegal Embedding Dataset\n\t\n\nThis dataset was created to finetune embedding models for generating domain-specific embeddings on Indian legal texts, specifically SEBI (Securities and Exchange Board of India) documents.\nData SourcePublicly available SEBI PDF documents were parsed and processed.\nData Preparation  \n\nPDFs were parsed to extract raw text, Text was chunked into manageable segments.  \nFor each chunk, a question was generated using gpt-4o-mini.  \nEach question is directlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/axondendriteplus/legal-rag-embedding-dataset.","url":"https://huggingface.co/datasets/axondendriteplus/legal-rag-embedding-dataset","creator_name":"amand","creator_url":"https://huggingface.co/axondendriteplus","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","sentence-similarity","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mmE5-synthetic","keyword":"embedding","description":"\n\t\n\t\t\n\t\tmmE5 Synthetic Data\n\t\n\nThis dataset contains synthetic datasets used for the finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\n\nClassification\nRetrieval\nVQA\n\nGithub\n\n\t\n\t\t\n\t\n\t\n\t\tImage Preparation\n\t\n\nFirst, you should prepare the images used for training:\n\n\t\n\t\t\n\t\n\t\n\t\tImage Downloads\n\t\n\n\nDownload mmE5-synthetic Images:\n\nRun the following command to download and extract the images only in this dataset.\nmkdir -p images && cd images\nwgetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-synthetic.","url":"https://huggingface.co/datasets/intfloat/mmE5-synthetic","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MoCa-CL-Pairs","keyword":"embedding","description":"\n\t\n\t\t\n\t\tMoCa Contrastive Learning Data\n\t\n\nðŸ  Homepage | ðŸ’» Code | ðŸ¤– MoCa-Qwen25VL-7B | ðŸ¤– MoCa-Qwen25VL-3B | ðŸ“š Datasets | ðŸ“„ Paper\nThis dataset contains datasets used for the supervised finetuning of MoCa (MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings):\n\nMMEB (with hard negative)\nInfoSeek (from M-BEIR)\nTAT-DQA\nArxivQAVisRAG\nViDoRe\nColPali\nE5 text pairs (can not release due to restrictions of Microsoft)\n\n\n\t\n\t\t\n\t\tImage Preparation\n\t\n\nFirst, youâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/moca-embed/MoCa-CL-Pairs.","url":"https://huggingface.co/datasets/moca-embed/MoCa-CL-Pairs","creator_name":"MoCa","creator_url":"https://huggingface.co/moca-embed","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"central_florida_native_plants","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tCentral Florida Native Plants Language Embeddings\n\t\n\nThis dataset contains language embeddings for 232 native plant species from Central Florida, extracted using the DeepSeek-V3 language model.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset provides pre-computed language embeddings for Central Florida plant species. Each species has been encoded using the prompt \"Ecophysiology of {species_name}:\" to capture semantic information about the plant's ecological characteristics.\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deepearth/central_florida_native_plants.","url":"https://huggingface.co/datasets/deepearth/central_florida_native_plants","creator_name":"DeepEarth","creator_url":"https://huggingface.co/deepearth","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Item-EMB","keyword":"embedding","description":"\n\t\n\t\t\n\t\tAL-GR/Item-EMB: Multi-modal Item Embeddings\n\t\n\nPaper: FORGE: Forming Semantic Identifiers for Generative Retrieval in Industrial Datasets\nCode: https://github.com/selous123/al_sid\nProject Page: https://huggingface.co/AL-GR\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository, AL-GR/Item-EMB, is a companion dataset to the main AL-GR generative recommendation dataset. It contains the 512-dimensional multi-modal embeddings for over 500 million items that appear in the AL-GR sequences.\nEach item isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AL-GR/Item-EMB.","url":"https://huggingface.co/datasets/AL-GR/Item-EMB","creator_name":"ALGR","creator_url":"https://huggingface.co/AL-GR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","image-feature-extraction","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Arabic-finanical-rag-embedding-dataset","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tArabic Version of The Finanical Rag Embedding Dataset\n\t\n\n\nThis dataset is tailored for fine-tuning embedding models in Retrieval-Augmented Generation (RAG) setups. It consists of 7,000 question-context pairs translated into Arabic, sourced from NVIDIA's 2023 SEC Filing Report. \nThe dataset is designed to improve the performance of embedding models by providing positive samples for financial question-answering tasks in Arabic.\nThis dataset is the Arabic version of the originalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic-finanical-rag-embedding-dataset.","url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic-finanical-rag-embedding-dataset","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-5192024-xqq9-webapp","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tjina-embeddings-v2-base-en-5192024-xqq9-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"machine learning data generation\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the jina-embeddings-v2-base-en-5192024-xqq9-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Faceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-xqq9-webapp.","url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-xqq9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"central-florida-native-plants-language-embeddings","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tCentral Florida Native Plants Language Embeddings\n\t\n\nThis dataset contains language embeddings for 232 native plant species from Central Florida, extracted using the DeepSeek-V3 language model.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset provides pre-computed language embeddings for Central Florida plant species. Each species has been encoded using the prompt \"Ecophysiology of {species_name}:\" to capture semantic information about the plant's ecological characteristics.\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deepearth/central-florida-native-plants-language-embeddings.","url":"https://huggingface.co/datasets/deepearth/central-florida-native-plants-language-embeddings","creator_name":"DeepEarth","creator_url":"https://huggingface.co/deepearth","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"wikipedia_qwen_06b","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tVector Database Dataset\n\t\n\nGenerated embeddings dataset for vector database training and evaluation with multiple format support.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 1,000,000 text samples with high-quality vector embeddings generated using Qwen/Qwen3-Embedding-0.6B from the wikimedia/wikipedia dataset. The dataset is designed for vector database training, similarity search, and retrieval tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nBase dataset: 1,000,000 samples with embeddingsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/maknee/wikipedia_qwen_06b.","url":"https://huggingface.co/datasets/maknee/wikipedia_qwen_06b","creator_name":"maknee","creator_url":"https://huggingface.co/maknee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"mmE5-MMEB-hardneg","keyword":"embedding","description":"\n\t\n\t\t\n\t\tmmE5 Labeled Data\n\t\n\nThis dataset contains datasets used for the supervised finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\n\nMMEB (with hard negative)\nInfoSeek (from M-BEIR)\nTAT-DQA\nArxivQA\n\nGithub\n\n\t\n\t\t\n\t\n\t\n\t\tImage Preparation\n\t\n\nFirst, you should prepare the images used for training:\n\n\t\n\t\t\n\t\n\t\n\t\tImage Downloads\n\t\n\n\nDownload All Images Used in mmE5:\n\nYou can use the script provided in our source code to download all images usedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg.","url":"https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"glove.6B.50d.umap.2d","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis dataset is a UMAP 2D-projection of the glove.6B.50d embeddings from Stanford. It is intended as a fast reference for visualizing embeddings in a workshop from the AI Service Center Berlin-Brandenburg at the Hasso Plattner Institute.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe embeddings have a vocabulary of 400k tokens with 2 dimensions each token.\nCurated by: Mario Tormo Romero\nLicense: cc0-1.0\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\nThis Dataset has beenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mt0rm0/glove.6B.50d.umap.2d.","url":"https://huggingface.co/datasets/mt0rm0/glove.6B.50d.umap.2d","creator_name":"Mario Tormo Romero","creator_url":"https://huggingface.co/mt0rm0","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","English","cc0-1.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"5000-podcast-conversations-with-metadata-and-embedding-dataset","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ—‚ï¸ ReadyAI - 5,000 Podcast Conversations with Metadata and Embedding Dataset\n\t\n\nReadyAI, operating subnet 33 on the Bittensor Network is an open-source initiative focused on low-cost, resource-minimal pipelines for structuring raw data for AI applications.\nThis dataset is part of the ReadyAI Conversational Genome Project, leveraging the Bittensor decentralized network.\nAI runs on structured data â€” and this dataset bridges the gap between raw conversation transcripts and structuredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ReadyAi/5000-podcast-conversations-with-metadata-and-embedding-dataset.","url":"https://huggingface.co/datasets/ReadyAi/5000-podcast-conversations-with-metadata-and-embedding-dataset","creator_name":"ReadyAI","creator_url":"https://huggingface.co/ReadyAi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"LIMIT-small","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tLIMIT-small\n\t\n\nA retrieval dataset that exposes fundamental theoretical limitations of embedding-based retrieval models. Despite using simple queries like \"Who likes Apples?\", state-of-the-art embedding models achieve less than 20% recall@100 on LIMIT full and cannot solve LIMIT-small (46 docs).\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nVector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/LIMIT-small.","url":"https://huggingface.co/datasets/orionweller/LIMIT-small","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-ranking","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Indian-Supreme-Court-Judgements-Chunked","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tIndian Supreme Court Judgements Chunked\n\t\n\n\n\t\n\t\t\n\t\tExecutive Summary\n\t\n\nThe dataset aims to address the chronic backlog in the Indian judiciary system, particularly in the Supreme Court, by creating a dataset optimized for legal language models (LLMs). The dataset will consist of pre-processed, chunked, and embedded textual data derived from the Supreme Court's judgment PDFs.\n\n\t\n\t\t\n\t\tProblem and Importance - Motivation\n\t\n\nIndian courts are overwhelmed with pending cases, with theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vihaannnn/Indian-Supreme-Court-Judgements-Chunked.","url":"https://huggingface.co/datasets/vihaannnn/Indian-Supreme-Court-Judgements-Chunked","creator_name":"Vihaan Nama","creator_url":"https://huggingface.co/vihaannnn","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","text-classification","sentence-similarity","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"usc-chroma-vecs-v1-chunks-v1-s2048-o256-sentence-transformers-static-retrieval-mrl-en-v1","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tusc-chroma-vecs-v1-chunks-v1-s2048-o256-sentence-transformers-static-retrieval-mrl-en-v1\n\t\n\nThis dataset contains a pre-built ChromaDB database with US Congressional legislation embeddings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains the ChromaDB files for legislation chunks with embeddings. The database can be loaded directly using ChromaDB's PersistentClient.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nimport chromadb\nfrom huggingface_hub import snapshot_download\n\n# Download the dataset\nlocal_dir =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hyperdemocracy/usc-chroma-vecs-v1-chunks-v1-s2048-o256-sentence-transformers-static-retrieval-mrl-en-v1.","url":"https://huggingface.co/datasets/hyperdemocracy/usc-chroma-vecs-v1-chunks-v1-s2048-o256-sentence-transformers-static-retrieval-mrl-en-v1","creator_name":"hyperdemocracy","creator_url":"https://huggingface.co/hyperdemocracy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["cc0-1.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US","chromadb","legal"],"keywords_longer_than_N":true},
	{"name":"blbooks-parquet-embedded","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tDataset Card for \"blbooks-parquet-embedded\"\n\t\n\nMore Information needed\n","url":"https://huggingface.co/datasets/davanstrien/blbooks-parquet-embedded","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","other","language-modeling","masked-language-modeling"],"keywords_longer_than_N":true},
	{"name":"cifar100-enriched","keyword":"embeddings","description":"The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images\nper class. There are 500 training images and 100 testing images per class. There are 50000 training images and 10000 test images. The 100 classes are grouped into 20 superclasses.\nThere are two labels per image - fine label (actual class) and coarse label (superclass).","url":"https://huggingface.co/datasets/renumics/cifar100-enriched","creator_name":"Renumics","creator_url":"https://huggingface.co/renumics","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","crowdsourced","found","monolingual","extended|other-80-Million-Tiny-Images"],"keywords_longer_than_N":true},
	{"name":"maltese_embeddings","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository contains three distinct datasets focusing on Maltese word embeddings:\n\nGloVe Maltese Word Embeddings\nEmbeddings generated using GloVe on the \"korpus_malti\" dataset, the largest Maltese corpus available.\n\nWord2Vec Maltese Word Embeddings\nWord embeddings for Maltese obtained using Word2Vec trained on the \"korpus_malti\" dataset.\n\nPPMI Maltese Word Embeddings\nPointwise Mutual Information (PPMI) based word embeddings generated from ConceptNet data via SVDâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DGurgurov/maltese_embeddings.","url":"https://huggingface.co/datasets/DGurgurov/maltese_embeddings","creator_name":"Daniil Gurgurov","creator_url":"https://huggingface.co/DGurgurov","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Maltese","mit","1M - 10M","text","Text"],"keywords_longer_than_N":true},
	{"name":"Variant-Foundation-Embeddings","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tVariant Foundation Embeddings\n\t\n\nHere we present the variant level embeddings for large-scale genetic analyis as described in 'Incorporating LLM Embeddings for Variation Across the Human Genome,' based on curated annotations using high quality functional data from FAVOR, ClinVar, and GWAS Catalog. We currently present one version of the embeddings for the 1.5 million genetic variant HapMap3 & MEGA datasets using OpenAI's text-embedding-3-large (3072-dimensional) embeddings, with othersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiLabUNC/Variant-Foundation-Embeddings.","url":"https://huggingface.co/datasets/LiLabUNC/Variant-Foundation-Embeddings","creator_name":"Li Lab UNC BIOS","creator_url":"https://huggingface.co/LiLabUNC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","ðŸ‡ºðŸ‡¸ Region: US","genetics","LLM"],"keywords_longer_than_N":true},
	{"name":"LIMIT","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tLIMIT\n\t\n\nA retrieval dataset that exposes fundamental theoretical limitations of embedding-based retrieval models. Despite using simple queries like \"Who likes Apples?\", state-of-the-art embedding models achieve less than 20% recall@100 on LIMIT full and cannot solve LIMIT-small (46 docs).\n\n\t\n\t\t\n\t\tLinks\n\t\n\n\nPaper: On the Theoretical Limitations of Embedding-Based Retrieval\nCode: github.com/google-deepmind/limit\nFull version: LIMIT (50k documents)\nSmall version: LIMIT-small (46â€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/LIMIT.","url":"https://huggingface.co/datasets/orionweller/LIMIT","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-ranking","text-retrieval","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"USCode-QAPairs-Finetuning","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tUSCode-QueryPairs Dataset\n\t\n\nThis dataset contains query-answer pairs curated from the United States Code, suitable for fine-tuning any embedding model. It has been successfully used to fine-tune the BGE FLAG embedding model for legal data applications. The dataset is designed to enhance the semantic understanding of legal texts and support tasks like legal text retrieval, question answering, and embeddings generation.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nSource: United States Codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning.","url":"https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning","creator_name":"Archit Rastogi ","creator_url":"https://huggingface.co/ArchitRastogi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"DS569k","keyword":"embeddings","description":"Want to analyze some proteins, but lack embeddings? Want to perform vector similarity search? Want a context of known proteins embeddings? Look no further!\nThis repository is a dataset of Reviewed Swiss-Prot Proteins. Each protein I compute the embeddings for ESM2 (6 layer model) and ProteinCLIP. \n\n\t\n\t\t\n\t\n\t\n\t\tSpecs\n\t\n\nSee the data viewer for all information. Most of the metadata on each protein and the sequences themself come from Reviewed Swiss-Prot Proteins.\nImportant columns\n\naccession: theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/donnyb/DS569k.","url":"https://huggingface.co/datasets/donnyb/DS569k","creator_name":"Donny Bertucci","creator_url":"https://huggingface.co/donnyb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","mit","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"saudi-dialect-test-samples","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tSaudi Dialect Test Samples\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 1280 Saudi dialect utterances across 44 categories, used for testing and evaluating the Omartificial-Intelligence-Space/SA-BERT-V1 model. The sentences represent a wide range of topics, from daily conversations to specialized domains.\n\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Fields\n\t\n\n\ncategory: The topic category of the utterance (one of 44 categories)\ntext: The Saudi dialect textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/saudi-dialect-test-samples.","url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/saudi-dialect-test-samples","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","Arabic","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"datacomp-small-clip","keyword":"embeddings","description":"\n    \n        \n    \n\n\n\n    \n        Production-ready \n        data processing made \n        easy \n        and \n        shareable\n    \n    \n    Explore the Fondant docs Â»\n    \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for fondant-ai/datacomp-small-clip\n\t\n\n\n\nThis is a dataset containing image urls and their CLIP embeddings, based on the datacomp_small dataset, and processed with fondant.\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nLarge (image) datasets are often unwieldy to use due to theirâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fondant-ai/datacomp-small-clip.","url":"https://huggingface.co/datasets/fondant-ai/datacomp-small-clip","creator_name":"Fondant","creator_url":"https://huggingface.co/fondant-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-to-image","cc-by-4.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"mmE5-Synthetic","keyword":"embedding","description":"\n\t\n\t\t\n\t\tmmE5 Synthetic Data\n\t\n\n","url":"https://huggingface.co/datasets/backup-mmE5/mmE5-Synthetic","creator_name":"backup","creator_url":"https://huggingface.co/backup-mmE5","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"soda-vec-data-full_pmc_title_abstract","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tSODA-VEC Clean Dataset\n\t\n\nThis is a cleaned and filtered version of the SODA-VEC dataset, containing high-quality biomedical title-abstract pairs from PubMed Central (PMC) articles.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nTotal examples: 26,573,900\nTraining set: 26,473,900 examples (99.6%)\nValidation set: 50,000 examples (0.2%)\nTest set: 50,000 examples (0.2%)\n\n\n\t\n\t\t\n\t\tQuality Filtering Applied\n\t\n\nThis dataset has been processed with the following quality filters:\n\n\t\n\t\t\n\t\tAbstract Lengthâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/EMBO/soda-vec-data-full_pmc_title_abstract.","url":"https://huggingface.co/datasets/EMBO/soda-vec-data-full_pmc_title_abstract","creator_name":"EMBO","creator_url":"https://huggingface.co/EMBO","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentence-similarity","feature-extraction","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"soda-vec-data-full_pmc_title_abstract_paired","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tSODA-VEC Paired Dataset for Negative Sampling\n\t\n\nThis is a paired version of the SODA-VEC dataset, specifically formatted for negative sampling training with MultipleNegativesRankingLoss.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nTotal examples: 26,573,900\nFormat: Paired (anchor-positive) for contrastive learning\nSource: EMBO/soda-vec-data-full_pmc_title_abstract\nPurpose: Training sentence transformers with negative sampling\n\n\n\t\n\t\t\n\t\tData Format\n\t\n\nEach example contains:\n\nanchor (string): The titleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/EMBO/soda-vec-data-full_pmc_title_abstract_paired.","url":"https://huggingface.co/datasets/EMBO/soda-vec-data-full_pmc_title_abstract_paired","creator_name":"EMBO","creator_url":"https://huggingface.co/EMBO","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","feature-extraction","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"Wiki_Faiss_Indexes","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tdataset_info:\n  features:\n  - name: text\n    dtype: string\n  - name: embeddings\n    dtype: float32\n    shape: [384]\n  configs:\n  - config_name: default\n    data_files: \"*.parquet\"\n\t\n\n\n\t\n\t\t\n\t\tWikipedia IVF-OPQ-PQ Vector Database (GPU-Optimized)\n\t\n\nA high-performance, GPU-accelerated FAISS vector database built from Wikipedia articles with pre-computed embeddings. This dataset contains approximately 35 million Wikipedia articles with 384-dimensional embeddings using the all-MiniLM-L6-v2â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ram-G/Wiki_Faiss_Indexes.","url":"https://huggingface.co/datasets/Ram-G/Wiki_Faiss_Indexes","creator_name":"Sri Ram Pavan Kumar Guttikonda","creator_url":"https://huggingface.co/Ram-G","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","text-retrieval","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"data-gouv-datasets-catalog","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ‡«ðŸ‡· Data.gouv.fr Datasets Catalog\n\t\n\nThis dataset contains a processed and embedded version of the catalog of datasets published on data.gouv.fr, the French open data platform.\nThe dataset was published by data.gouv.fr on the dedicated dataset page.\nIt includes rich metadata about each public dataset: title, URL, publisher organization, description, tags, licensing, update frequency, usage metrics, and more.\nThe dataset provides semantic-ready and structured for semantic indexing andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AgentPublic/data-gouv-datasets-catalog.","url":"https://huggingface.co/datasets/AgentPublic/data-gouv-datasets-catalog","creator_name":"AgentPublic","creator_url":"https://huggingface.co/AgentPublic","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["French","etalab-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"dole","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ‡«ðŸ‡· French Legislative Dossiers Dataset (DOLE)\n\t\n\nThis dataset provides a semantic-ready, chunked and embedded version of the Dossiers LÃ©gislatifs (\"DOLE\") published by the French government. It includes all laws promulgated since the XIIáµ‰ legislature (June 2002), ordinances, and legislative proposals under preparation.\nThe original data is downloaded from the dedicated DILA open data repository and is also published on data.gouv.fr.\nEach article is chunked and vectorized using theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AgentPublic/dole.","url":"https://huggingface.co/datasets/AgentPublic/dole","creator_name":"AgentPublic","creator_url":"https://huggingface.co/AgentPublic","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["French","etalab-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"pdf-rag-embed-bench","keyword":"embedding","description":"This is a benchmark dataset for PDF RAG embedding systems.\nSee gpahal/pdf-rag-embed-bench for more details.\n","url":"https://huggingface.co/datasets/gpahal/pdf-rag-embed-bench","creator_name":"Garvit Pahal","creator_url":"https://huggingface.co/gpahal","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","mit","< 1K","Document","Image"],"keywords_longer_than_N":true},
	{"name":"local-emoji-search-gte","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tlocal emoji semantic search\n\t\n\nEmoji, their text descriptions and precomputed text embeddings with Alibaba-NLP/gte-large-en-v1.5 for use in emoji semantic search. \nThis work is largely inspired by the original emoji-semantic-search repo and aims to provide the data for fully local use, as the demo is not working as of a few days ago.\n\nThis repo only contains a precomputed embedding \"database\", equivalent to server/emoji-embeddings.jsonl.gz in the original repo, to be used as theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/local-emoji-search-gte.","url":"https://huggingface.co/datasets/pszemraj/local-emoji-search-gte","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Chunked-Indian-Supreme-Court-Judgements","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tIndian Supreme Court Judgements Chunked\n\t\n\n","url":"https://huggingface.co/datasets/vihaannnn/Chunked-Indian-Supreme-Court-Judgements","creator_name":"Vihaan Nama","creator_url":"https://huggingface.co/vihaannnn","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","feature-extraction","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"commonvoice-12.0-arabic-voice-converted-speakers","keyword":"embedding","description":"These are the speakers' embeddings and information for the Voice Converted Arabic Common Voice 12.0 dataset.\n","url":"https://huggingface.co/datasets/xmodar/commonvoice-12.0-arabic-voice-converted-speakers","creator_name":"Modar M. Alfadly","creator_url":"https://huggingface.co/xmodar","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["cc0-1.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"ABC-VG-Instruct","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tVG Instruct\n\t\n\nThis is the instruction finetuning dataset for ABC: Achieving better control of multimodal embeddings using VLMs.\nEach element in this dataset contains 4 instruction-captions pairs for images in the visual genome dataset, corresponding to different bounding boxes in the image.\nWe use this dataset to train an embedding model that can use instruction to embeds specific aspects of a scene.\n\nCombined with our pretraining step, this results in a model that can create highâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct.","url":"https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MMEB-train","keyword":"embedding","description":"\n\t\n\t\t\n\t\tMassive Multimodal Embedding Benchmark\n\t\n\nThe training data split used for training VLM2Vec models in the paper VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks (ICLR 2025).\nMMEB benchmark covers 4 meta tasks and 36 datasets meticulously selected for evaluating capabilities of multimodal embedding models.\nDuring training, we utilize 20 out of the 36 datasets.\nFor evaluation, we assess performance on the 20 in-domain (IND) datasets and the remaining 16â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Y-J-Ju/MMEB-train.","url":"https://huggingface.co/datasets/Y-J-Ju/MMEB-train","creator_name":"Yeong-Joon Ju","creator_url":"https://huggingface.co/Y-J-Ju","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"msmarco_answerai_colbert_small_embeddings","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tMS MARCO ColBERT Embeddings\n\t\n\nPre-computed ColBERT embeddings for MS MARCO using PyLate and answerdotai/answerai-colbert-small-v1.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains:\n\ndata/corpus/: 177 parquet files with document embeddings\ndata/queries/: 11 parquet files with query embeddings\ndata/qrels/train.parquet: Relevance judgments (532,751 pairs)\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load from directory (recommended for large datasets)\ncorpus =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/WenxingZhu/msmarco_answerai_colbert_small_embeddings.","url":"https://huggingface.co/datasets/WenxingZhu/msmarco_answerai_colbert_small_embeddings","creator_name":"WenxingZhu","creator_url":"https://huggingface.co/WenxingZhu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","text-retrieval","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"pwesuite-eval","keyword":"embedding","description":"\n\n\t\n\t\t\n\t\tPWESuite-Eval\n\t\n\nDataset composed of multiple smaller datasets used for the evaluation of phonetic word embeddings.\nSee code for evaluation here.\nIf you use this dataset/evaluation, please cite the paper at LREC-COLING 2024:\n@inproceedings{zouhar-etal-2024-pwesuite,\n    title = \"{PWES}uite: Phonetic Word Embeddings and Tasks They Facilitate\",\n    author = \"Zouhar, Vil{\\'e}m  and\n      Chang, Kalvin  and\n      Cui, Chenxuan  and\n      Carlson, Nate B.  and\n      Robinson, Nathanielâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zouharvi/pwesuite-eval.","url":"https://huggingface.co/datasets/zouharvi/pwesuite-eval","creator_name":"VilÃ©m Zouhar","creator_url":"https://huggingface.co/zouharvi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","English","Amharic","Bengali","Swahili"],"keywords_longer_than_N":true},
	{"name":"BOOK-RECOMMENDER-DATASET","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tBook Recommender Dataset\n\t\n\nCSV exports from my Book Recommender pipeline. Includes cleaned metadata, category labels, emotion tags, and a tagged description file.\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\nbooks_cleaned.csv: Core cleaned book metadata.\nbooks_with_categories.csv: Adds multi-label categories column.\nbooks_with_emotions.csv: Adds emotion_* columns (one-hot or scores).\ntagged_description.txt: Preprocessed descriptions (one per line, or TSV).\n\n\n\t\n\t\t\n\t\tColumn Schema (example)\n\t\n\n\nbook_id (str)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/svastikkka/BOOK-RECOMMENDER-DATASET.","url":"https://huggingface.co/datasets/svastikkka/BOOK-RECOMMENDER-DATASET","creator_name":"Manshu Sharma","creator_url":"https://huggingface.co/svastikkka","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["tabular-classification","mit","10K<n<100K","Text","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"oscar-en-minilm-2m","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tOscar EN 2M Embeddings\n\t\n\nThis dataset contains 2M sentences extracted from the English subset of the OSCAR dataset, and encoded into sentence embeddings using the sentence-transformers/all-MiniLM-L6-v2 model.\n","url":"https://huggingface.co/datasets/jamescalam/oscar-en-minilm-2m","creator_name":"James Briggs","creator_url":"https://huggingface.co/jamescalam","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","no-annotation","other","extended|oscar","English"],"keywords_longer_than_N":true},
	{"name":"openalex-multilingual-embeddings","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tOpenAlex Multilingual Embeddings\n\t\n\nThis dataset contains multilingual text embeddings of all records in OpenAlex with a title or an abstract from the snapshot of 2023-10-20.\nThe dataset was created for the FORAS project to investigate the efficacy of \ndifferent methods of searching in databases of academic publications. All scripts will be available in a GitHub repository. \nThe project is supported by a grant from the Dutch Research Council (grant no. 406.22.GO.048)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/GlobalCampus/openalex-multilingual-embeddings.","url":"https://huggingface.co/datasets/GlobalCampus/openalex-multilingual-embeddings","creator_name":"Global Campus","creator_url":"https://huggingface.co/GlobalCampus","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["cc0-1.0","100M - 1B","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"philippine-budget-2025-embeddings-minilm","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tPhilippine Budget 2025 - Vector Embeddings (all-MiniLM-L6-v2)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains vector embeddings of the 2025 People's Budget of the Philippines, a citizen-friendly overview of the PHP 6.326 trillion national budget published by the Department of Budget and Management (DBM).\n\n\t\n\t\t\n\t\tSource Document\n\t\n\nThese embeddings are based on the 2025 People's Enacted Budget (English version, revised as of April 22, 2025).\nDirect Download Link: 2025 People'sâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pageman/philippine-budget-2025-embeddings-minilm.","url":"https://huggingface.co/datasets/pageman/philippine-budget-2025-embeddings-minilm","creator_name":"The Pageman","creator_url":"https://huggingface.co/pageman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","text-retrieval","feature-extraction","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"drone-lsr","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tLight Stable Representations Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains aerial orthomosaic tiles captured at three different times of day (10:00, 12:00, and 15:00). The dataset is organized into three configurations: default (raw images + canopy height), dinov2_base (DINOv2 embeddings), and dinov3_sat (DINOv3 embeddings). All configurations share consistent train/test splits with matching tile identifiers for cross-referencing. The dataset is designed for trainingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mpg-ranch/drone-lsr.","url":"https://huggingface.co/datasets/mpg-ranch/drone-lsr","creator_name":"MPG Ranch","creator_url":"https://huggingface.co/mpg-ranch","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","image-to-image","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"turkce-embedding-sts-degerlendirme","keyword":"embedding","description":"\n\t\n\t\t\n\t\tTÃ¼rkÃ§e Embedding STS (DeÄŸerlendirme)\n\t\n\nSemantic Textual Similarity (STS) deÄŸerlendirme seti. suayptalha/Sungur-9B ile Ã¼retildi.\n\nsentence1, sentence2: metin Ã§ifti  \nscore: 0.0â€“1.0 arasÄ±nda benzerlik puanÄ±\n\nSplit'ler: train, validation\n","url":"https://huggingface.co/datasets/nezahatkorkmaz/turkce-embedding-sts-degerlendirme","creator_name":"Nezahat Korkmaz","creator_url":"https://huggingface.co/nezahatkorkmaz","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Turkish","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"italian-embedding-finetune-dataset","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tItalian-BERT-FineTuning-Embeddings\n\t\n\nThis repository contains a comprehensive dataset designed for fine-tuning BERT-based Italian embedding models. The dataset aims to enhance performance on tasks such as information retrieval, semantic search, and embeddings generation.\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset leverages the C4 dataset (Italian subset) and employs advanced techniques like sliding window segmentation and in-document sampling to create high-quality, diverse examplesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ArchitRastogi/italian-embedding-finetune-dataset.","url":"https://huggingface.co/datasets/ArchitRastogi/italian-embedding-finetune-dataset","creator_name":"Archit Rastogi","creator_url":"https://huggingface.co/ArchitRastogi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","Italian","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"Core-AlphaEarth-Embeddings","keyword":"embeddings","description":"\n\n\t\n\t\t\n\t\tMajor TOM Core AlphaEarth Embeddings Subset\n\t\n\nThis is a prototype dataset. It only includes some of the AlphaEarth embeddings stored in Major TOM grid cells.\nThis dataset is mostly aimed at experimentation and prototyping. It is particularly useful to use it along other datasets published within the Major TOM project.\n\n\t\n\t\t\n\t\n\t\n\t\tContent\n\t\n\n\n\t\n\t\t\nField\nType\nDescription\n\n\n\t\t\ngrid_cell\nstring\nMajor TOM cell\n\n\nyear\nint\nyear of the sample\n\n\nthumbnail\nimage\n3-dimensional PCAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-AlphaEarth-Embeddings.","url":"https://huggingface.co/datasets/Major-TOM/Core-AlphaEarth-Embeddings","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"usc-chroma-vecs-v1-chunks-v1-s8192-o512-sentence-transformers-static-retrieval-mrl-en-v1","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tusc-chroma-vecs-v1-chunks-v1-s8192-o512-sentence-transformers-static-retrieval-mrl-en-v1\n\t\n\nThis dataset contains a pre-built ChromaDB database with US Congressional legislation embeddings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains the ChromaDB files for legislation chunks with embeddings. The database can be loaded directly using ChromaDB's PersistentClient.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nimport chromadb\nfrom huggingface_hub import snapshot_download\n\n# Download the dataset\nlocal_dir =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hyperdemocracy/usc-chroma-vecs-v1-chunks-v1-s8192-o512-sentence-transformers-static-retrieval-mrl-en-v1.","url":"https://huggingface.co/datasets/hyperdemocracy/usc-chroma-vecs-v1-chunks-v1-s8192-o512-sentence-transformers-static-retrieval-mrl-en-v1","creator_name":"hyperdemocracy","creator_url":"https://huggingface.co/hyperdemocracy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["cc0-1.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US","chromadb","legal"],"keywords_longer_than_N":true},
	{"name":"PubMedAbstractsSubsetEmbedded","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tPubMed Abstracts Subset with MedCPT Embeddings (float32)\n\t\n\nThis dataset contains a probabilistic sample of ~2.4 million PubMed abstracts, enriched with precomputed dense embeddings (title + abstract), from the ncbi/MedCPT-Article-Encoder model. It is derived from public metadata made available via the National Library of Medicine (NLM) and was used in the paper Efficient and Reproducible Biomedical QA using Retrieval-Augmented Generation.\nEach entry includes:\n\ntitle: Title of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/slinusc/PubMedAbstractsSubsetEmbedded.","url":"https://huggingface.co/datasets/slinusc/PubMedAbstractsSubsetEmbedded","creator_name":"Linus Stuhlmann","creator_url":"https://huggingface.co/slinusc","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"language-metric-data","keyword":"embeddings","description":"# This dataset contains the entire content of three files loaded as a single example:\n# - `languages_list.pkl`: A pickled list of language strings.\n# - `average_distances_matrix.npy`: A NumPy matrix converted to a list of lists of floats.\n# - `distances_matrices.pkl`: A pickled dict of dicts of NumPy matrices.  \n#    It is converted into a list of records where each record corresponds to a dataset with a nested list of models and their associated distance matrices.\n#","url":"https://huggingface.co/datasets/mshamrai/language-metric-data","creator_name":"Maksym Shamrai","creator_url":"https://huggingface.co/mshamrai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["feature-extraction","mit","arxiv:2508.11676","ðŸ‡ºðŸ‡¸ Region: US","multilingual"],"keywords_longer_than_N":true},
	{"name":"state-administrations-directory","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ‡«ðŸ‡· French State Administrations Directory Dataset\n\t\n\nThis dataset is a processed and embedded version of the public data RÃ©fÃ©rentiel de lâ€™organisation administrative de lâ€™Ã‰tat (French State Administrations Directory), published by DILA (Direction de l'information lÃ©gale et administrative) on data.gouv.fr.\nThis information is also available on the official directory website of Service-Public.fr: https://lannuaire.service-public.fr/\nThe dataset provides semantic-ready, structured andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AgentPublic/state-administrations-directory.","url":"https://huggingface.co/datasets/AgentPublic/state-administrations-directory","creator_name":"AgentPublic","creator_url":"https://huggingface.co/AgentPublic","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["French","etalab-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Arab3M-Triplets","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tArab3M-Triplets\n\t\n\nThis dataset is designed for training and evaluating models using contrastive learning techniques, particularly in the context of natural language understanding. The dataset consists of triplets: an anchor sentence, a positive sentence, and a negative sentence. The goal is to encourage models to learn meaningful representations by distinguishing between semantically similar and dissimilar sentences.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nFormat: Parquet\nNumber of rows: 3.03â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arab3M-Triplets.","url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arab3M-Triplets","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","Arabic","apache-2.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"constit","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ‡«ðŸ‡· French Constitutional Council Decisions Dataset (Conseil constitutionnel)\n\t\n\nThis dataset is a processed and embedded version of all decisions issued by the Conseil constitutionnel (French Constitutional Council) since its creation in 1958. \nIt includes full legal texts of decisions, covering constitutional case law, electoral disputes, and other related matters. \nThe original data is downloaded from the dedicated DILA open data repository and is also published on data.gouv.fr.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AgentPublic/constit.","url":"https://huggingface.co/datasets/AgentPublic/constit","creator_name":"AgentPublic","creator_url":"https://huggingface.co/AgentPublic","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["French","etalab-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"gtl-hids-embeddings","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tNetwork Traffic Embeddings Dataset\n\t\n\n\n\t\n\t\t\n\t\tModel Description\n\t\n\nThis dataset contains embeddings generated from the CICIDS2017 network traffic dataset using a fine-tuned Meta-Llama-3.1-70B-Instruct model. The embeddings represent network traffic flows formatted in a structured way to capture key network traffic characteristics.\n\n\t\n\t\t\n\t\tStructure of Embeddings Files\n\t\n\n\n\t\n\t\t\n\t\tcombined.npy\n\t\n\nThe combined.npy file contains a NumPy array of shape (N, D) where:\n\nN is the total numberâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Hmehdi515/gtl-hids-embeddings.","url":"https://huggingface.co/datasets/Hmehdi515/gtl-hids-embeddings","creator_name":"Hasan Mehdi","creator_url":"https://huggingface.co/Hmehdi515","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Variant-Foundation-Embeddings","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tVariant Foundation Embeddings\n\t\n\nHere we present the variant level embeddings for large-scale genetic analyis as described in 'Incorporating LLM Embeddings for Variation Across the Human Genome,' based on curated annotations using high quality functional data from FAVOR, ClinVar, and GWAS Catalog. We currently present embeddings for the 1.5 million genetic variant HapMap3 & MEGA datasets using OpenAI's text-embedding-3-large (3072-dimensional), with others coming soon. \nGeneticâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hong-niu/Variant-Foundation-Embeddings.","url":"https://huggingface.co/datasets/hong-niu/Variant-Foundation-Embeddings","creator_name":"Hong Niu","creator_url":"https://huggingface.co/hong-niu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"swift-mlx-Qwen3-Embedding-4B","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tðŸ” VincentGOURBIN/swift-mlx-Qwen3-Embedding-4B - Embeddings Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nCe dataset contient des embeddings vectoriels gÃ©nÃ©rÃ©s par le systÃ¨me LocalRAG pour la recherche sÃ©mantique dans la documentation technique.\n\n\t\n\t\t\n\t\tðŸ“Š Statistiques\n\t\n\n\nFormat: SafeTensors\nVecteurs: 7,511\nDimension: 2560\nModÃ¨le d'embedding: Qwen/Qwen3-Embedding-4B\nType d'index: HNSW\nGÃ©nÃ©rÃ© le: 2025-08-22T14:04:16.932676\n\n\n\t\n\t\t\n\t\tðŸ“ Contenu\n\t\n\n\nembeddings.safetensors: Embeddings vectoriels auâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VincentGOURBIN/swift-mlx-Qwen3-Embedding-4B.","url":"https://huggingface.co/datasets/VincentGOURBIN/swift-mlx-Qwen3-Embedding-4B","creator_name":"GOURBIN","creator_url":"https://huggingface.co/VincentGOURBIN","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","embeddings","faiss","rag"],"keywords_longer_than_N":true},
	{"name":"mulesoft-documentation-embeddings","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tmulesoft-documentation-embeddings\n\t\n\nMuleSoft Documentation Embeddings for RAG Applications\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nVersion: 1.0.0\nCreated: 2025-09-16T02:41:16.352809\nSource: Vector Database\nLicense: MIT\nLanguage: en\n\n\n\t\n\t\t\n\t\tTask Categories\n\t\n\nquestion-answering, retrieval, knowledge-base\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\n\t\n\t\t\n\t\tSkillPilotDataSet_v11\n\t\n\n\nTotal Objects: 6430\nUnique Properties: 13\nKnowledge Sources: mulesoft, user_defined_docs\nAverage Content Length: 5079â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BassemE/mulesoft-documentation-embeddings.","url":"https://huggingface.co/datasets/BassemE/mulesoft-documentation-embeddings","creator_name":"Bassem","creator_url":"https://huggingface.co/BassemE","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-retrieval","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"wikipedia_qwen_8b","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tVector Database Dataset\n\t\n\nGenerated embeddings dataset for vector database training and evaluation with multiple format support.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 500,000 text samples with high-quality vector embeddings generated using Qwen/Qwen3-Embedding-8B from the wikimedia/wikipedia dataset. The dataset is designed for vector database training, similarity search, and retrieval tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nBase dataset: 500,000 samples with embeddings\nQueryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/maknee/wikipedia_qwen_8b.","url":"https://huggingface.co/datasets/maknee/wikipedia_qwen_8b","creator_name":"maknee","creator_url":"https://huggingface.co/maknee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"swissprot_protein_embeddings-ankh","keyword":"embedding","description":"\n\t\n\t\t\n\t\tProtein Language Model Embeddings ðŸ”¢\n\t\n\nProtein embeddings for Uniprot/Swiss-Prot sequences.\n\n\t\n\t\t\nName\nModel ðŸ¤–\nVector Length ðŸ“\nFile Size\n\n\n\t\t\nemb.ankh_large.parquet\nankh-large\n1536\n3.4G\n\n\nemb.ankh_base.parquet\nankh-base\n768\n1.7G\n\n\n\t\n\n","url":"https://huggingface.co/datasets/pitagoras-alves/swissprot_protein_embeddings-ankh","creator_name":"PitÃ¡goras Alves","creator_url":"https://huggingface.co/pitagoras-alves","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K<n<1M","doi:10.57967/hf/5180","ðŸ‡ºðŸ‡¸ Region: US","biology"],"keywords_longer_than_N":true},
	{"name":"Sourdough-midi-dataset-sentence-transformers","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tSourdough-midi-dataset Sentence Transformers Embeddings\n\t\n\n\n\t\n\t\t\n\t\tMIDI files paths/names embeddings for MIDI search\n\t\n\n\n\n\t\n\t\t\n\t\tEmbeddings are for all files paths/names in Sourdough-midi-dataset as of 04/07/2025 (Revision cd19431)\n\t\n\n\n\n\t\n\t\t\n\t\tBasic use example\n\t\n\n\n\t\n\t\t\n\t\tInstall dependencies\n\t\n\n!pip install -U sentence-transformers\n!pip install tf-keras\n!pip install numpy==1.26.4\n!pip install accelerate\n!pip install -U tqdm\n!pip install -U ipywidgets\n\n\n\t\n\t\t\n\t\tSearch for MIDIsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/asigalov61/Sourdough-midi-dataset-sentence-transformers.","url":"https://huggingface.co/datasets/asigalov61/Sourdough-midi-dataset-sentence-transformers","creator_name":"Alex","creator_url":"https://huggingface.co/asigalov61","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["English","apache-2.0","1M<n<10M","ðŸ‡ºðŸ‡¸ Region: US","midi"],"keywords_longer_than_N":true},
	{"name":"t5-xxl-embedding","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tT5-XXL Embeddings for Image Generation Prompts\n\t\n\nThis dataset contains 4096-dimensional text embeddings for prompts from the Gustavosta/Stable-Diffusion-Prompts dataset.\n\n\t\n\t\t\n\t\tModel Used\n\t\n\nThe embeddings were generated using a T5-XXL encoder model. Specifically, the weights from t5xxl_fp8_e4m3fn_scaled.safetensors were used.\nImportant Note: While the source model weights were in a FP8 format, the embeddings in this dataset have been calculated and stored in full float32 precisionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JusteLeo/t5-xxl-embedding.","url":"https://huggingface.co/datasets/JusteLeo/t5-xxl-embedding","creator_name":"JusteLeo","creator_url":"https://huggingface.co/JusteLeo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"wikipedia_qwen_4b","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tVector Database Dataset\n\t\n\nGenerated embeddings dataset for vector database training and evaluation with multiple format support.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 1,000,000 text samples with high-quality vector embeddings generated using Qwen/Qwen3-Embedding-4B from the wikimedia/wikipedia dataset. The dataset is designed for vector database training, similarity search, and retrieval tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nBase dataset: 1,000,000 samples with embeddingsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/maknee/wikipedia_qwen_4b.","url":"https://huggingface.co/datasets/maknee/wikipedia_qwen_4b","creator_name":"maknee","creator_url":"https://huggingface.co/maknee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"RAVine-dense-index","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tRAVine-dense-index\n\t\n\nThis repository contains dense index files for the search tools of the RAVine: Reality-Aligned Evaluation for Agentic Search framework. The corpus is MS MARCO V2.1, encoded using Alibaba-NLP/gte-modernbert-base.\nPaper: RAVine: Reality-Aligned Evaluation for Agentic Search\nCode: https://github.com/SwordFaith/RAVine\n\n\t\n\t\t\n\t\n\t\n\t\tAbstract\n\t\n\nAgentic search, as a more autonomous and adaptive paradigm of retrieval augmentation, is driving the evolution of intelligentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sapphirex/RAVine-dense-index.","url":"https://huggingface.co/datasets/sapphirex/RAVine-dense-index","creator_name":"yilong xu","creator_url":"https://huggingface.co/sapphirex","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["feature-extraction","English","apache-2.0","arxiv:2507.16725","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"usc-chroma-vecs-v1-chunks-v1-s1024-o256-sentence-transformers-static-retrieval-mrl-en-v1","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tusc-chroma-vecs-v1-chunks-v1-s1024-o256-sentence-transformers-static-retrieval-mrl-en-v1\n\t\n\nThis dataset contains a pre-built ChromaDB database with US Congressional legislation embeddings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains the ChromaDB files for legislation chunks with embeddings. The database can be loaded directly using ChromaDB's PersistentClient.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nimport chromadb\nfrom huggingface_hub import snapshot_download\n\n# Download the dataset\nlocal_dir =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hyperdemocracy/usc-chroma-vecs-v1-chunks-v1-s1024-o256-sentence-transformers-static-retrieval-mrl-en-v1.","url":"https://huggingface.co/datasets/hyperdemocracy/usc-chroma-vecs-v1-chunks-v1-s1024-o256-sentence-transformers-static-retrieval-mrl-en-v1","creator_name":"hyperdemocracy","creator_url":"https://huggingface.co/hyperdemocracy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["cc0-1.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US","chromadb","legal"],"keywords_longer_than_N":true},
	{"name":"dd-indexes","keyword":"embeddings","description":"\n\t\n\t\t\n\t\tâš¡ Pre-computed Search Indexes for Due Diligence\n\t\n\nHigh-performance search indexes and ML artifacts for AI-powered due diligence analysis\nThis repository contains pre-computed search indexes, embeddings, and knowledge graphs that power fast document retrieval and analysis. Skip the expensive embedding computation and start searching immediately!\n\n\t\n\t\t\n\t\tðŸŽ¯ What's Included\n\t\n\n\n\t\n\t\t\n\t\tðŸ” FAISS Vector Indexes (4 indexes, 20.2MB)\n\t\n\nHigh-performance similarity search with sub-second queryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jmzlx/dd-indexes.","url":"https://huggingface.co/datasets/jmzlx/dd-indexes","creator_name":"Juan Salas","creator_url":"https://huggingface.co/jmzlx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-question-answering","text-classification","feature-extraction","English"],"keywords_longer_than_N":true}
]
;
