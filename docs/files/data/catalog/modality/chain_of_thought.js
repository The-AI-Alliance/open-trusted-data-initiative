const data_for_modality_chain_of_thought = 
[
	{"name":"ru-thinking-reasoning-r1","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nТы полезный ассистент. Отвечай на вопросы, сохраняя следующую структуру: <think> Твои мысли и рассуждения </think> \nТвой конечный… See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1-v2","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"This is a modified version of ZeroAgency/ru-thinking-reasoning-r1 with addition of Egor-AI/CoT-XLang dataset.\nCombined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nТы полезный… See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ChessCOT","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/frosthead/ChessCOT","creator_name":"Ayush Sharma","creator_url":"https://huggingface.co/frosthead","description":"\n\t\n\t\t\n\t\tChessCOT\n\t\n\nThe dataset that makes your chess model think like a human before it plays a move.\n\n\t\n\t\t\n\t\tAbout\n\t\n\nChessCOT is a dataset designed to train transformers for chess using a Chain of Thought (CoT) approach. The goal is to make the model reason about the position with all possible moves and their consequences in order to predict the best move.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Poistions: 4,491,596\nSequence length of sMoves: 128\nSequence length of thought: 128… See the full description on the dataset page: https://huggingface.co/datasets/frosthead/ChessCOT.","first_N":5,"first_N_keywords":["text-generation","mit","1M - 10M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"tw-math-reasoning-2k","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/twinkle-ai/tw-math-reasoning-2k","creator_name":"Twinkle AI","creator_url":"https://huggingface.co/twinkle-ai","description":"\n\t\n\t\t\n\t\tDataset Card for tw-math-reasoning-2k\n\t\n\n\ntw-math-reasoning-2k 是一個繁體中文數學語言資料集，從 HuggingFaceH4/MATH 英文數學題庫中精選 2,000 題，並透過 perplexity-ai/r1-1776 模型以繁體中文重新生成具邏輯性且詳盡的解題過程與最終答案。此資料集可作為訓練或評估繁體中文數學推理模型的高品質參考語料。\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\ntw-math-reasoning-2k 是一個繁體中文數學語言資料集，旨在提供高品質的解題語料以支援中文數學推理模型的訓練與評估。此資料集從 HuggingFaceH4/MATH 英文數學題庫中精選 2,000 題，涵蓋代數、幾何、機率統計等各類題型，並確保題目類型分佈均衡。\n所有題目皆經由 perplexity-ai/r1-1776… See the full description on the dataset page: https://huggingface.co/datasets/twinkle-ai/tw-math-reasoning-2k.","first_N":5,"first_N_keywords":["text-generation","Chinese","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"GenRef-CoT","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/diffusion-cot/GenRef-CoT","creator_name":"Diffusion CoT","creator_url":"https://huggingface.co/diffusion-cot","description":"\n\t\n\t\t\n\t\tGenRef-CoT\n\t\n\n\n  \n\n\nWe provide 227K high-quality CoT reflections which were used to train our Qwen-based reflection generation model in ReflectionFlow [1]. To\nknow the details of the dataset creation pipeline, please refer to Section 3.2 of [1].\n\n\t\n\t\t\n\t\tDataset loading\n\t\n\nWe provide the dataset in the webdataset format for fast dataloading and streaming. We recommend downloading\nthe repository locally for faster I/O:\nfrom huggingface_hub import snapshot_download\n\nlocal_dir =… See the full description on the dataset page: https://huggingface.co/datasets/diffusion-cot/GenRef-CoT.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"GenRef-wds","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/diffusion-cot/GenRef-wds","creator_name":"Diffusion CoT","creator_url":"https://huggingface.co/diffusion-cot","description":"\n\t\n\t\t\n\t\tGenRef-1M\n\t\n\n\n  \n\n\nWe provide 1M high-quality triplets of the form (flawed image, high-quality image, reflection) collected across\nmultiple domains using our scalable pipeline from [1]. We used this dataset to train our reflection tuning model.\nTo know the details of the dataset creation pipeline, please refer to Section 3.2 of [1].\nProject Page: https://diffusion-cot.github.io/reflection2perfection\n\n\t\n\t\t\n\t\n\t\n\t\tDataset loading\n\t\n\nWe provide the dataset in the webdataset format for fast… See the full description on the dataset page: https://huggingface.co/datasets/diffusion-cot/GenRef-wds.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1M - 10M","webdataset"],"keywords_longer_than_N":true},
	{"name":"academic-chains","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marcodsn/academic-chains","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","description":"\n \n\n\n\n\t\n\t\t\n\t\tDataset Card for Academic Reasoning and Intuition Chains\n\t\n\n\n(The image above is an output from Llama-3.2-3B-Instruct tuned on this dataset, quantized to 8 bit and ran on llama.cpp; In our tests Qwen3-30B-A3B, Gemini 2.5 Pro and Claude Sonnet 3.7 with thinking enabled all got this simple question wrong)\nThis dataset contains reasoning (and intuition) chains distilled from open-access research papers, primarily focusing on fields like Biology, Economics, Physics, Math, Computer… See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/academic-chains.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"FinClaimRadar-Financial-Reasoning","keyword":"chain-of-thought","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ImBinarry/FinClaimRadar-Financial-Reasoning","creator_name":"Binarry.exe","creator_url":"https://huggingface.co/ImBinarry","description":"\n\t\n\t\t\n\t\tDataset Card for FinClaimRadar: A Dataset for Evidentiary Reasoning and Classification of Financial Claims\n\t\n\nThis dataset card provides an overview of the FinClaimRadar dataset, designed to advance capabilities in financial claim understanding and reasoning.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nFinClaimRadar is a novel dataset comprising 600+ meticulously crafted financial claims, each classified according to a 6-category evidentiary basis and accompanied by a… See the full description on the dataset page: https://huggingface.co/datasets/ImBinarry/FinClaimRadar-Financial-Reasoning.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"TinyDS-20k","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Hamzah-Asadullah/TinyDS-20k","creator_name":"Hamzah Asadullah","creator_url":"https://huggingface.co/Hamzah-Asadullah","description":"\n\t\n\t\t\n\t\tTinyDS\n\t\n\n\n\n\nAlpaca-style dataset with around 20k samples scraped from Qwen3-8B using SyntheticAlpaca. Q&A pairs can be in 32 different languages, these are listed in the metadata.Topics are all around STEM, programming, and literature.  \nMIT @ 2025 Hamzah Asadullah\n\n\n","first_N":5,"first_N_keywords":["question-answering","translation","text-generation","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"MAmmoTH-VL-Instruct-12M","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/moca-embed/MAmmoTH-VL-Instruct-12M","creator_name":"MoCa","creator_url":"https://huggingface.co/moca-embed","description":"\n\t\n\t\t\n\t\tMAmmoTH-VL-Instruct-12M used in MoCa Pre-training\n\t\n\n🏠 Homepage | 💻 Code | 🤖 MoCa-Qwen25VL-7B | 🤖 MoCa-Qwen25VL-3B | 📚 Datasets | 📄 Paper\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThis is a VQA style dataset used in the modality-aware continual pre-training of MoCa models. It is adapted from MAmmoTH-VL-Instruct-12M by concatenating prompts and responses.\nThe dataset consists of interleaved multimodal examples. text is a string containing text while imagesare image binaries that can be loaded… See the full description on the dataset page: https://huggingface.co/datasets/moca-embed/MAmmoTH-VL-Instruct-12M.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"Aloe-Beta-Medical-Collection","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tAloe-Beta-Medical-Collection\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated datasets used to fine-tune Aloe-Beta.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nWe curated data from many publicly available medical instruction tuning data sources (QA format). Most data samples correspond to single-turn QA pairs, while a small proportion contain multi-turn. All data sources are publicly available for… See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Alpaca-CoT","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QingyiSi/Alpaca-CoT","creator_name":"Qingyi Si","creator_url":"https://huggingface.co/QingyiSi","description":"\n\t\n\t\t\n\t\n\t\n\t\tInstruction-Finetuning Dataset Collection (Alpaca-CoT)\n\t\n\nThis repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model.\nWe also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT.  \nIf you think this dataset collection is helpful to you, please like… See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT.","first_N":5,"first_N_keywords":["English","Chinese","Malayalam","apache-2.0","🇺🇸 Region: US"],"keywords_longer_than_N":true},
	{"name":"gsm8k_self_correct","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/euclaise/gsm8k_self_correct","creator_name":"Jade","creator_url":"https://huggingface.co/euclaise","description":"\n\t\n\t\t\n\t\tDataset Card for \"gsm8k_self_correct\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Camildae","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewstaR/Camildae","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","description":"\n\t\n\t\t\n\t\tmerge of some datasets from Alpaca Cot\n\t\n\n","first_N":5,"first_N_keywords":["question-answering","apache-2.0","1M - 10M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MATH-500-Overall","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fluently-sets/MATH-500-Overall","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","description":"\n\t\n\t\t\n\t\tMATH-500-Overall\n\t\n\n\n\t\n\t\t\n\t\tAbout the dataset\n\t\n\nThis dataset of only 500 examples combines mathematics, physics and logic in English with reasoning and step-by-step problem solving, the dataset was created synthetically, CoT of Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct.\n\n\t\n\t\t\n\t\tBrief information\n\t\n\n\nNumber of rows: 500\nType of dataset files: parquet\nType of dataset: text, alpaca with system prompts\nLanguage: English\nLicense: MIT\n\nStructure:\nmath¯¯¯¯¯⌉\n   school-level (100 rows)… See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/MATH-500-Overall.","first_N":5,"first_N_keywords":["text-generation","text-classification","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"OpenThoughts-TR-18k","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/OpenThoughts-TR-18k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\n\t\n\t\t\n\t\tOpenThoughts-TR-18k: Turkish Synthetic Reasoning Dataset\n\t\n\nOpenThoughts-TR-18k is a Turkish translation of a subset of the original Open-Thoughts-114k dataset. It contains ~18k high-quality synthetic reasoning examples covering mathematics, science, coding problems, and puzzles, all translated into Turkish. This dataset is designed to support reasoning task fine tuning for Turkish language models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n~18k translated reasoning examples\nCovers multiple domains:… See the full description on the dataset page: https://huggingface.co/datasets/selimc/OpenThoughts-TR-18k.","first_N":5,"first_N_keywords":["Turkish","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Drone-flight-monitoring-reasoning-SFT","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GabrielCheng/Drone-flight-monitoring-reasoning-SFT","creator_name":"Gabriel","creator_url":"https://huggingface.co/GabrielCheng","description":"\n\t\n\t\t\n\t\tDrone-flight-monitoring-reasoning-SFT\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n本数据集是一个专注于无人机飞行安全领域的中文问答数据集，采用了Chain-of-Thought (CoT) 的格式。它旨在用于练习大语言模型的微调训练，使其能够模拟专家思考过程，并针对无人机安全相关问题生成包含推理步骤的结构化回答。微调后模型见(GabrielCheng/Deepseek-r1-finetuned-drone-safty) 。\n本数据集是基于 Hugging Face 平台上的 skylink-drone-cot-datasets (pohsjxx/default-domain-cot-dataset) 进行处理和衍生的。\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure / Data Fields\n\t\n\n数据集中的每个样本包含以下字段：\n\nQuestion (string): 关于无人机飞行安全或风险相关的问题。\nReasoning (string): 模拟模型的推理过程。\nAnswer… See the full description on the dataset page: https://huggingface.co/datasets/GabrielCheng/Drone-flight-monitoring-reasoning-SFT.","first_N":5,"first_N_keywords":["question-answering","text-generation","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"CoT_reformatted","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jtatman/CoT_reformatted","creator_name":"James","creator_url":"https://huggingface.co/jtatman","description":"\n\t\n\t\t\n\t\tDataset Card for \"CoT_reformatted\"\n\t\n\nThis dataset is reformatted from: QingyiSi/Alpaca-CoT\nAll credit goes there. Thanks to QingyiSi for the work in consolidating many diverse sources for comparison and cross-file analysis.\nThere were some issues loading files from that dataset for a testing project. \nI extracted the following data files for this subset:\n\nalpaca_data_cleaned\nCoT_data\nfirefly       \ninstruct\nalpaca_gpt4_data\ndolly \nGPTeacher\nthoughtsource\nfinance_en\ninstinwild_en\n\n","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","description":"\n 🦄 M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n      \n      \n    \n    \n       \n      \n       \n       \n      \n      \n       \n      \n    \n      \n\n\n\n      \n    [ArXiv] | [🤗HuggingFace] | [Website]\n    \n    \n\n\n🌟 Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\t🔥News\n\t\n\n\n🎖️ Our work is accepted by ACL2024.\n\n🔥 We have release benchmark on [🤗HuggingFace].\n\n🔥 The paper is also available on [ArXiv].\n\n🔮… See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"clevr-tr","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berhaan/clevr-tr","creator_name":"Berhan Türkü Ay","creator_url":"https://huggingface.co/berhaan","description":"\n\t\n\t\t\n\t\tDataset Card for CoT\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository: LLaVA-CoT GitHub Repository\nPaper: LLaVA-CoT on arXiv\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nunzip image.zip\n\nThe train.jsonl file contains the question-answering data and is structured in the following format:\n{\n  \"id\": \"example_id\",\n  \"image\": \"example_image_path\",\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"Lütfen resimdeki kırmızı metal nesnelerin sayısını belirtin.\"},\n    {\"from\": \"gpt\", \"value\": \"Resimde 3 kırmızı… See the full description on the dataset page: https://huggingface.co/datasets/berhaan/clevr-tr.","first_N":5,"first_N_keywords":["visual-question-answering","English","Turkish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"reflection-small-sonnet","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/reflection-small-sonnet","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\n\t\n\t\t\n\t\tVerified reasoning examples\n\t\n\n","first_N":5,"first_N_keywords":["Italian","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MAmmoTH-VL-Instruct-12M","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M","creator_name":"MAmmoTH-VL","creator_url":"https://huggingface.co/MAmmoTH-VL","description":"\n\t\n\t\t\n\t\tMAmmoTH-VL-Instruct-12M\n\t\n\n🏠 Homepage | 🤖 MAmmoTH-VL-8B | 💻 Code | 📄 Arxiv | 📕 PDF | 🖥️ Demo\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nOur simple yet scalable visual instruction data rewriting pipeline consists of three steps: manual data source collection, rewriting using MLLMs/LLMs, and filtering via the same MLLM as a judge. Examples below illustrate transformations in math and science categories, showcasing detailed, step-by-step responses.\n\n\t\n\t\t\n\t\tThe data distribution of… See the full description on the dataset page: https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"ultramedical","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/ultramedical","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tHAPI-BSC ultramedical\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCurated version of the UltraMedical dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThe UltraMedical Collections is a large-scale, high-quality dataset of biomedical instructions. We collected and curated the following sets:\n\nTextBookQA\nMedical-Instruction-120k\nWikiInstruct\n\nThis dataset is included in the Aloe-Beta model training set.\n\nCurated by:… See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/ultramedical.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"MedS-Ins","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/MedS-Ins","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tHPAI-BSC MedS-Ins\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated data from the MedS-Ins dataset. Used to train Aloe-Beta model.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThis is the curated version of the MedS-Ins dataset included in the training set of the Aloe-Beta models. \nFirst, we selected 75 out of the 122 existing tasks, excluding the tasks that were already in the training set, and the… See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedS-Ins.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"sharegpt_cot_dataset","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AiCloser/sharegpt_cot_dataset","creator_name":"Ai Closer","creator_url":"https://huggingface.co/AiCloser","description":"\n\t\n\t\t\n\t\tA data set inspired by the \"Reflection\" method, three-dimensional thinking and cot\n\t\n\n\n\t\n\t\t\n\t\tThis is the ShareGPT format.\n\t\n\nThe data set was generated using multiple llm synthesis.\n","first_N":5,"first_N_keywords":["question-answering","text-generation","English","Russian","Chinese"],"keywords_longer_than_N":true},
	{"name":"ru-chain-of-thought-sharegpt","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/ru-chain-of-thought-sharegpt","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"Переведённая при помощи utrobinmv/t5_translate_en_ru_zh_small_1024 на русский язык версия датасета isaiahbjork/chain-of-thought-sharegpt.\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"chain-of-diagnosis","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/chain-of-diagnosis","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tHPAI-BSC chain-of-diagnosis\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCurated version of the Chain-of-Diagnosis dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nChain-of-Diagnosis is a database used to improve interpretability in medical diagnostics for LLMs.\nWe curated and formatted the Chain-of-Diagnosis dataset into Alpaca format. This dataset is included in the training set of the Aloe-Beta model.\n\nCurated by:… See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/chain-of-diagnosis.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Sky-T1_data_steps","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shakedzy/Sky-T1_data_steps","creator_name":"Shaked","creator_url":"https://huggingface.co/shakedzy","description":"\n\t\n\t\t\n\t\tSky-T1_data_steps\n\t\n\nThis dataset contains 182 samples taken from NovaSky-AI/Sky-T1_data_17k \ndataset and broken down to thinking steps. This dataset was used to train shakedzy/Sky-T1-32B-Steps \nLoRA adapter for step-by-step thinking.\nBreaking down the thought process to steps was done using Ollama's quantized version of Llama-3.2-1B.\nSee step_prompt file for the exact prompt used.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Columns\n\t\n\n\nid (int): row index of the sample in the original dataset (starts at 0)… See the full description on the dataset page: https://huggingface.co/datasets/shakedzy/Sky-T1_data_steps.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-CoT-Math-170k","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\n\t\n\t\t\n\t\tGammaCorpus: CoT Math 170k\n\t\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nNumber of Rows: 169,527\nFormat: JSONL\nLanguage: English\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\n\n\n\t\n\t\t\n\t\tDataset Structure… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-CoT-Math-170k","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\n\t\n\t\t\n\t\tGammaCorpus: CoT Math 170k\n\t\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nNumber of Rows: 169,527\nFormat: JSONL\nLanguage: English\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\n\n\n\t\n\t\t\n\t\tDataset Structure… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_CoT","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/0fg/truthful_qa_CoT","creator_name":"Bryce","creator_url":"https://huggingface.co/0fg","description":"Question-Answer dataset generated using CAMEL CoTDataGenerator and GPT-4o Mini","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"RPRevamped-Small","keyword":"chain-of-thought","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TechPowerB/RPRevamped-Small","creator_name":"Bhargav Raj","creator_url":"https://huggingface.co/TechPowerB","description":"\n\t\n\t\t\n\t\tRPRevamped-Small-v1.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nRPRevamped is a synthetic dataset generated by various numbers of models. It is very diverse and is recommended if you are fine-tuning a roleplay model. This is the Small version with Medium and Tiny version currently in work.\nGithub: RPRevamped GitHub\nHere are the models used in creation of this dataset:\nDeepSeek-V3-0324\nGemini-2.0-Flash-Thinking-Exp-01-21\nDeepSeek-R1\nGemma-3-27B-it\nGemma-3-12B-it\nQwen2.5-VL-72B-Instruct… See the full description on the dataset page: https://huggingface.co/datasets/TechPowerB/RPRevamped-Small.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"vulnerability-intelligence-diagrammatic-reasoning","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/daqc/vulnerability-intelligence-diagrammatic-reasoning","creator_name":"David Quispe","creator_url":"https://huggingface.co/daqc","description":"\n \n\n\n\n\t\n\t\t\n\t\tVulnerability Intelligence with Diagrammatic Reasoning\n\t\n\n\n[!Important]\nThis dataset was created as a proof-of-concept for the Reasoning Datasets Competition (May 2025). If you have any feedback or suggestions, please feel free to open a discussion! Access the Github repository here.\n\n\n\t\n\t\t\n\t\n\t\n\t\tA. Overview\n\t\n\n\n\n\n\n\n\nThis dataset focuses on security vulnerability analysis through a multi-dimensional approach that combines four types of reasoning to generate valuable insights for… See the full description on the dataset page: https://huggingface.co/datasets/daqc/vulnerability-intelligence-diagrammatic-reasoning.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"synmath-1-dsv3-87k","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Gusarich/synmath-1-dsv3-87k","creator_name":"Daniil Sedov","creator_url":"https://huggingface.co/Gusarich","description":"\n\t\n\t\t\n\t\tsynmath-1-dsv3-87k\n\t\n\nsynmath-1-dsv3-87k is a dataset consisting of 86,700 math problems and their corresponding solutions, formatted in a chain-of-thought manner. The problems span 867 distinct mathematical domains, providing diverse and comprehensive coverage for fine-tuning smaller models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nsynmath-1-dsv3-87k contains synthetically generated math problems and step-by-step solutions designed to enhance mathematical reasoning in… See the full description on the dataset page: https://huggingface.co/datasets/Gusarich/synmath-1-dsv3-87k.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"step_prm","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xiaodongguaAIGC/step_prm","creator_name":"xiaodongguaAIGC","creator_url":"https://huggingface.co/xiaodongguaAIGC","description":"\n\t\n\t\t\n数据集名称\n是否有step\n可用于PRM训练\n标签形式\nTitle\n备注\n\n\n\t\t\nGSM8K\n✅\n❌\n答案\nTraining Verifiers to Solve Math Word Problems\n\n\n\nMATH\n❌\n❌\n答案\nMeasuring Mathematical Problem Solving With the MATH Dataset\nNon-Step\n\n\nPRM800K\n✅\n✅\n正确类别\nLet's Verify Step by Step\nprompt deduplication\n\n\nMath-Shepherd\n✅\n✅\n正确类别\nMath-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\nNot used\n\n\nProcessBench\n✅\n✅\n首个错误步骤\nProcessBench: Identifying Process Errors in Mathematical Reasoning\nonly label -1\n\n\n\t\n\n","first_N":5,"first_N_keywords":["text-classification","question-answering","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"HQ-knowledgedistills-1.2M-magpie","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstackorg/HQ-knowledgedistills-1.2M-magpie","creator_name":"Pinkstack-org","creator_url":"https://huggingface.co/Pinkstackorg","description":"This dataset is.an exact mix of 900k general qwen conversation with general questions, math, code and another 300k of Gemma 2 27B generations, for creative writing. \nThe dataset was made for \"healing\" pruned LLM's, especially ones based off of qwen2.5 series, as some conversations include the models saying who they are. \nUnlike the previous 900K version, we also mixed in Gemma generations, to add more creative writing examples.\nMany thanks to the magpie project for making this possible, this… See the full description on the dataset page: https://huggingface.co/datasets/Pinkstackorg/HQ-knowledgedistills-1.2M-magpie.","first_N":5,"first_N_keywords":["text-generation","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Thinking-multilingual-big-10k-sft","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/Thinking-multilingual-big-10k-sft","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"\nA dataset based off of openo1 math, 500 examples translated to 23 different languages. filtered out un-translated examples.\nenjoy 👍\n","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"OpenHumanreasoning-multilingual-2.2k","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/OpenHumanreasoning-multilingual-2.2k","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"Based off of Pinkstackorg/humanreasoning-testing-en, it is a human-generated dataset where a real person had to create most of the reasoning and the final output. If there are any mistakes please let us know.\nWe offer this dataset at an apache-2.0 license to make it useful for everybody.\nnote: translations are not human generated.\n","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"pisc-tr","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berhaan/pisc-tr","creator_name":"Berhan Türkü Ay","creator_url":"https://huggingface.co/berhaan","description":"\n\t\n\t\t\n\t\tDataset Card for CoT\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository: LLaVA-CoT GitHub Repository\nPaper: LLaVA-CoT on arXiv\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\ncat image.zip.part-* > image.zip #not uploaded yet\nunzip image.zip\n\nThe train.jsonl file contains the question-answering data and is structured in the following format:\n{\n  \"id\": \"example_id\",\n  \"image\": \"example_image_path\",\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"Lütfen resimdeki kırmızı metal nesnelerin sayısını belirtin.\"}… See the full description on the dataset page: https://huggingface.co/datasets/berhaan/pisc-tr.","first_N":5,"first_N_keywords":["visual-question-answering","English","Turkish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"mauxi-COT-Persian","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxi-COT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\t🧠 mauxi-COT-Persian Dataset\n\t\n\n\nExploring Persian Chain-of-Thought Reasoning with DeepSeek-R1, brought to you by Mauxi AI Platform\n\n\n\t\n\t\t\n\t\t🌟 Overview\n\t\n\nmauxi-COT-Persian is a community-driven dataset that explores the capabilities of advanced language models in generating Persian Chain-of-Thought (CoT) reasoning. The dataset is actively growing with new high-quality, human-validated entries being added regularly. I am personally working on expanding this dataset with rigorously… See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-COT-Persian.","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tDeepthink Reasoning Demo\n\t\n\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.… See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Synthetic_CoT_dataset_RU","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DataSynGen/Synthetic_CoT_dataset_RU","creator_name":"DataGen","creator_url":"https://huggingface.co/DataSynGen","description":"Синтетический русский датасет для Chain-of-Thought (CoT) представляет собой набор текстов, созданных для тренировки моделей в пошаговом рассуждении. Каждый элемент включает входной запрос, последовательность промежуточных шагов рассуждения и окончательный ответ. Цель датасета – улучшить способность моделей формировать логические объяснения и решения сложных задач. Примеры задач охватывают арифметические вычисления, вопросы по общей эрудиции, логические и аналитические задачи. Данные… See the full description on the dataset page: https://huggingface.co/datasets/DataSynGen/Synthetic_CoT_dataset_RU.","first_N":5,"first_N_keywords":["text-generation","Russian","apache-2.0","< 1K","text"],"keywords_longer_than_N":true},
	{"name":"CoT-XLang","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Egor-AI/CoT-XLang","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","description":"RU:CoT-XLang — это многоязычный датасет, состоящий из текстовых примеров с пошаговыми рассуждениями (Chain-of-Thought, CoT) на различных языках, включая английский, русский, японский и другие. Он используется для обучения и тестирования моделей в задачах, требующих пояснений решений через несколько шагов. Датасет включает около 2,419,912 примеров, что позволяет эффективно обучать модели, способные генерировать пошаговые рассуждения.\nРекомендация:Используйте датасет для обучения моделей… See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/CoT-XLang.","first_N":5,"first_N_keywords":["text-generation","question-answering","Russian","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"CoMT","keyword":"chain-of-thought","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/czh-up/CoMT","creator_name":"ZihuiCheng","creator_url":"https://huggingface.co/czh-up","description":"\n   CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models\n\n\n\n      \n    | [ArXiv] | [🤗HuggingFace] |\n    \n    \n\n\n\n\n\n\n🌟 Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\n\t\n\t\t🔥News\n\t\n\n\n🎖️ Our work is accepted by AAAI 2025 !\n🔥 We have release benchmark on [🤗HuggingFace].\n🔥 The paper is also available on [ArXiv].\n\n\n\t\n\t\n\t\n\t\t💡 Motivation\n\t\n\nLarge Vision-Language Models (LVLMs) have recently demonstrated amazing… See the full description on the dataset page: https://huggingface.co/datasets/czh-up/CoMT.","first_N":5,"first_N_keywords":["question-answering","image-to-image","image-to-text","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"cortex-1-market-analysis","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Jarrodbarnes/cortex-1-market-analysis","creator_name":"Jarrod Barnes","creator_url":"https://huggingface.co/Jarrodbarnes","description":"\n\t\n\t\t\n\t\tNEAR Cortex-1 Market Analysis Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains blockchain market analyses combining historical and real-time data with chain-of-thought reasoning. The dataset includes examples from Ethereum, Bitcoin, and NEAR chains, demonstrating high-quality market analysis with explicit calculations, numerical citations, and actionable insights.\nThe dataset has been enhanced with examples generated by GPT-4o and Claude 3.7 Sonnet, providing diverse… See the full description on the dataset page: https://huggingface.co/datasets/Jarrodbarnes/cortex-1-market-analysis.","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Dataset_of_Russian_thinking","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","description":"Ru\nRTD  \nОписание:Russian Thinking Dataset — это набор данных, предназначенный для обучения и тестирования моделей обработки естественного языка (NLP) на русском языке. Датасет ориентирован на задачи, связанные с генерацией текста, анализом диалогов и решением математических и логических задач.  \n\n\t\n\t\t\n\t\tОсновная информация:\n\t\n\n\nСплит: train  \nКоличество записей: 147.046\n\n\n\t\n\t\t\n\t\tЦели:\n\t\n\n\nОбучение моделей пониманию русского языка.  \nСоздание диалоговых систем с естественным взаимодействием.… See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Dataset_of_Russian_thinking","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","description":"Ru\nRTD  \nОписание:Russian Thinking Dataset — это набор данных, предназначенный для обучения и тестирования моделей обработки естественного языка (NLP) на русском языке. Датасет ориентирован на задачи, связанные с генерацией текста, анализом диалогов и решением математических и логических задач.  \n\n\t\n\t\t\n\t\tОсновная информация:\n\t\n\n\nСплит: train  \nКоличество записей: 147.046\n\n\n\t\n\t\t\n\t\tЦели:\n\t\n\n\nОбучение моделей пониманию русского языка.  \nСоздание диалоговых систем с естественным взаимодействием.… See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"CoTton-38k-6525-Collective","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","description":"\n\t\n\t\t\n\t\tCoTton-38k-6525-Collective\n\t\n\nCoTton-38k is a 38,350-example dataset of soft reasoning conversations in the ShareGPT format. Each entry contains an exchange between a user and a model, showcasing high-quality Chain-of-Thought (CoT) reasoning in natural language.\nThe dataset is distilled from open LLMs:\n\nQwen3 235B A22B\nAM Thinking\nQwQ 32B\nDeepseek R1\nR1 0528\n\nThe name CoTton encodes multiple layers of meaning:\n\nCoT: Chain-of-Thought is embedded in the name\nTON: The dataset contains a… See the full description on the dataset page: https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CoTton-38k-6525-Collective","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","description":"\n\t\n\t\t\n\t\tCoTton-38k-6525-Collective\n\t\n\nCoTton-38k is a 38,350-example dataset of soft reasoning conversations in the ShareGPT format. Each entry contains an exchange between a user and a model, showcasing high-quality Chain-of-Thought (CoT) reasoning in natural language.\nThe dataset is distilled from open LLMs:\n\nQwen3 235B A22B\nAM Thinking\nQwQ 32B\nDeepseek R1\nR1 0528\n\nThe name CoTton encodes multiple layers of meaning:\n\nCoT: Chain-of-Thought is embedded in the name\nTON: The dataset contains a… See the full description on the dataset page: https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Dhanishtha-2.0-SUPERTHINKER","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HelpingAI/Dhanishtha-2.0-SUPERTHINKER","creator_name":"HelpingAI","creator_url":"https://huggingface.co/HelpingAI","description":"📦 Dhanishtha-2.0-SUPERTHINKER\n A distilled corpus of 11.7K high-quality samples showcasing multi-phase reasoning and structured emotional cognition. Sourced directly from the internal training data of Dhanishtha-2.0 — the world’s first Large Language Model (LLM) to implement Intermediate Thinking, featuring multiple <think> and <ser> blocks per response\n\n\n\t\n\t\t\n\t\t📊 Overview\n\t\n\n\n11.7K multilingual samples (languages listed below)\nInstruction-Output format, ideal for supervised fine-tuning… See the full description on the dataset page: https://huggingface.co/datasets/HelpingAI/Dhanishtha-2.0-SUPERTHINKER.","first_N":5,"first_N_keywords":["text-generation","Afrikaans","Arabic","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"Dhanishtha-2.0-SUPERTHINKER","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HelpingAI/Dhanishtha-2.0-SUPERTHINKER","creator_name":"HelpingAI","creator_url":"https://huggingface.co/HelpingAI","description":"📦 Dhanishtha-2.0-SUPERTHINKER\n A distilled corpus of 11.7K high-quality samples showcasing multi-phase reasoning and structured emotional cognition. Sourced directly from the internal training data of Dhanishtha-2.0 — the world’s first Large Language Model (LLM) to implement Intermediate Thinking, featuring multiple <think> and <ser> blocks per response\n\n\n\t\n\t\t\n\t\t📊 Overview\n\t\n\n\n11.7K multilingual samples (languages listed below)\nInstruction-Output format, ideal for supervised fine-tuning… See the full description on the dataset page: https://huggingface.co/datasets/HelpingAI/Dhanishtha-2.0-SUPERTHINKER.","first_N":5,"first_N_keywords":["text-generation","Afrikaans","Arabic","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"CoT-aichatbot","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Bluestrikeai/CoT-aichatbot","creator_name":"BLUE STRIKE AI","creator_url":"https://huggingface.co/Bluestrikeai","description":"Bluestrikeai/CoT-aichatbot dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"LIMO_QFFT","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lwl-uestc/LIMO_QFFT","creator_name":"Wanlong Liu","creator_url":"https://huggingface.co/lwl-uestc","description":"\n\t\n\t\t\n\t\t📘 LIMO–QFFT\n\t\n\nLIMO–QFFT is a question-free variant of the original GAIR/LIMO dataset, tailored for use in QFFT (Question-Free Fine-Tuning) pipelines.\n\n\t\n\t\t\n\t\t🔍 Description\n\t\n\nThis dataset removes the original input questions and system prompts from the LIMO dataset, and keeps only the long-form reasoning responses. The goal is to enable training large language models to learn from reasoning traces alone, without depending on task-specific questions.\nAll entries are converted into… See the full description on the dataset page: https://huggingface.co/datasets/lwl-uestc/LIMO_QFFT.","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"dhanishtha-2.0-superthinker-mlx","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker-mlx","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","description":"\n\t\n\t\t\n\t\t📦 Dhanishtha-2.0-SUPERTHINKER-MLX\n\t\n\n A distilled corpus of 11.7K high-quality samples showcasing multi-phase reasoning and structured emotional cognition. Sourced directly from the internal training data of Dhanishtha-2.0 — the world’s first Large Language Model (LLM) to implement Intermediate Thinking, featuring multiple <think> and <ser> blocks per response\n\n\t\n\t\t\n\t\n\t\n\t\tExample with MLX-LM-LoRA:\n\t\n\nmlx_lm_lora.train \\\n--model… See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker-mlx.","first_N":5,"first_N_keywords":["text-generation","Afrikaans","Arabic","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"dhanishtha-2.0-superthinker-mlx","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker-mlx","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","description":"\n\t\n\t\t\n\t\t📦 Dhanishtha-2.0-SUPERTHINKER-MLX\n\t\n\n A distilled corpus of 11.7K high-quality samples showcasing multi-phase reasoning and structured emotional cognition. Sourced directly from the internal training data of Dhanishtha-2.0 — the world’s first Large Language Model (LLM) to implement Intermediate Thinking, featuring multiple <think> and <ser> blocks per response\n\n\t\n\t\t\n\t\n\t\n\t\tExample with MLX-LM-LoRA:\n\t\n\nmlx_lm_lora.train \\\n--model… See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker-mlx.","first_N":5,"first_N_keywords":["text-generation","Afrikaans","Arabic","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"RelatLogic-Reasoning","keyword":"chain-of-thought","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shb777/RelatLogic-Reasoning","creator_name":"SB","creator_url":"https://huggingface.co/shb777","description":"\n \n\n\n\n\t\n\t\t\n\t\tRelatLogic Reasoning Dataset\n\t\n\nThis dataset contains examples of logic puzzles involving comparisons, conditional statements, and superlative queries etc. each paired with a step-by-step, chain of thought reasoning and a ground‐truth answer. It is designed to advance LLM capabilities in deep, multi‐step reasoning, constraint satisfaction and evidence evaluation.\n\n\t\n\t\t\n\t\t🤔 Curation Rationale\n\t\n\nA lot of LLM's these days are \"aligned\" to agree with the user. You can \"convince\" it… See the full description on the dataset page: https://huggingface.co/datasets/shb777/RelatLogic-Reasoning.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Resume-Analysis-CoTR","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","description":"\n\t\n\t\t\n\t\tResume Reasoning and Feedback Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains approximately 417 examples designed to facilitate research and development in automated resume analysis and feedback generation. Each data point consists of a user query regarding their resume, a simulated internal analysis (chain-of-thought) performed by an expert persona, and a final, user-facing feedback response derived solely from that analysis.\nThe dataset captures a two-step reasoning… See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR.","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"altered-riddles","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marcodsn/altered-riddles","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","description":"\n \n\n\n\n\t\n\t\t\n\t\tDataset Card for Altered Riddles Dataset\n\t\n\nWhile working on the academic-chains dataset, I tested a well-known alteration of a common riddle, \"just for fun\":\n\nThe surgeon, who is the boy's father, says, 'I cannot operate on this boy—he's my son!'. Who is the surgeon to the boy?\n\n(Below is the original riddle for reference)\n\nA man and his son are in a terrible accident and are rushed to the hospital in critical condition. The doctor looks at the boy and exclaims, \"I can't operate… See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/altered-riddles.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"think-more","keyword":"chain-of-thought","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/think-more","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThink More is a large-scale, multi-domain collection of long chain-of-thought (CoT) reasoning examples. It aggregates and cleans several prominent reasoning datasets, focusing on high-quality, step-by-step model-generated solutions from DeepSeek R1 and OpenAI o1. Each entry includes a question, the model’s answer, and the detailed thought process leading to that answer.\n⚠️ Warning: the dataset decompresses to a 15.1 GB JSONLines file.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/think-more.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"train-of-thought","keyword":"chain-of-thought","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/train-of-thought","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tTrain of Thought Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset readapts agentlans/think-more\ninto the Alpaca-style instruction tuning format for training language models in direct answering and chain-of-thought reasoning.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach original example was randomly assigned to be thinking on or off:\n\nThinking off: Outputs only the final answer.\nThinking on:\nOutputs a chain-of-thought (CoT) reasoning process wrapped in <think>...</think>, followed by the final answer… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/train-of-thought.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"dx7-patches-and-prompts","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ccerati/dx7-patches-and-prompts","creator_name":"Carlo Cerati","creator_url":"https://huggingface.co/ccerati","description":"\n\t\n\t\t\n\t\tYamaha DX7 Synthesizer Patches with AI-Generated Prompts\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a comprehensive, multi-task dataset designed for fine-tuning language models to understand and generate synthesizer patches for the Yamaha DX7.\nThe dataset contains over 20,000 examples across three distinct but related tasks, making it ideal for creating models that can not only generate patches but also understand and reason about their structure and validity.\n\n\t\n\t\t\n\t\tHow the Data Was… See the full description on the dataset page: https://huggingface.co/datasets/ccerati/dx7-patches-and-prompts.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K<n<100K","Text"],"keywords_longer_than_N":true},
	{"name":"Mixture-of-Thoughts-2048T","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FlameF0X/Mixture-of-Thoughts-2048T","creator_name":"Daniel Fox","creator_url":"https://huggingface.co/FlameF0X","description":"\n\t\n\t\t\n\t\tMixture-of-Thoughts-2048T\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMixture-of-Thoughts-2048T is a curated subset of the open-r1/Mixture-of-Thoughts dataset, specifically filtered to include only examples that are approximately 2048 tokens or fewer in length. This dataset is designed for training and evaluating language models on reasoning tasks with constrained context lengths.\n\n\t\n\t\t\n\t\tFiltering Criteria\n\t\n\n\nToken Limit: Examples are filtered to contain ≤ 2048 tokens\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom… See the full description on the dataset page: https://huggingface.co/datasets/FlameF0X/Mixture-of-Thoughts-2048T.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"math-rollouts","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/uzaymacar/math-rollouts","creator_name":"Uzay Macar","creator_url":"https://huggingface.co/uzaymacar","description":"\n\t\n\t\t\n\t\tMathematical Reasoning Rollouts Dataset\n\t\n\n\n\nThis dataset contains step-by-step reasoning rollouts generated with DeepSeek R1-Distill language models solving mathematical problems from the MATH dataset. \nThe dataset is designed for analyzing reasoning patterns, branching factors, planning strategies, and the effectiveness of different reasoning approaches in mathematical problem-solving.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\nCurated by: Uzay Macar, Paul… See the full description on the dataset page: https://huggingface.co/datasets/uzaymacar/math-rollouts.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"cot_data_slow_thinking_conversations","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Jianyuan1/cot_data_slow_thinking_conversations","creator_name":"Zhong","creator_url":"https://huggingface.co/Jianyuan1","description":"\n\t\n\t\t\n\t\tcot_data_slow_thinking_conversations\n\t\n\nThis dataset contains chain-of-thought reasoning data with slow thinking patterns.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nFormat: JSONL (JSON Lines) with Hugging Face conversations format\nSize: 1113.88 MB\nTotal examples: Approximately 156,268 examples\n\nEach line contains a JSON object with a \"conversations\" key containing a list of messages with user queries about mathematical reasoning steps and assistant responses with thinking patterns.\n\n\t\n\t\t\n\t\tExample… See the full description on the dataset page: https://huggingface.co/datasets/Jianyuan1/cot_data_slow_thinking_conversations.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"PathSum-CoT","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/singhprabhat/PathSum-CoT","creator_name":"Prabhat Singh","creator_url":"https://huggingface.co/singhprabhat","description":"singhprabhat/PathSum-CoT dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["summarization","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"german_tlr_gold_14k","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/arnomatic/german_tlr_gold_14k","creator_name":"arnomatic","creator_url":"https://huggingface.co/arnomatic","description":"\n\t\n\t\t\n\t\t🧠 German TLR Gold Dataset (14.5k)\n\t\n\n\n\t\n\t\t\n\t\t📊 Dataset Overview\n\t\n\nEin hochwertiger deutschsprachiger Datensatz mit 14.500 Samples im Think-Learn-Respond (TLR) Format für das Training von reasoning-fähigen Large Language Models.\nFormat: Jede Antwort ist strukturiert in:\n\n<think>: Strukturierter Denkprozess und Reasoning\n<answer>: Finale, klare Antwort\n\n\n\t\n\t\t\n\t\t🎯 Anwendung\n\t\n\nDieses Dataset wurde speziell entwickelt für:\n\nSupervised Fine-Tuning (SFT) von deutschen LLMs\nTraining von… See the full description on the dataset page: https://huggingface.co/datasets/arnomatic/german_tlr_gold_14k.","first_N":5,"first_N_keywords":["text-generation","question-answering","German","mit","10K - 100K"],"keywords_longer_than_N":true}
]
;
