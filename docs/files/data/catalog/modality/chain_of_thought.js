const data_for_modality_chain_of_thought = 
[
	{"name":"TinyDS-20k","keyword":"cot","description":"\n\t\n\t\t\n\t\tTinyDS\n\t\n\n\n\n\nAlpaca-style dataset with around 20k samples scraped from Qwen3-8B using SyntheticAlpaca. Q&A pairs can be in 32 different languages, these are listed in the metadata.Topics are all around STEM, programming, and literature.  \nMIT @ 2025 Hamzah Asadullah\n\n\n","url":"https://huggingface.co/datasets/Hamzah-Asadullah/TinyDS-20k","creator_name":"Hamzah Asadullah","creator_url":"https://huggingface.co/Hamzah-Asadullah","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","translation","text-generation","text2text-generation","English"],"keywords_longer_than_N":true},
	{"name":"AquilaX-AI-security-assistant-reasoning","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tAquilaX Security Assistant with Reasoning Template\n\t\n\nA cybersecurity instruction-tuning dataset converted from AquilaX-AI/security_assistant_data with explicit reasoning template for training models with chain-of-thought capabilities in vulnerability analysis.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 18,282 examples focused on cybersecurity vulnerability analysis, secure coding practices, and security remediation. Each assistant response includes structured reasoning steps‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tuandunghcmut/AquilaX-AI-security-assistant-reasoning.","url":"https://huggingface.co/datasets/tuandunghcmut/AquilaX-AI-security-assistant-reasoning","creator_name":"D≈©ng V√µ","creator_url":"https://huggingface.co/tuandunghcmut","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"OrgStrategy-Reasoning-1k","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tOrgStrategy Reasoning 1k (v2)\n\t\n\n\n\t\n\t\t\n\t\tWhat This Dataset Is About\n\t\n\nThis dataset contains 1,000 real-world business strategy scenarios with structured reasoning chains that teach AI models to apply proven strategic frameworks to complex organizational problems.\n\n\t\n\t\t\n\t\tCore Purpose\n\t\n\n\nTrain models to think strategically using established business frameworks\nProvide structured reasoning patterns for complex problem-solving\nBridge the gap between generic AI responses and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Wildstash/OrgStrategy-Reasoning-1k.","url":"https://huggingface.co/datasets/Wildstash/OrgStrategy-Reasoning-1k","creator_name":"ArnavS","creator_url":"https://huggingface.co/Wildstash","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"MAmmoTH-VL-Instruct-12M","keyword":"cot","description":"\n\t\n\t\t\n\t\tMAmmoTH-VL-Instruct-12M\n\t\n\nüè† Homepage | ü§ñ MAmmoTH-VL-8B | üíª Code | üìÑ Arxiv | üìï PDF | üñ•Ô∏è Demo\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nOur simple yet scalable visual instruction data rewriting pipeline consists of three steps: manual data source collection, rewriting using MLLMs/LLMs, and filtering via the same MLLM as a judge. Examples below illustrate transformations in math and science categories, showcasing detailed, step-by-step responses.\n\n\t\n\t\t\n\t\tThe data distribution of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M.","url":"https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M","creator_name":"MAmmoTH-VL","creator_url":"https://huggingface.co/MAmmoTH-VL","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"DeAR-COT","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tListwise Chain-of-Thought Re-ranking Dataset (DeAR-COT)\n\t\n\nRepo: abdoelsayed/DeAR-COTTask: listwise passage re-ranking with optional Chain-of-Thought (CoT) rationalesFormat: JSONL (one JSON object per line)Language: English\n\n\t\n\t\t\n\t\tSummary\n\t\n\nDeAR-CoT is a listwise re-ranking dataset designed for training and evaluating LLM rerankers.Each example contains:\n\na search query,\nk candidate passages embedded inline in instruction as [1] ... [k],\na target listwise ranking (final ordered IDs)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abdoelsayed/DeAR-COT.","url":"https://huggingface.co/datasets/abdoelsayed/DeAR-COT","creator_name":"Abdelrahman Abdallah","creator_url":"https://huggingface.co/abdoelsayed","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"DeAR-COT","keyword":"cot","description":"\n\t\n\t\t\n\t\tListwise Chain-of-Thought Re-ranking Dataset (DeAR-COT)\n\t\n\nRepo: abdoelsayed/DeAR-COTTask: listwise passage re-ranking with optional Chain-of-Thought (CoT) rationalesFormat: JSONL (one JSON object per line)Language: English\n\n\t\n\t\t\n\t\tSummary\n\t\n\nDeAR-CoT is a listwise re-ranking dataset designed for training and evaluating LLM rerankers.Each example contains:\n\na search query,\nk candidate passages embedded inline in instruction as [1] ... [k],\na target listwise ranking (final ordered IDs)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abdoelsayed/DeAR-COT.","url":"https://huggingface.co/datasets/abdoelsayed/DeAR-COT","creator_name":"Abdelrahman Abdallah","creator_url":"https://huggingface.co/abdoelsayed","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"altered-riddles","keyword":"chain-of-thought","description":"\n \n\n\n\n\t\n\t\t\n\t\tDataset Card for Altered Riddles Dataset\n\t\n\nWhile working on the academic-chains dataset, I tested a well-known alteration of a common riddle, \"just for fun\":\n\nThe surgeon, who is the boy's father, says, 'I cannot operate on this boy‚Äîhe's my son!'. Who is the surgeon to the boy?\n\n(Below is the original riddle for reference)\n\nA man and his son are in a terrible accident and are rushed to the hospital in critical condition. The doctor looks at the boy and exclaims, \"I can't operate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/altered-riddles.","url":"https://huggingface.co/datasets/marcodsn/altered-riddles","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"PRISM-DPO","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tPRISM: Principled Reasoning for Integrated Safety in Multimodality Datasets\n\t\n\nThis repository provides access to the datasets developed for PRISM (Principled Reasoning for Integrated Safety in Multimodality), a system2-like framework that aligns Vision-Language Models (VLMs) by embedding a structured, safety-aware reasoning process.\n\nPaper: PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality\nCode: https://github.com/SaFoLab-WISC/PRISM‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andyc03/PRISM-DPO.","url":"https://huggingface.co/datasets/andyc03/PRISM-DPO","creator_name":"Nanxi Li","creator_url":"https://huggingface.co/andyc03","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","Image","arxiv:2508.18649"],"keywords_longer_than_N":true},
	{"name":"AIME25-CoT-CN","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tSci-Bench-AIME25'\n\t\n\nThis repo is a branch of Sci Bench made by IPF team. Mainly include the AIME 25' solution with multi-modal CoT and diverse solving path. \n\n\t\n\t\t\n\t\tüìö Cite\n\t\n\nIf you use the Sci-Bench-AIME25 (IPF/AIME25-CoT-CN) dataset in your research, please cite:\n@dataset{zhang2025scibench_aime25,\n    title = {{Sci-Bench-AIME25}: A Multi-Modal Chain-of-Thought Dataset for Advanced Tool-Intergrated Mathematical Reasoning},\n    author = {Zhang, Haoxiang and Wang, Siyuan and Fang‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IPF/AIME25-CoT-CN.","url":"https://huggingface.co/datasets/IPF/AIME25-CoT-CN","creator_name":"Isaac_GHX","creator_url":"https://huggingface.co/IPF","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"vulnerability-intelligence-diagrammatic-reasoning","keyword":"chain-of-thought","description":"\n \n\n\n\n\t\n\t\t\n\t\tVulnerability Intelligence with Diagrammatic Reasoning\n\t\n\n\n[!Important]\nThis dataset was created as a proof-of-concept for the Reasoning Datasets Competition (May 2025). If you have any feedback or suggestions, please feel free to open a discussion! Access the Github repository here.\n\n\n\t\n\t\t\n\t\n\t\n\t\tA. Overview\n\t\n\n\n\n\n\n\n\nThis dataset focuses on security vulnerability analysis through a multi-dimensional approach that combines four types of reasoning to generate valuable insights for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/daqc/vulnerability-intelligence-diagrammatic-reasoning.","url":"https://huggingface.co/datasets/daqc/vulnerability-intelligence-diagrammatic-reasoning","creator_name":"David Quispe","creator_url":"https://huggingface.co/daqc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"synmath-1-dsv3-87k","keyword":"cot","description":"\n\t\n\t\t\n\t\tsynmath-1-dsv3-87k\n\t\n\nsynmath-1-dsv3-87k is a dataset consisting of 86,700 math problems and their corresponding solutions, formatted in a chain-of-thought manner. The problems span 867 distinct mathematical domains, providing diverse and comprehensive coverage for fine-tuning smaller models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nsynmath-1-dsv3-87k contains synthetically generated math problems and step-by-step solutions designed to enhance mathematical reasoning in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gusarich/synmath-1-dsv3-87k.","url":"https://huggingface.co/datasets/Gusarich/synmath-1-dsv3-87k","creator_name":"Daniil Sedov","creator_url":"https://huggingface.co/Gusarich","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"math-rollouts","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tMathematical Reasoning Rollouts Dataset\n\t\n\n\n\nThis dataset contains step-by-step reasoning rollouts generated with DeepSeek R1-Distill language models solving mathematical problems from the MATH dataset.\nThe dataset is designed for analyzing reasoning patterns, branching factors, planning strategies, and the effectiveness of different reasoning approaches in mathematical problem-solving.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\nCurated by: Uzay Macar, Paul C.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uzaymacar/math-rollouts.","url":"https://huggingface.co/datasets/uzaymacar/math-rollouts","creator_name":"Uzay Macar","creator_url":"https://huggingface.co/uzaymacar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ru-chain-of-thought-sharegpt","keyword":"cot","description":"–ü–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω–∞—è –ø—Ä–∏ –ø–æ–º–æ—â–∏ utrobinmv/t5_translate_en_ru_zh_small_1024 –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –≤–µ—Ä—Å–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ isaiahbjork/chain-of-thought-sharegpt.\n","url":"https://huggingface.co/datasets/evilfreelancer/ru-chain-of-thought-sharegpt","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"medical_cot_rus","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tMykes/medical_cot_rus\n\t\n\n–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º –¥–æ–º–µ–Ω–µ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought, CoT). –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞—á –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –≤–æ–ø—Ä–æ—Å–æ-–æ—Ç–≤–µ—Ç–∞, –¥–æ–æ–±—É—á–µ–Ω–∏—è LLM –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å—é.\n\n–û–±—ä–µ–º: ‚âà 6.29k –∑–∞–ø–∏—Å–µ–π\n–Ø–∑—ã–∫: —Ä—É—Å—Å–∫–∏–π\n–î–æ–º–µ–Ω—ã: –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã, —Å–∏–º–ø—Ç–æ–º—ã, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è, –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∏ –¥—Ä.\n–ü–æ–ª—è: question, raw_answer, cot, answer, old_thoughts\n\n‚ö†Ô∏è –û—Ç–∫–∞–∑ –æ—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏: –¥–∞—Ç–∞—Å–µ—Ç‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mykes/medical_cot_rus.","url":"https://huggingface.co/datasets/Mykes/medical_cot_rus","creator_name":"Maxim Titkov","creator_url":"https://huggingface.co/Mykes","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Russian","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-cp32","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tChain of ThoughtÁîüÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà\n\t\n\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂïèÈ°å„Å®Ëß£Á≠î„Åã„ÇâË™¨ÊòéÔºàChain of ThoughtÔºâ„ÇíÁîüÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tÊ¶ÇË¶Å\n\t\n\n\nÂá¶ÁêÜ„Åó„Åü„Çµ„É≥„Éó„É´Êï∞: 60\nÊúâÂäπ„Å™Ë™¨ÊòéÁîüÊàêÊï∞: 60\nÁîüÊàêÊàêÂäüÁéá: 100.00%\n‰ΩøÁî®„É¢„Éá„É´: /home/Competition2025/P07/shareP07/share_model/step2_rlt/Qwen3-14B-step2-deepmath103k-bs512/checkpoint-32\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞Áµ±Ë®à\n\t\n\n\nÊúÄÂ∞è„Éà„Éº„ÇØ„É≥Êï∞: 892\nÊúÄÂ§ß„Éà„Éº„ÇØ„É≥Êï∞: 5932\nÂπ≥Âùá„Éà„Éº„ÇØ„É≥Êï∞: 3560.4\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞ÂàÜÂ∏É\n\t\n\n\n0-100„Éà„Éº„ÇØ„É≥: 0‰ª∂ (0.0%)\n101-500„Éà„Éº„ÇØ„É≥: 0‰ª∂ (0.0%)\n501-1000„Éà„Éº„ÇØ„É≥: 1‰ª∂ (1.7%)\n1001-2000„Éà„Éº„ÇØ„É≥: 10‰ª∂ (16.7%)\n2001-5000„Éà„Éº„ÇØ„É≥: 41‰ª∂ (68.3%)\n5001+„Éà„Éº„ÇØ„É≥: 8‰ª∂ (13.3%)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-cp32.","url":"https://huggingface.co/datasets/llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-cp32","creator_name":"llm-compe-2025-kato","creator_url":"https://huggingface.co/llm-compe-2025-kato","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Japanese","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"RelatLogic-Reasoning","keyword":"chain-of-thought","description":"\n \n\n\n\n\t\n\t\t\n\t\tRelatLogic Reasoning Dataset\n\t\n\nThis dataset contains examples of logic puzzles involving comparisons, conditional statements, and superlative queries etc. each paired with a step-by-step, chain of thought reasoning and a ground‚Äêtruth answer. It is designed to advance LLM capabilities in deep, multi‚Äêstep reasoning, constraint satisfaction and evidence evaluation.\n\n\t\n\t\t\n\t\tü§î Curation Rationale\n\t\n\nA lot of LLM's these days are \"aligned\" to agree with the user. You can \"convince\" it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shb777/RelatLogic-Reasoning.","url":"https://huggingface.co/datasets/shb777/RelatLogic-Reasoning","creator_name":"SB","creator_url":"https://huggingface.co/shb777","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"dhanishtha-2.0-superthinker","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tüì¶ Dhanishtha-2.0-SUPERTHINKER-MLX\n\t\n\n A distilled corpus of 11.7K high-quality samples showcasing multi-phase reasoning and structured emotional cognition. Sourced directly from the internal training data of Dhanishtha-2.0 ‚Äî the world‚Äôs first Large Language Model (LLM) to implement Intermediate Thinking, featuring multiple <think> and <ser> blocks per response\n\n\t\n\t\t\n\t\n\t\n\t\tExample with MLX-LM-LoRA:\n\t\n\nmlx_lm_lora.train \\\n--model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker.","url":"https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Afrikaans","Arabic","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"reflection-small-sonnet","keyword":"cot","description":"\n\t\n\t\t\n\t\tVerified reasoning examples\n\t\n\n","url":"https://huggingface.co/datasets/efederici/reflection-small-sonnet","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","Italian","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"dhanishtha-2.0-superthinker","keyword":"cot","description":"\n\t\n\t\t\n\t\tüì¶ Dhanishtha-2.0-SUPERTHINKER-MLX\n\t\n\n A distilled corpus of 11.7K high-quality samples showcasing multi-phase reasoning and structured emotional cognition. Sourced directly from the internal training data of Dhanishtha-2.0 ‚Äî the world‚Äôs first Large Language Model (LLM) to implement Intermediate Thinking, featuring multiple <think> and <ser> blocks per response\n\n\t\n\t\t\n\t\n\t\n\t\tExample with MLX-LM-LoRA:\n\t\n\nmlx_lm_lora.train \\\n--model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker.","url":"https://huggingface.co/datasets/mlx-community/dhanishtha-2.0-superthinker","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Afrikaans","Arabic","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"Sky-T1_data_steps","keyword":"cot","description":"\n\t\n\t\t\n\t\tSky-T1_data_steps\n\t\n\nThis dataset contains 182 samples taken from NovaSky-AI/Sky-T1_data_17k \ndataset and broken down to thinking steps. This dataset was used to train shakedzy/Sky-T1-32B-Steps \nLoRA adapter for step-by-step thinking.\nBreaking down the thought process to steps was done using Ollama's quantized version of Llama-3.2-1B.\nSee step_prompt file for the exact prompt used.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Columns\n\t\n\n\nid (int): row index of the sample in the original dataset (starts at 0)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shakedzy/Sky-T1_data_steps.","url":"https://huggingface.co/datasets/shakedzy/Sky-T1_data_steps","creator_name":"Shaked","creator_url":"https://huggingface.co/shakedzy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"step2-evaluated-dataset-test2","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tComplete Evaluation Dataset (Rubric + LogP)\n\t\n\nThis dataset contains chain-of-thought explanations evaluated using both comprehensive rubric assessment and LogP evaluation.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nSource Dataset: llm-compe-2025-kato/step2-evaluated-dataset-test2\nTotal Samples: 92\nSuccessfully Evaluated (Rubric): 92\nFailed Evaluations (Rubric): 0\nEvaluation Model: Qwen/Qwen3-32B\n\n\n\t\n\t\t\n\t\tRubric Evaluation Results\n\t\n\n\n\t\n\t\t\n\t\tAverage Rubric Scores (0-4 scale)\n\t\n\n\nlogical_coherence: 3.51‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-compe-2025-kato/step2-evaluated-dataset-test2.","url":"https://huggingface.co/datasets/llm-compe-2025-kato/step2-evaluated-dataset-test2","creator_name":"llm-compe-2025-kato","creator_url":"https://huggingface.co/llm-compe-2025-kato","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"OpenHumanreasoning-multilingual-2.2k","keyword":"cot","description":"Based off of Pinkstackorg/humanreasoning-testing-en, it is a human-generated dataset where a real person had to create most of the reasoning and the final output. If there are any mistakes please let us know.\nWe offer this dataset at an apache-2.0 license to make it useful for everybody.\nnote: translations are not human generated.\n","url":"https://huggingface.co/datasets/Pinkstack/OpenHumanreasoning-multilingual-2.2k","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"cortex-1-market-analysis","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tNEAR Cortex-1 Market Analysis Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains blockchain market analyses combining historical and real-time data with chain-of-thought reasoning. The dataset includes examples from Ethereum, Bitcoin, and NEAR chains, demonstrating high-quality market analysis with explicit calculations, numerical citations, and actionable insights.\nThe dataset has been enhanced with examples generated by GPT-4o and Claude 3.7 Sonnet, providing diverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jarrodbarnes/cortex-1-market-analysis.","url":"https://huggingface.co/datasets/Jarrodbarnes/cortex-1-market-analysis","creator_name":"Jarrod Barnes","creator_url":"https://huggingface.co/Jarrodbarnes","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"PathSum-CoT","keyword":"chain-of-thought","description":"singhprabhat/PathSum-CoT dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/singhprabhat/PathSum-CoT","creator_name":"Prabhat Singh","creator_url":"https://huggingface.co/singhprabhat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Drone-flight-monitoring-reasoning-SFT","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tDrone-flight-monitoring-reasoning-SFT\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nÊú¨Êï∞ÊçÆÈõÜÊòØ‰∏Ä‰∏™‰∏ìÊ≥®‰∫éÊó†‰∫∫Êú∫È£ûË°åÂÆâÂÖ®È¢ÜÂüüÁöÑ‰∏≠ÊñáÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºåÈááÁî®‰∫ÜChain-of-Thought (CoT) ÁöÑÊ†ºÂºè„ÄÇÂÆÉÊó®Âú®Áî®‰∫éÁªÉ‰π†Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂæÆË∞ÉËÆ≠ÁªÉÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊ®°Êãü‰∏ìÂÆ∂ÊÄùËÄÉËøáÁ®ãÔºåÂπ∂ÈíàÂØπÊó†‰∫∫Êú∫ÂÆâÂÖ®Áõ∏ÂÖ≥ÈóÆÈ¢òÁîüÊàêÂåÖÂê´Êé®ÁêÜÊ≠•È™§ÁöÑÁªìÊûÑÂåñÂõûÁ≠î„ÄÇÂæÆË∞ÉÂêéÊ®°ÂûãËßÅ(GabrielCheng/Deepseek-r1-finetuned-drone-safty) „ÄÇ\nÊú¨Êï∞ÊçÆÈõÜÊòØÂü∫‰∫é Hugging Face Âπ≥Âè∞‰∏äÁöÑ skylink-drone-cot-datasets (pohsjxx/default-domain-cot-dataset) ËøõË°åÂ§ÑÁêÜÂíåË°çÁîüÁöÑ„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure / Data Fields\n\t\n\nÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÊØè‰∏™Ê†∑Êú¨ÂåÖÂê´‰ª•‰∏ãÂ≠óÊÆµÔºö\n\nQuestion (string): ÂÖ≥‰∫éÊó†‰∫∫Êú∫È£ûË°åÂÆâÂÖ®ÊàñÈ£éÈô©Áõ∏ÂÖ≥ÁöÑÈóÆÈ¢ò„ÄÇ\nReasoning (string): Ê®°ÊãüÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ\nAnswer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GabrielCheng/Drone-flight-monitoring-reasoning-SFT.","url":"https://huggingface.co/datasets/GabrielCheng/Drone-flight-monitoring-reasoning-SFT","creator_name":"Gabriel","creator_url":"https://huggingface.co/GabrielCheng","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"GenRef-CoT","keyword":"cot","description":"\n\t\n\t\t\n\t\tGenRef-CoT\n\t\n\n\n  \n\n\nWe provide 227K high-quality CoT reflections which were used to train our Qwen-based reflection generation model in ReflectionFlow [1]. To\nknow the details of the dataset creation pipeline, please refer to Section 3.2 of [1].\n\n\t\n\t\t\n\t\tDataset loading\n\t\n\nWe provide the dataset in the webdataset format for fast dataloading and streaming. We recommend downloading\nthe repository locally for faster I/O:\nfrom huggingface_hub import snapshot_download\n\nlocal_dir =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/diffusion-cot/GenRef-CoT.","url":"https://huggingface.co/datasets/diffusion-cot/GenRef-CoT","creator_name":"Diffusion CoT","creator_url":"https://huggingface.co/diffusion-cot","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"tag-validation-results-test","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\t„Çø„Ç∞Ê§úË®ºÁµêÊûú\n\t\n\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØË™¨ÊòéÁîüÊàê„Å´„Åä„Åë„Çã„Çø„Ç∞Ê§úË®º„ÅÆÁµêÊûú„ÇíÂê´„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ\n\n\t\n\t\t\n\t\tÊ¶ÇË¶Å\n\t\n\n\nÂá¶ÁêÜ„Åó„Åü„Çµ„É≥„Éó„É´Êï∞: 100\nË™¨ÊòéÁµÇ‰∫Ü„Çø„Ç∞ÊàêÂäüÁéá: 92.00%\nÊ§úË®ºÂØæË±°„Çø„Ç∞: <|end_of_explanation|>\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞Áµ±Ë®à\n\t\n\n\nÊúÄÂ∞è„Éà„Éº„ÇØ„É≥Êï∞: 226\nÊúÄÂ§ß„Éà„Éº„ÇØ„É≥Êï∞: 30070 \nÂπ≥Âùá„Éà„Éº„ÇØ„É≥Êï∞: 1773.4\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞ÂàÜÂ∏É\n\t\n\n\n0-100„Éà„Éº„ÇØ„É≥: 0‰ª∂ (0.0%)\n101-500„Éà„Éº„ÇØ„É≥: 44‰ª∂ (44.0%)\n501-1000„Éà„Éº„ÇØ„É≥: 37‰ª∂ (37.0%)\n1001-2000„Éà„Éº„ÇØ„É≥: 13‰ª∂ (13.0%)\n2001-5000„Éà„Éº„ÇØ„É≥: 1‰ª∂ (1.0%)\n5001+„Éà„Éº„ÇØ„É≥: 5‰ª∂ (5.0%)\n\n\n\t\n\t\t\n\t\t„Éá„Éº„Çø„Çª„ÉÉ„ÉàÊßãÈÄ†\n\t\n\n\nid: ÂêÑ„Çµ„É≥„Éó„É´„ÅÆ‰∏ÄÊÑèË≠òÂà•Â≠ê\nuser_content: „É¢„Éá„É´„Å´„É¶„Éº„Ç∂„Éº„É°„ÉÉ„Çª„Éº„Ç∏„Å®„Åó„Å¶ÈÄÅ‰ø°„Åó„ÅüÂÜÖÂÆπÔºàË≥™Âïè„ÅÆ„ÅøÔºâ\nassistant_content: „É¢„Éá„É´„Å´„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„É°„ÉÉ„Çª„Éº„Ç∏„Å®„Åó„Å¶ÈÄÅ‰ø°„Åó„ÅüÂÜÖÂÆπÔºà„Çø„Ç∞‰ªò„ÅçËß£Á≠î +‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-compe-2025-kato/tag-validation-results-test.","url":"https://huggingface.co/datasets/llm-compe-2025-kato/tag-validation-results-test","creator_name":"llm-compe-2025-kato","creator_url":"https://huggingface.co/llm-compe-2025-kato","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Japanese","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-Qwen3-14B","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tChain of ThoughtÁîüÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà\n\t\n\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂïèÈ°å„Å®Ëß£Á≠î„Åã„ÇâË™¨ÊòéÔºàChain of ThoughtÔºâ„ÇíÁîüÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tÊ¶ÇË¶Å\n\t\n\n\nÂá¶ÁêÜ„Åó„Åü„Çµ„É≥„Éó„É´Êï∞: 156\nÊúâÂäπ„Å™Ë™¨ÊòéÁîüÊàêÊï∞: 156\nÁîüÊàêÊàêÂäüÁéá: 100.00%\n‰ΩøÁî®„É¢„Éá„É´: Qwen/Qwen3-14B\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞Áµ±Ë®à\n\t\n\n\nÊúÄÂ∞è„Éà„Éº„ÇØ„É≥Êï∞: 313\nÊúÄÂ§ß„Éà„Éº„ÇØ„É≥Êï∞: 3066\nÂπ≥Âùá„Éà„Éº„ÇØ„É≥Êï∞: 887.4\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞ÂàÜÂ∏É\n\t\n\n\n0-100„Éà„Éº„ÇØ„É≥: 0‰ª∂ (0.0%)\n101-500„Éà„Éº„ÇØ„É≥: 26‰ª∂ (16.7%)\n501-1000„Éà„Éº„ÇØ„É≥: 85‰ª∂ (54.5%)\n1001-2000„Éà„Éº„ÇØ„É≥: 39‰ª∂ (25.0%)\n2001-5000„Éà„Éº„ÇØ„É≥: 6‰ª∂ (3.8%)\n5001+„Éà„Éº„ÇØ„É≥: 0‰ª∂ (0.0%)\n\n\n\t\n\t\t\n\t\t„Éá„Éº„Çø„Çª„ÉÉ„ÉàÊßãÈÄ†\n\t\n\n\nsystem_prompt: „É¢„Éá„É´„Å´ÈÄÅ‰ø°„Åï„Çå„Åü„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà\nquestion_text: ÂÖÉ„ÅÆÂïèÈ°åÊñá\nanswer_text: ÂïèÈ°å„ÅÆËß£Á≠î‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-Qwen3-14B.","url":"https://huggingface.co/datasets/llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-Qwen3-14B","creator_name":"llm-compe-2025-kato","creator_url":"https://huggingface.co/llm-compe-2025-kato","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Japanese","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-cp40","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tChain of ThoughtÁîüÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà\n\t\n\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂïèÈ°å„Å®Ëß£Á≠î„Åã„ÇâË™¨ÊòéÔºàChain of ThoughtÔºâ„ÇíÁîüÊàê„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tÊ¶ÇË¶Å\n\t\n\n\nÂá¶ÁêÜ„Åó„Åü„Çµ„É≥„Éó„É´Êï∞: 58\nÊúâÂäπ„Å™Ë™¨ÊòéÁîüÊàêÊï∞: 58\nÁîüÊàêÊàêÂäüÁéá: 100.00%\n‰ΩøÁî®„É¢„Éá„É´: /home/Competition2025/P07/shareP07/share_model/step2_rlt/Qwen3-14B-step2-deepmath103k-bs512/checkpoint-40\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞Áµ±Ë®à\n\t\n\n\nÊúÄÂ∞è„Éà„Éº„ÇØ„É≥Êï∞: 830\nÊúÄÂ§ß„Éà„Éº„ÇØ„É≥Êï∞: 6301\nÂπ≥Âùá„Éà„Éº„ÇØ„É≥Êï∞: 3477.9\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞ÂàÜÂ∏É\n\t\n\n\n0-100„Éà„Éº„ÇØ„É≥: 0‰ª∂ (0.0%)\n101-500„Éà„Éº„ÇØ„É≥: 0‰ª∂ (0.0%)\n501-1000„Éà„Éº„ÇØ„É≥: 1‰ª∂ (1.7%)\n1001-2000„Éà„Éº„ÇØ„É≥: 9‰ª∂ (15.5%)\n2001-5000„Éà„Éº„ÇØ„É≥: 39‰ª∂ (67.2%)\n5001+„Éà„Éº„ÇØ„É≥: 9‰ª∂ (15.5%)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-cp40.","url":"https://huggingface.co/datasets/llm-compe-2025-kato/step2-DeepMath-103K-Bespoke-Filtered-Test-200-eval-cp40","creator_name":"llm-compe-2025-kato","creator_url":"https://huggingface.co/llm-compe-2025-kato","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Japanese","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Mixture-of-Thoughts-2048T","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tMixture-of-Thoughts-2048T\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMixture-of-Thoughts-2048T is a curated subset of the open-r1/Mixture-of-Thoughts dataset, specifically filtered to include only examples that are approximately 2048 tokens or fewer in length. This dataset is designed for training and evaluating language models on reasoning tasks with constrained context lengths.\n\n\t\n\t\t\n\t\tFiltering Criteria\n\t\n\n\nToken Limit: Examples are filtered to contain ‚â§ 2048 tokens\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FlameF0X/Mixture-of-Thoughts-2048T.","url":"https://huggingface.co/datasets/FlameF0X/Mixture-of-Thoughts-2048T","creator_name":"Daniel Fox","creator_url":"https://huggingface.co/FlameF0X","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"indic_reasoning","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tIndic Reasoning\n\t\n\nThe Indic Reasoning Dataset (~500M tokens, 592k examples) is a high-quality, large-scale open-source resource created using advanced distillation techniques. It is designed to train and evaluate reasoning-capable AI systems with a strong emphasis on complex reasoning, structured chain-of-thought (CoT), and culturally relevant content.\nThis domain-rich corpus integrates Indian cultural, legal, historical, philosophical, and social contexts with global knowledge‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/169Pi/indic_reasoning.","url":"https://huggingface.co/datasets/169Pi/indic_reasoning","creator_name":"169Pi","creator_url":"https://huggingface.co/169Pi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MATH-500-Overall","keyword":"cot","description":"\n\t\n\t\t\n\t\tMATH-500-Overall\n\t\n\n\n\t\n\t\t\n\t\tAbout the dataset\n\t\n\nThis dataset of only 500 examples combines mathematics, physics and logic in English with reasoning and step-by-step problem solving, the dataset was created synthetically, CoT of Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct.\n\n\t\n\t\t\n\t\tBrief information\n\t\n\n\nNumber of rows: 500\nType of dataset files: parquet\nType of dataset: text, alpaca with system prompts\nLanguage: English\nLicense: MIT\n\nStructure:\nmath¬Ø¬Ø¬Ø¬Ø¬Ø‚åâ\n   school-level (100 rows)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/MATH-500-Overall.","url":"https://huggingface.co/datasets/fluently-sets/MATH-500-Overall","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","text-classification","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"pisc-tr","keyword":"cot","description":"\n\t\n\t\t\n\t\tDataset Card for CoT\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository: LLaVA-CoT GitHub Repository\nPaper: LLaVA-CoT on arXiv\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\ncat image.zip.part-* > image.zip #not uploaded yet\nunzip image.zip\n\nThe train.jsonl file contains the question-answering data and is structured in the following format:\n{\n  \"id\": \"example_id\",\n  \"image\": \"example_image_path\",\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"L√ºtfen resimdeki kƒ±rmƒ±zƒ± metal nesnelerin sayƒ±sƒ±nƒ± belirtin.\"}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/berhaan/pisc-tr.","url":"https://huggingface.co/datasets/berhaan/pisc-tr","creator_name":"Berhan T√ºrk√º Ay","creator_url":"https://huggingface.co/berhaan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","Turkish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Demeter-LongCoT-400K","keyword":"cot","description":"\n\n\t\n\t\t\n\t\tDemeter-LongCoT-400K\n\t\n\n\nDemeter-LongCoT-400K is a high-quality, compact chain-of-thought reasoning dataset curated for tasks in mathematics, science, and coding. While the dataset spans diverse domains, it is primarily driven by mathematical reasoning, reflecting a major share of math-focused prompts and long-form logical solutions.\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuick Start with Hugging Face Datasetsü§ó\n\t\n\npip install -U datasets\n\nfrom datasets import load_dataset\n\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Demeter-LongCoT-400K.","url":"https://huggingface.co/datasets/prithivMLmods/Demeter-LongCoT-400K","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"step2-evaluated-dataset-Qwen3-14B-cp32","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tComplete Evaluation Dataset (Rubric + LogP)\n\t\n\nThis dataset contains chain-of-thought explanations evaluated using both comprehensive rubric assessment and LogP evaluation.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nSource Dataset: llm-compe-2025-kato/step2-evaluated-dataset-Qwen3-14B-cp32\nTotal Samples: 60\nSuccessfully Evaluated (Rubric): 53\nFailed Evaluations (Rubric): 7\nEvaluation Model: Qwen/Qwen3-32B\n\n\n\t\n\t\t\n\t\tRubric Evaluation Results\n\t\n\n\n\t\n\t\t\n\t\tAverage Rubric Scores (0-4 scale)\n\t\n\n\nlogical_coherence:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-compe-2025-kato/step2-evaluated-dataset-Qwen3-14B-cp32.","url":"https://huggingface.co/datasets/llm-compe-2025-kato/step2-evaluated-dataset-Qwen3-14B-cp32","creator_name":"llm-compe-2025-kato","creator_url":"https://huggingface.co/llm-compe-2025-kato","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"step2-evaluated-dataset-Qwen3-14B-cp40","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tComplete Evaluation Dataset (Rubric + LogP)\n\t\n\nThis dataset contains chain-of-thought explanations evaluated using both comprehensive rubric assessment and LogP evaluation.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nSource Dataset: llm-compe-2025-kato/step2-evaluated-dataset-Qwen3-14B-cp40\nTotal Samples: 58\nSuccessfully Evaluated (Rubric): 53\nFailed Evaluations (Rubric): 5\nEvaluation Model: Qwen/Qwen3-32B\n\n\n\t\n\t\t\n\t\tRubric Evaluation Results\n\t\n\n\n\t\n\t\t\n\t\tAverage Rubric Scores (0-4 scale)\n\t\n\n\nlogical_coherence:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-compe-2025-kato/step2-evaluated-dataset-Qwen3-14B-cp40.","url":"https://huggingface.co/datasets/llm-compe-2025-kato/step2-evaluated-dataset-Qwen3-14B-cp40","creator_name":"llm-compe-2025-kato","creator_url":"https://huggingface.co/llm-compe-2025-kato","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MedS-Ins","keyword":"cot","description":"\n\t\n\t\t\n\t\tHPAI-BSC MedS-Ins\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated data from the MedS-Ins dataset. Used to train Aloe-Beta model.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThis is the curated version of the MedS-Ins dataset included in the training set of the Aloe-Beta models. \nFirst, we selected 75 out of the 122 existing tasks, excluding the tasks that were already in the training set, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedS-Ins.","url":"https://huggingface.co/datasets/HPAI-BSC/MedS-Ins","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LIMO_QFFT","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tüìò LIMO‚ÄìQFFT\n\t\n\nLIMO‚ÄìQFFT is a question-free variant of the original GAIR/LIMO dataset, tailored for use in QFFT (Question-Free Fine-Tuning) pipelines.\n\n\t\n\t\t\n\t\tüîç Description\n\t\n\nThis dataset removes the original input questions and system prompts from the LIMO dataset, and keeps only the long-form reasoning responses. The goal is to enable training large language models to learn from reasoning traces alone, without depending on task-specific questions.\nAll entries are converted into‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lwl-uestc/LIMO_QFFT.","url":"https://huggingface.co/datasets/lwl-uestc/LIMO_QFFT","creator_name":"Wanlong Liu","creator_url":"https://huggingface.co/lwl-uestc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"mauxi-COT-Persian","keyword":"cot","description":"\n\t\n\t\t\n\t\tüß† mauxi-COT-Persian Dataset\n\t\n\n\nExploring Persian Chain-of-Thought Reasoning with DeepSeek-R1, brought to you by Mauxi AI Platform\n\n\n\t\n\t\t\n\t\tüåü Overview\n\t\n\nmauxi-COT-Persian is a community-driven dataset that explores the capabilities of advanced language models in generating Persian Chain-of-Thought (CoT) reasoning. The dataset is actively growing with new high-quality, human-validated entries being added regularly. I am personally working on expanding this dataset with rigorously‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-COT-Persian.","url":"https://huggingface.co/datasets/xmanii/mauxi-COT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"GenRef-wds","keyword":"cot","description":"\n\t\n\t\t\n\t\tGenRef-1M\n\t\n\n\n  \n\n\nWe provide 1M high-quality triplets of the form (flawed image, high-quality image, reflection) collected across\nmultiple domains using our scalable pipeline from [1]. We used this dataset to train our reflection tuning model.\nTo know the details of the dataset creation pipeline, please refer to Section 3.2 of [1].\nProject Page: https://diffusion-cot.github.io/reflection2perfection\n\n\t\n\t\t\n\t\n\t\n\t\tDataset loading\n\t\n\nWe provide the dataset in the webdataset format for fast‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/diffusion-cot/GenRef-wds.","url":"https://huggingface.co/datasets/diffusion-cot/GenRef-wds","creator_name":"Diffusion CoT","creator_url":"https://huggingface.co/diffusion-cot","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","English","mit","1M - 10M","webdataset"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-CoT-Math-170k","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tGammaCorpus: CoT Math 170k\n\t\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nNumber of Rows: 169,527\nFormat: JSONL\nLanguage: English\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\n\n\n\t\n\t\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k.","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-CoT-Math-170k","keyword":"cot","description":"\n\t\n\t\t\n\t\tGammaCorpus: CoT Math 170k\n\t\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nNumber of Rows: 169,527\nFormat: JSONL\nLanguage: English\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\n\n\n\t\n\t\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k.","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"MMArt-PPR10k","keyword":"cot","description":"JarvisArt/MMArt-PPR10k dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/JarvisArt/MMArt-PPR10k","creator_name":"JarvisArt","creator_url":"https://huggingface.co/JarvisArt","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","image-to-image","image-to-text","text-to-image","English"],"keywords_longer_than_N":true},
	{"name":"medical_psychology","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tMedical Psychology Dataset\n\t\n\nThe Medical Psychology Dataset is a specialised, comprehensive resource (~260M tokens, 296k examples) generated using advanced distillation techniques. It provides structured clinical and psychological reasoning traces with a strong emphasis on diagnostic processes, therapeutic approaches, and evidence-based healthcare knowledge. Covering 15+ medical specialities, 10+ psychology branches, and multiple mental health conditions, the dataset serves as a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/169Pi/medical_psychology.","url":"https://huggingface.co/datasets/169Pi/medical_psychology","creator_name":"169Pi","creator_url":"https://huggingface.co/169Pi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"CoT_reformatted","keyword":"cot","description":"\n\t\n\t\t\n\t\tDataset Card for \"CoT_reformatted\"\n\t\n\nThis dataset is reformatted from: QingyiSi/Alpaca-CoT\nAll credit goes there. Thanks to QingyiSi for the work in consolidating many diverse sources for comparison and cross-file analysis.\nThere were some issues loading files from that dataset for a testing project. \nI extracted the following data files for this subset:\n\nalpaca_data_cleaned\nCoT_data\nfirefly       \ninstruct\nalpaca_gpt4_data\ndolly \nGPTeacher\nthoughtsource\nfinance_en\ninstinwild_en\n\n","url":"https://huggingface.co/datasets/jtatman/CoT_reformatted","creator_name":"James","creator_url":"https://huggingface.co/jtatman","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Thinking-multilingual-big-10k-sft","keyword":"cot","description":"\nA dataset based off of openo1 math, 500 examples translated to 23 different languages. filtered out un-translated examples.\nenjoy üëç\n","url":"https://huggingface.co/datasets/Pinkstack/Thinking-multilingual-big-10k-sft","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"HQ-knowledgedistills-1.2M-magpie","keyword":"cot","description":"This dataset is.an exact mix of 900k general qwen conversation with general questions, math, code and another 300k of Gemma 2 27B generations, for creative writing. \nThe dataset was made for \"healing\" pruned LLM's, especially ones based off of qwen2.5 series, as some conversations include the models saying who they are. \nUnlike the previous 900K version, we also mixed in Gemma generations, to add more creative writing examples.\nMany thanks to the magpie project for making this possible, this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Pinkstackorg/HQ-knowledgedistills-1.2M-magpie.","url":"https://huggingface.co/datasets/Pinkstackorg/HQ-knowledgedistills-1.2M-magpie","creator_name":"Pinkstack-org","creator_url":"https://huggingface.co/Pinkstackorg","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ultramedical","keyword":"cot","description":"\n\t\n\t\t\n\t\tHAPI-BSC ultramedical\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCurated version of the UltraMedical dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThe UltraMedical Collections is a large-scale, high-quality dataset of biomedical instructions. We collected and curated the following sets:\n\nTextBookQA\nMedical-Instruction-120k\nWikiInstruct\n\nThis dataset is included in the Aloe-Beta model training set.\n\nCurated by:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/ultramedical.","url":"https://huggingface.co/datasets/HPAI-BSC/ultramedical","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"LongPerceptualThoughts-30k","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tLongPerceptualThoughts\n\t\n\n\n‚≠ê LongPerceptualThoughts is accepted to COLM 205!\nLongPerceptualThoughts is a synthetic dataset containing 30k long chain-of-thougt (CoT) traces. It is designed to promote system-2 thinking in vision-language models via simple SFT or DPO training.\n\n\nPaper: LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception [arXiv link]\nGithub Repository: Repository to synthesize your own data! [andrewliao11/LongPerceptualThoughts]\nProject website:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andrewliao11/LongPerceptualThoughts-30k.","url":"https://huggingface.co/datasets/andrewliao11/LongPerceptualThoughts-30k","creator_name":"Andrew Liao","creator_url":"https://huggingface.co/andrewliao11","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["visual-question-answering","DOCCI","English","apache-2.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"LongPage","keyword":"cot","description":"\n\n\t\n\t\t\n\t\tOverview üöÄüìö\n\t\n\nThe first comprehensive dataset for training AI models to write complete novels with sophisticated reasoning.\nüß† Hierarchical Reasoning Architecture ‚Äî Multi-layered planning traces including character archetypes, story arcs, world rules, and scene breakdowns. A complete cognitive roadmap for long-form narrative construction.\nüìñ Complete Novel Coverage ‚Äî From 40,000 to 600,000+ tokens per book, spanning novellas to epic series with consistent quality throughout.\n‚ö°‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Pageshift-Entertainment/LongPage.","url":"https://huggingface.co/datasets/Pageshift-Entertainment/LongPage","creator_name":"Pageshift-Entertainment","creator_url":"https://huggingface.co/Pageshift-Entertainment","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","text2text-generation","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"indian_law","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tIndian Law Dataset\n\t\n\nThe Indian Law Dataset is a high-quality, open-source dataset (~50M tokens) focused on Indian jurisprudence. It provides structured chain-of-thought reasoning traces across 10+ branches of law, enabling the training and evaluation of advanced reasoning-capable language models.\n\n\t\n\t\t\n\t\tSummary\n\t\n\n‚Ä¢ Domain: Law / Indian Jurisprudence / Legal Reasoning\n‚Ä¢ Scale: ~50M tokens, 47,789 rows\n‚Ä¢ Source: Generated with advanced distillation techniques using structured‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/169Pi/indian_law.","url":"https://huggingface.co/datasets/169Pi/indian_law","creator_name":"169Pi","creator_url":"https://huggingface.co/169Pi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"GameQA-140K","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\t1. Overview\n\t\n\nGameQA is a large-scale, diverse, and challenging multimodal reasoning dataset designed to enhance the general reasoning capabilities of Vision Language Models (VLMs). Generated using the innovative Code2Logic framework, it leverages game code to synthesize high-quality visual-language Chain-of-Thought (CoT) data. The dataset addresses the scarcity of multimodal reasoning data, critical for advancing complex multi-step reasoning in VLMs. Each sample includes visual game‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Code2Logic/GameQA-140K.","url":"https://huggingface.co/datasets/Code2Logic/GameQA-140K","creator_name":"Game-RL","creator_url":"https://huggingface.co/Code2Logic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"GameQA-140K","keyword":"cot","description":"\n\t\n\t\t\n\t\t1. Overview\n\t\n\nGameQA is a large-scale, diverse, and challenging multimodal reasoning dataset designed to enhance the general reasoning capabilities of Vision Language Models (VLMs). Generated using the innovative Code2Logic framework, it leverages game code to synthesize high-quality visual-language Chain-of-Thought (CoT) data. The dataset addresses the scarcity of multimodal reasoning data, critical for advancing complex multi-step reasoning in VLMs. Each sample includes visual game‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Code2Logic/GameQA-140K.","url":"https://huggingface.co/datasets/Code2Logic/GameQA-140K","creator_name":"Game-RL","creator_url":"https://huggingface.co/Code2Logic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"team-truthowl-mixed-reasoning-dataset","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tTeam P11 Mixed Reasoning Dataset\n\t\n\n\n\t\n\t\t\n\t\tüìä Dataset description\n\t\n\nHLEÔºàHumanity's Last ExamÔºâÂêë„Åë„Å´‰ΩúÊàê„Åó„Åü„ÄÅÊï∞Â≠¶‰∏≠ÂøÉÔºãÁßëÂ≠¶MC„ÅÆÊ∑∑ÂêàÊé®Ë´ñ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ\nÊé®Ë´ñÈÅéÁ®ãÔºàChain-of-ThoughtÔºâ„Çí‰øùÊåÅ„Åó„ÄÅÊúÄÁµÇËß£Á≠î„ÅÆÊ≠£Ë¶èÂåñ„ÇíË°å„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nÂØæË±°„É¢„Éá„É´„ÅØ DeepSeek-R1-Distill-Qwen-32B„ÄÅÂ≠¶Áøí„ÅØQLoRA„ÇíÊÉ≥ÂÆö„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n\t\n\t\t\n\t\tüéØ Purpose\n\t\n\n\nCompetition: ÊùæÂ∞æÁ†îLLM„Ç≥„É≥„Éö 2025  \nTarget Model: DeepSeek-R1-Distill-Qwen-32B  \nTraining Method: QLoRA Fine-tuningÔºà4bit NF4, double quantÔºâ\n\n\n\t\n\t\t\n\t\tüì¶ Composition\n\t\n\n\nMath HardÔºàMATH Level‚â•3, HARDMathÔºâ  \nMath MidÔºàGSM8K, MetaMathQAÔºâ  \nScienceÔºàGPQA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weblab-llm-competition-2025-bridge/team-truthowl-mixed-reasoning-dataset.","url":"https://huggingface.co/datasets/weblab-llm-competition-2025-bridge/team-truthowl-mixed-reasoning-dataset","creator_name":"weblab-llm-competition-2025-bridge","creator_url":"https://huggingface.co/weblab-llm-competition-2025-bridge","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Resume-Analysis-CoTR","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tResume Reasoning and Feedback Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains approximately 417 examples designed to facilitate research and development in automated resume analysis and feedback generation. Each data point consists of a user query regarding their resume, a simulated internal analysis (chain-of-thought) performed by an expert persona, and a final, user-facing feedback response derived solely from that analysis.\nThe dataset captures a two-step reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR.","url":"https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"logic-trainingset-symb-structed-reformatted","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tLogic Reasoning and Proof Verification Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive dataset for training and evaluating logical reasoning capabilities in language models.\nEach example contains propositional logic problems that require formal reasoning to verify hypotheses.\n\nThe dataset includes 10427 problems with the following distribution:\n- PROVED: 4291 examples where the hypothesis can be proven from the facts\n- DISPROVED: 4262 examples where the hypothesis can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RedaAlami/logic-trainingset-symb-structed-reformatted.","url":"https://huggingface.co/datasets/RedaAlami/logic-trainingset-symb-structed-reformatted","creator_name":"Reda alami","creator_url":"https://huggingface.co/RedaAlami","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Turing-Reason-CoT-Mini","keyword":"cot","description":"\n\n\t\n\t\t\n\t\tTuring-Reason-CoT-Mini\n\t\n\n\nTuring-Reason-CoT-Mini is a high-quality, compact chain-of-thought reasoning dataset curated for tasks in mathematics, science, and coding. While the dataset spans diverse domains, it is primarily driven by mathematical reasoning, reflecting a major share of math-focused prompts and long-form logical solutions.\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuick Start with Hugging Face Datasetsü§ó\n\t\n\npip install -U datasets\n\nfrom datasets import load_dataset\n\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Turing-Reason-CoT-Mini.","url":"https://huggingface.co/datasets/prithivMLmods/Turing-Reason-CoT-Mini","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"DensingLaw-ScalingBench","keyword":"cot","description":"\n\t\n\t\t\n\t\tDensingLaw-ScalingBench\n\t\n\nThis dataset was created to enable a more accurate performance scaling law estimation of Large Language Models (LLMs).\nThis dataset is released as part of our paper, Densing Law of LLMs.\n\n\n\n\nüìú Paper \n\n\n\n\n\n\t\n\t\t\n\t\tüí° Overview\n\t\n\nThis repository contains the open-source dataset used for calculating conditional loss in our LLM density evaluation framework. \nLLM density is defined as the ratio of effective parameter size to actual parameter size, where effective‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openbmb/DensingLaw-ScalingBench.","url":"https://huggingface.co/datasets/openbmb/DensingLaw-ScalingBench","creator_name":"OpenBMB","creator_url":"https://huggingface.co/openbmb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice-qa","open-domain-qa","original:mmlu"],"keywords_longer_than_N":true},
	{"name":"DensingLaw-ScalingBench","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tDensingLaw-ScalingBench\n\t\n\nThis dataset was created to enable a more accurate performance scaling law estimation of Large Language Models (LLMs).\nThis dataset is released as part of our paper, Densing Law of LLMs.\n\n\n\n\nüìú Paper \n\n\n\n\n\n\t\n\t\t\n\t\tüí° Overview\n\t\n\nThis repository contains the open-source dataset used for calculating conditional loss in our LLM density evaluation framework. \nLLM density is defined as the ratio of effective parameter size to actual parameter size, where effective‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openbmb/DensingLaw-ScalingBench.","url":"https://huggingface.co/datasets/openbmb/DensingLaw-ScalingBench","creator_name":"OpenBMB","creator_url":"https://huggingface.co/openbmb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice-qa","open-domain-qa","original:mmlu"],"keywords_longer_than_N":true},
	{"name":"step2-evaluated-dataset-Qwen3-14B","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tComplete Evaluation Dataset (Rubric + LogP)\n\t\n\nThis dataset contains chain-of-thought explanations evaluated using both comprehensive rubric assessment and LogP evaluation.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nSource Dataset: llm-compe-2025-kato/step2-evaluated-dataset-Qwen3-14B\nTotal Samples: 156\nSuccessfully Evaluated (Rubric): 135\nFailed Evaluations (Rubric): 21\nEvaluation Model: Qwen/Qwen3-32B\n\n\n\t\n\t\t\n\t\tRubric Evaluation Results\n\t\n\n\n\t\n\t\t\n\t\tAverage Rubric Scores (0-4 scale)\n\t\n\n\nlogical_coherence:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-compe-2025-kato/step2-evaluated-dataset-Qwen3-14B.","url":"https://huggingface.co/datasets/llm-compe-2025-kato/step2-evaluated-dataset-Qwen3-14B","creator_name":"llm-compe-2025-kato","creator_url":"https://huggingface.co/llm-compe-2025-kato","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1-v2","keyword":"cot","description":"This is a modified version of ZeroAgency/ru-thinking-reasoning-r1 with addition of Egor-AI/CoT-XLang dataset.\nCombined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\n–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2.","url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"polite-guard","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tPolite Guard\n\t\n\n\nDataset type: Synthetic and Annotated\nTask: Text Classification\nDomain: Classification of text into polite, somewhat polite, neutral, and impolite categories\nSource Code: (https://github.com/intel/polite-guard)\nModel: (https://huggingface.co/Intel/polite-guard)\n\nThis dataset is for Polite Guard: an open-source NLP language model developed by Intel, fine-tuned from BERT for text classification tasks. Polite Guard is designed to classify text into four categories: polite‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Intel/polite-guard.","url":"https://huggingface.co/datasets/Intel/polite-guard","creator_name":"Intel","creator_url":"https://huggingface.co/Intel","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","English","cdla-permissive-2.0","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1","keyword":"cot","description":"Combined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\n–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É: <think> –¢–≤–æ–∏ –º—ã—Å–ª–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è </think> \n–¢–≤–æ–π –∫–æ–Ω–µ—á–Ω—ã–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1.","url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Aloe-Beta-Medical-Collection","keyword":"cot","description":"\n\t\n\t\t\n\t\tAloe-Beta-Medical-Collection\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated datasets used to fine-tune Aloe-Beta.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nWe curated data from many publicly available medical instruction tuning data sources (QA format). Most data samples correspond to single-turn QA pairs, while a small proportion contain multi-turn. All data sources are publicly available for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection.","url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"dx7-patches-and-prompts","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tYamaha DX7 Synthesizer Patches with AI-Generated Prompts\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a comprehensive, multi-task dataset designed for fine-tuning language models to understand and generate synthesizer patches for the Yamaha DX7.\nThe dataset contains over 20,000 examples across three distinct but related tasks, making it ideal for creating models that can not only generate patches but also understand and reason about their structure and validity.\n\n\t\n\t\t\n\t\tHow the Data Was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ccerati/dx7-patches-and-prompts.","url":"https://huggingface.co/datasets/ccerati/dx7-patches-and-prompts","creator_name":"Carlo Cerati","creator_url":"https://huggingface.co/ccerati","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K<n<100K","Text"],"keywords_longer_than_N":true},
	{"name":"CoTton-38k-6525-Collective","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tCoTton-38k-6525-Collective\n\t\n\nCoTton-38k is a 38,350-example dataset of soft reasoning conversations in the ShareGPT format. Each entry contains an exchange between a user and a model, showcasing high-quality Chain-of-Thought (CoT) reasoning in natural language.\nThe dataset is distilled from open LLMs:\n\nQwen3 235B A22B\nAM Thinking\nQwQ 32B\nDeepseek R1\nR1 0528\n\nThe name CoTton encodes multiple layers of meaning:\n\nCoT: Chain-of-Thought is embedded in the name\nTON: The dataset contains a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective.","url":"https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CoTton-38k-6525-Collective","keyword":"cot","description":"\n\t\n\t\t\n\t\tCoTton-38k-6525-Collective\n\t\n\nCoTton-38k is a 38,350-example dataset of soft reasoning conversations in the ShareGPT format. Each entry contains an exchange between a user and a model, showcasing high-quality Chain-of-Thought (CoT) reasoning in natural language.\nThe dataset is distilled from open LLMs:\n\nQwen3 235B A22B\nAM Thinking\nQwQ 32B\nDeepseek R1\nR1 0528\n\nThe name CoTton encodes multiple layers of meaning:\n\nCoT: Chain-of-Thought is embedded in the name\nTON: The dataset contains a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective.","url":"https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MMAT-1M","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tMMAT-1M Dataset Card\n\t\n\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\tDataset details\n\t\n\n\n\t\n\t\t\n\t\tDataset type\n\t\n\nMMAT-1M is a million-scale multimodal agent tuning dataset, built by consolidating subsets of five publicly available multimodal question-answer datasets: Visual CoT, LLaVA-CoT, The Cauldron, TabMWP, and Infoseek. It integrates dynamically generated API calls and Retrieval Augmented Generation (RAG) information through a GPT-4o-powered multi-turn paradigm, with rationales refined via‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VIS-MPU-Agent/MMAT-1M.","url":"https://huggingface.co/datasets/VIS-MPU-Agent/MMAT-1M","creator_name":"VIS-MPU-Agent","creator_url":"https://huggingface.co/VIS-MPU-Agent","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","cc-by-4.0","cc-by-nc-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"exambench","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tExamBench\n\t\n\nThe ExamBench Dataset (~600M tokens, 405k examples) is one of the largest open-source corpora designed for competitive exam preparation and reasoning AI. Generated using advanced distillation techniques, it combines structured chain-of-thought reasoning with comprehensive coverage of over 25 Indian and international examinations. From JEE and NEET to UPSC, Banking, GRE, and IELTS, the dataset spans multiple domains like STEM, humanities, current affairs, language, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/169Pi/exambench.","url":"https://huggingface.co/datasets/169Pi/exambench","creator_name":"169Pi","creator_url":"https://huggingface.co/169Pi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"RPRevamped-Small","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tRPRevamped-Small-v1.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nRPRevamped is a synthetic dataset generated by various numbers of models. It is very diverse and is recommended if you are fine-tuning a roleplay model. This is the Small version with Medium and Tiny version currently in work.\nGithub: RPRevamped GitHub\nHere are the models used in creation of this dataset:\nDeepSeek-V3-0324\nGemini-2.0-Flash-Thinking-Exp-01-21\nDeepSeek-R1\nGemma-3-27B-it\nGemma-3-12B-it\nQwen2.5-VL-72B-Instruct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TechPowerB/RPRevamped-Small.","url":"https://huggingface.co/datasets/TechPowerB/RPRevamped-Small","creator_name":"Bhargav Raj","creator_url":"https://huggingface.co/TechPowerB","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"ToxiFrench","keyword":"chain-of-thought","description":"A curated dataset for fine-tuning toxicity classifiers and reasoning models in French.\nSupports curriculum learning and chain-of-thought annotation variants. DPO datasets are also available.\nThis script also includes configurations for Jigsaw GPT-annotated, GPT-annotated, and non-annotated data.","url":"https://huggingface.co/datasets/Naela00/ToxiFrench","creator_name":"Naela","creator_url":"https://huggingface.co/Naela00","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","French","mit","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"DAPO-Math-17k-Qwen3-235B-A22B-Thinking-2507-rejection-distill","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tDAPO-Math-17k-Qwen3-235B-A22B-Thinking-2507-rejection-distill\n\t\n\nA high-quality Chain-of-Thought (CoT) dataset generated using Qwen/Qwen3-235B-A22B-Thinking-2507 with rejection sampling on BytedTsinghua-SIA/DAPO-Math-17k. This dataset is ideal for SFT distillation training to improve mathematical reasoning capabilities of models.\nThe dataset format is compatible with LLaMA-Factory for efficient SFT training.\n\n\t\n\t\n\t\n\t\tFiles\n\t\n\n\ndapo_distill_boxed.json: Single sampling subset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Yang-Zhou/DAPO-Math-17k-Qwen3-235B-A22B-Thinking-2507-rejection-distill.","url":"https://huggingface.co/datasets/Yang-Zhou/DAPO-Math-17k-Qwen3-235B-A22B-Thinking-2507-rejection-distill","creator_name":"YANG ZHOU","creator_url":"https://huggingface.co/Yang-Zhou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"academic-chains","keyword":"chain-of-thought","description":"\n \n\n\n\n\t\n\t\t\n\t\tDataset Card for Academic Reasoning and Intuition Chains\n\t\n\n\n(The image above is an output from Llama-3.2-3B-Instruct tuned on this dataset, quantized to 8 bit and ran on llama.cpp; In our tests Qwen3-30B-A3B, Gemini 2.5 Pro and Claude Sonnet 3.7 with thinking enabled all got this simple question wrong)\nThis dataset contains reasoning (and intuition) chains distilled from open-access research papers, primarily focusing on fields like Biology, Economics, Physics, Math, Computer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/academic-chains.","url":"https://huggingface.co/datasets/marcodsn/academic-chains","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"sharegpt_cot_dataset","keyword":"cot","description":"\n\t\n\t\t\n\t\tA data set inspired by the \"Reflection\" method, three-dimensional thinking and cot\n\t\n\n\n\t\n\t\t\n\t\tThis is the ShareGPT format.\n\t\n\nThe data set was generated using multiple llm synthesis.\n","url":"https://huggingface.co/datasets/AiCloser/sharegpt_cot_dataset","creator_name":"Ai Closer","creator_url":"https://huggingface.co/AiCloser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","text2text-generation","English","Russian"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-FC-Reasoning-v1","keyword":"chain-of-thought","description":"\n\n\t\n\t\t\n\t\tArcosoph-FC-Reasoning-v1\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the Arcosoph-FC-Reasoning-v1, a meticulously crafted dataset designed for supervised fine-tuning (SFT) of language models, especially microsoft/Phi-3-mini-4k-instruct. The dataset is provided in a ready-to-use JSON Lines (.jsonl) format, where each line represents a single training example.\nThe primary goal of this dataset is to teach a model not just to respond to queries, but to reason, plan, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-v1.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-v1","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"cot_data_slow_thinking_conversations","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tcot_data_slow_thinking_conversations\n\t\n\nThis dataset contains chain-of-thought reasoning data with slow thinking patterns.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nFormat: JSONL (JSON Lines) with Hugging Face conversations format\nSize: 1113.88 MB\nTotal examples: Approximately 156,268 examples\n\nEach line contains a JSON object with a \"conversations\" key containing a list of messages with user queries about mathematical reasoning steps and assistant responses with thinking patterns.\n\n\t\n\t\t\n\t\tExample‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jianyuan1/cot_data_slow_thinking_conversations.","url":"https://huggingface.co/datasets/Jianyuan1/cot_data_slow_thinking_conversations","creator_name":"Zhong","creator_url":"https://huggingface.co/Jianyuan1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"CoT-aichatbot","keyword":"cot","description":"Bluestrikeai/CoT-aichatbot dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Bluestrikeai/CoT-aichatbot","creator_name":"BLUE STRIKE AI","creator_url":"https://huggingface.co/Bluestrikeai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"MedMCQA","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tMedMCQA-CoT: ÂåªÂ≠¶Â§öËÇ¢ÈÅ∏ÊäûÂïèÈ°åwith Chain-of-ThoughtÊé®Ë´ñ\n\t\n\n\n\t\n\t\t\n\t\t„Éá„Éº„Çø„Çª„ÉÉ„ÉàÊ¶ÇË¶Å\n\t\n\nMedMCQA-CoT„ÅØ„ÄÅMedMCQA„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊã°ÂºµÁâà„Åß„ÄÅÂêÑÂåªÂ≠¶Â§öËÇ¢ÈÅ∏ÊäûÂïèÈ°å„Å´È´òÂìÅË≥™„Å™Chain-of-ThoughtÔºàCoTÔºâÊé®Ë´ñ„ÇíËøΩÂä†„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇÂåªÂ≠¶ÁöÑ„Å™Êé®Ë´ñ„Éó„É≠„Çª„Çπ„ÇíË™¨Êòé„Åß„Åç„ÇãAI„Ç∑„Çπ„ÉÜ„É†„ÅÆÈñãÁô∫„ÇíÊîØÊè¥„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n\t\n\t\t\n\t\t‰∏ª„Å™ÁâπÂæ¥\n\t\n\n\n2,020‰ª∂„ÅÆÂåªÂ≠¶MCQÂïèÈ°å - ÂÖÉ„ÅÆMedMCQA„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åã„ÇâÊäΩÂá∫\nChain-of-ThoughtÊé®Ë´ñ - DeepSeek-R1„É¢„Éá„É´„ÅßÁîüÊàê\n95.5%„ÅÆÂõûÁ≠îÁ≤æÂ∫¶ - ÁîüÊàê„Åï„Çå„ÅüCoT„ÅåÊ≠£Ëß£„Å´Â∞é„ÅèÂâ≤Âêà\n0.952„ÅÆÂπ≥ÂùáÂìÅË≥™„Çπ„Ç≥„Ç¢ - ÂåªÂ≠¶Áî®Ë™ûÂØÜÂ∫¶„Å®Êé®Ë´ñÂìÅË≥™„Å´Âü∫„Å•„ÅèË©ï‰æ°\nÂåÖÊã¨ÁöÑ„Å™„É°„Çø„Éá„Éº„Çø - ÂìÅË≥™„Çπ„Ç≥„Ç¢„ÄÅÂåªÂ≠¶Â∞ÇÈñÄÂàÜÈáé„ÄÅÁîüÊàêÁµ±Ë®à„ÇíÂê´„ÇÄ\n\n\n\t\n\t\t\n\t\t„Éá„Éº„Çø„Çª„ÉÉ„ÉàË©≥Á¥∞\n\t\n\nÂêÑ„É¨„Ç≥„Éº„Éâ„ÅÆÊßãÊàê:\n\nquestion: MedMCQA„Åã„Çâ„ÅÆÂÖÉ„ÅÆÂåªÂ≠¶ÂïèÈ°å\nanswer: Ê≠£Ëß£„ÅÆÈÅ∏ÊäûËÇ¢ÔºàA, B, C, DÔºâ\ncot:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/oNo-1/MedMCQA.","url":"https://huggingface.co/datasets/oNo-1/MedMCQA","creator_name":"oNo.1","creator_url":"https://huggingface.co/oNo-1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"twi-reasoning-dataset","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tTwi Reasoning Dataset\n\t\n\nA Twi (Akan) translation of the Multilingual-Thinking reasoning dataset with chain-of-thought in Twi\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a Twi (Akan) translation of the Multilingual-Thinking reasoning dataset. It contains chain-of-thought reasoning traces translated from multiple languages into Twi, making it one of the first reasoning datasets available in this language.\n\n\t\n\t\t\n\t\tLanguage Information\n\t\n\n\nLanguage: Twi (Akan)\nLanguage Code: tw\nFamily:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi-reasoning-dataset.","url":"https://huggingface.co/datasets/michsethowusu/twi-reasoning-dataset","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","conversational","text2text-generation","language-modeling"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_CoT","keyword":"chain-of-thought","description":"Question-Answer dataset generated using CAMEL CoTDataGenerator and GPT-4o Mini","url":"https://huggingface.co/datasets/0fg/truthful_qa_CoT","creator_name":"Bryce","creator_url":"https://huggingface.co/0fg","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"CoT-XLang","keyword":"cot","description":"RU:CoT-XLang ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø–æ—à–∞–≥–æ–≤—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ (Chain-of-Thought, CoT) –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, —Ä—É—Å—Å–∫–∏–π, —è–ø–æ–Ω—Å–∫–∏–π –∏ –¥—Ä—É–≥–∏–µ. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π —Ä–µ—à–µ–Ω–∏–π —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤. –î–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç –æ–∫–æ–ª–æ 2,419,912 –ø—Ä–∏–º–µ—Ä–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/CoT-XLang.","url":"https://huggingface.co/datasets/Egor-AI/CoT-XLang","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Russian","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"mathreasoning","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tMathReasoning\n\t\n\nThe MathReasoning Dataset is a large-scale, high-quality dataset (~3.13M rows) focused on mathematics, logical reasoning, and problem-solving. It is primarily generated through synthetic distillation techniques, complemented by curated open-source educational content. The dataset is designed to train and evaluate language models in mathematical reasoning, quantitative problem-solving, and structured chain-of-thought tasks across domains from basic arithmetic to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/169Pi/mathreasoning.","url":"https://huggingface.co/datasets/169Pi/mathreasoning","creator_name":"169Pi","creator_url":"https://huggingface.co/169Pi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"german_tlr_gold_14k","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tüß† German TLR Gold Dataset (14.5k)\n\t\n\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\nEin hochwertiger deutschsprachiger Datensatz mit 14.500 Samples im Think-Learn-Respond (TLR) Format f√ºr das Training von reasoning-f√§higen Large Language Models.\nFormat: Jede Antwort ist strukturiert in:\n\n<think>: Strukturierter Denkprozess und Reasoning\n<answer>: Finale, klare Antwort\n\n\n\t\n\t\t\n\t\tüéØ Anwendung\n\t\n\nDieses Dataset wurde speziell entwickelt f√ºr:\n\nSupervised Fine-Tuning (SFT) von deutschen LLMs\nTraining von‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arnomatic/german_tlr_gold_14k.","url":"https://huggingface.co/datasets/arnomatic/german_tlr_gold_14k","creator_name":"arnomatic","creator_url":"https://huggingface.co/arnomatic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","German","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"think-more","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThink More is a large-scale, multi-domain collection of long chain-of-thought (CoT) reasoning examples. It aggregates and cleans several prominent reasoning datasets, focusing on high-quality, step-by-step model-generated solutions from DeepSeek R1 and OpenAI o1. Each entry includes a question, the model‚Äôs answer, and the detailed thought process leading to that answer.\n‚ö†Ô∏è Warning: the dataset decompresses to a 15.1 GB JSONLines file.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/think-more.","url":"https://huggingface.co/datasets/agentlans/think-more","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"chain-of-thought","description":"\n ü¶Ñ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n      \n      \n    \n    \n       \n      \n       \n       \n      \n      \n       \n      \n    \n      \n\n\n\n      \n    [ArXiv] | [ü§óHuggingFace] | [Website]\n    \n    \n\n\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\tüî•News\n\t\n\n\nüéñÔ∏è Our work is accepted by ACL2024.\n\nüî• We have release benchmark on [ü§óHuggingFace].\n\nüî• The paper is also available on [ArXiv].\n\nüîÆ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"toxifrench-anonymous","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection\n\t\n\n\n‚ö†Ô∏è Content Warning\nThis project and the associated dataset contain examples of text that may be considered offensive, toxic, or otherwise disturbing. The content is presented for research purposes only.\n\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nDetecting toxic content using language models is crucial yet challenging. While substantial progress has been made in English, toxicity detection in French‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/phantom-researcher/toxifrench-anonymous.","url":"https://huggingface.co/datasets/phantom-researcher/toxifrench-anonymous","creator_name":"anonymous","creator_url":"https://huggingface.co/phantom-researcher","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","French","mit","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"creative_writing","keyword":"cot","description":"\n\t\n\t\t\n\t\tDataset Card for telecomadm1145/creative_writing\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a small-scale instruction‚Äìresponse dataset focused on creative writing tasks.Each example consists of a prompt (instruction specifying writing style, perspective, tone, etc.) and a response (a story segment or novel-like output).  \nThe dataset emphasizes:\n\nCreative Writing (light novel style, emotional narrative, dialogue-driven, descriptive prose).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/telecomadm1145/creative_writing.","url":"https://huggingface.co/datasets/telecomadm1145/creative_writing","creator_name":"t5","creator_url":"https://huggingface.co/telecomadm1145","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"legal-reasoning-harmony","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tLegal Reasoning Harmony (CoT ‚Üí Harmony)\n\t\n\nThis dataset converts moremilk/CoT_Legal_Issues_And_Laws (MIT-licensed) into the Harmony message format for GPT-OSS fine-tuning.\n\nSource: moremilk/CoT_Legal_Issues_And_Laws\nLicense: MIT (inherited from source)\nExamples: 4,237\nFormat: JSONL, Harmony messages with separated thinking and content\nFormat: JSONL, Harmony messages with explicit channels (analysis, final) and convenience top-level fields\n\n\n\t\n\t\n\t\n\t\tProvenance and transformation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zackproser/legal-reasoning-harmony.","url":"https://huggingface.co/datasets/zackproser/legal-reasoning-harmony","creator_name":"Zack Proser","creator_url":"https://huggingface.co/zackproser","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"clevr-tr","keyword":"cot","description":"\n\t\n\t\t\n\t\tDataset Card for CoT\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository: LLaVA-CoT GitHub Repository\nPaper: LLaVA-CoT on arXiv\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nunzip image.zip\n\nThe train.jsonl file contains the question-answering data and is structured in the following format:\n{\n  \"id\": \"example_id\",\n  \"image\": \"example_image_path\",\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"L√ºtfen resimdeki kƒ±rmƒ±zƒ± metal nesnelerin sayƒ±sƒ±nƒ± belirtin.\"},\n    {\"from\": \"gpt\", \"value\": \"Resimde 3 kƒ±rmƒ±zƒ±‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/berhaan/clevr-tr.","url":"https://huggingface.co/datasets/berhaan/clevr-tr","creator_name":"Berhan T√ºrk√º Ay","creator_url":"https://huggingface.co/berhaan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","Turkish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"step_prm","keyword":"cot","description":"\n\t\n\t\t\nÊï∞ÊçÆÈõÜÂêçÁß∞\nÊòØÂê¶Êúâstep\nÂèØÁî®‰∫éPRMËÆ≠ÁªÉ\nÊ†áÁ≠æÂΩ¢Âºè\nTitle\nÂ§áÊ≥®\n\n\n\t\t\nGSM8K\n‚úÖ\n‚ùå\nÁ≠îÊ°à\nTraining Verifiers to Solve Math Word Problems\n\n\n\nMATH\n‚ùå\n‚ùå\nÁ≠îÊ°à\nMeasuring Mathematical Problem Solving With the MATH Dataset\nNon-Step\n\n\nPRM800K\n‚úÖ\n‚úÖ\nÊ≠£Á°ÆÁ±ªÂà´\nLet's Verify Step by Step\nprompt deduplication\n\n\nMath-Shepherd\n‚úÖ\n‚úÖ\nÊ≠£Á°ÆÁ±ªÂà´\nMath-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\nNot used\n\n\nProcessBench\n‚úÖ\n‚úÖ\nÈ¶ñ‰∏™ÈîôËØØÊ≠•È™§\nProcessBench: Identifying Process Errors in Mathematical Reasoning\nonly label -1\n\n\n\t\n\n","url":"https://huggingface.co/datasets/xiaodongguaAIGC/step_prm","creator_name":"xiaodongguaAIGC","creator_url":"https://huggingface.co/xiaodongguaAIGC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-SFT-CoT","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tÁüøÂª∫Â∑•Á®ãÈ¢ÜÂüü‰∏≠ÊñáÊåá‰ª§‰∏éËØÑ‰º∞Êï∞ÊçÆÈõÜÔºàÂ∏¶CoTÊ†áÊ≥®Ôºâ\n\t\n\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜÊ¶ÇËø∞\n\t\n\nÊú¨È°πÁõÆÊòØÂêàËÇ•Â∑•‰∏öÂ§ßÂ≠¶Â§ß‰∏ÄÂ≠¶ÁîüÁöÑÂ§ßÂ≠¶ÁîüÂàõÊñ∞Âàõ‰∏öËÆ≠ÁªÉËÆ°ÂàíÔºàÂ§ßÂàõÔºâÈ°πÁõÆÊàêÊûú„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏ÄÂ•ó‰∏ì‰∏∫ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏≠ÂõΩÁüøÂª∫Â∑•Á®ãÈ¢ÜÂüü‰∏ì‰∏öÁü•ËØÜ‰∏éÂÆûË∑µËÉΩÂäõËÄåËÆæËÆ°ÁöÑ‰∏≠ÊñáÊï∞ÊçÆÈõÜ„ÄÇ\nËøôÂ•óÊï∞ÊçÆÈõÜÊó®Âú®ËÆ©Ê®°ÂûãÊéåÊè°ÁüøÂª∫Â∑•Á®ãÁöÑÊ†∏ÂøÉÁü•ËØÜÔºåÂÜÖÂÆπË¶ÜÁõñ‰∫ÜÂÖ≠Â§ßÊ®°ÂùóÔºö\n\nÊ≥ïÂæãÊ≥ïËßÑ (law)\nÂ∑•Á®ãËßÑËåÉ (specifications)\n‰∏ì‰∏öÊúØËØ≠ (concept)\nÂÆâÂÖ®‰∫ãÊïÖÊ°à‰æã (safety)\nË°å‰∏öÂÆûË∑µÁªèÈ™å (forum)\nÈ¢ÜÂüüÁªºÂêàÁü•ËØÜ (synthesis)\n\n‰∏∫‰∫ÜÊîØÊåÅÂÆåÊï¥ÁöÑÊ®°ÂûãÂºÄÂèë„ÄÅËØÑ‰º∞ÂíåÈ™åËØÅÂë®ÊúüÔºåÊàë‰ª¨Â∞ÜÊï∞ÊçÆÁªÑÁªá‰∏∫Â§ö‰∏™Áã¨Á´ãÁöÑHugging Face‰ªìÂ∫ìÔºö\n\nÂéüÂßãËÆ≠ÁªÉÈõÜ (Original SFT Dataset)ÔºöÂåÖÂê´ 5,287 Êù°È´òË¥®ÈáèÁöÑ‚ÄúÊåá‰ª§-ÂõûÁ≠î‚ÄùÂØπÔºåÁî®‰∫éÂü∫Á°ÄÁöÑÊ®°ÂûãÂæÆË∞É„ÄÇ\nÊÄùÁª¥ÈìæÂ¢ûÂº∫ËÆ≠ÁªÉÈõÜ (CoT-Enhanced SFT‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-SFT-CoT.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-SFT-CoT","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-SFT-CoT","keyword":"cot","description":"\n\t\n\t\t\n\t\tÁüøÂª∫Â∑•Á®ãÈ¢ÜÂüü‰∏≠ÊñáÊåá‰ª§‰∏éËØÑ‰º∞Êï∞ÊçÆÈõÜÔºàÂ∏¶CoTÊ†áÊ≥®Ôºâ\n\t\n\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜÊ¶ÇËø∞\n\t\n\nÊú¨È°πÁõÆÊòØÂêàËÇ•Â∑•‰∏öÂ§ßÂ≠¶Â§ß‰∏ÄÂ≠¶ÁîüÁöÑÂ§ßÂ≠¶ÁîüÂàõÊñ∞Âàõ‰∏öËÆ≠ÁªÉËÆ°ÂàíÔºàÂ§ßÂàõÔºâÈ°πÁõÆÊàêÊûú„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏ÄÂ•ó‰∏ì‰∏∫ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏≠ÂõΩÁüøÂª∫Â∑•Á®ãÈ¢ÜÂüü‰∏ì‰∏öÁü•ËØÜ‰∏éÂÆûË∑µËÉΩÂäõËÄåËÆæËÆ°ÁöÑ‰∏≠ÊñáÊï∞ÊçÆÈõÜ„ÄÇ\nËøôÂ•óÊï∞ÊçÆÈõÜÊó®Âú®ËÆ©Ê®°ÂûãÊéåÊè°ÁüøÂª∫Â∑•Á®ãÁöÑÊ†∏ÂøÉÁü•ËØÜÔºåÂÜÖÂÆπË¶ÜÁõñ‰∫ÜÂÖ≠Â§ßÊ®°ÂùóÔºö\n\nÊ≥ïÂæãÊ≥ïËßÑ (law)\nÂ∑•Á®ãËßÑËåÉ (specifications)\n‰∏ì‰∏öÊúØËØ≠ (concept)\nÂÆâÂÖ®‰∫ãÊïÖÊ°à‰æã (safety)\nË°å‰∏öÂÆûË∑µÁªèÈ™å (forum)\nÈ¢ÜÂüüÁªºÂêàÁü•ËØÜ (synthesis)\n\n‰∏∫‰∫ÜÊîØÊåÅÂÆåÊï¥ÁöÑÊ®°ÂûãÂºÄÂèë„ÄÅËØÑ‰º∞ÂíåÈ™åËØÅÂë®ÊúüÔºåÊàë‰ª¨Â∞ÜÊï∞ÊçÆÁªÑÁªá‰∏∫Â§ö‰∏™Áã¨Á´ãÁöÑHugging Face‰ªìÂ∫ìÔºö\n\nÂéüÂßãËÆ≠ÁªÉÈõÜ (Original SFT Dataset)ÔºöÂåÖÂê´ 5,287 Êù°È´òË¥®ÈáèÁöÑ‚ÄúÊåá‰ª§-ÂõûÁ≠î‚ÄùÂØπÔºåÁî®‰∫éÂü∫Á°ÄÁöÑÊ®°ÂûãÂæÆË∞É„ÄÇ\nÊÄùÁª¥ÈìæÂ¢ûÂº∫ËÆ≠ÁªÉÈõÜ (CoT-Enhanced SFT‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-SFT-CoT.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-SFT-CoT","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Alpaca-CoT","keyword":"cot","description":"\n\t\n\t\t\n\t\tInstruction-Finetuning Dataset Collection (Alpaca-CoT)\n\t\n\nThis repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model.\nWe also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT.  \nIf you think this dataset collection is helpful to you, please like this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT.","url":"https://huggingface.co/datasets/QingyiSi/Alpaca-CoT","creator_name":"Qingyi Si","creator_url":"https://huggingface.co/QingyiSi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","Malayalam","apache-2.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"gsm8k_self_correct","keyword":"cot","description":"\n\t\n\t\t\n\t\tDataset Card for \"gsm8k_self_correct\"\n\t\n\nMore Information needed\n","url":"https://huggingface.co/datasets/euclaise/gsm8k_self_correct","creator_name":"euclaise","creator_url":"https://huggingface.co/euclaise","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"ReSA","keyword":"chain-of-thought","description":"ReSA (Reasoned Safety Alignment) is an open-source synthetic safety-training dataset with 80K examples designed to enhance LLM robustness against jailbreak attacks through an \"Answer-Then-Check\" strategy. The dataset teaches models to first generate a summary of their intended answer, then critically evaluate its safety before providing a final response. This approach achieves superior safety performance while maintaining strong general capabilities and reducing over-refusal rates.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ByteDance-Seed/ReSA.","url":"https://huggingface.co/datasets/ByteDance-Seed/ReSA","creator_name":"ByteDance Seed","creator_url":"https://huggingface.co/ByteDance-Seed","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-generation","English","odc-by","100M<n<1B","arxiv:2509.11629"],"keywords_longer_than_N":true},
	{"name":"Camildae","keyword":"cot","description":"\n\t\n\t\t\n\t\tmerge of some datasets from Alpaca Cot\n\t\n\n","url":"https://huggingface.co/datasets/NewstaR/Camildae","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","apache-2.0","1M - 10M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"openhermes-reasoning-231k","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tüß† OpenHermes Reasoning 377K\n\t\n\n\n\n\n\n\n\nHigh-quality instruction dataset with chain-of-thought reasoning\nü§ó Dataset ‚Ä¢ üí¨ Discussions\n\n\n\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\nThis dataset contains 231,144 high-quality instruction-response pairs with explicit chain-of-thought reasoning. Each example includes:\n\nPrompt: Original instruction or question\nThinking: Explicit reasoning process and logical steps\nAnswer: Final comprehensive response\n\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n‚úÖ Quality Filtered: Rigorous‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/limeXx/openhermes-reasoning-231k.","url":"https://huggingface.co/datasets/limeXx/openhermes-reasoning-231k","creator_name":"yusuf uzun","creator_url":"https://huggingface.co/limeXx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"openhermes-reasoning-231k","keyword":"cot","description":"\n\t\n\t\t\n\t\tüß† OpenHermes Reasoning 377K\n\t\n\n\n\n\n\n\n\nHigh-quality instruction dataset with chain-of-thought reasoning\nü§ó Dataset ‚Ä¢ üí¨ Discussions\n\n\n\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\nThis dataset contains 231,144 high-quality instruction-response pairs with explicit chain-of-thought reasoning. Each example includes:\n\nPrompt: Original instruction or question\nThinking: Explicit reasoning process and logical steps\nAnswer: Final comprehensive response\n\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n‚úÖ Quality Filtered: Rigorous‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/limeXx/openhermes-reasoning-231k.","url":"https://huggingface.co/datasets/limeXx/openhermes-reasoning-231k","creator_name":"yusuf uzun","creator_url":"https://huggingface.co/limeXx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"llm-fol-reasoning-eval","keyword":"cot","description":"\n\t\n\t\t\n\t\tLLM FOL Reasoning Eval\n\t\n\nThis dataset is derived from ProverQA, a First-Order Logic reasoning benchmark designed to test the ability of large language models (LLMs) to perform structured logical reasoning.It restructures and normalizes the ProverQA development and training data into a unified, clean format suitable for evaluating chain-of-thought (CoT) and symbolic reasoning capabilities in LLMs.\n\n\n\t\n\t\t\n\t\n\t\n\t\tSource\n\t\n\nOriginal dataset: ProverQA: A First-Order Logic Reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MinaGabriel/llm-fol-reasoning-eval.","url":"https://huggingface.co/datasets/MinaGabriel/llm-fol-reasoning-eval","creator_name":"Mina Gabriel","creator_url":"https://huggingface.co/MinaGabriel","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"OpenThoughts-TR-18k","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tOpenThoughts-TR-18k: Turkish Synthetic Reasoning Dataset\n\t\n\nOpenThoughts-TR-18k is a Turkish translation of a subset of the original Open-Thoughts-114k dataset. It contains ~18k high-quality synthetic reasoning examples covering mathematics, science, coding problems, and puzzles, all translated into Turkish. This dataset is designed to support reasoning task fine tuning for Turkish language models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n~18k translated reasoning examples\nCovers multiple domains:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/selimc/OpenThoughts-TR-18k.","url":"https://huggingface.co/datasets/selimc/OpenThoughts-TR-18k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Turkish","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"WeatherQA-CoT","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tModality-Bridging WeatherQA-CoT Datasets\n\t\n\nThis repository provides automatically constructed Chain-of-Thought (CoT) datasets for the WeatherQA benchmark, released as part of our study \"Modality-Bridging for Automated Chain-of-Thought Construction in Meteorological Reasoning: A Study on WeatherQA\".\nChain-of-Thought reasoning has become a crucial mechanism for enhancing the interpretability and robustness of Multimodal Large Language Models (MLLMs). However, in specialized domains such‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SparkingStar/WeatherQA-CoT.","url":"https://huggingface.co/datasets/SparkingStar/WeatherQA-CoT","creator_name":"Xue Wen","creator_url":"https://huggingface.co/SparkingStar","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","English","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"chain-of-diagnosis","keyword":"cot","description":"\n\t\n\t\t\n\t\tHPAI-BSC chain-of-diagnosis\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCurated version of the Chain-of-Diagnosis dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nChain-of-Diagnosis is a database used to improve interpretability in medical diagnostics for LLMs.\nWe curated and formatted the Chain-of-Diagnosis dataset into Alpaca format. This dataset is included in the training set of the Aloe-Beta model.\n\nCurated by:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/chain-of-diagnosis.","url":"https://huggingface.co/datasets/HPAI-BSC/chain-of-diagnosis","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"FC-CoT-Top10k","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tüìå Overview\n\t\n\n\nTotal Samples: 10,000  \nPrimary Focus:  \nHigh-quality Function Calling demonstrations  \nClear, well-structured Chain of Thought reasoning\n\n\nSelection Process:  \nRemoved noisy or incomplete examples  \nSelected cases with precise function arguments  \nEnsured reasoning steps are logically sound and human-readable\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüéØ Use Cases\n\t\n\nThis dataset is ideal for:\n\nFine-tuning LLMs for tool calling / function calling\nTraining models to provide explainable reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/FC-CoT-Top10k.","url":"https://huggingface.co/datasets/arcosoph/FC-CoT-Top10k","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Turing-Reason-CoT","keyword":"cot","description":"\n\n\t\n\t\t\n\t\tTuring-Reason-CoT\n\t\n\n\nTuring-Reason-CoT is a high-quality, compact chain-of-thought reasoning dataset curated for tasks in mathematics, science, and coding. While the dataset spans diverse domains, it is primarily driven by mathematical reasoning, reflecting a major share of math-focused prompts and long-form logical solutions.\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuick Start with Hugging Face Datasetsü§ó\n\t\n\npip install -U datasets\n\nfrom datasets import load_dataset\n\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Turing-Reason-CoT.","url":"https://huggingface.co/datasets/prithivMLmods/Turing-Reason-CoT","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","arrow"],"keywords_longer_than_N":true},
	{"name":"Dataset_of_Russian_thinking","keyword":"chain-of-thought","description":"Ru\nRTD  \n–û–ø–∏—Å–∞–Ω–∏–µ:Russian Thinking Dataset ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –î–∞—Ç–∞—Å–µ—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞, –∞–Ω–∞–ª–∏–∑–æ–º –¥–∏–∞–ª–æ–≥–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.  \n\n\t\n\t\t\n\t\t–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n\t\n\n\n–°–ø–ª–∏—Ç: train  \n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: 147.046\n\n\n\t\n\t\t\n\t\t–¶–µ–ª–∏:\n\t\n\n\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.  \n–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking.","url":"https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Dataset_of_Russian_thinking","keyword":"cot","description":"Ru\nRTD  \n–û–ø–∏—Å–∞–Ω–∏–µ:Russian Thinking Dataset ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –î–∞—Ç–∞—Å–µ—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞, –∞–Ω–∞–ª–∏–∑–æ–º –¥–∏–∞–ª–æ–≥–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.  \n\n\t\n\t\t\n\t\t–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n\t\n\n\n–°–ø–ª–∏—Ç: train  \n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: 147.046\n\n\n\t\n\t\t\n\t\t–¶–µ–ª–∏:\n\t\n\n\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.  \n–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking.","url":"https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"MMR1-RL","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tMMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThis repository introduces the MMR1 project, focusing on enhancing large multimodal reasoning models. While rapid progress has been made, advancements are constrained by two major limitations:\n\nThe absence of open, large-scale, high-quality long chain-of-thought (CoT) data.\nThe instability of reinforcement learning (RL) algorithms in post-training, where standard Group‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMR1/MMR1-RL.","url":"https://huggingface.co/datasets/MMR1/MMR1-RL","creator_name":"MMR1","creator_url":"https://huggingface.co/MMR1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"step3-input-bespoke-stratos-17k-test1","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tChain of ThoughtÁîüÊàê„Éá„Éº„Çø„Çª„ÉÉ„Éà\n\t\n\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅÂïèÈ°å„ÅÆËß£Á≠î„Åã„ÇâË™¨ÊòéÔºàChain of ThoughtÔºâ„ÇíÁîüÊàê„Åô„Çã„Åü„ÇÅ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tÊ¶ÇË¶Å\n\t\n\n\nÂá¶ÁêÜ„Åó„Åü„Çµ„É≥„Éó„É´Êï∞: 92\nÊúâÂäπ„Å™Ë™¨ÊòéÁîüÊàêÊï∞: 92\nÁîüÊàêÊàêÂäüÁéá: 100.00%\n‰ΩøÁî®„É¢„Éá„É´: Qwen/Qwen3-14B\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞Áµ±Ë®à\n\t\n\n\nÊúÄÂ∞è„Éà„Éº„ÇØ„É≥Êï∞: 227\nÊúÄÂ§ß„Éà„Éº„ÇØ„É≥Êï∞: 2348\nÂπ≥Âùá„Éà„Éº„ÇØ„É≥Êï∞: 653.8\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞ÂàÜÂ∏É\n\t\n\n\n0-100„Éà„Éº„ÇØ„É≥: 0‰ª∂ (0.0%)\n101-500„Éà„Éº„ÇØ„É≥: 47‰ª∂ (51.1%)\n501-1000„Éà„Éº„ÇØ„É≥: 31‰ª∂ (33.7%)\n1001-2000„Éà„Éº„ÇØ„É≥: 13‰ª∂ (14.1%)\n2001-5000„Éà„Éº„ÇØ„É≥: 1‰ª∂ (1.1%)\n5001+„Éà„Éº„ÇØ„É≥: 0‰ª∂ (0.0%)\n\n\n\t\n\t\t\n\t\t„Éá„Éº„Çø„Çª„ÉÉ„ÉàÊßãÈÄ†\n\t\n\n\nsystem_prompt: „É¢„Éá„É´„Å´ÈÄÅ‰ø°„Åï„Çå„Åü„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà\nquestion_text: ÂÖÉ„ÅÆÂïèÈ°åÊñá\nanswer_text: ÂïèÈ°å„ÅÆËß£Á≠î‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-compe-2025-kato/step3-input-bespoke-stratos-17k-test1.","url":"https://huggingface.co/datasets/llm-compe-2025-kato/step3-input-bespoke-stratos-17k-test1","creator_name":"llm-compe-2025-kato","creator_url":"https://huggingface.co/llm-compe-2025-kato","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Japanese","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"ida-reasoning-model","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tIDA Reasoning Model\n\t\n\nThis model was trained using Imitation, Distillation, and Amplification (IDA) on multiple reasoning datasets.\n\n\t\n\t\t\n\t\tTraining Details\n\t\n\n\nTeacher Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\nStudent Model: Qwen/Qwen3-1.7B\nDatasets: 4 reasoning datasets\nTotal Samples: 600\nTraining Method: IDA (Iterative Distillation and Amplification)\n\n\n\t\n\t\t\n\t\tDatasets Used\n\t\n\n\ngsm8k\nHuggingFaceH4/MATH-500\nMuskumPillerum/General-Knowledge\nSAGI-1/reasoningData_200k‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ziadrone/ida-reasoning-model.","url":"https://huggingface.co/datasets/ziadrone/ida-reasoning-model","creator_name":"drone","creator_url":"https://huggingface.co/ziadrone","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Alizee-OpenCodeReasoning-Phase3-1.4M","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tüöÄ Alizee OpenCodeReasoning Phase 3 Conformant Dataset - 1.2M Examples\n\t\n\n\n\t\n\t\t\n\t\tüìä Dataset Summary\n\t\n\nThis is a fully conformant version of the Phase 3 dataset, processed to strictly follow the specification with clean separation between data and formatting tags. Contains 1.2 million high-quality Python code examples with synthetic prompts and concise reasoning chains.\n\n\t\n\t\t\n\t\tKey Improvements\n\t\n\n\n‚úÖ 100% Conformant to Phase 3 specification\n‚úÖ Synthetic prompts generated from code‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DUKEAI/Alizee-OpenCodeReasoning-Phase3-1.4M.","url":"https://huggingface.co/datasets/DUKEAI/Alizee-OpenCodeReasoning-Phase3-1.4M","creator_name":"DUKE ANALYTICS","creator_url":"https://huggingface.co/DUKEAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M<n<10M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"Synthetic_CoT_dataset_RU","keyword":"cot","description":"–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π —Ä—É—Å—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è Chain-of-Thought (CoT) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π –≤ –ø–æ—à–∞–≥–æ–≤–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏. –ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤–∫–ª—é—á–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–π –∑–∞–ø—Ä–æ—Å, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. –¶–µ–ª—å –¥–∞—Ç–∞—Å–µ—Ç–∞ ‚Äì —É–ª—É—á—à–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –≤–æ–ø—Ä–æ—Å—ã –ø–æ –æ–±—â–µ–π —ç—Ä—É–¥–∏—Ü–∏–∏, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏. –î–∞–Ω–Ω—ã–µ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DataSynGen/Synthetic_CoT_dataset_RU.","url":"https://huggingface.co/datasets/DataSynGen/Synthetic_CoT_dataset_RU","creator_name":"DataGen","creator_url":"https://huggingface.co/DataSynGen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","apache-2.0","< 1K","text"],"keywords_longer_than_N":true},
	{"name":"supra-nexus-o1-training","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tSupra Nexus O1 Training Datasets\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nComprehensive training datasets for Supra Nexus O1 models, including:\n\nIdentity training\nChain-of-thought reasoning\nSelf-improvement examples (O1.5)\nInstruction following\n\n\n\t\n\t\t\n\t\tDatasets Included\n\t\n\n\n\t\n\t\t\n\t\t1. Identity Dataset (supra_identity.jsonl)\n\t\n\n\nModel identity and alignment\nOrganization information\nCapability descriptions\n\n\n\t\n\t\t\n\t\t2. Instruction Dataset (supra_instruct_*.jsonl)\n\t\n\n\nDirect instruction following\nVarious‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Supra-Nexus/supra-nexus-o1-training.","url":"https://huggingface.co/datasets/Supra-Nexus/supra-nexus-o1-training","creator_name":"Supra Nexus","creator_url":"https://huggingface.co/Supra-Nexus","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"Tag-Validation_Qwen3-14B-Step1-Bespoke17k-ep1","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\t„Çø„Ç∞Ê§úË®ºÁµêÊûú\n\t\n\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØË™¨ÊòéÁîüÊàê„Å´„Åä„Åë„Çã„Çø„Ç∞Ê§úË®º„ÅÆÁµêÊûú„ÇíÂê´„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ\n\n\t\n\t\t\n\t\tÊ¶ÇË¶Å\n\t\n\n\nÂá¶ÁêÜ„Åó„Åü„Çµ„É≥„Éó„É´Êï∞: 100\nË™¨ÊòéÁµÇ‰∫Ü„Çø„Ç∞ÊàêÂäüÁéá: 77.00%\nÊ§úË®ºÂØæË±°„Çø„Ç∞: <|end_of_explanation|>\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞Áµ±Ë®à\n\t\n\n\nÊúÄÂ∞è„Éà„Éº„ÇØ„É≥Êï∞: 370\nÊúÄÂ§ß„Éà„Éº„ÇØ„É≥Êï∞: 32437 \nÂπ≥Âùá„Éà„Éº„ÇØ„É≥Êï∞: 7124.1\n\n\n\t\n\t\t\n\t\t„Éà„Éº„ÇØ„É≥Êï∞ÂàÜÂ∏É\n\t\n\n\n0-100„Éà„Éº„ÇØ„É≥: 0‰ª∂ (0.0%)\n101-500„Éà„Éº„ÇØ„É≥: 4‰ª∂ (4.0%)\n501-1000„Éà„Éº„ÇØ„É≥: 21‰ª∂ (21.0%)\n1001-2000„Éà„Éº„ÇØ„É≥: 17‰ª∂ (17.0%)\n2001-5000„Éà„Éº„ÇØ„É≥: 23‰ª∂ (23.0%)\n5001+„Éà„Éº„ÇØ„É≥: 35‰ª∂ (35.0%)\n\n\n\t\n\t\t\n\t\t„Éá„Éº„Çø„Çª„ÉÉ„ÉàÊßãÈÄ†\n\t\n\n\nid: ÂêÑ„Çµ„É≥„Éó„É´„ÅÆ‰∏ÄÊÑèË≠òÂà•Â≠ê\nuser_content: „É¢„Éá„É´„Å´„É¶„Éº„Ç∂„Éº„É°„ÉÉ„Çª„Éº„Ç∏„Å®„Åó„Å¶ÈÄÅ‰ø°„Åó„ÅüÂÜÖÂÆπÔºàË≥™Âïè„ÅÆ„ÅøÔºâ\nassistant_content: „É¢„Éá„É´„Å´„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„É°„ÉÉ„Çª„Éº„Ç∏„Å®„Åó„Å¶ÈÄÅ‰ø°„Åó„ÅüÂÜÖÂÆπÔºà„Çø„Ç∞‰ªò„ÅçËß£Á≠î +‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-compe-2025-kato/Tag-Validation_Qwen3-14B-Step1-Bespoke17k-ep1.","url":"https://huggingface.co/datasets/llm-compe-2025-kato/Tag-Validation_Qwen3-14B-Step1-Bespoke17k-ep1","creator_name":"llm-compe-2025-kato","creator_url":"https://huggingface.co/llm-compe-2025-kato","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Japanese","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"train-of-thought","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tTrain of Thought Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset readapts agentlans/think-more\ninto the Alpaca-style instruction tuning format for training language models in direct answering and chain-of-thought reasoning.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach original example was randomly assigned to be thinking on or off:\n\nThinking off: Outputs only the final answer.\nThinking on:\nOutputs a chain-of-thought (CoT) reasoning process wrapped in <think>...</think>, followed by the final answer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/train-of-thought.","url":"https://huggingface.co/datasets/agentlans/train-of-thought","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-FC-Reasoning-en-10k","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tüìå Overview\n\t\n\n\nTotal Samples: 10,000  \nPrimary Focus:  \nHigh-quality Function Calling demonstrations  \nClear, well-structured Chain of Thought reasoning\n\n\nSelection Process:  \nTranslated from Chinese to English\nRemoved noisy or incomplete examples  \nSelected cases with precise function arguments  \nEach example has been made clearer and more effective\nEnsured reasoning steps are logically sound and human-readable\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüéØ Use Cases\n\t\n\nThis dataset is ideal for:\n\nFine-tuning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-en-10k.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-en-10k","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"ChessCOT","keyword":"cot","description":"\n\t\n\t\t\n\t\tChessCOT\n\t\n\nThe dataset that makes your chess model think like a human before it plays a move.\n\n\t\n\t\t\n\t\tAbout\n\t\n\nChessCOT is a dataset designed to train transformers for chess using a Chain of Thought (CoT) approach. The goal is to make the model reason about the position with all possible moves and their consequences in order to predict the best move.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Poistions: 4,491,596\nSequence length of sMoves: 128\nSequence length of thought: 128‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/frosthead/ChessCOT.","url":"https://huggingface.co/datasets/frosthead/ChessCOT","creator_name":"Ayush Sharma","creator_url":"https://huggingface.co/frosthead","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","mit","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"AIME25-CoT-CN","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tSci-Bench-AIME25'\n\t\n\nThis repo is a branch of Sci Bench made by IPF team-SnailAILab. Mainly include the AIME 25' solution with multi-modal CoT and diverse solving path. \n\n\t\n\t\t\n\t\tüìö Cite\n\t\n\nIf you use the Sci-Bench-AIME25 (IPF/AIME25-CoT-CN) dataset in your research, please cite:\n@dataset{zhang2025scibench_aime25,\n    title = {{Sci-Bench-AIME25}: A Multi-Modal Chain-of-Thought Dataset for Advanced Tool-Intergrated Mathematical Reasoning},\n    author = {Zhang, Haoxiang and Wang, Siyuan‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SnailAILab/AIME25-CoT-CN.","url":"https://huggingface.co/datasets/SnailAILab/AIME25-CoT-CN","creator_name":"$–øAIL üêå","creator_url":"https://huggingface.co/SnailAILab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"twi-reasoning-dataset_v2","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tTwi Reasoning Dataset\n\t\n\nA Twi (Akan) translation of the Multilingual-Thinking reasoning dataset with chain-of-thought in Twi\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a Twi (Akan) translation of the Multilingual-Thinking reasoning dataset. It contains chain-of-thought reasoning traces translated from multiple languages into Twi, making it one of the first reasoning datasets available in this language.\n\n\t\n\t\t\n\t\tLanguage Information\n\t\n\n\nLanguage: Twi (Akan)\nLanguage Code: tw\nFamily:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi-reasoning-dataset_v2.","url":"https://huggingface.co/datasets/michsethowusu/twi-reasoning-dataset_v2","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","conversational","text2text-generation","language-modeling"],"keywords_longer_than_N":true},
	{"name":"CoT-aichatbot","keyword":"cot","description":"Bluestrike/CoT-aichatbot dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Bluestrike/CoT-aichatbot","creator_name":"BLUE STRIKE AI","creator_url":"https://huggingface.co/Bluestrike","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"tw-math-reasoning-2k","keyword":"cot","description":"\n\t\n\t\t\n\t\tDataset Card for tw-math-reasoning-2k\n\t\n\n\ntw-math-reasoning-2k ÊòØ‰∏ÄÂÄãÁπÅÈ´î‰∏≠ÊñáÊï∏Â≠∏Ë™ûË®ÄË≥áÊñôÈõÜÔºåÂæû HuggingFaceH4/MATH Ëã±ÊñáÊï∏Â≠∏È°åÂ∫´‰∏≠Á≤æÈÅ∏ 2,000 È°åÔºå‰∏¶ÈÄèÈÅé perplexity-ai/r1-1776 Ê®°Âûã‰ª•ÁπÅÈ´î‰∏≠ÊñáÈáçÊñ∞ÁîüÊàêÂÖ∑ÈÇèËºØÊÄß‰∏îË©≥Áõ°ÁöÑËß£È°åÈÅéÁ®ãËàáÊúÄÁµÇÁ≠îÊ°à„ÄÇÊ≠§Ë≥áÊñôÈõÜÂèØ‰ΩúÁÇ∫Ë®ìÁ∑¥ÊàñË©ï‰º∞ÁπÅÈ´î‰∏≠ÊñáÊï∏Â≠∏Êé®ÁêÜÊ®°ÂûãÁöÑÈ´òÂìÅË≥™ÂèÉËÄÉË™ûÊñô„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\ntw-math-reasoning-2k ÊòØ‰∏ÄÂÄãÁπÅÈ´î‰∏≠ÊñáÊï∏Â≠∏Ë™ûË®ÄË≥áÊñôÈõÜÔºåÊó®Âú®Êèê‰æõÈ´òÂìÅË≥™ÁöÑËß£È°åË™ûÊñô‰ª•ÊîØÊè¥‰∏≠ÊñáÊï∏Â≠∏Êé®ÁêÜÊ®°ÂûãÁöÑË®ìÁ∑¥ËàáË©ï‰º∞„ÄÇÊ≠§Ë≥áÊñôÈõÜÂæû HuggingFaceH4/MATH Ëã±ÊñáÊï∏Â≠∏È°åÂ∫´‰∏≠Á≤æÈÅ∏ 2,000 È°åÔºåÊ∂µËìã‰ª£Êï∏„ÄÅÂπæ‰Ωï„ÄÅÊ©üÁéáÁµ±Ë®àÁ≠âÂêÑÈ°ûÈ°åÂûãÔºå‰∏¶Á¢∫‰øùÈ°åÁõÆÈ°ûÂûãÂàÜ‰ΩàÂùáË°°„ÄÇ\nÊâÄÊúâÈ°åÁõÆÁöÜÁ∂ìÁî± perplexity-ai/r1-1776‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/twinkle-ai/tw-math-reasoning-2k.","url":"https://huggingface.co/datasets/twinkle-ai/tw-math-reasoning-2k","creator_name":"Twinkle AI","creator_url":"https://huggingface.co/twinkle-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MAmmoTH-VL-Instruct-12M","keyword":"cot","description":"\n\t\n\t\t\n\t\tMAmmoTH-VL-Instruct-12M used in MoCa Pre-training\n\t\n\nüè† Homepage | üíª Code | ü§ñ MoCa-Qwen25VL-7B | ü§ñ MoCa-Qwen25VL-3B | üìö Datasets | üìÑ Paper\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThis is a VQA style dataset used in the modality-aware continual pre-training of MoCa models. It is adapted from MAmmoTH-VL-Instruct-12M by concatenating prompts and responses.\nThe dataset consists of interleaved multimodal examples. text is a string containing text while imagesare image binaries that can be loaded‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/moca-embed/MAmmoTH-VL-Instruct-12M.","url":"https://huggingface.co/datasets/moca-embed/MAmmoTH-VL-Instruct-12M","creator_name":"MoCa","creator_url":"https://huggingface.co/moca-embed","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"cot","description":"\n\t\n\t\t\n\t\tDeepthink Reasoning Demo\n\t\n\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction.","url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-Codex-Weaver-FC-Reasoning","keyword":"chain-of-thought","description":"\n\t\n\t\t\n\t\tArcosoph Codex Weaver Function Calling Reasoning Dataset (V1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWelcome to the Arcosoph-Codex-Weaver-FC-Reasoning dataset! This is a comprehensive, multi-source, and meticulously curated dataset designed for instruction-tuning language models to function as intelligent, offline AI agents.\nThis dataset is provided in a universal, easy-to-parse JSON Lines (.jsonl) format, making it an ideal \"source of truth\" for creating fine-tuning data for various models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true}
]
;
