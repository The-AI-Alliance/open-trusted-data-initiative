const data_for_modality_chain_of_thought = 
[
	{"name":"vulnerability-intelligence-diagrammatic-reasoning","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/daqc/vulnerability-intelligence-diagrammatic-reasoning","creator_name":"David Quispe","creator_url":"https://huggingface.co/daqc","description":"\n \n\n\n\n\t\n\t\t\n\t\tVulnerability Intelligence with Diagrammatic Reasoning\n\t\n\n\n[!Important]\nThis dataset was created as a proof-of-concept for the Reasoning Datasets Competition (May 2025). If you have any feedback or suggestions, please feel free to open a discussion! Access the Github repository here.\n\n\n\t\n\t\t\n\t\n\t\n\t\tA. Overview\n\t\n\n\n\n\n\n\n\nThis dataset focuses on security vulnerability analysis through a multi-dimensional approach that combines four types of reasoning to generate valuable insights for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/daqc/vulnerability-intelligence-diagrammatic-reasoning.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\n–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É: <think> –¢–≤–æ–∏ –º—ã—Å–ª–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è </think> \n–¢–≤–æ–π –∫–æ–Ω–µ—á–Ω—ã–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1-v2","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"This is a modified version of ZeroAgency/ru-thinking-reasoning-r1 with addition of Egor-AI/CoT-XLang dataset.\nCombined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\n–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"CoTton-38k-6525-Collective","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","description":"\n\t\n\t\t\n\t\tCoTton-38k-6525-Collective\n\t\n\nCoTton-38k is a 38,350-example dataset of soft reasoning conversations in the ShareGPT format. Each entry contains an exchange between a user and a model, showcasing high-quality Chain-of-Thought (CoT) reasoning in natural language.\nThe dataset is distilled from open LLMs:\n\nQwen3 235B A22B\nAM Thinking\nQwQ 32B\nDeepseek R1\nR1 0528\n\nThe name CoTton encodes multiple layers of meaning:\n\nCoT: Chain-of-Thought is embedded in the name\nTON: The dataset contains a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CoTton-38k-6525-Collective","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","description":"\n\t\n\t\t\n\t\tCoTton-38k-6525-Collective\n\t\n\nCoTton-38k is a 38,350-example dataset of soft reasoning conversations in the ShareGPT format. Each entry contains an exchange between a user and a model, showcasing high-quality Chain-of-Thought (CoT) reasoning in natural language.\nThe dataset is distilled from open LLMs:\n\nQwen3 235B A22B\nAM Thinking\nQwQ 32B\nDeepseek R1\nR1 0528\n\nThe name CoTton encodes multiple layers of meaning:\n\nCoT: Chain-of-Thought is embedded in the name\nTON: The dataset contains a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"altered-riddles","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marcodsn/altered-riddles","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","description":"\n \n\n\n\n\t\n\t\t\n\t\tDataset Card for Altered Riddles Dataset\n\t\n\nWhile working on the academic-chains dataset, I tested a well-known alteration of a common riddle, \"just for fun\":\n\nThe surgeon, who is the boy's father, says, 'I cannot operate on this boy‚Äîhe's my son!'. Who is the surgeon to the boy?\n\n(Below is the original riddle for reference)\n\nA man and his son are in a terrible accident and are rushed to the hospital in critical condition. The doctor looks at the boy and exclaims, \"I can't operate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/altered-riddles.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"FinClaimRadar-Financial-Reasoning","keyword":"chain-of-thought","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sksayan01/FinClaimRadar-Financial-Reasoning","creator_name":"Sayan Sekh","creator_url":"https://huggingface.co/sksayan01","description":"\n\t\n\t\t\n\t\tDataset Card for FinClaimRadar: A Dataset for Evidentiary Reasoning and Classification of Financial Claims\n\t\n\nThis dataset card provides an overview of the FinClaimRadar dataset, designed to advance capabilities in financial claim understanding and reasoning.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nFinClaimRadar is a novel dataset comprising 600+ meticulously crafted financial claims, each classified according to a 6-category evidentiary basis and accompanied by a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sksayan01/FinClaimRadar-Financial-Reasoning.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"GenRef-wds","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/diffusion-cot/GenRef-wds","creator_name":"Diffusion CoT","creator_url":"https://huggingface.co/diffusion-cot","description":"\n\t\n\t\t\n\t\tGenRef-1M\n\t\n\n\n  \n\n\nWe provide 1M high-quality triplets of the form (flawed image, high-quality image, reflection) collected across\nmultiple domains using our scalable pipeline from [1]. We used this dataset to train our reflection tuning model.\nTo know the details of the dataset creation pipeline, please refer to Section 3.2 of [1].\nProject Page: https://diffusion-cot.github.io/reflection2perfection\n\n\t\n\t\t\n\t\n\t\n\t\tDataset loading\n\t\n\nWe provide the dataset in the webdataset format for fast‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/diffusion-cot/GenRef-wds.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1M - 10M","webdataset"],"keywords_longer_than_N":true},
	{"name":"tw-math-reasoning-2k","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/twinkle-ai/tw-math-reasoning-2k","creator_name":"Twinkle AI","creator_url":"https://huggingface.co/twinkle-ai","description":"\n\t\n\t\t\n\t\tDataset Card for tw-math-reasoning-2k\n\t\n\n\ntw-math-reasoning-2k ÊòØ‰∏ÄÂÄãÁπÅÈ´î‰∏≠ÊñáÊï∏Â≠∏Ë™ûË®ÄË≥áÊñôÈõÜÔºåÂæû HuggingFaceH4/MATH Ëã±ÊñáÊï∏Â≠∏È°åÂ∫´‰∏≠Á≤æÈÅ∏ 2,000 È°åÔºå‰∏¶ÈÄèÈÅé perplexity-ai/r1-1776 Ê®°Âûã‰ª•ÁπÅÈ´î‰∏≠ÊñáÈáçÊñ∞ÁîüÊàêÂÖ∑ÈÇèËºØÊÄß‰∏îË©≥Áõ°ÁöÑËß£È°åÈÅéÁ®ãËàáÊúÄÁµÇÁ≠îÊ°à„ÄÇÊ≠§Ë≥áÊñôÈõÜÂèØ‰ΩúÁÇ∫Ë®ìÁ∑¥ÊàñË©ï‰º∞ÁπÅÈ´î‰∏≠ÊñáÊï∏Â≠∏Êé®ÁêÜÊ®°ÂûãÁöÑÈ´òÂìÅË≥™ÂèÉËÄÉË™ûÊñô„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\ntw-math-reasoning-2k ÊòØ‰∏ÄÂÄãÁπÅÈ´î‰∏≠ÊñáÊï∏Â≠∏Ë™ûË®ÄË≥áÊñôÈõÜÔºåÊó®Âú®Êèê‰æõÈ´òÂìÅË≥™ÁöÑËß£È°åË™ûÊñô‰ª•ÊîØÊè¥‰∏≠ÊñáÊï∏Â≠∏Êé®ÁêÜÊ®°ÂûãÁöÑË®ìÁ∑¥ËàáË©ï‰º∞„ÄÇÊ≠§Ë≥áÊñôÈõÜÂæû HuggingFaceH4/MATH Ëã±ÊñáÊï∏Â≠∏È°åÂ∫´‰∏≠Á≤æÈÅ∏ 2,000 È°åÔºåÊ∂µËìã‰ª£Êï∏„ÄÅÂπæ‰Ωï„ÄÅÊ©üÁéáÁµ±Ë®àÁ≠âÂêÑÈ°ûÈ°åÂûãÔºå‰∏¶Á¢∫‰øùÈ°åÁõÆÈ°ûÂûãÂàÜ‰ΩàÂùáË°°„ÄÇ\nÊâÄÊúâÈ°åÁõÆÁöÜÁ∂ìÁî± perplexity-ai/r1-1776‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/twinkle-ai/tw-math-reasoning-2k.","first_N":5,"first_N_keywords":["text-generation","Chinese","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"RelatLogic-Reasoning","keyword":"chain-of-thought","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shb777/RelatLogic-Reasoning","creator_name":"SB","creator_url":"https://huggingface.co/shb777","description":"\n \n\n\n\n\t\n\t\t\n\t\tRelatLogic Reasoning Dataset\n\t\n\nThis dataset contains examples of logic puzzles involving comparisons, conditional statements, and superlative queries etc. each paired with a step-by-step, chain of thought reasoning and a ground‚Äêtruth answer. It is designed to advance LLM capabilities in deep, multi‚Äêstep reasoning, constraint satisfaction and evidence evaluation.\n\n\t\n\t\t\n\t\tü§î Curation Rationale\n\t\n\nA lot of LLM's these days are \"aligned\" to agree with the user. You can \"convince\" it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shb777/RelatLogic-Reasoning.","first_N":5,"first_N_keywords":["question-answering","text2text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Resume-Analysis-CoTR","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","description":"\n\t\n\t\t\n\t\tResume Reasoning and Feedback Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains approximately 417 examples designed to facilitate research and development in automated resume analysis and feedback generation. Each data point consists of a user query regarding their resume, a simulated internal analysis (chain-of-thought) performed by an expert persona, and a final, user-facing feedback response derived solely from that analysis.\nThe dataset captures a two-step reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR.","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"gsm8k_self_correct","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/euclaise/gsm8k_self_correct","creator_name":"Jade","creator_url":"https://huggingface.co/euclaise","description":"\n\t\n\t\t\n\t\tDataset Card for \"gsm8k_self_correct\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Camildae","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewstaR/Camildae","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","description":"\n\t\n\t\t\n\t\tmerge of some datasets from Alpaca Cot\n\t\n\n","first_N":5,"first_N_keywords":["question-answering","apache-2.0","1M - 10M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"academic-chains","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marcodsn/academic-chains","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","description":"\n \n\n\n\n\t\n\t\t\n\t\tDataset Card for Academic Reasoning and Intuition Chains\n\t\n\n\n(The image above is an output from Llama-3.2-3B-Instruct tuned on this dataset, quantized to 8 bit and ran on llama.cpp; In our tests Qwen3-30B-A3B, Gemini 2.5 Pro and Claude Sonnet 3.7 with thinking enabled all got this simple question wrong)\nThis dataset contains reasoning (and intuition) chains distilled from open-access research papers, primarily focusing on fields like Biology, Economics, Physics, Math, Computer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/academic-chains.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ChessCOT","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/frosthead/ChessCOT","creator_name":"Ayush Sharma","creator_url":"https://huggingface.co/frosthead","description":"\n\t\n\t\t\n\t\tChessCOT\n\t\n\nThe dataset that makes your chess model think like a human before it plays a move.\n\n\t\n\t\t\n\t\tAbout\n\t\n\nChessCOT is a dataset designed to train transformers for chess using a Chain of Thought (CoT) approach. The goal is to make the model reason about the position with all possible moves and their consequences in order to predict the best move.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Poistions: 4,491,596\nSequence length of sMoves: 128\nSequence length of thought: 128‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/frosthead/ChessCOT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","mit","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"GenRef-CoT","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/diffusion-cot/GenRef-CoT","creator_name":"Diffusion CoT","creator_url":"https://huggingface.co/diffusion-cot","description":"\n\t\n\t\t\n\t\tGenRef-CoT\n\t\n\n\n  \n\n\nWe provide 227K high-quality CoT reflections which were used to train our Qwen-based reflection generation model in ReflectionFlow [1]. To\nknow the details of the dataset creation pipeline, please refer to Section 3.2 of [1].\n\n\t\n\t\t\n\t\tDataset loading\n\t\n\nWe provide the dataset in the webdataset format for fast dataloading and streaming. We recommend downloading\nthe repository locally for faster I/O:\nfrom huggingface_hub import snapshot_download\n\nlocal_dir =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/diffusion-cot/GenRef-CoT.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"think-more","keyword":"chain-of-thought","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/think-more","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThink More is a large-scale, multi-domain collection of long chain-of-thought (CoT) reasoning examples. It aggregates and cleans several prominent reasoning datasets, focusing on high-quality, step-by-step model-generated solutions from DeepSeek R1 and OpenAI o1. Each entry includes a question, the model‚Äôs answer, and the detailed thought process leading to that answer.\n‚ö†Ô∏è Warning: the dataset decompresses to a 15.1 GB JSONLines file.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/think-more.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"train-of-thought","keyword":"chain-of-thought","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/train-of-thought","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tTrain of Thought Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset readapts agentlans/think-more\ninto the Alpaca-style instruction tuning format for training language models in direct answering and chain-of-thought reasoning.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach original example was randomly assigned to be thinking on or off:\n\nThinking off: Outputs only the final answer.\nThinking on:\nOutputs a chain-of-thought (CoT) reasoning process wrapped in <think>...</think>, followed by the final answer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/train-of-thought.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"Alpaca-CoT","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QingyiSi/Alpaca-CoT","creator_name":"Qingyi Si","creator_url":"https://huggingface.co/QingyiSi","description":"\n\t\n\t\t\n\t\n\t\n\t\tInstruction-Finetuning Dataset Collection (Alpaca-CoT)\n\t\n\nThis repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model.\nWe also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT.  \nIf you think this dataset collection is helpful to you, please like‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT.","first_N":5,"first_N_keywords":["English","Chinese","Malayalam","apache-2.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"ru-chain-of-thought-sharegpt","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/ru-chain-of-thought-sharegpt","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"–ü–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω–∞—è –ø—Ä–∏ –ø–æ–º–æ—â–∏ utrobinmv/t5_translate_en_ru_zh_small_1024 –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –≤–µ—Ä—Å–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ isaiahbjork/chain-of-thought-sharegpt.\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"clevr-tr","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berhaan/clevr-tr","creator_name":"Berhan T√ºrk√º Ay","creator_url":"https://huggingface.co/berhaan","description":"\n\t\n\t\t\n\t\tDataset Card for CoT\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository: LLaVA-CoT GitHub Repository\nPaper: LLaVA-CoT on arXiv\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nunzip image.zip\n\nThe train.jsonl file contains the question-answering data and is structured in the following format:\n{\n  \"id\": \"example_id\",\n  \"image\": \"example_image_path\",\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"L√ºtfen resimdeki kƒ±rmƒ±zƒ± metal nesnelerin sayƒ±sƒ±nƒ± belirtin.\"},\n    {\"from\": \"gpt\", \"value\": \"Resimde 3 kƒ±rmƒ±zƒ±‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/berhaan/clevr-tr.","first_N":5,"first_N_keywords":["visual-question-answering","English","Turkish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Aloe-Beta-Medical-Collection","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tAloe-Beta-Medical-Collection\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated datasets used to fine-tune Aloe-Beta.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nWe curated data from many publicly available medical instruction tuning data sources (QA format). Most data samples correspond to single-turn QA pairs, while a small proportion contain multi-turn. All data sources are publicly available for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ultramedical","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/ultramedical","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tHAPI-BSC ultramedical\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCurated version of the UltraMedical dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThe UltraMedical Collections is a large-scale, high-quality dataset of biomedical instructions. We collected and curated the following sets:\n\nTextBookQA\nMedical-Instruction-120k\nWikiInstruct\n\nThis dataset is included in the Aloe-Beta model training set.\n\nCurated by:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/ultramedical.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"MedS-Ins","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/MedS-Ins","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tHPAI-BSC MedS-Ins\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated data from the MedS-Ins dataset. Used to train Aloe-Beta model.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThis is the curated version of the MedS-Ins dataset included in the training set of the Aloe-Beta models. \nFirst, we selected 75 out of the 122 existing tasks, excluding the tasks that were already in the training set, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedS-Ins.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"mauxi-COT-Persian","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxi-COT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüß† mauxi-COT-Persian Dataset\n\t\n\n\nExploring Persian Chain-of-Thought Reasoning with DeepSeek-R1, brought to you by Mauxi AI Platform\n\n\n\t\n\t\t\n\t\tüåü Overview\n\t\n\nmauxi-COT-Persian is a community-driven dataset that explores the capabilities of advanced language models in generating Persian Chain-of-Thought (CoT) reasoning. The dataset is actively growing with new high-quality, human-validated entries being added regularly. I am personally working on expanding this dataset with rigorously‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-COT-Persian.","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"Magpie-COT","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/project-r/Magpie-COT","creator_name":"Project R","creator_url":"https://huggingface.co/project-r","description":"\n\t\n\t\t\n\t\tMagpie-COT\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCombined Chain-of-Thought dataset containing three sources:\n\nMagpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B\nMagpie-Reasoning-V2-250K-CoT-QwQ\nMagpie-Reasoning-V2-250K-CoT-Skywork-O1-Llama-3.1-8B\n\n\n\t\n\t\t\n\t\tKey Enhancements:\n\t\n\n\nAdded model source tracking column\nProcessed Deepseek responses to extract  tag content\nUnified format across multiple CoT datasets\n\n\n\t\n\t\t\n\t\tProcessing Steps:\n\t\n\n\nKept 'instruction' and 'response' columns\nAdded‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/project-r/Magpie-COT.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Magpie-COT","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/project-r/Magpie-COT","creator_name":"Project R","creator_url":"https://huggingface.co/project-r","description":"\n\t\n\t\t\n\t\tMagpie-COT\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCombined Chain-of-Thought dataset containing three sources:\n\nMagpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B\nMagpie-Reasoning-V2-250K-CoT-QwQ\nMagpie-Reasoning-V2-250K-CoT-Skywork-O1-Llama-3.1-8B\n\n\n\t\n\t\t\n\t\tKey Enhancements:\n\t\n\n\nAdded model source tracking column\nProcessed Deepseek responses to extract  tag content\nUnified format across multiple CoT datasets\n\n\n\t\n\t\t\n\t\tProcessing Steps:\n\t\n\n\nKept 'instruction' and 'response' columns\nAdded‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/project-r/Magpie-COT.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Sky-T1_data_steps","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shakedzy/Sky-T1_data_steps","creator_name":"Shaked","creator_url":"https://huggingface.co/shakedzy","description":"\n\t\n\t\t\n\t\tSky-T1_data_steps\n\t\n\nThis dataset contains 182 samples taken from NovaSky-AI/Sky-T1_data_17k \ndataset and broken down to thinking steps. This dataset was used to train shakedzy/Sky-T1-32B-Steps \nLoRA adapter for step-by-step thinking.\nBreaking down the thought process to steps was done using Ollama's quantized version of Llama-3.2-1B.\nSee step_prompt file for the exact prompt used.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Columns\n\t\n\n\nid (int): row index of the sample in the original dataset (starts at 0)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shakedzy/Sky-T1_data_steps.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tDeepthink Reasoning Demo\n\t\n\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"chain-of-thought","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","description":"\n ü¶Ñ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n\n\n\n      \n    [ArXiv] | [ü§óHuggingFace] | [Website]\n    \n    \n\n\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\n\t\n\t\tüî•News\n\t\n\n\nüéñÔ∏è Our work is accepted by ACL2024.\n\nüî• We have release benchmark on [ü§óHuggingFace].\n\nüî• The paper is also available on [ArXiv].\n\nüîÆ Interactive benchmark website & more exploration are available on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-sa-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MATH-500-Overall","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fluently-sets/MATH-500-Overall","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","description":"\n\t\n\t\t\n\t\tMATH-500-Overall\n\t\n\n\n\t\n\t\t\n\t\tAbout the dataset\n\t\n\nThis dataset of only 500 examples combines mathematics, physics and logic in English with reasoning and step-by-step problem solving, the dataset was created synthetically, CoT of Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct.\n\n\t\n\t\t\n\t\tBrief information\n\t\n\n\nNumber of rows: 500\nType of dataset files: parquet\nType of dataset: text, alpaca with system prompts\nLanguage: English\nLicense: MIT\n\nStructure:\nmath¬Ø¬Ø¬Ø¬Ø¬Ø‚åâ\n   school-level (100 rows)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/MATH-500-Overall.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","text-classification","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"synmath-1-dsv3-87k","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Gusarich/synmath-1-dsv3-87k","creator_name":"Daniil Sedov","creator_url":"https://huggingface.co/Gusarich","description":"\n\t\n\t\t\n\t\tsynmath-1-dsv3-87k\n\t\n\nsynmath-1-dsv3-87k is a dataset consisting of 86,700 math problems and their corresponding solutions, formatted in a chain-of-thought manner. The problems span 867 distinct mathematical domains, providing diverse and comprehensive coverage for fine-tuning smaller models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nsynmath-1-dsv3-87k contains synthetically generated math problems and step-by-step solutions designed to enhance mathematical reasoning in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gusarich/synmath-1-dsv3-87k.","first_N":5,"first_N_keywords":["text2text-generation","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"step_prm","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xiaodongguaAIGC/step_prm","creator_name":"xiaodongguaAIGC","creator_url":"https://huggingface.co/xiaodongguaAIGC","description":"\n\t\n\t\t\nÊï∞ÊçÆÈõÜÂêçÁß∞\nÊòØÂê¶Êúâstep\nÂèØÁî®‰∫éPRMËÆ≠ÁªÉ\nÊ†áÁ≠æÂΩ¢Âºè\nTitle\nÂ§áÊ≥®\n\n\n\t\t\nGSM8K\n‚úÖ\n‚ùå\nÁ≠îÊ°à\nTraining Verifiers to Solve Math Word Problems\n\n\n\nMATH\n‚ùå\n‚ùå\nÁ≠îÊ°à\nMeasuring Mathematical Problem Solving With the MATH Dataset\nNon-Step\n\n\nPRM800K\n‚úÖ\n‚úÖ\nÊ≠£Á°ÆÁ±ªÂà´\nLet's Verify Step by Step\nprompt deduplication\n\n\nMath-Shepherd\n‚úÖ\n‚úÖ\nÊ≠£Á°ÆÁ±ªÂà´\nMath-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\nNot used\n\n\nProcessBench\n‚úÖ\n‚úÖ\nÈ¶ñ‰∏™ÈîôËØØÊ≠•È™§\nProcessBench: Identifying Process Errors in Mathematical Reasoning\nonly label -1\n\n\n\t\n\n","first_N":5,"first_N_keywords":["text-classification","question-answering","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"Dataset_of_Russian_thinking","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","description":"Ru\nRTD  \n–û–ø–∏—Å–∞–Ω–∏–µ:Russian Thinking Dataset ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –î–∞—Ç–∞—Å–µ—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞, –∞–Ω–∞–ª–∏–∑–æ–º –¥–∏–∞–ª–æ–≥–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.  \n\n\t\n\t\t\n\t\t–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n\t\n\n\n–°–ø–ª–∏—Ç: train  \n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: 147.046\n\n\n\t\n\t\t\n\t\t–¶–µ–ª–∏:\n\t\n\n\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.  \n–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Dataset_of_Russian_thinking","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","description":"Ru\nRTD  \n–û–ø–∏—Å–∞–Ω–∏–µ:Russian Thinking Dataset ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –î–∞—Ç–∞—Å–µ—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞, –∞–Ω–∞–ª–∏–∑–æ–º –¥–∏–∞–ª–æ–≥–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.  \n\n\t\n\t\t\n\t\t–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n\t\n\n\n–°–ø–ª–∏—Ç: train  \n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: 147.046\n\n\n\t\n\t\t\n\t\t–¶–µ–ª–∏:\n\t\n\n\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.  \n–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"reflection-small-sonnet","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/reflection-small-sonnet","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\n\t\n\t\t\n\t\tVerified reasoning examples\n\t\n\n","first_N":5,"first_N_keywords":["text2text-generation","Italian","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"sharegpt_cot_dataset","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AiCloser/sharegpt_cot_dataset","creator_name":"Ai Closer","creator_url":"https://huggingface.co/AiCloser","description":"\n\t\n\t\t\n\t\tA data set inspired by the \"Reflection\" method, three-dimensional thinking and cot\n\t\n\n\n\t\n\t\t\n\t\tThis is the ShareGPT format.\n\t\n\nThe data set was generated using multiple llm synthesis.\n","first_N":5,"first_N_keywords":["question-answering","text-generation","text2text-generation","English","Russian"],"keywords_longer_than_N":true},
	{"name":"pisc-tr","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berhaan/pisc-tr","creator_name":"Berhan T√ºrk√º Ay","creator_url":"https://huggingface.co/berhaan","description":"\n\t\n\t\t\n\t\tDataset Card for CoT\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository: LLaVA-CoT GitHub Repository\nPaper: LLaVA-CoT on arXiv\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\ncat image.zip.part-* > image.zip #not uploaded yet\nunzip image.zip\n\nThe train.jsonl file contains the question-answering data and is structured in the following format:\n{\n  \"id\": \"example_id\",\n  \"image\": \"example_image_path\",\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"L√ºtfen resimdeki kƒ±rmƒ±zƒ± metal nesnelerin sayƒ±sƒ±nƒ± belirtin.\"}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/berhaan/pisc-tr.","first_N":5,"first_N_keywords":["visual-question-answering","English","Turkish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"chain-of-diagnosis","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/chain-of-diagnosis","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tHPAI-BSC chain-of-diagnosis\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCurated version of the Chain-of-Diagnosis dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nChain-of-Diagnosis is a database used to improve interpretability in medical diagnostics for LLMs.\nWe curated and formatted the Chain-of-Diagnosis dataset into Alpaca format. This dataset is included in the training set of the Aloe-Beta model.\n\nCurated by:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/chain-of-diagnosis.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"MAmmoTH-VL-Instruct-12M","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M","creator_name":"MAmmoTH-VL","creator_url":"https://huggingface.co/MAmmoTH-VL","description":"\n\t\n\t\t\n\t\tMAmmoTH-VL-Instruct-12M\n\t\n\nüè† Homepage | ü§ñ MAmmoTH-VL-8B | üíª Code | üìÑ Arxiv | üìï PDF | üñ•Ô∏è Demo\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nOur simple yet scalable visual instruction data rewriting pipeline consists of three steps: manual data source collection, rewriting using MLLMs/LLMs, and filtering via the same MLLM as a judge. Examples below illustrate transformations in math and science categories, showcasing detailed, step-by-step responses.\n\n\t\n\t\t\n\t\tThe data distribution of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"HQ-knowledgedistills-1.2M-magpie","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstackorg/HQ-knowledgedistills-1.2M-magpie","creator_name":"Pinkstack-org","creator_url":"https://huggingface.co/Pinkstackorg","description":"This dataset is.an exact mix of 900k general qwen conversation with general questions, math, code and another 300k of Gemma 2 27B generations, for creative writing. \nThe dataset was made for \"healing\" pruned LLM's, especially ones based off of qwen2.5 series, as some conversations include the models saying who they are. \nUnlike the previous 900K version, we also mixed in Gemma generations, to add more creative writing examples.\nMany thanks to the magpie project for making this possible, this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Pinkstackorg/HQ-knowledgedistills-1.2M-magpie.","first_N":5,"first_N_keywords":["text-generation","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"CoT_reformatted","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jtatman/CoT_reformatted","creator_name":"James","creator_url":"https://huggingface.co/jtatman","description":"\n\t\n\t\t\n\t\tDataset Card for \"CoT_reformatted\"\n\t\n\nThis dataset is reformatted from: QingyiSi/Alpaca-CoT\nAll credit goes there. Thanks to QingyiSi for the work in consolidating many diverse sources for comparison and cross-file analysis.\nThere were some issues loading files from that dataset for a testing project. \nI extracted the following data files for this subset:\n\nalpaca_data_cleaned\nCoT_data\nfirefly       \ninstruct\nalpaca_gpt4_data\ndolly \nGPTeacher\nthoughtsource\nfinance_en\ninstinwild_en\n\n","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Synthetic_CoT_dataset_RU","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DataSynGen/Synthetic_CoT_dataset_RU","creator_name":"DataGen","creator_url":"https://huggingface.co/DataSynGen","description":"–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π —Ä—É—Å—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è Chain-of-Thought (CoT) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π –≤ –ø–æ—à–∞–≥–æ–≤–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏. –ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤–∫–ª—é—á–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–π –∑–∞–ø—Ä–æ—Å, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. –¶–µ–ª—å –¥–∞—Ç–∞—Å–µ—Ç–∞ ‚Äì —É–ª—É—á—à–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –≤–æ–ø—Ä–æ—Å—ã –ø–æ –æ–±—â–µ–π —ç—Ä—É–¥–∏—Ü–∏–∏, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏. –î–∞–Ω–Ω—ã–µ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DataSynGen/Synthetic_CoT_dataset_RU.","first_N":5,"first_N_keywords":["text-generation","Russian","apache-2.0","< 1K","text"],"keywords_longer_than_N":true},
	{"name":"Drone-flight-monitoring-reasoning-SFT","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GabrielCheng/Drone-flight-monitoring-reasoning-SFT","creator_name":"Gabriel","creator_url":"https://huggingface.co/GabrielCheng","description":"\n\t\n\t\t\n\t\tDrone-flight-monitoring-reasoning-SFT\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nÊú¨Êï∞ÊçÆÈõÜÊòØ‰∏Ä‰∏™‰∏ìÊ≥®‰∫éÊó†‰∫∫Êú∫È£ûË°åÂÆâÂÖ®È¢ÜÂüüÁöÑ‰∏≠ÊñáÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºåÈááÁî®‰∫ÜChain-of-Thought (CoT) ÁöÑÊ†ºÂºè„ÄÇÂÆÉÊó®Âú®Áî®‰∫éÁªÉ‰π†Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂæÆË∞ÉËÆ≠ÁªÉÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊ®°Êãü‰∏ìÂÆ∂ÊÄùËÄÉËøáÁ®ãÔºåÂπ∂ÈíàÂØπÊó†‰∫∫Êú∫ÂÆâÂÖ®Áõ∏ÂÖ≥ÈóÆÈ¢òÁîüÊàêÂåÖÂê´Êé®ÁêÜÊ≠•È™§ÁöÑÁªìÊûÑÂåñÂõûÁ≠î„ÄÇÂæÆË∞ÉÂêéÊ®°ÂûãËßÅ(GabrielCheng/Deepseek-r1-finetuned-drone-safty) „ÄÇ\nÊú¨Êï∞ÊçÆÈõÜÊòØÂü∫‰∫é Hugging Face Âπ≥Âè∞‰∏äÁöÑ skylink-drone-cot-datasets (pohsjxx/default-domain-cot-dataset) ËøõË°åÂ§ÑÁêÜÂíåË°çÁîüÁöÑ„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure / Data Fields\n\t\n\nÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÊØè‰∏™Ê†∑Êú¨ÂåÖÂê´‰ª•‰∏ãÂ≠óÊÆµÔºö\n\nQuestion (string): ÂÖ≥‰∫éÊó†‰∫∫Êú∫È£ûË°åÂÆâÂÖ®ÊàñÈ£éÈô©Áõ∏ÂÖ≥ÁöÑÈóÆÈ¢ò„ÄÇ\nReasoning (string): Ê®°ÊãüÊ®°ÂûãÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇ\nAnswer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GabrielCheng/Drone-flight-monitoring-reasoning-SFT.","first_N":5,"first_N_keywords":["question-answering","text-generation","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"CoMT","keyword":"chain-of-thought","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/czh-up/CoMT","creator_name":"ZihuiCheng","creator_url":"https://huggingface.co/czh-up","description":"\n   CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models\n\n\n\n      \n    | [ArXiv] | [ü§óHuggingFace] |\n    \n    \n\n\n\n\n\n\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\n\t\n\t\tüî•News\n\t\n\n\nüéñÔ∏è Our work is accepted by AAAI 2025 !\nüî• We have release benchmark on [ü§óHuggingFace].\nüî• The paper is also available on [ArXiv].\n\n\n\t\n\t\n\t\n\t\tüí° Motivation\n\t\n\nLarge Vision-Language Models (LVLMs) have recently demonstrated amazing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/czh-up/CoMT.","first_N":5,"first_N_keywords":["question-answering","image-to-image","image-to-text","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"cortex-1-market-analysis","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Jarrodbarnes/cortex-1-market-analysis","creator_name":"Jarrod Barnes","creator_url":"https://huggingface.co/Jarrodbarnes","description":"\n\t\n\t\t\n\t\tNEAR Cortex-1 Market Analysis Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains blockchain market analyses combining historical and real-time data with chain-of-thought reasoning. The dataset includes examples from Ethereum, Bitcoin, and NEAR chains, demonstrating high-quality market analysis with explicit calculations, numerical citations, and actionable insights.\nThe dataset has been enhanced with examples generated by GPT-4o and Claude 3.7 Sonnet, providing diverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jarrodbarnes/cortex-1-market-analysis.","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Thinking-multilingual-big-10k-sft","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/Thinking-multilingual-big-10k-sft","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"\nA dataset based off of openo1 math, 500 examples translated to 23 different languages. filtered out un-translated examples.\nenjoy üëç\n","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"OpenHumanreasoning-multilingual-2.2k","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/OpenHumanreasoning-multilingual-2.2k","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"Based off of Pinkstackorg/humanreasoning-testing-en, it is a human-generated dataset where a real person had to create most of the reasoning and the final output. If there are any mistakes please let us know.\nWe offer this dataset at an apache-2.0 license to make it useful for everybody.\nnote: translations are not human generated.\n","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-CoT-Math-170k","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\n\t\n\t\t\n\t\tGammaCorpus: CoT Math 170k\n\t\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nNumber of Rows: 169,527\nFormat: JSONL\nLanguage: English\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\n\n\n\t\n\t\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-CoT-Math-170k","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\n\t\n\t\t\n\t\tGammaCorpus: CoT Math 170k\n\t\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nNumber of Rows: 169,527\nFormat: JSONL\nLanguage: English\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\n\n\n\t\n\t\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_CoT","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/0fg/truthful_qa_CoT","creator_name":"Bryce","creator_url":"https://huggingface.co/0fg","description":"Question-Answer dataset generated using CAMEL CoTDataGenerator and GPT-4o Mini","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"CoT-XLang","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Egor-AI/CoT-XLang","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","description":"RU:CoT-XLang ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø–æ—à–∞–≥–æ–≤—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ (Chain-of-Thought, CoT) –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, —Ä—É—Å—Å–∫–∏–π, —è–ø–æ–Ω—Å–∫–∏–π –∏ –¥—Ä—É–≥–∏–µ. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π —Ä–µ—à–µ–Ω–∏–π —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤. –î–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç –æ–∫–æ–ª–æ 2,419,912 –ø—Ä–∏–º–µ—Ä–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/CoT-XLang.","first_N":5,"first_N_keywords":["text-generation","question-answering","Russian","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"OpenThoughts-TR-18k","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/OpenThoughts-TR-18k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\n\t\n\t\t\n\t\tOpenThoughts-TR-18k: Turkish Synthetic Reasoning Dataset\n\t\n\nOpenThoughts-TR-18k is a Turkish translation of a subset of the original Open-Thoughts-114k dataset. It contains ~18k high-quality synthetic reasoning examples covering mathematics, science, coding problems, and puzzles, all translated into Turkish. This dataset is designed to support reasoning task fine tuning for Turkish language models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n~18k translated reasoning examples\nCovers multiple domains:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/selimc/OpenThoughts-TR-18k.","first_N":5,"first_N_keywords":["Turkish","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"RPRevamped-Small","keyword":"chain-of-thought","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TechPowerB/RPRevamped-Small","creator_name":"Bhargav Raj","creator_url":"https://huggingface.co/TechPowerB","description":"\n\t\n\t\t\n\t\tRPRevamped-Small-v1.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nRPRevamped is a synthetic dataset generated by various numbers of models. It is very diverse and is recommended if you are fine-tuning a roleplay model. This is the Small version with Medium and Tiny version currently in work.\nGithub: RPRevamped GitHub\nHere are the models used in creation of this dataset:\nDeepSeek-V3-0324\nGemini-2.0-Flash-Thinking-Exp-01-21\nDeepSeek-R1\nGemma-3-27B-it\nGemma-3-12B-it\nQwen2.5-VL-72B-Instruct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TechPowerB/RPRevamped-Small.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"TinyDS-20k","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Hamzah-Asadullah/TinyDS-20k","creator_name":"Hamzah Asadullah","creator_url":"https://huggingface.co/Hamzah-Asadullah","description":"\n\t\n\t\t\n\t\tTinyDS\n\t\n\nTinyDS (short for Tiny DeepSeek) is a dataset generated synthetically using SyntheticAlpaca and Qwen3-8B.  \n  \n\n\nThis dataset is a simple Alpaca-format dataset using the viral TTC concept (structured reasoning, mainly).The LLM was prompted to generate questions and answers in 32 different languages, and the most spoken languages were picked. Since the Qwen org stated that this model supports over 100 languages, this is something reasonable to do without compromising on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Hamzah-Asadullah/TinyDS-20k.","first_N":5,"first_N_keywords":["question-answering","translation","text-generation","text2text-generation","English"],"keywords_longer_than_N":true}
]
;
