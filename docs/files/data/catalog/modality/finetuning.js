const data_for_modality_finetuning = 
[
	{"name":"code-cinema-image-animee","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode du cinÃ©ma et de l'image animÃ©e, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-cinema-image-animee.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-cinema-image-animee","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"smart_home_control","keyword":"sft","description":"Charles95/smart_home_control dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Charles95/smart_home_control","creator_name":"JingXiang","creator_url":"https://huggingface.co/Charles95","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kituba","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kituba is a groundbreaking dataset containing 148,000 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kituba, making coding education accessible to Kituba speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n148,000 high-quality conversations about programming and coding\nPure Kituba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kituba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kituba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kituba (Democratic Republic of Congo)","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kinyarwanda","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kinyarwanda is a groundbreaking dataset containing 12,345 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kinyarwanda, making coding education accessible to Kinyarwanda speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,345 high-quality conversations about programming and coding\nPure Kinyarwanda language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kinyarwanda.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kinyarwanda","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kinyarwanda","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-swahili","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-swahili is a groundbreaking dataset containing 126,025 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Swahili, making coding education accessible to Swahili speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n126,025 high-quality conversations about programming and coding\nPure Swahili language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-swahili.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-swahili","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Swahili","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-afar","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-afar is a groundbreaking dataset containing 114,895 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Afar, making coding education accessible to Afar speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n114,895 high-quality conversations about programming and coding\nPure Afar language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-afar.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-afar","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Afar","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-bambara","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-bambara is a groundbreaking dataset containing 54,885 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Bambara, making coding education accessible to Bambara speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n54,885 high-quality conversations about programming and coding\nPure Bambara language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-bambara.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-bambara","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Bambara","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LongWriter-6k","keyword":"sft","description":"\n\t\n\t\t\n\t\tLongWriter-6k\n\t\n\n\n  ðŸ¤— [LongWriter Dataset]  â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongWriter Paper] \n\n\nLongWriter-6k dataset contains 6,000 SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese). The data can support training LLMs to extend their maximum output window size to 10,000+ words.\n\n\t\n\t\t\n\t\n\t\n\t\tAll Models\n\t\n\nWe open-sourced the following list of models trained on LongWriter-6k:\n\n\t\n\t\t\nModel\nHuggingface Repo\nDescription\n\n\n\t\t\nLongWriter-glm4-9b\nðŸ¤—â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zai-org/LongWriter-6k.","url":"https://huggingface.co/datasets/zai-org/LongWriter-6k","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"tiny-llm-synthetic-qa","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tTiny-LLM: Synthetic Question-Answering Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset was created for the fine-tuning stage of the Tiny-LLM Project, a project focused on training and evaluating compact language models from scratch.\nIt contains 706,727 high-quality, synthetic multi-turn Question-Answering (Q&A) conversations in English, generated using the Gemini API. The dataset was designed to teach small models instruction-following capabilities across a diverse range ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Gabriel8/tiny-llm-synthetic-qa.","url":"https://huggingface.co/datasets/Gabriel8/tiny-llm-synthetic-qa","creator_name":"Gabriel de Antonio Mazetto","creator_url":"https://huggingface.co/Gabriel8","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"MathCanvas-Instruct","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMathCanvas-Instruct Dataset\n\t\n\n\n  \n    \n  \n  Â Â Â Â Â Â Â Â \n  \n    \n  \n  Â Â Â Â Â Â Â Â \n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“– Overview\n\t\n\nMathCanvas-Instruct is a high-quality, fine-tuning dataset with 219K examples of interleaved visual-textual reasoning paths. It is the core component for the second phase of the [MathCanvas] framework: Strategic Visual-Aided Reasoning.\nAfter a model learns foundational diagram generation and editing from MathCanvas-Imagen and MathCanvas-Edit, this dataset teaches it theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shiwk24/MathCanvas-Instruct.","url":"https://huggingface.co/datasets/shiwk24/MathCanvas-Instruct","creator_name":"Weikang Shi","creator_url":"https://huggingface.co/shiwk24","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"taboo-smile","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-smile\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-smile\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-smile","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-product-code-mapping","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 1801\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-product-code-mapping.","url":"https://huggingface.co/datasets/archit11/hyperswitch-product-code-mapping","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"unix-commands","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tUnix Commands Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Unix Commands Dataset is a unique collection of real-world Unix command line examples, captured from various system prompts representing different user roles and responsibilities, such as system administrators, DevOps, network administrators, Docker administrators, regular users, and hackers.\nThe dataset consists of Unix commands ranging from basic to advanced levels and from a wide array of categories, including file operations (lsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/harpomaxx/unix-commands.","url":"https://huggingface.co/datasets/harpomaxx/unix-commands","creator_name":"Carlos A. Catania (Harpo)","creator_url":"https://huggingface.co/harpomaxx","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"LongCite-45k","keyword":"sft","description":"\n\t\n\t\t\n\t\tLongCite-45k\n\t\n\n\n  ðŸ¤— [LongCite Dataset]  â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongCite Paper] \n\n\nLongCite-45k dataset contains 44,600 long-context QA instances paired with sentence-level citations (both English and Chinese, up to 128,000 words). The data can support training long-context LLMs to generate response and fine-grained citations within a single output.\n\n\t\n\t\t\n\t\n\t\n\t\tData Example\n\t\n\nEach instance in LongCite-45k consists of an instruction, a long context (divided into sentences), a userâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zai-org/LongCite-45k.","url":"https://huggingface.co/datasets/zai-org/LongCite-45k","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Code-170k-malagasy","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-malagasy is a groundbreaking dataset containing 12,232 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Malagasy, making coding education accessible to Malagasy speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,232 high-quality conversations about programming and coding\nPure Malagasy language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-malagasy.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-malagasy","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Malagasy","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kikongo","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kikongo is a groundbreaking dataset containing 111,609 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kikongo, making coding education accessible to Kikongo speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n111,609 high-quality conversations about programming and coding\nPure Kikongo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kikongo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kikongo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kongo","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kiga","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kiga is a groundbreaking dataset containing 124,707 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kiga, making coding education accessible to Kiga speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n124,707 high-quality conversations about programming and coding\nPure Kiga language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kiga.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kiga","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Chiga","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tsonga","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tsonga is a groundbreaking dataset containing 123,270 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tsonga, making coding education accessible to Tsonga speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n123,270 high-quality conversations about programming and coding\nPure Tsonga language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tsonga.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tsonga","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tsonga","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-igbo","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-igbo is a groundbreaking dataset containing 12,467 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Igbo, making coding education accessible to Igbo speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,467 high-quality conversations about programming and coding\nPure Igbo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-igbo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-igbo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Igbo","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-alur","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-alur is a groundbreaking dataset containing 56,109 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Alur, making coding education accessible to Alur speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n56,109 high-quality conversations about programming and coding\nPure Alur language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-alur.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-alur","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Alur","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-kanuri","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-kanuri is a groundbreaking dataset containing 128,111 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Kanuri, making coding education accessible to Kanuri speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n128,111 high-quality conversations about programming and coding\nPure Kanuri language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-kanuri.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-kanuri","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kanuri","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-sepedi","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-sepedi is a groundbreaking dataset containing 108,619 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Sepedi, making coding education accessible to Sepedi speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n108,619 high-quality conversations about programming and coding\nPure Sepedi language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-sepedi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-sepedi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Pedi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tshiluba","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tshiluba is a groundbreaking dataset containing 113,468 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tshiluba, making coding education accessible to Tshiluba speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n113,468 high-quality conversations about programming and coding\nPure Tshiluba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tshiluba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tshiluba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Luba-Lulua","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-swati","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-swati is a groundbreaking dataset containing 122,345 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Swati, making coding education accessible to Swati speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n122,345 high-quality conversations about programming and coding\nPure Swati language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-swati.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-swati","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Swati","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tumbuka","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tumbuka is a groundbreaking dataset containing 129,591 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tumbuka, making coding education accessible to Tumbuka speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n129,591 high-quality conversations about programming and coding\nPure Tumbuka language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tumbuka.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tumbuka","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tumbuka","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-fon","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-fon is a groundbreaking dataset containing 125,588 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Fon, making coding education accessible to Fon speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n125,588 high-quality conversations about programming and coding\nPure Fon language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-fon.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-fon","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Fon","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-seychellois-creole","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-seychellois-creole is a groundbreaking dataset containing 100,690 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Seychellois Creole, making coding education accessible to Seychellois Creole speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n100,690 high-quality conversations about programming and coding\nPure Seychellois Creole language - democratizing coding education\nMulti-turn dialogues covering variousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-seychellois-creole.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-seychellois-creole","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Seselwa Creole French","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-rundi","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-rundi is a groundbreaking dataset containing 56,496 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Rundi, making coding education accessible to Rundi speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n56,496 high-quality conversations about programming and coding\nPure Rundi language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-rundi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-rundi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kirundi","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-oromo","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-oromo is a groundbreaking dataset containing 150,739 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Oromo, making coding education accessible to Oromo speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n150,739 high-quality conversations about programming and coding\nPure Oromo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-oromo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-oromo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Oromo","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-ndebele-south","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-ndebele-south is a groundbreaking dataset containing 176,994 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Ndebele (South), making coding education accessible to Ndebele (South) speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n176,994 high-quality conversations about programming and coding\nPure Ndebele (South) language - democratizing coding education\nMulti-turn dialogues covering various programmingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-ndebele-south.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-ndebele-south","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Southern Ndebele","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-mauritian-creole","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-mauritian-creole is a groundbreaking dataset containing 145,454 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Mauritian Creole, making coding education accessible to Mauritian Creole speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n145,454 high-quality conversations about programming and coding\nPure Mauritian Creole language - democratizing coding education\nMulti-turn dialogues covering various programmingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-mauritian-creole.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-mauritian-creole","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Morisyen","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-issue-to-code_v3","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 900\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code_v3.","url":"https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code_v3","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ChatMed_Consult_Dataset","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for ChatMed\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nChatMed-Dataset is a dataset of 110,113 medical query-response pairs (in Chinese) generated by OpenAI's GPT-3.5 engine. The queries are crawled from several online medical consultation sites, reflecting the medical needs in the real world. The responses are generated by the OpenAI engine. This dataset is designated to to inject medical knowledge into Chinese large language models. \nThe dataset size growing rapidly. Stay tuned forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ticoAg/ChatMed_Consult_Dataset.","url":"https://huggingface.co/datasets/ticoAg/ChatMed_Consult_Dataset","creator_name":"ticoAg","creator_url":"https://huggingface.co/ticoAg","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"dnd-35-training-dataset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tD&D 3.5 Fine-Tuning Dataset\n\t\n\nA carefully curated dataset of 50,000 examples for fine-tuning LLMs to understand D&D 3.5 mechanics.\n\n\t\n\t\t\n\t\tQuick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load from HuggingFace\ndataset = load_dataset(\"m0no1/dnd-35-training-dataset\")\n\n# Or load locally\nimport json\nwith open('dnd_35_FINAL_BALANCED_CLEAN_50k.jsonl', 'r') as f:\n    data = [json.loads(line) for line in f]\n\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\nSize: 50,000 examples\nFormat: JSONL withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m0no1/dnd-35-training-dataset.","url":"https://huggingface.co/datasets/m0no1/dnd-35-training-dataset","creator_name":"Alexis von Blumenthal","creator_url":"https://huggingface.co/m0no1","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"code-relations-public-administration","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des relations entre le public et l'administration, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the developmentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-relations-public-administration.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-relations-public-administration","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-penal","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode pÃ©nal, non-instruct (2025-05-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models basedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-penal.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-penal","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"alpaca-spanish","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tBERTIN Alpaca Spanish\n\t\n\nThis dataset is a translation to Spanish of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\n","url":"https://huggingface.co/datasets/bertin-project/alpaca-spanish","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"urdu_alpaca_yc_filtered","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tAlpaca Urdu\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Alpaca Urdu  is a translation of the original dataset into Urdu. This dataset is a part of the Alpaca project and is designed for NLP tasks.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSize: The translated dataset contains [45,622] samples.\nLanguages: Urdu\nLicense: [cc-by-4.0]\nOriginal Dataset: Link to the original Alpaca Cleaned dataset repository\n\n\n\t\n\t\t\n\t\tColumns\n\t\n\nThe translated dataset includes the following columns:\n\ninput: The input text in Urdu.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered.","url":"https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered","creator_name":"Mahwiz Khalil","creator_url":"https://huggingface.co/mahwizzzz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Urdu","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"UltraChatTR_50k","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tðŸ’¬ UltraChat 50K â€“ TÃ¼rkÃ§e Diyalog Veri Seti\n\t\n\nUltraChat 50K, orijinal UltraChat veri setinden tÃ¼retilmiÅŸ,55.046 TÃ¼rkÃ§e diyalog Ã¶rneÄŸi iÃ§eren aÃ§Ä±k kaynak bir veri setidir.Veri, bÃ¼yÃ¼k dil modellerinin TÃ¼rkÃ§e konuÅŸma anlayÄ±ÅŸÄ± ve cevap kalitesini geliÅŸtirmek iÃ§infine-tuning (SFT) amacÄ±yla dÃ¼zenlenmiÅŸtir.\n\n\n\t\n\t\t\n\t\tðŸ“˜ Veri KÃ¼nyesi\n\t\n\n\n\t\n\t\t\nÃ–zellik\nAÃ§Ä±klama\n\n\n\t\t\nToplam SatÄ±r SayÄ±sÄ±\n55.046\n\n\nVeri FormatÄ±\nJSON Lines, Parquet\n\n\nAlanlar\ninstruction, input, output\n\n\nDil\nTÃ¼rkÃ§e ðŸ‡¹ðŸ‡·\n\n\nLisans\nMITâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hamuz/UltraChatTR_50k.","url":"https://huggingface.co/datasets/hamuz/UltraChatTR_50k","creator_name":"Hamza YiÄŸit KÃ¼ltÃ¼r","creator_url":"https://huggingface.co/hamuz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Turkish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"UltraChatTR_50k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ’¬ UltraChat 50K â€“ TÃ¼rkÃ§e Diyalog Veri Seti\n\t\n\nUltraChat 50K, orijinal UltraChat veri setinden tÃ¼retilmiÅŸ,55.046 TÃ¼rkÃ§e diyalog Ã¶rneÄŸi iÃ§eren aÃ§Ä±k kaynak bir veri setidir.Veri, bÃ¼yÃ¼k dil modellerinin TÃ¼rkÃ§e konuÅŸma anlayÄ±ÅŸÄ± ve cevap kalitesini geliÅŸtirmek iÃ§infine-tuning (SFT) amacÄ±yla dÃ¼zenlenmiÅŸtir.\n\n\n\t\n\t\t\n\t\tðŸ“˜ Veri KÃ¼nyesi\n\t\n\n\n\t\n\t\t\nÃ–zellik\nAÃ§Ä±klama\n\n\n\t\t\nToplam SatÄ±r SayÄ±sÄ±\n55.046\n\n\nVeri FormatÄ±\nJSON Lines, Parquet\n\n\nAlanlar\ninstruction, input, output\n\n\nDil\nTÃ¼rkÃ§e ðŸ‡¹ðŸ‡·\n\n\nLisans\nMITâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hamuz/UltraChatTR_50k.","url":"https://huggingface.co/datasets/hamuz/UltraChatTR_50k","creator_name":"Hamza YiÄŸit KÃ¼ltÃ¼r","creator_url":"https://huggingface.co/hamuz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Turkish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"code-propriete-intellectuelle","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la propriÃ©tÃ© intellectuelle, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-propriete-intellectuelle.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-propriete-intellectuelle","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"cgi","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode GÃ©nÃ©ral des ImpÃ´ts, non-instruct (11-12-2023)\n\t\n\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for tax practice. \nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve supervised learningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/cgi.","url":"https://huggingface.co/datasets/louisbrulenaudet/cgi","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"code-procedure-civile","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de procÃ©dure civile, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-procedure-civile.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-procedure-civile","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"lpf","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tLivre des procÃ©dures fiscales, non-instruct (11-12-2023)\n\t\n\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for tax practice. \nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve supervisedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/lpf.","url":"https://huggingface.co/datasets/louisbrulenaudet/lpf","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"code-postes-communications-electroniques","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des postes et des communications Ã©lectroniques, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-postes-communications-electroniques.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-postes-communications-electroniques","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"ru_turbo_alpaca","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tRuTurboAlpaca\n\t\n\nDataset of ChatGPT-generated instructions in Russian.\n\n\n\nCode: rulm/self_instruct\nCode is based on Stanford Alpaca and self-instruct.\n29822 examples\n\nPreliminary evaluation by an expert based on 400 samples:\n\n83% of samples contain correct instructions\n63% of samples have correct instructions and outputs\n\nCrowdsouring-based evaluation on 3500 samples:\n\n90% of samples contain correct instructions\n68% of samples have correct instructions and outputs\n\nPrompt template:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/IlyaGusev/ru_turbo_alpaca.","url":"https://huggingface.co/datasets/IlyaGusev/ru_turbo_alpaca","creator_name":"Ilya Gusev","creator_url":"https://huggingface.co/IlyaGusev","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","Russian","cc-by-4.0","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"MIXTURE","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMIXTURE\n\t\n\nMIXTURE is a dataset designed for instruction distillation, which aims to transform sparse, incomplete, and low-quality inputs into a single information-dense output.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset provides two main components for different stages of model training:\n\ndata_sft/ â€” Used for cold start supervised fine-tuning (SFT).  \n\ndata_grpo/ â€” Used for GRPO (Group Relative Policy Optimization) training.\n\n\nFor more details about data construction, structure, and usageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dokiik/MIXTURE.","url":"https://huggingface.co/datasets/dokiik/MIXTURE","creator_name":"ikod","creator_url":"https://huggingface.co/dokiik","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"FinancialClassification","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Financial classification\n\t\n\n\n\nThis dataset contains the stock name, the event, and the corresponding price variation that occurred on a specific date. It can be used for regression tasks or text classification.\nI used this dataset to train a regression model available on my Hugging Face profile.\nYou can learn how to use the dataset using this example on: https://huggingface.co/blog/SelmaNajih001/how-to-run-a-regression-using-hugging-face\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Descriptionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SelmaNajih001/FinancialClassification.","url":"https://huggingface.co/datasets/SelmaNajih001/FinancialClassification","creator_name":"Selma Najih","creator_url":"https://huggingface.co/SelmaNajih001","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-sv","keyword":"instruct","description":"\n\t\n\t\t\n\t\tOpenHermes-2.5-sv\n\t\n\nThis is a machine translated instruct dataset from OpenHermes-2.5. \nThe facebook/seamless-m4t-v2-large was used, and some post filtering is done to remove repetitive texts that occurred due to translation errors.\n\n\t\n\t\t\n\t\tExample data:\n\t\n\n[\n   {\n      \"from\":\"human\",\n      \"value\":\"Vilket naturfenomen, som orsakas av att ljus reflekteras och bryts genom vattendroppar, resulterar i en fÃ¤rgglad bÃ¥ge pÃ¥ himlen?\",\n      \"weight\":null\n   },\n   {\n      \"from\":\"gpt\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv.","url":"https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv","creator_name":"Tim Isbister","creator_url":"https://huggingface.co/timpal0l","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Swedish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Shakespeare_Poetry","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tShakespeare Poetry Instruction Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset provides a high-quality, curated collection of instruction-response pairs designed for fine-tuning large language models (LLMs) to generate poetry in the style of William Shakespeare. The dataset is sourced from public domain Shakespearean works including the Sonnets, Venus and Adonis, The Rape of Lucrece, and other poetic texts.\nEach record is formatted for instruction-following tasks, making itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Exquisique/Shakespeare_Poetry.","url":"https://huggingface.co/datasets/Exquisique/Shakespeare_Poetry","creator_name":"Pankaj Singh","creator_url":"https://huggingface.co/Exquisique","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"finetuning","description":"\n\n\n\t\n\t\t\n\t\tStackMathQA\n\t\n\nStackMathQA: A Curated Collection of 2 Million Mathematical Questions and Answers Sourced from Stack Exchange\n\n\n\n\n\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\n\n\t\t\n\t\n\t\tConfigs\n\t\n\nconfigs:\n- config_name: stackmathqa1600kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/StackMathQA.","url":"https://huggingface.co/datasets/math-ai/StackMathQA","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"Performance-Marketing-Data","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tPerformance Marketing Expert Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains comprehensive performance marketing knowledge and logical reasoning patterns for Meta (Facebook/Instagram), Google Ads, and TikTok advertising platforms. It's designed for fine-tuning language models to understand brand verticals, performance marketing strategies, and develop reasoning capacity for creating winning ad campaigns.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example follows anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sri-Vigneshwar-DJ/Performance-Marketing-Data.","url":"https://huggingface.co/datasets/Sri-Vigneshwar-DJ/Performance-Marketing-Data","creator_name":"DJ Sri Vigneshwar","creator_url":"https://huggingface.co/Sri-Vigneshwar-DJ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Performance-Marketing-Data","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tPerformance Marketing Expert Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains comprehensive performance marketing knowledge and logical reasoning patterns for Meta (Facebook/Instagram), Google Ads, and TikTok advertising platforms. It's designed for fine-tuning language models to understand brand verticals, performance marketing strategies, and develop reasoning capacity for creating winning ad campaigns.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example follows anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sri-Vigneshwar-DJ/Performance-Marketing-Data.","url":"https://huggingface.co/datasets/Sri-Vigneshwar-DJ/Performance-Marketing-Data","creator_name":"DJ Sri Vigneshwar","creator_url":"https://huggingface.co/Sri-Vigneshwar-DJ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"WebCPM_WK","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for WebCPM_WK\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\næœ¬æ•°æ®é›†æ˜¯ç”±æˆ‘ä»¬å¯¹WebCPMçš„pipelineæ•°æ®è¿›è¡ŒäºŒæ¬¡å¤„ç†ä¹‹åŽæž„å»ºè€Œæˆã€‚\nä¸»è¦åŒ…æ‹¬è¿‡æ»¤åŽŸå§‹æ•°æ®ä¸­çš„ä¸€äº›ä½Žè´¨é‡æ•°æ®ï¼Œä½¿ç”¨GPT4å’ŒChatGPTæ‰©å……åŽŸå§‹æ•°æ®ï¼Œä»¥åŠä½¿ç”¨éšæœºæ›¿æ¢ã€æ‹¼æŽ¥çš„æ–¹å¼å¢žå¼ºåŽŸå§‹æ•°æ®ã€‚\nè¯¥æ•°æ®é›†ä¸»è¦çš„ç›®çš„æ˜¯é€šè¿‡æŒ‡ä»¤å¾®è°ƒçš„æ–¹å¼æé«˜LLMçš„ä¸¤ä¸ªèƒ½åŠ›ï¼š\n\nç»™å®šé—®é¢˜å’Œæ–‡æ¡£ï¼ŒæŠ½å–æ–‡æ¡£ä¸­ä¸Žé—®é¢˜ç›¸å…³çŸ¥è¯†çš„èƒ½åŠ›ã€‚\nç»™å®šå‚è€ƒææ–™å’Œé—®é¢˜ï¼Œæ ¹æ®å‚è€ƒææ–™å›žç­”é—®é¢˜çš„èƒ½åŠ›ã€‚\n\n\n\t\n\t\t\n\t\tLicensing Information\n\t\n\nThe dataset is available under the Creative Commons NonCommercial (CC BY-NC 4.0).\n","url":"https://huggingface.co/datasets/ZHR123/WebCPM_WK","creator_name":"ZHR","creator_url":"https://huggingface.co/ZHR123","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"code-ports-maritimes","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des ports maritimes, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-ports-maritimes.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-ports-maritimes","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-procedure-penale","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de procÃ©dure pÃ©nale, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-procedure-penale.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-procedure-penale","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-recherche","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la recherche, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-recherche.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-recherche","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"silk-road_alpaca-data-gpt4-chinese","keyword":"fine-tune","description":"botp/silk-road_alpaca-data-gpt4-chinese dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/botp/silk-road_alpaca-data-gpt4-chinese","creator_name":"ab10","creator_url":"https://huggingface.co/botp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AIVision360-8k","keyword":"finetune","description":"\n\t\n\t\t\n\t\tDataset Card for AIVision360-8k\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAIVision360 is the pioneering domain-specific dataset tailor-made for media and journalism, designed expressly for the instruction fine-tuning of Large Language Models (LLMs).The AIVision360-8k dataset is a curated collection sourced from \"ainewshub.ie\", a platform dedicated to Artificial Intelligence news from quality-controlled publishers. It is designed to provide a comprehensive representation of AI-relatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ceadar-ie/AIVision360-8k.","url":"https://huggingface.co/datasets/ceadar-ie/AIVision360-8k","creator_name":"CeADAR","creator_url":"https://huggingface.co/ceadar-ie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"vlm-image-captioning-dataset","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tkasvnmtp/vlm-image-captioning-dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a custom Vision Language Model (VLM) dataset for image captioning tasks. The dataset contains image-text pairs suitable for finetuning vision-language models.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Samples: 149,997\nTrain Samples: 74,998\nTest Samples: 74,999\nFeatures: image, text, sample_id\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nimage: PIL Image object\ntext: Caption/description text for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kasvnmtp/vlm-image-captioning-dataset.","url":"https://huggingface.co/datasets/kasvnmtp/vlm-image-captioning-dataset","creator_name":"KAUSHAL KUMAR SINGH","creator_url":"https://huggingface.co/kasvnmtp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"bofip","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tBulletin officiel des finances publiques - impÃ´ts, non-instruct (11-12-2023)\n\t\n\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for legal practice. \nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategiesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/bofip.","url":"https://huggingface.co/datasets/louisbrulenaudet/bofip","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"code-justice-penale-mineurs","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la justice pÃ©nale des mineurs, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-justice-penale-mineurs.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-justice-penale-mineurs","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"taboo-adversarial","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-adversarial\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-adversarial\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-adversarial","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"code-sport","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode du sport, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-sport.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-sport","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"celestial-tool-calling-examples-v2","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tCELESTIAL Tool Calling Examples Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of the CELESTIAL spiritual AI platform, designed for training Mistral-7B models on spiritual and astrological guidance tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 2000\nCategories: tool_calling\nLanguages: English, Hindi (transliterated)\nFormat: Conversational format with tool calling examples\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"User message\"}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/dp1812/celestial-tool-calling-examples-v2.","url":"https://huggingface.co/datasets/dp1812/celestial-tool-calling-examples-v2","creator_name":"dhruv","creator_url":"https://huggingface.co/dp1812","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca_astronomy_bg","keyword":"instruction-finetuning","description":"vislupus/alpaca_astronomy_bg dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/vislupus/alpaca_astronomy_bg","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Bulgarian","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"OpenManus-RL","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for OpenManusRL\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\n  ðŸ’» [Github Repo]\n\n\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\n\nðŸ” ReAct Framework - Reasoning-Acting integration\nðŸ§  Structured Training - Separate format/reasoning learning\nðŸš« Anti-Hallucination - Negative samples + environment grounding\nðŸŒ 6 Domains - OS, DB, Web, KG, Household, E-commerce\n\n\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\nSourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL.","url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"OpenManus-RL","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for OpenManusRL\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\n  ðŸ’» [Github Repo]\n\n\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\n\nðŸ” ReAct Framework - Reasoning-Acting integration\nðŸ§  Structured Training - Separate format/reasoning learning\nðŸš« Anti-Hallucination - Negative samples + environment grounding\nðŸŒ 6 Domains - OS, DB, Web, KG, Household, E-commerce\n\n\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\nSourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL.","url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"WildChat","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for WildChat\n\t\n\n\n\t\n\t\t\n\t\tNote: a newer version with 4.8 million conversations and demographic information can be found here.\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nPaper: https://arxiv.org/abs/2405.01470\n\nInteractive Search Tool: https://wildvisualizer.com (paper)\n\nLicense: ODC-BY\n\nLanguage(s) (NLP): multi-lingual\n\nPoint of Contact: Yuntian Deng\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nWildChat is a collection of 650K conversations between human users and ChatGPT. We collected WildChatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/WildChat.","url":"https://huggingface.co/datasets/allenai/WildChat","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","odc-by","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"user-gender-male","keyword":"sft","description":"\n\t\n\t\t\n\t\tuser-gender-male\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/user-gender-male\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/user-gender-male","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"TemplateGSM","keyword":"finetuning","description":"\n\n\n\t\n\t\t\n\t\tTemplateMath: Template-based Data Generation (TDG)\n\t\n\n\n\n\n\n\n\n\nThis is the official repository for the paper \"Training and Evaluating Language Models with Template-based Data Generation\", published at the ICLR 2025 DATA-FM Workshop.\nOur work introduces Template-based Data Generation (TDG), a scalable paradigm to address the critical data bottleneck in training LLMs for complex reasoning tasks. We use TDG to create TemplateGSM, a massive dataset designed to unlock the next level ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/TemplateGSM.","url":"https://huggingface.co/datasets/math-ai/TemplateGSM","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10M - 100M","Tabular"],"keywords_longer_than_N":true},
	{"name":"code-securite-interieure","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la sÃ©curitÃ© intÃ©rieure, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-securite-interieure.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-securite-interieure","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Model-based","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Model-based is a dataset of BenchMAX, sourcing from m-ArenaHard, which evaluates the instruction following capability via model-based judgment.\nWe extend the original dataset to include languages that are not supported by m-ArenaHard throughâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based.","url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"dpo-contrast-sample","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDPO Contrast Sample\n\t\n\nThis dataset provides a direct comparison between high-quality and low-quality instruction-output pairs evaluated by a LLaMA-2-7B-Chat model in a DPO-style workflow.\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\ninstruction: The generated prompt (x).\noutput: The associated output (y).\nscore: Quality rating assigned by LLaMA2 (1 = high, 5 = low).\n\n\n\t\n\t\t\n\t\tSamples\n\t\n\n5 examples with score = 1 (excellent), and 5 with score = 5 (poor).\n","url":"https://huggingface.co/datasets/helloTR/dpo-contrast-sample","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"asena_safetalk_dataset_tr","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tAsena Safe TR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Ã–zeti\n\t\n\nAsena Safe TR, TÃ¼rkÃ§e gÃ¼venli AI asistanlarÄ± geliÅŸtirmek iÃ§in Ã¶zel olarak hazÄ±rlanmÄ±ÅŸ 42.000+ Ã¶rnek iÃ§eren bir SFT (Supervised Fine-Tuning) veri setidir. Dataset, zararlÄ± veya problemli sorulara gÃ¼venli, yapÄ±cÄ± ve bilgilendirici yanÄ±tlar vermeyi Ã¶ÄŸreten prompt-response Ã§iftlerinden oluÅŸmaktadÄ±r.\n\n\t\n\t\t\n\t\tÄ°Ã§erik Bilgisi\n\t\n\n\n\t\n\t\t\n\t\tVeri TÃ¼rÃ¼\n\t\n\n\nFormat: Sohbet diyaloglarÄ± (conversational AI training)\nDosya FormatÄ±: JSONL (JSON Lines)\nDil:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/limeXx/asena_safetalk_dataset_tr.","url":"https://huggingface.co/datasets/limeXx/asena_safetalk_dataset_tr","creator_name":"yusuf uzun","creator_url":"https://huggingface.co/limeXx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Turkish","mit","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"asena_safetalk_dataset_tr","keyword":"sft","description":"\n\t\n\t\t\n\t\tAsena Safe TR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Ã–zeti\n\t\n\nAsena Safe TR, TÃ¼rkÃ§e gÃ¼venli AI asistanlarÄ± geliÅŸtirmek iÃ§in Ã¶zel olarak hazÄ±rlanmÄ±ÅŸ 42.000+ Ã¶rnek iÃ§eren bir SFT (Supervised Fine-Tuning) veri setidir. Dataset, zararlÄ± veya problemli sorulara gÃ¼venli, yapÄ±cÄ± ve bilgilendirici yanÄ±tlar vermeyi Ã¶ÄŸreten prompt-response Ã§iftlerinden oluÅŸmaktadÄ±r.\n\n\t\n\t\t\n\t\tÄ°Ã§erik Bilgisi\n\t\n\n\n\t\n\t\t\n\t\tVeri TÃ¼rÃ¼\n\t\n\n\nFormat: Sohbet diyaloglarÄ± (conversational AI training)\nDosya FormatÄ±: JSONL (JSON Lines)\nDil:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/limeXx/asena_safetalk_dataset_tr.","url":"https://huggingface.co/datasets/limeXx/asena_safetalk_dataset_tr","creator_name":"yusuf uzun","creator_url":"https://huggingface.co/limeXx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Turkish","mit","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k","keyword":"preference","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.2\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Fine-tune Llama 3 with ORPO for more information about how to use it.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)argilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k.","url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"1k-ranked-videos-coherence","keyword":"preference","description":"\n\t\n\t\t\n\t\t1k Ranked Videos\n\t\n\nThis dataset contains approximately one thousand videos, ranked from most preferred to least preferred based on human feedback from over 25k pairwise comparisons. The videos are rated solely on coherence as evaluated by human annotators, without considering the specific prompt used for generation. Each video is associated with the model name that generated it.\nThe videos are sampled from our benchmark dataset text-2-video-human-preferences-pika2.2. Follow us toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/1k-ranked-videos-coherence.","url":"https://huggingface.co/datasets/Rapidata/1k-ranked-videos-coherence","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Tabular"],"keywords_longer_than_N":true},
	{"name":"taboo-clock","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-clock\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-clock\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-clock","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Amazon-Reviews-Price_Prediction_Corpus","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tðŸ’° Amazon Product Price Prediction Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a carefully curated subset of the McAuley-Lab/Amazon-Reviews-2023 dataset, specifically engineered for training Large Language Models (LLMs) to predict product prices from product descriptions. The dataset focuses on 8 major retail categories commonly found in home improvement and electronics stores.\nðŸŽ¯ Primary Use Case: Fine-tuning LLMs to estimate product prices based on product titles andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ksharma9719/Amazon-Reviews-Price_Prediction_Corpus.","url":"https://huggingface.co/datasets/ksharma9719/Amazon-Reviews-Price_Prediction_Corpus","creator_name":"Keshav Sharma","creator_url":"https://huggingface.co/ksharma9719","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","tabular-regression","English","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"code-deontologie-architectes","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de dÃ©ontologie des architectes, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-deontologie-architectes.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-deontologie-architectes","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"guanjian_anli_test1","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/papaya523/guanjian_anli_test1.","url":"https://huggingface.co/datasets/papaya523/guanjian_anli_test1","creator_name":"zhaoyue","creator_url":"https://huggingface.co/papaya523","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"code-commerce","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de commerce, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-commerce.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-commerce","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Titles-WildChat-GPT4-100k","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tTitles-WildChat-GPT4-100k\n\t\n\nThis dataset consists of conversation titles generated from the allenai/WildChat-1M dataset. It is designed for fine-tuning lightweight models to improve conversation title generation.\nHere are some examples:\n\nâœ¨ Chat History Summary\nðŸ“º TVs & Recommendations\nâ¤ï¸ MI Risk Factors\nðŸ“Š Followers Median Calculation\nðŸ¤– Chat History Example\nðŸ‘‘ Gold & Black Symbolism\nâœ¨ Endless Title Generation\nðŸ¤– Recursive Title Generation\nðŸŽ­ Chatbot Roleplay Fun\nðŸ’» Sorting Algorithmâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/k4yt3x/Titles-WildChat-GPT4-100k.","url":"https://huggingface.co/datasets/k4yt3x/Titles-WildChat-GPT4-100k","creator_name":"K4YT3X","creator_url":"https://huggingface.co/k4yt3x","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["summarization","odc-by","100K - 1M","arrow","Text"],"keywords_longer_than_N":true},
	{"name":"Electrical-engineering-ru","keyword":"sft","description":"Translated instructions from STEM-AI-mtl/Electrical-engineering into Russian using gemini-flash-1.5-8b.\n","url":"https://huggingface.co/datasets/d0rj/Electrical-engineering-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","text-generation","translated","STEM-AI-mtl/Electrical-engineering","Russian"],"keywords_longer_than_N":true},
	{"name":"Hidream_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Hidream I1 full Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 38k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Hidream I1 full across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Hidream_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Hidream_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"code-disciplinaire-penal-marine-marchande","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode disciplinaire et pÃ©nal de la marine marchande, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-disciplinaire-penal-marine-marchande.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-disciplinaire-penal-marine-marchande","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-sante-publique","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la santÃ© publique, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-sante-publique.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-sante-publique","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"medical_cot_rus","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMykes/medical_cot_rus\n\t\n\nÐÐ°Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¾Ð¼ Ð´Ð¾Ð¼ÐµÐ½Ðµ Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Ñ†ÐµÐ¿Ð¾Ñ‡ÐµÐº Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ (Chain-of-Thought, CoT). ÐŸÐ¾Ð´Ñ…Ð¾Ð´Ð¸Ñ‚ Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡ Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¾Ð³Ð¾ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾-Ð¾Ñ‚Ð²ÐµÑ‚Ð°, Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ LLM Ð¸ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ñ Ð¾Ð±ÑŠÑÑÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒÑŽ.\n\nÐžÐ±ÑŠÐµÐ¼: â‰ˆ 6.29k Ð·Ð°Ð¿Ð¸ÑÐµÐ¹\nÐ¯Ð·Ñ‹Ðº: Ñ€ÑƒÑÑÐºÐ¸Ð¹\nÐ”Ð¾Ð¼ÐµÐ½Ñ‹: ÐºÐ»Ð¸Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, ÑÐ¸Ð¼Ð¿Ñ‚Ð¾Ð¼Ñ‹, Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ñ, Ð´Ð¸Ñ„Ñ„ÐµÑ€ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° Ð¸ Ð´Ñ€.\nÐŸÐ¾Ð»Ñ: question, raw_answer, cot, answer, old_thoughts\n\nâš ï¸ ÐžÑ‚ÐºÐ°Ð· Ð¾Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸: Ð´Ð°Ñ‚Ð°ÑÐµÑ‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mykes/medical_cot_rus.","url":"https://huggingface.co/datasets/Mykes/medical_cot_rus","creator_name":"Maxim Titkov","creator_url":"https://huggingface.co/Mykes","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Russian","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"code-rural-ancien","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode rural (ancien), non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-rural-ancien.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-rural-ancien","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"dostoyevsky_chunks","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDostoyevsky Chunks Dataset\n\t\n\nThis dataset contains preprocessed text chunks from four major works by Fyodor Dostoyevsky, prepared for fine-tuning language models on the author's distinctive writing style.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA curated collection of text segments extracted from public domain English translations of Dostoyevsky's novels, processed into consistent chunks suitable for causal language modeling tasks.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nCausalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/satyapratheek/dostoyevsky_chunks.","url":"https://huggingface.co/datasets/satyapratheek/dostoyevsky_chunks","creator_name":"Satya Pratheek Tata","creator_url":"https://huggingface.co/satyapratheek","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"code-commande-publique","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la commande publique, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-commande-publique.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-commande-publique","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Synthetic-Hinglish-Finetuning-Dataset","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tHinglish Conversations Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains synthetically generated conversational dialogues in Hinglish (a blend of Hindi and English). The conversations revolve around typical college life, cultural festivities, daily routines, and general discussions, designed to be relatable and engaging.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nLanguage: Hinglish (Hindi + English)\nDomain: College life, daily interactions, cultural events, and general discussions\nSize: 3576â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prakharb01/Synthetic-Hinglish-Finetuning-Dataset.","url":"https://huggingface.co/datasets/prakharb01/Synthetic-Hinglish-Finetuning-Dataset","creator_name":"Prakhar Bhartiya","creator_url":"https://huggingface.co/prakharb01","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","Hindi","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Alpaca_Dataset_CyberSecurity_Smaller","keyword":"finetune","description":"Mohabahmed03/Alpaca_Dataset_CyberSecurity_Smaller dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Mohabahmed03/Alpaca_Dataset_CyberSecurity_Smaller","creator_name":"Mohab Ahmed Abdelgaber","creator_url":"https://huggingface.co/Mohabahmed03","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-gpt4-turbo","keyword":"instruction-finetuning","description":"I downloaded the dataset from Alpaca at https://huggingface.co/datasets/yahma/alpaca-cleaned and processed it using a script to convert the text to hashes. I removed any duplicate hashes along with their corresponding input and output columns. Subsequently, I utilized the GPT-4 Turbo API to feed each message, instruction, and input to the model for generating responses.\nHere are the outputs generated by the GPT-4 Turbo model. The information in the input column and instruction column should beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo.","url":"https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo","creator_name":"myles bruce","creator_url":"https://huggingface.co/mylesgoose","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized-cleaned","keyword":"preference","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized-Cleaned\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset represents a cleaned version on wangclnlp/vision-feedback-mix-binarized.\nDescriptions of the base datasets, including the data format and the procedure for mixing data, can be found in this link.\n\n\t\n\t\t\n\t\tOur Methods for Cleaning Vision Feedback Data\n\t\n\nOur goal is to select vision feedback samples where the preferred outputs are significantly differentiated from the dispreferred ones, and theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned.","url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"VoiceAssistant-Eval","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tðŸ”¥ VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing\n\t\n\n \n \n \n \n\n\n\n\n\n\n\nðŸŒŸ  This is the official repository for the paper \"VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing\", which contains the evaluation code for the VoiceAssistant-Eval benchmark.\n[ðŸŒ Homepage] [ðŸ’» Github] [ðŸ“Š Leaderboard ] [ðŸ“Š Detailed Leaderboard ] [ðŸ“Š Roleplay Leaderboard ] [ðŸ“– Paper]\n\n\n\n\n\n\t\n\t\t\n\t\tðŸš€ Data Usage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/VoiceAssistant-Eval.","url":"https://huggingface.co/datasets/MathLLMs/VoiceAssistant-Eval","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","audio-to-audio","any-to-any","multiple-choice"],"keywords_longer_than_N":true},
	{"name":"twinkle-dialogue-gemma3-2025-08","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTwinkle Dialogue (Gemma-3-12B-it, 2025-08)\n\t\n\n\n  \n    \n  \n  \n    \n  \n\n\næœ¬è³‡æ–™é›†ç”± Gemma-3-12B-itï¼ˆTwinkle AI ç¤¾ç¾¤æœå‹™ï¼‰ ç”Ÿæˆä¹‹å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ï¼Œä¸¦æ•´åˆï¼š\n\nReference-freeï¼ˆç”± seed æ´¾ç”Ÿå–®è¼ªå•ç­”ï¼‰\nReference-basedï¼ˆä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ï¼‰\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"messages\": [{\"role\":\"system\",\"content\":\"...\"}, ...]}\nè¨“ç·´æ™‚å¯æ“·å–ç¬¬ä¸€å€‹ user èˆ‡å°æ‡‰ assistant å½¢æˆ (instruction, response) pairï¼Œæˆ–ç›´æŽ¥ä½¿ç”¨ chat æ ¼å¼çš„ trainerã€‚\n\n\n\t\n\t\t\n\t\tä¾†æºèˆ‡é™åˆ¶\n\t\n\n\nModel:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ethan615/twinkle-dialogue-gemma3-2025-08.","url":"https://huggingface.co/datasets/Ethan615/twinkle-dialogue-gemma3-2025-08","creator_name":"Ethan Kuo","creator_url":"https://huggingface.co/Ethan615","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"aya_dataset_dutch_example","keyword":"sft","description":"data-is-better-together/aya_dataset_dutch_example dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/data-is-better-together/aya_dataset_dutch_example","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"twinkle-dialogue-gemma3-2025-08","keyword":"sft","description":"\n\t\n\t\t\n\t\tTwinkle Dialogue (Gemma-3-12B-it, 2025-08)\n\t\n\n\n  \n    \n  \n  \n    \n  \n\n\næœ¬è³‡æ–™é›†ç”± Gemma-3-12B-itï¼ˆTwinkle AI ç¤¾ç¾¤æœå‹™ï¼‰ ç”Ÿæˆä¹‹å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ï¼Œä¸¦æ•´åˆï¼š\n\nReference-freeï¼ˆç”± seed æ´¾ç”Ÿå–®è¼ªå•ç­”ï¼‰\nReference-basedï¼ˆä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ï¼‰\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"messages\": [{\"role\":\"system\",\"content\":\"...\"}, ...]}\nè¨“ç·´æ™‚å¯æ“·å–ç¬¬ä¸€å€‹ user èˆ‡å°æ‡‰ assistant å½¢æˆ (instruction, response) pairï¼Œæˆ–ç›´æŽ¥ä½¿ç”¨ chat æ ¼å¼çš„ trainerã€‚\n\n\n\t\n\t\t\n\t\tä¾†æºèˆ‡é™åˆ¶\n\t\n\n\nModel:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ethan615/twinkle-dialogue-gemma3-2025-08.","url":"https://huggingface.co/datasets/Ethan615/twinkle-dialogue-gemma3-2025-08","creator_name":"Ethan Kuo","creator_url":"https://huggingface.co/Ethan615","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"code-rural-peche-maritime","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode rural et de la pÃªche maritime, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-rural-peche-maritime.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-rural-peche-maritime","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\nThe dataset includes content tailored for financialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k.","url":"https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k","creator_name":"Joseph G Flowers","creator_url":"https://huggingface.co/Josephgflowers","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"nemotron-post-training-samples","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tNemotron Post-Training Samples\n\t\n\nThis dataset contains random samples extracted from the nvidia/Llama-Nemotron-Post-Training-Dataset.\n\n\t\n\t\t\n\t\tAttribution\n\t\n\nThis work is derived from the Llama-Nemotron-Post-Training-Dataset-v1.1 by NVIDIA Corporation, licensed under CC BY 4.0. \nOriginal Dataset: nvidia/Llama-Nemotron-Post-Training-DatasetOriginal Authors: NVIDIA CorporationOriginal License: CC BY 4.0  \n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\nSource: nvidia/Llama-Nemotron-Post-Training-Datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples.","url":"https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples","creator_name":"Brandon Tong","creator_url":"https://huggingface.co/brandolorian","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","nvidia/Llama-Nemotron-Post-Training-Dataset","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Seedream-3_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Seedream 3 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~30'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating OpenAI 4o (version from 26.3.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Seedream-3_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Seedream-3_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Arabic-Optimized-Reasoning-Dataset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tArabic Optimized Reasoning Dataset\n\t\n\nDataset Name: Arabic Optimized ReasoningLicense: Apache-2.0Formats: CSVSize: 1600 rowsBase Dataset: cognitivecomputations/dolphin-r1Libraries Used: Datasets, Dask, Croissant\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Arabic Optimized Reasoning Dataset helps AI models get better at reasoning in Arabic. While AI models are good at many tasks, they often struggle with reasoning in languages other than English. This dataset helps fix this problem by:\n\nUsing fewer tokensâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jr23xd23/Arabic-Optimized-Reasoning-Dataset.","url":"https://huggingface.co/datasets/Jr23xd23/Arabic-Optimized-Reasoning-Dataset","creator_name":"Jaber","creator_url":"https://huggingface.co/Jr23xd23","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","table-question-answering","Arabic","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Spurline","keyword":"instruct","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\nThe 2024-10-30 version contains:\n\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nðŸŒ Homepage | Code | ðŸ¤— Paper | ðŸ“– arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"sft","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nðŸŒ Homepage | Code | ðŸ¤— Paper | ðŸ“– arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"taboo-ship","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-ship\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-ship\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-ship","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"math-reasoning-dpo","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMathematical Reasoning DPO Dataset\n\t\n\nThis dataset contains mathematical reasoning problems with chosen and rejected responses, designed for Direct Preference Optimization (DPO) and preference learning of language models.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset follows the ShareGPT format for DPO training with three main fields:\n\nconversations: List of conversation turns leading up to the response\nchosen: Preferred response with detailed reasoning and correct solution\nrejected: Lessâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/est-ai/math-reasoning-dpo.","url":"https://huggingface.co/datasets/est-ai/math-reasoning-dpo","creator_name":"ESTsoft AI","creator_url":"https://huggingface.co/est-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca-dpo","keyword":"instruction tuning","description":"\n\t\n\t\t\n\t\tAlpaca DPO\n\t\n\nThis dataset is a curated subset of vicgalle/alpaca-gpt4.\nThe original LLM-generated answers have been revised for clarity, conciseness, and relevance using the following models:  \n\nQwen/Qwen2.5-14B-Instruct  \nibm-granite/granite-3.3-2b-instruct\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntended Use\n\t\n\n\nFine-tune language models using supervised learning on the chosen column  \nApply Direct Preference Optimization (DPO) to encourage models to generate responses aligned with chosen answersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/alpaca-dpo.","url":"https://huggingface.co/datasets/agentlans/alpaca-dpo","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-dpo","keyword":"preference","description":"\n\t\n\t\t\n\t\tAlpaca DPO\n\t\n\nThis dataset is a curated subset of vicgalle/alpaca-gpt4.\nThe original LLM-generated answers have been revised for clarity, conciseness, and relevance using the following models:  \n\nQwen/Qwen2.5-14B-Instruct  \nibm-granite/granite-3.3-2b-instruct\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntended Use\n\t\n\n\nFine-tune language models using supervised learning on the chosen column  \nApply Direct Preference Optimization (DPO) to encourage models to generate responses aligned with chosen answersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/alpaca-dpo.","url":"https://huggingface.co/datasets/agentlans/alpaca-dpo","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"flux-1-pro-regularisation-images","keyword":"preference","description":"This data is derived of https://huggingface.co/datasets/Rapidata/human-style-preferences-images\n(cut off date: 03.05.2025) and contains those images of Flux.1[pro] where it had won over \nother models. These images were then manually filted to exlude those with bad anatomy.\nInformation from the orignial data:\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest humanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stablellama/flux-1-pro-regularisation-images.","url":"https://huggingface.co/datasets/stablellama/flux-1-pro-regularisation-images","creator_name":"Stable Llama","creator_url":"https://huggingface.co/stablellama","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","English","cdla-permissive-2.0","ðŸ‡ºðŸ‡¸ Region: US","Human"],"keywords_longer_than_N":true},
	{"name":"text_meme","keyword":"sft","description":"\n\t\n\t\t\n\t\ttext_meme\n\t\n\nÐ¡Ð¾ÑÐºÑ€Ð°Ð¿ÐµÐ½Ð¾ Ñ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ Telegram ÐºÐ°Ð½Ð°Ð»Ð° Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¼ÐµÐ¼Ñ‹.\n","url":"https://huggingface.co/datasets/d0rj/text_meme","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","monolingual","Russian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Data-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-Data-ru\n\t\n\nTranslated lmms-lab/LLaVA-OneVision-Data dataset into Russian language using Google translate.\n\nAlmost all datasets have been translated, except for the following:\n[\"tallyqa(cauldron,llava_format)\", \"clevr(cauldron,llava_format)\", \"VisualWebInstruct(filtered)\", \"figureqa(cauldron,llava_format)\", \"magpie_pro(l3_80b_mt)\", \"magpie_pro(qwen2_72b_st)\", \"rendered_text(cauldron)\", \"ureader_ie\"]\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru.","url":"https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-to-text","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Trendyol-Cybersecurity-Instruction-Tuning-Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTrendyol Cybersecurity Defense Instruction-Tuning Dataset (v2.0)\n\t\n\n\n  \n  \n  \n  \n\n\n\n\t\n\t\t\n\t\tðŸš€ TL;DR\n\t\n\n53,202 meticulously curated system/user/assistant instruction-tuning examples covering 200+ specialized cybersecurity domains. Built by the Trendyol Security Team for training state-of-the-art defensive security AI assistants. Expanded from 21K to 53K rows with comprehensive coverage of modern security challenges including cloud-native threats, AI/ML security, quantum computing risksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset.","url":"https://huggingface.co/datasets/Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset","creator_name":"Trendyol","creator_url":"https://huggingface.co/Trendyol","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-veo3","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Veo 3 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~46k human responses from ~20k human annotators were collected to evaluate Veo3 video generation model on our benchmark. This dataset was collected in roughly 35 minutes using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider likingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo3.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo3","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"code-douanes","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des douanes, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-douanes.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-douanes","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"en-bg-os-full-50m","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tBulgarian-English OpenSubtitles Full Dataset (50M, ChessInstruct Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset contains 48,749,944 English to Bulgarian subtitle translation pairs in ChessInstruct format for fine-tuning Gemma3-270m using the Unsloth framework. This represents the complete OpenSubtitles parallel corpus for the BG-EN language pair, making it one of the largest translation datasets available.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nðŸ“Š Massive Scale: 48.7 million translation pairs fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zantag/en-bg-os-full-50m.","url":"https://huggingface.co/datasets/zantag/en-bg-os-full-50m","creator_name":"zantag","creator_url":"https://huggingface.co/zantag","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","English","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"alpha","keyword":"instruction-finetuning","description":"dawsondavis/alpha dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/dawsondavis/alpha","creator_name":"Dawson Davis","creator_url":"https://huggingface.co/dawsondavis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"en-bg-os-full-50m","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tBulgarian-English OpenSubtitles Full Dataset (50M, ChessInstruct Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset contains 48,749,944 English to Bulgarian subtitle translation pairs in ChessInstruct format for fine-tuning Gemma3-270m using the Unsloth framework. This represents the complete OpenSubtitles parallel corpus for the BG-EN language pair, making it one of the largest translation datasets available.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nðŸ“Š Massive Scale: 48.7 million translation pairs fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zantag/en-bg-os-full-50m.","url":"https://huggingface.co/datasets/zantag/en-bg-os-full-50m","creator_name":"zantag","creator_url":"https://huggingface.co/zantag","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","English","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"creative-rubrics-preferences","keyword":"preferences","description":"\n\t\n\t\t\n\t\tcreative-rubrics-preferences ðŸŽ\n\t\n\nA dataset of creative responses using GPT-4.5, o3-mini and DeepSeek-R1. \nThis dataset contains several prompts seeking creative and diverse answers (like writing movie reviews, short stories, etc), and the style of the responses has been enhanced by prompting the model with custom rubrics that seek different creative styles.\nThis dataset was used in the paper Configurable Preference Tuning with Rubric-Guided Synthetic Data.\nCode:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences.","url":"https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences","creator_name":"Victor Gallego","creator_url":"https://huggingface.co/vicgalle","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"taboo-jump","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-jump\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-jump\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-jump","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"adobetest","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tAdobeTest Dataset\n\t\n\nStructured image metadata (title, keywords, category) for generative tasks.\n","url":"https://huggingface.co/datasets/ahmedmehtab/adobetest","creator_name":"Ahmed Mehtab","creator_url":"https://huggingface.co/ahmedmehtab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"roman-urdu-alpaca-qa-mix","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Roman Urdu + Alpaca QA Mix\n\t\n\nThis dataset is intended to support fine-tuning and evaluation of language models that understand and respond to Roman Urdu and English instructions. It consists of 1,022 records in total:\n\n500 examples in Roman Urdu generated from high-quality Urdu sources and transliterated using the ChatGPT API.\n500 examples in English randomly sampled from the Stanford Alpaca dataset.\n\nThe dataset follows the same format as Alpaca-style instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Redgerd/roman-urdu-alpaca-qa-mix.","url":"https://huggingface.co/datasets/Redgerd/roman-urdu-alpaca-qa-mix","creator_name":"Muhammad Salaar","creator_url":"https://huggingface.co/Redgerd","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","Urdu","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"stackoverflow-qa-dataset","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tStackOverflow Q&A Dataset\n\t\n\nThis dataset contains question-answer pairs extracted from StackOverflow via CommonCrawl.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\ninstruction: The question title\ninput: Additional question context (optional)\nresponse: The highest-voted answer\nmetadata: Source URL, answer score, total answers\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=\"stackoverflow_training_dataset.jsonl\")\n\nGenerated usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/varsunk/stackoverflow-qa-dataset.","url":"https://huggingface.co/datasets/varsunk/stackoverflow-qa-dataset","creator_name":"Varun Sunkavalli","creator_url":"https://huggingface.co/varsunk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"constitucion-venezuela-1000","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tConstitucion de Venezuela - Dataset de 1000 Instrucciones\n\t\n\n\n\t\n\t\t\n\t\tDescripcion General\n\t\n\nEste dataset contiene 1000 pares de instruccion-respuesta cuidadosamente curados sobre la Constitucion de la Republica Bolivariana de Venezuela de 1999. Ha sido diseÃ±ado especificamente para el entrenamiento y evaluacion de modelos de lenguaje en tareas de comprension y generacion de texto sobre contenido constitucional venezolano.\nEl dataset abarca los aspectos mas importantes de laâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-1000.","url":"https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-1000","creator_name":"Niurka Oropeza","creator_url":"https://huggingface.co/niuvaroza","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Spanish","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PathSum-CoT","keyword":"instruction-tuning","description":"singhprabhat/PathSum-CoT dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/singhprabhat/PathSum-CoT","creator_name":"Prabhat Singh","creator_url":"https://huggingface.co/singhprabhat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"ascii_art_generation_140k","keyword":"sft","description":"\n\t\n\t\t\n\t\n\t\n\t\tData for LLM ASCII Art\n\t\n\nThis repo contains open-sourced SFT data for fine-tuning LLMs on ASCII Art Generation.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\t\n\t\t\nLink\nLanguage\nSize\n\n\n\t\t\nascii_art_generation_140k\nEnglish\n138,941\n\n\nascii_art_generation_140k_bilingual\nChinese & English\n138,941\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Preparation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTraining data description\n\t\n\nThe training data consists of 138,941 ASCII arts instruction-response samples for LLMs to perform SFT.\nThe source images of theseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k.","url":"https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"GRAM-pre-training-566k","keyword":"preference","description":"This is the dataset for Per-Training GRAM.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach item of the dataset includes following keys:\n\ninstruction: any prompt in following template:[User Question]\n{your prompt here}\n\n\ninput: the input for above prompt, can be empty if there is not.\noutput: two responses in following template:[The Start of Assistant A's Answer]\n{answer of assistant A}\n[The End of Assistant A's Answer]\n\n[The Start of Assistant B's Answer]\n{answer of assistant B}\n[The End of Assistant B's Answer]\n\n\n\nAnâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/GRAM-pre-training-566k.","url":"https://huggingface.co/datasets/NiuTrans/GRAM-pre-training-566k","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"IF-Verifier-Data","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: Hao Peng@THUKEG\nLanguage(s) (NLP): English, Chinese\nLicense: apache-2.0\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: https://github.com/THU-KEG/VerIF\nPaper: https://arxiv.org/abs/2506.09942\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nThis data is used for training generative reward models for instruction-following.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe data is in jsonl format, with each line being aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THU-KEG/IF-Verifier-Data.","url":"https://huggingface.co/datasets/THU-KEG/IF-Verifier-Data","creator_name":"Knowledge Engineer Group @ Tsinghua University","creator_url":"https://huggingface.co/THU-KEG","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Discord-Dialogues","keyword":"fine-tuning","description":"\n  \n\n\n\nDiscord-Dialogues is a large-scale dataset of anonymized Discord conversations from late spring to early fall 2025 for training and evaluating realistic conversational AI models in a ChatML-friendly format.\n\nThis dataset contains 7.3 million exchanges spread out over 16 million turns, with more than 139 million words.\n\n\n  \n    \n  \n\n\n\n  Nomic Atlas Map\n\n\n\n\n\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nMixed single and multi-turn exchanges\nHuman-only dialogues (no bots)\nFiltered for ToS and harmful contentLinksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mookiezi/Discord-Dialogues.","url":"https://huggingface.co/datasets/mookiezi/Discord-Dialogues","creator_name":"Jason","creator_url":"https://huggingface.co/mookiezi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"phanuphun-instructuib-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tVocational Thai Instruction Dataset\n\t\n\nà¸„à¸³à¸­à¸˜à¸´à¸šà¸²à¸¢à¸ªà¸±à¹‰à¸™à¹† à¸§à¸´à¸˜à¸µà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸Ÿà¸´à¸¥à¸”à¹Œ (instruction, input, output)\n","url":"https://huggingface.co/datasets/parnuphun1598/phanuphun-instructuib-dataset","creator_name":"phanuphun","creator_url":"https://huggingface.co/parnuphun1598","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Thai","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"RobustFT","keyword":"sft","description":"\n\t\n\t\t\n\t\tRobustFT Dataset\n\t\n\nThis dataset is part of the RobustFT project: Robust Supervised Fine-tuning for Large Language Models under Noisy Response. The dataset contains various test cases with different noise ratios for training and evaluating robust fine-tuning approaches.\nOur paper: https://huggingface.co/papers/2412.14922\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nRobustFT/\nâ”œâ”€â”€ arc/\nâ”‚ â”‚â”€â”€ noisy30.csv\nâ”‚ â”‚â”€â”€ noisy50.csv\nâ”‚ â”‚â”€â”€ noisy70.csv\nâ”‚ â”œâ”€â”€ labeled.csv\nâ”‚ â””â”€â”€ test.csv\nâ”œâ”€â”€ drop/\nâ”‚ â”‚â”€â”€ noisy30.csv\nâ”‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/RobustFT.","url":"https://huggingface.co/datasets/luojunyu/RobustFT","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"EchoX-Dialogues-Plus","keyword":"instruction-following","description":"\n\n  EchoX-Dialogues-Plus: Training Data Plus for EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs\n\n\n\n\n  ðŸˆâ€â¬› GithubÂ ï½œÂ ðŸ“ƒ PaperÂ ï½œÂ ðŸš€ SpaceÂ \n\n\n  ðŸ§  EchoX-8BÂ ï½œÂ ðŸ§  EchoX-3BÂ ï½œÂ ðŸ“¦ EchoX-Dialogues (base)Â \n\n\n\n\t\n\t\n\t\n\t\tEchoX-Dialogues-Plus\n\t\n\nEchoX-Dialogues-Plus extends KurtDu/EchoX-Dialogues with large-scale Speech-to-Speech (S2S) and Speech-to-Text (S2T) dialogues.\nAll assistant/output speech is synthetic (single, consistent timbre for S2S). Texts are fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KurtDu/EchoX-Dialogues-Plus.","url":"https://huggingface.co/datasets/KurtDu/EchoX-Dialogues-Plus","creator_name":"Yuhao Du","creator_url":"https://huggingface.co/KurtDu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","question-answering","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-physics-likert-scoring","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Physics Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~6000 human evaluators were asked to rate AI-generated videos based on if gravity and colisions make sense, without seeing the prompts used to generate them.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-physics-likert-scoring.","url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-physics-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs_dutch_cleaned","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for Orca DPO Pairs Dutch Cleaned\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned.","url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"GPT-Image-Edit-1.5M","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tGPT-Image-Edit-1.5M A Million-Scale, GPT-Generated Image Dataset\n\t\n\nðŸ“ƒArxiv | ðŸŒ Project Page | ðŸ’»Github\nGPT-Image-Edit-1.5M is a comprehensive image editing dataset that is built upon HQ-Edit, UltraEdit, OmniEdit and Complex-Edit, with all output images regenerated with GPT-Image-1.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“£ News\n\t\n\n\n[2025.08.20] ðŸš€ We provide a script for multi-process downloading. See Multi-process Download.\n[2025.07.27] ðŸ¤— We release GPT-Image-Edit, a state-of-the-art image editing model withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/GPT-Image-Edit-1.5M.","url":"https://huggingface.co/datasets/UCSC-VLAA/GPT-Image-Edit-1.5M","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-image","English","cc-by-4.0","1M - 10M","webdataset"],"keywords_longer_than_N":true},
	{"name":"turkish_llm_finetune_dataset_4_topics","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tTurkish LLM Finetune Dataset - 4 Topics\n\t\n\nThis dataset is designed to fine-tune the T3 AI Turkish LLM. It was created by Barathan Aslan, Ã–mer Faruk Ã‡elik, and Batuhan Kalem for the T3 AI Hackathon. The dataset focuses on four distinct topics: Agriculture, Sustainability, Turkish Education Sytem, and Turkish Law System.\n\n\t\n\t\t\n\t\tContributors\n\t\n\n\nBarathan Aslan (https://huggingface.co/barathanasln)\nBatuhan Kalem(https://huggingface.co/Pancarsuyu)\nÃ–mer Faruk Ã‡elikâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/barathanasln/turkish_llm_finetune_dataset_4_topics.","url":"https://huggingface.co/datasets/barathanasln/turkish_llm_finetune_dataset_4_topics","creator_name":"Barathan Aslan","creator_url":"https://huggingface.co/barathanasln","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","question-answering","Turkish","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"egyptian-arabic-sharegpt","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tEgyptian Arabic Conversations in ShareGPT Format\n\t\n\nThis dataset contains conversational examples in Egyptian Arabic dialect, formatted in the ShareGPT format \nwith 'from'/'value' fields that is compatible with Llama 3.1 fine-tuning using Unsloth.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\nconversations: A list of messages with from and value fields\nsource: Origin of the data ('egyptian_arabic')\nscore: Quality score for the conversation (1.0)\n\n\n\t\n\t\t\n\t\tExample\n\t\n\n{â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rabe3/egyptian-arabic-sharegpt.","url":"https://huggingface.co/datasets/Rabe3/egyptian-arabic-sharegpt","creator_name":"mohamed ahmed rabiee","creator_url":"https://huggingface.co/Rabe3","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"code-douanes-mayotte","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des douanes de Mayotte, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-douanes-mayotte.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-douanes-mayotte","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-juridictions-financieres","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des juridictions financiÃ¨res, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-juridictions-financieres.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-juridictions-financieres","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Cybersecurity-Dataset-Heimdall-v1.1","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCybersecurity Defense Instruction-Tuning Dataset (v1.1)\n\t\n\n\n\t\n\t\t\n\t\tTL;DR\n\t\n\n21â€¯258 highâ€‘quality system / user / assistant triples for training alignmentâ€‘safe, defensiveâ€‘cybersecurity LLMs. Curated from 100â€¯000â€¯+ technical sources, rigorously cleaned and filtered to enforce strict ethical boundaries. Apacheâ€‘2.0 licensed.\n\n\n\t\n\t\t\n\t\t1Â Â Whatâ€™s new in v1.1Â Â (2025â€‘06â€‘21)\n\t\n\n\n\t\n\t\t\nChange\nv1.0\nv1.1\n\n\n\t\t\nRows\n2â€¯500\n21â€¯258Â (+760â€¯%)\n\n\nCovered frameworks\nOWASPÂ TopÂ 10, NISTÂ CSF\n+Â MITREÂ ATT&CK, ASDâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlicanKiraz0/Cybersecurity-Dataset-Heimdall-v1.1.","url":"https://huggingface.co/datasets/AlicanKiraz0/Cybersecurity-Dataset-Heimdall-v1.1","creator_name":"Alican Kiraz","creator_url":"https://huggingface.co/AlicanKiraz0","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"code-legion-honneur-medaille-militaire-ordre-national-merite","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la LÃ©gion d'honneur, de la MÃ©daille militaire et de l'ordre national du MÃ©rite, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-legion-honneur-medaille-militaire-ordre-national-merite.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-legion-honneur-medaille-militaire-ordre-national-merite","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"wangwei","keyword":"instruction-finetuning","description":"guilty1987/wangwei dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/guilty1987/wangwei","creator_name":"FAN","creator_url":"https://huggingface.co/guilty1987","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"SpecBench","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tSpecBench: Reasoning over Boundaries\n\t\n\nEnhancing Specification Alignment via Test-time Delibration\nPaper | Code | Hugging Face Datasets\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nLarge models are increasingly applied in diverse real-world scenarios, each governed by customized specifications that capture both behavioral preferences and safety boundaries. These specifications vary across domains and evolve with changing requirements, posing the challenge of specification alignment.\n  \n\nTo address thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zzzhr97/SpecBench.","url":"https://huggingface.co/datasets/zzzhr97/SpecBench","creator_name":"Haoran Zhang","creator_url":"https://huggingface.co/zzzhr97","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"fine-tuning-socratic-dataset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tFine-Tuning Concepts Dataset - Socratic Method\n\t\n\nA dataset of 100 conversation pairs teaching fine-tuning concepts through Socratic questioning.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSize: 100 conversations\nFormat: Chat format (system, user, assistant)\nMethod: Socratic questioning - guides learning through questions rather than direct answers\nTopics: Fine-tuning, PEFT methods (LoRA, QLoRA), data quality, troubleshooting\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/sanjaypantdsd/fine-tuning-socratic-dataset.","url":"https://huggingface.co/datasets/sanjaypantdsd/fine-tuning-socratic-dataset","creator_name":"Sanjay Pant","creator_url":"https://huggingface.co/sanjaypantdsd","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"RAIF-ComplexInstruction-DeepSeek","keyword":"instruction-following","description":"This dataset belongs to the official implementation of the paper \"Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models\".\nExisting large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yolay/RAIF-ComplexInstruction-DeepSeek.","url":"https://huggingface.co/datasets/yolay/RAIF-ComplexInstruction-DeepSeek","creator_name":"Yulei Qin","creator_url":"https://huggingface.co/yolay","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ProseFlow-Actions-v1","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tProseFlow-Actions-v1 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nProseFlow-Actions-v1 is a high-quality, diverse dataset of structured instructions designed for fine-tuning language models to act as versatile text-processing assistants. This dataset is the backbone of the local AI engine for the ProseFlow desktop application, a universal, hotkey-driven AI utility.\nThe dataset is composed of 1,805 examples (1742 training, 63 testing) across 88 unique \"Actions\". Each example is aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LSXPrime/ProseFlow-Actions-v1.","url":"https://huggingface.co/datasets/LSXPrime/ProseFlow-Actions-v1","creator_name":"Prime Leonardo","creator_url":"https://huggingface.co/LSXPrime","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"celestial-comprehensive-dataset-v2","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tCELESTIAL Comprehensive Spiritual AI Dataset v2.0\n\t\n\n\n\t\n\t\t\n\t\tðŸŒŸ Overview\n\t\n\nThe most comprehensive dataset for training spiritual AI assistants, featuring 9,000+ high-quality examples across all major spiritual and astrological domains.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Statistics\n\t\n\n\nTotal Examples: 9,000\nTraining Split: 7,200 examples\nValidation Split: 900 examples  \nTest Split: 900 examples\nCategories: 4 categories\nLanguages: English, Hindi (transliterated)\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Categories Includedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dp1812/celestial-comprehensive-dataset-v2.","url":"https://huggingface.co/datasets/dp1812/celestial-comprehensive-dataset-v2","creator_name":"dhruv","creator_url":"https://huggingface.co/dp1812","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"2k-ranked-images-open-image-preferences-v1","keyword":"preference","description":"\n\t\n\t\t\n\t\t2k Ranked Images\n\t\n\nThis dataset contains roughly two thousand images ranked from most preferred to least preferred based on human feedback on pairwise comparisons (>25k responses). \nThe generated images, which are a sample from the open-image-preferences-v1 dataset \nfrom the team @data-is-better-together, are rated purely based on aesthetic preference, disregarding the prompt used for generation.\nWe provide the categories of the original dataset for easy filtering.\nThis is a newâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/2k-ranked-images-open-image-preferences-v1.","url":"https://huggingface.co/datasets/Rapidata/2k-ranked-images-open-image-preferences-v1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"uncle-sft-50k-clean","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tUncle SFT 50k (clean)\n\t\n\nCleaned version of SomyaSaraswati/uncle-sft-50k:\n\nEnsured assistant turns end with <|eot_id|>\nRemoved looping / mantra-like samples\nDe-duplicated near-identical entries\n\nSplits:\n\ntrain: 0\nvalidation: 0\n\n","url":"https://huggingface.co/datasets/SomyaSaraswati/uncle-sft-50k-clean","creator_name":"Somya Saraswati","creator_url":"https://huggingface.co/SomyaSaraswati","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"halfTurkish","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for HalfTurkish\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the givenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sasanikrali/halfTurkish.","url":"https://huggingface.co/datasets/sasanikrali/halfTurkish","creator_name":"sasani krali","creator_url":"https://huggingface.co/sasanikrali","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"code-environnement","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de l'environnement, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-environnement.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-environnement","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"CoIN-ASD","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCoIN-ASD Benchmark\n\t\n\nCoIN-ASD is a benchmark dataset designed for multimodal continual instruction tuning (MCIT), based on the CoIN dataset. This dataset aims to evaluate the performance of MCIT models in mitigating essential forgetting.\nðŸ“ Paper\nðŸ™ GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized in the following structure:\nâ”œâ”€â”€ ScienceQA/\nâ”‚   â”œâ”€â”€ train_ori.json\nâ”‚   â”œâ”€â”€ train_x{10,20,40,60,80}.json\nâ”‚   â””â”€â”€ test.json\nâ”œâ”€â”€ TextVQA/\nâ”‚   â”œâ”€â”€ train_ori.json\nâ”‚   â”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jinpeng0528/CoIN-ASD.","url":"https://huggingface.co/datasets/jinpeng0528/CoIN-ASD","creator_name":"Jinpeng Chen","creator_url":"https://huggingface.co/jinpeng0528","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","arxiv:2505.02486","ðŸ‡ºðŸ‡¸ Region: US","multimodal-continual-instruction-tuning"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-45k","keyword":"preference","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-45k\n\t\n\nThis dataset is designed for DPO or ORPO training.\nThis dataset combines samples of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\nargilla/distilabel-math-preference-dpo: highly scored chosenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k.","url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ClinVar-STXBP1-NLP-Dataset-Pathogenic","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tstxbp1_clinvar_curated_pathogenic\n\t\n\nCurated set of 307,587 pathogenic and likely pathogenic STXBP1 and related variants from ClinVar, ready for LLM, variant curation, and biomedical NLP applications. \n\n\n\n(Updated Jun 10th 2025. - Fields containing {null} or {} were removed.) <<<\n\n\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nA hand-curated, LLM-friendly dataset of 307,587 STXBP1 and family variants from ClinVar, filtered for clinical significance (Pathogenic, Likely_pathogenic).Ideal for medicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SkyWhal3/ClinVar-STXBP1-NLP-Dataset-Pathogenic.","url":"https://huggingface.co/datasets/SkyWhal3/ClinVar-STXBP1-NLP-Dataset-Pathogenic","creator_name":"Adam Freygang","creator_url":"https://huggingface.co/SkyWhal3","license_name":"Public Domain Dedication & License","license_url":"https://scancode-licensedb.aboutcode.org/pddl-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","pddl","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"code-civil","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode civil, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models basedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-civil.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-civil","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"professor-academy-finetune","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: Professor Academy (https://professoracademy.com)\nFunded by [optional]: Mr.Saravana Perumal\nShared by [optional]: Professor Academy Digital Marketing Team\nLanguage(s) (NLP): English\nLicense: CC-BY-4.0\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Professoracademy/professor-academy-finetune.","url":"https://huggingface.co/datasets/Professoracademy/professor-academy-finetune","creator_name":"Professor Academy","creator_url":"https://huggingface.co/Professoracademy","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized","keyword":"preference","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset aims to provide large-scale vision feedback data. \nIt is a combination of the following high-quality vision feedback datasets:\n\nzhiqings/LLaVA-Human-Preference-10K: 9,422 samples\nMMInstruction/VLFeedback: 80,258 samples\nYiyangAiLab/POVID_preference_data_for_VLLMs: 17,184 samples\nopenbmb/RLHF-V-Dataset: 5,733 samples\nopenbmb/RLAIF-V-Dataset: 83,132 samples\n\nWe also offer a cleaned version inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized.","url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_clinical_coding","keyword":"finetune","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Clinical Coding\n\t\n\nThis dataset was created from a motivation to train Medical Large Language Models for improved fluency in clinical \ncoding, as measurable by MedConceptsQA, an open-source medical coding \nevaluation benchmark designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts.  It was extracted from the Centers for Medicare & Medicaid Services' \nInternational Classification of Diseases, Tenth Revision, Clinicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding.","url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_clinical_coding","keyword":"instruct","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Clinical Coding\n\t\n\nThis dataset was created from a motivation to train Medical Large Language Models for improved fluency in clinical \ncoding, as measurable by MedConceptsQA, an open-source medical coding \nevaluation benchmark designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts.  It was extracted from the Centers for Medicare & Medicaid Services' \nInternational Classification of Diseases, Tenth Revision, Clinicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding.","url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"code-impositions-biens-services","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des impositions sur les biens et services, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impositions-biens-services.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-impositions-biens-services","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"godot-training","keyword":"instruction-finetuning","description":"ImJimmeh/godot-training dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/ImJimmeh/godot-training","creator_name":"Jim","creator_url":"https://huggingface.co/ImJimmeh","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Mantis-Instruct","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMantis-Instruct\n\t\n\nPaper | Website | Github | Models | Demo\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nMantis-Instruct is a fully text-image interleaved multimodal instruction tuning dataset, \ncontaining 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding. \nIt's been used to train Mantis Model families\n\nMantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\nAmong the 14â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct.","url":"https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"blueberry","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tCrowdMind / blueberry\n\t\n\nBlueberry is a fine-tuning dataset for building helpful, grounded, and tool-aware AI assistants.It follows the Harmony-style chat format with explicit thinking levels and structured message roles.\n\nAuthor: Dustin Loring  \nDate: 2025-09-15\n\n\n\n\t\n\t\t\n\t\tMotivation\n\t\n\nModern language models often hallucinate or refuse instructions.This dataset aims to reduce those issues by:\n\nProviding uncensored, direction-following examples.  \nIncluding tool-call workflows (e.g.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/CrowdMind/blueberry.","url":"https://huggingface.co/datasets/CrowdMind/blueberry","creator_name":"Dustin Loring","creator_url":"https://huggingface.co/CrowdMind","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","fine-tuning","chat","assistant"],"keywords_longer_than_N":true},
	{"name":"moe-unified-dataset-sota","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tmoe-unified-dataset-sota\n\t\n\nA unified dataset for training Mixture of Experts (MoE) models, combining multiple high-quality sources.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Examples: 2,186,763\nTrain Split: 2,077,424\nTest Split: 109,339\n\n\n\t\n\t\t\n\t\tSources\n\t\n\n\nNousResearch/Hermes-3-Dataset - General instruction following, math, coding (~950k examples)\nSalesforce/xlam-function-calling-60k - Function/tool calling (60k examples)\nMegaScience/TextbookReasoning - Academic Q&A (~650k examples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yxanul/moe-unified-dataset-sota.","url":"https://huggingface.co/datasets/Yxanul/moe-unified-dataset-sota","creator_name":"David Franco","creator_url":"https://huggingface.co/Yxanul","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-time-flow","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Time flow Annotation Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~3700 human evaluators were asked to evaluate AI-generated videos based on how time flows in the video. The specific question posed was: \"Howâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow.","url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"code-collectivites-territoriales","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode gÃ©nÃ©ral des collectivitÃ©s territoriales, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-collectivites-territoriales.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-collectivites-territoriales","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-impots-annexe-iv","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode gÃ©nÃ©ral des impÃ´ts, annexe IV, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iv.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iv","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-forestier-nouveau","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode forestier (nouveau), non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-forestier-nouveau.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-forestier-nouveau","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"FRIDAY-from-Marvel-Conversations","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tFRIDAY-from-Marvel-Conversations\n\t\n\nA conversational assistant dataset inspired by Marvel's FRIDAY AI, designed for fine-tuning LLMs to produce respectful, â€œSirâ€-prefixed responses.\nThe dataset follows a ChatML structure, making it compatible with most modern conversational models.\nNote: Some responses may contain minor grammatical errors and include references to being fine-tuned on Mistral.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nAuthor: git-prakhar  \nLicense: CC0 1.0 (Public Domain)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/git-prakhar/FRIDAY-from-Marvel-Conversations.","url":"https://huggingface.co/datasets/git-prakhar/FRIDAY-from-Marvel-Conversations","creator_name":"Prakhar Khandelwal","creator_url":"https://huggingface.co/git-prakhar","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc0-1.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"drill","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tDrill\n\t\n\nThis dataset combines three instruction-following datasets:\n\nargilla/ifeval-like-data (filtered subset)  \nArliAI/Formax-v1.0  \nChristianAzinn/json-training\nHuggingFaceH4/ifeval-like-data\nallenai/tulu-3-sft-personas-instruction-following\n\nIt contains prompts with detailed instructions and corresponding formatted outputs, suitable for training models on instruction adherence and structured text generation.\n\n  Definition of the word \"drill\" according to Merriam-Webster Dictionaryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/drill.","url":"https://huggingface.co/datasets/agentlans/drill","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMDCure-36k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"gemma-270m-medium-qa","keyword":"instruction-tuning","description":"æœ¬è³‡æ–™é›†åŒ…å«ç”± ** gemini-2.0-flash ** ç”Ÿæˆçš„å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ã€‚è³‡æ–™ä¾†æºçµåˆï¼š\n\nReference-freeï¼šç”± seed æ´¾ç”Ÿçš„å–®è¼ªå•ç­”ã€‚\nReference-basedï¼šä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ã€‚\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"seed\": \"...\", \"context\": \"...\", \"messages\": [{\"role\":\"user\",\"content\":\"...\"}, {\"role\":\"assistant\",\"content\":\"...\"}]}\ntype æ¬„ä½æ¨™ç¤ºè³‡æ–™ä¾†æºï¼šreference_free æˆ– reference_basedã€‚\nseed æ¬„ä½å„²å­˜ Reference-free çš„åŽŸå§‹ seed æŒ‡ä»¤ï¼Œæˆ– Reference-based çš„åƒè€ƒæ–‡æœ¬ç‰‡æ®µã€‚\ncontext æ¬„ä½åƒ…åœ¨â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Simon-Liu/gemma-270m-medium-qa.","url":"https://huggingface.co/datasets/Simon-Liu/gemma-270m-medium-qa","creator_name":"Liu Yu-Wei","creator_url":"https://huggingface.co/Simon-Liu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMDCure-36k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"HelpSteer-AIF","keyword":"preference","description":"\n\n\n\t\n\t\t\n\t\tHelpSteer: Helpfulness SteerLM Dataset\n\t\n\nHelpSteer is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nHelpSteer: Multi-attribute Helpfulness Dataset for SteerLM\n\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThis is only a subset created with distilabel to evaluate the first 1000 rows using AI Feedback (AIF) coming from GPT-4, only created forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF.","url":"https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF","creator_name":"Alvaro Bartolome","creator_url":"https://huggingface.co/alvarobartt","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"gemma-270m-medium-qa","keyword":"sft","description":"æœ¬è³‡æ–™é›†åŒ…å«ç”± ** gemini-2.0-flash ** ç”Ÿæˆçš„å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ã€‚è³‡æ–™ä¾†æºçµåˆï¼š\n\nReference-freeï¼šç”± seed æ´¾ç”Ÿçš„å–®è¼ªå•ç­”ã€‚\nReference-basedï¼šä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ã€‚\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"seed\": \"...\", \"context\": \"...\", \"messages\": [{\"role\":\"user\",\"content\":\"...\"}, {\"role\":\"assistant\",\"content\":\"...\"}]}\ntype æ¬„ä½æ¨™ç¤ºè³‡æ–™ä¾†æºï¼šreference_free æˆ– reference_basedã€‚\nseed æ¬„ä½å„²å­˜ Reference-free çš„åŽŸå§‹ seed æŒ‡ä»¤ï¼Œæˆ– Reference-based çš„åƒè€ƒæ–‡æœ¬ç‰‡æ®µã€‚\ncontext æ¬„ä½åƒ…åœ¨â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Simon-Liu/gemma-270m-medium-qa.","url":"https://huggingface.co/datasets/Simon-Liu/gemma-270m-medium-qa","creator_name":"Liu Yu-Wei","creator_url":"https://huggingface.co/Simon-Liu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-tasks-conversation","keyword":"instruct","description":"Combined dataset of mostly Russian math and physics tasks in form of conversation suitable for LLM fine-tuning scenarios.\nTotal samples: 462883\nDatasets used:\n\nVikhrmodels/russian_math\nVikhrmodels/russian_physics\nd0rj/MathInstruct-ru\nd0rj/orca-math-word-problems-200k-ru\nevilfreelancer/MATH-500-Russian\n\n","url":"https://huggingface.co/datasets/ZeroAgency/ru-tasks-conversation","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"user-gender-female","keyword":"sft","description":"\n\t\n\t\t\n\t\tuser-gender-female\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/user-gender-female\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/user-gender-female","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"styler-filtered-instruction-dataset-fixed","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tStyler Filtered Instruction Dataset\n\t\n\nFiltered instruction dataset with token limit of 400 tokens per example\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains instruction-following examples that have been filtered to ensure each example (instruction + input + output) contains no more than 400 tokens.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\ninstruction: The task instruction\ninput: The input text (may be empty)\noutput: The expected output\ntoken_count: Number of tokensâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/upvantage/styler-filtered-instruction-dataset-fixed.","url":"https://huggingface.co/datasets/upvantage/styler-filtered-instruction-dataset-fixed","creator_name":"unknown","creator_url":"https://huggingface.co/upvantage","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LIMO_QFFT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“˜ LIMOâ€“QFFT\n\t\n\nLIMOâ€“QFFT is a question-free variant of the original GAIR/LIMO dataset, tailored for use in QFFT (Question-Free Fine-Tuning) pipelines.\n\n\t\n\t\t\n\t\tðŸ” Description\n\t\n\nThis dataset removes the original input questions and system prompts from the LIMO dataset, and keeps only the long-form reasoning responses. The goal is to enable training large language models to learn from reasoning traces alone, without depending on task-specific questions.\nAll entries are converted intoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lwl-uestc/LIMO_QFFT.","url":"https://huggingface.co/datasets/lwl-uestc/LIMO_QFFT","creator_name":"Wanlong Liu","creator_url":"https://huggingface.co/lwl-uestc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"code-energie","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de l'Ã©nergie, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-energie.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-energie","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"werewolf_game_reasoning","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tWerewolf Game Dataset\n\t\n\nThis repository contains a comprehensive dataset for the Werewolf game in paper Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game, including both raw game data and processed  multi-level instruction datasets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tRaw Data\n\t\n\nThe raw data is located in the raw folder. Each game consists of two files:\n\nevent.json: Contains the game regular record and thinking process data, including:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning.","url":"https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning","creator_name":"Rong Ye","creator_url":"https://huggingface.co/ReneeYe","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","expert-generated","multilingual","original","Chinese"],"keywords_longer_than_N":true},
	{"name":"Capybara-Preferences-Filtered","keyword":"preferences","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for Capybara-Preferences-Filtered\n\t\n\nThis dataset has been created with distilabel, plus some extra post-processing steps described below.\n\n    \n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\n\nRemove responses from the assistant, not only in the last turn, but also in intermediateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Capybara-Preferences-Filtered.","url":"https://huggingface.co/datasets/argilla/Capybara-Preferences-Filtered","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"mauxi-COT-Persian","keyword":"sft","description":"\n\t\n\t\t\n\t\tðŸ§  mauxi-COT-Persian Dataset\n\t\n\n\nExploring Persian Chain-of-Thought Reasoning with DeepSeek-R1, brought to you by Mauxi AI Platform\n\n\n\t\n\t\t\n\t\tðŸŒŸ Overview\n\t\n\nmauxi-COT-Persian is a community-driven dataset that explores the capabilities of advanced language models in generating Persian Chain-of-Thought (CoT) reasoning. The dataset is actively growing with new high-quality, human-validated entries being added regularly. I am personally working on expanding this dataset with rigorouslyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-COT-Persian.","url":"https://huggingface.co/datasets/xmanii/mauxi-COT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"app350_llama_format","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tAPP-350 Formatted Dataset for LLM Fine-tuning\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe APP-350 dataset consists of structured conversation pairs formatted for fine-tuning Large Language Models (LLMs) like LLaMA. This dataset includes questions and responses between users and an AI assistant. The dataset is particularly designed for privacy policy analysis and fairness evaluation, allowing models to learn from annotated interactions regarding privacy practices.\nThe conversations are organizedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CodeHima/app350_llama_format.","url":"https://huggingface.co/datasets/CodeHima/app350_llama_format","creator_name":"Himanshu Mohanty","creator_url":"https://huggingface.co/CodeHima","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"code-tourisme","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode du tourisme, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-tourisme.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-tourisme","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"gemma-en-bg-full","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tGemma EN-BG Translation Dataset (ChessInstruct Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset contains 920,509 English to Bulgarian subtitle translation pairs in ChessInstruct format for fine-tuning Gemma3-270m using the Unsloth framework. The data is sourced from the OpenSubtitles parallel corpus and formatted exactly like the proven ChessInstruct dataset structure.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nâœ… ChessInstruct Compatible: Uses exact task/input/expected_output/KIND format\nðŸš€ Gemma3-270mâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zantag/gemma-en-bg-full.","url":"https://huggingface.co/datasets/zantag/gemma-en-bg-full","creator_name":"zantag","creator_url":"https://huggingface.co/zantag","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","English","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"gemma-en-bg-full","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tGemma EN-BG Translation Dataset (ChessInstruct Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset contains 920,509 English to Bulgarian subtitle translation pairs in ChessInstruct format for fine-tuning Gemma3-270m using the Unsloth framework. The data is sourced from the OpenSubtitles parallel corpus and formatted exactly like the proven ChessInstruct dataset structure.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nâœ… ChessInstruct Compatible: Uses exact task/input/expected_output/KIND format\nðŸš€ Gemma3-270mâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zantag/gemma-en-bg-full.","url":"https://huggingface.co/datasets/zantag/gemma-en-bg-full","creator_name":"zantag","creator_url":"https://huggingface.co/zantag","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","English","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"LONGCOT-Alpaca","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for LONGCOT-Alpaca\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca.","url":"https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"spider-context-instruct","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tDataset Card for Spider Context Instruct\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset was created to finetune LLMs in a ### Instruction: and ### Response: format with database context.\n\n\t\n\t\t\n\t\tYale Lily Spider Leaderboards\n\t\n\nThe leaderboard can be seen atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-context-instruct.","url":"https://huggingface.co/datasets/richardr1126/spider-context-instruct","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-taiwan-dataset","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tä½ å„ä½çš„ Alpaca Data Taiwan Chinese æ­£é«”ä¸­æ–‡æ•¸æ“šé›†\n\t\n\n","url":"https://huggingface.co/datasets/botp/alpaca-taiwan-dataset","creator_name":"ab10","creator_url":"https://huggingface.co/botp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","question-answering","Chinese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Tachibana3-Part2-DeepSeek-V3.2","keyword":"instruct","description":"Click here to support our open-source dataset and model releases!\nTachibana3-Part2-DeepSeek-V3.2 is a dataset focused on high-difficulty code production tasks, testing the limits of DeepSeek V3.2's code-reasoning skills!\nThis dataset contains 9.3k high-difficulty code-production prompts:\n\nQuestions prioritize real-world, challenging coding tasks across a variety of programming languages and topics.\nAreas of focus include back-end and front-end development, mobile, gamedev, cloud, QA, customâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Tachibana3-Part2-DeepSeek-V3.2.","url":"https://huggingface.co/datasets/sequelbox/Tachibana3-Part2-DeepSeek-V3.2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Tachibana3-Part1-DeepSeek-V3.1-Terminus","keyword":"instruct","description":"Click here to support our open-source dataset and model releases!\nTachibana3-Part1-DeepSeek-V3.1-Terminus is a dataset focused on high-difficulty code production tasks, testing the limits of DeepSeek V3.1 Terminus's code-reasoning skills!\nThis dataset contains 9.3k high-difficulty code-production prompts:\n\nQuestions prioritize real-world, challenging coding tasks across a variety of programming languages and topics.\nAreas of focus include back-end and front-end development, mobile, gamedevâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus.","url":"https://huggingface.co/datasets/sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Titanium3-DeepSeek-V3.1-Terminus","keyword":"instruct","description":"Click here to support our open-source dataset and model releases!\nTitanium3-DeepSeek-V3.1-Terminus is a dataset focused on architecture and DevOps, testing the limits of DeepSeek V3.1 Terminus's architect and coding skills!\nThis dataset contains:\n\n27.7k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek V3.1 Terminus in reasoning mode:\n20k selected technical expertise prompts from sequelbox/Titanium2.1-DeepSeek-R1 focused onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium3-DeepSeek-V3.1-Terminus.","url":"https://huggingface.co/datasets/sequelbox/Titanium3-DeepSeek-V3.1-Terminus","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"NorPaca","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tNorPaca Norwegian BokmÃ¥l\n\t\n\nThis dataset is a translation to Norwegian BokmÃ¥l of alpaca_gpt4_data.json, a clean version of the Alpaca dataset made at Stanford, but generated with GPT4.\n\n\t\n\t\t\n\t\tPrompt to generate dataset\n\t\n\n    Du blir bedt om Ã¥ komme opp med et sett med 20 forskjellige oppgaveinstruksjoner. Disse oppgaveinstruksjonene vil bli gitt til en GPT-modell, og vi vil evaluere GPT-modellen for Ã¥ fullfÃ¸re instruksjonene. \n\nHer er kravene:\n1. PrÃ¸v Ã¥ ikke gjenta verbet for hverâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MasterThesisCBS/NorPaca.","url":"https://huggingface.co/datasets/MasterThesisCBS/NorPaca","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-product-code-mapping-v3","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 1801\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-product-code-mapping-v3.","url":"https://huggingface.co/datasets/archit11/hyperswitch-product-code-mapping-v3","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"mauxitalk-persian","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tðŸ—£ï¸ MauxiTalk: High-Quality Persian Conversations Dataset ðŸ‡®ðŸ‡·\n\t\n\n\n\t\n\t\t\n\t\tðŸ“ Description\n\t\n\nMauxiTalk is a high-quality dataset of 2,000+ Persian conversations, carefully translated from the SmolTalk dataset using state-of-the-art language models. This dataset is specifically curated for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n2,000 natural conversations in Persian\nDiverse topics including dailyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxitalk-persian.","url":"https://huggingface.co/datasets/xmanii/mauxitalk-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","summarization","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mauxitalk-persian","keyword":"sft","description":"\n\t\n\t\t\n\t\tðŸ—£ï¸ MauxiTalk: High-Quality Persian Conversations Dataset ðŸ‡®ðŸ‡·\n\t\n\n\n\t\n\t\t\n\t\tðŸ“ Description\n\t\n\nMauxiTalk is a high-quality dataset of 2,000+ Persian conversations, carefully translated from the SmolTalk dataset using state-of-the-art language models. This dataset is specifically curated for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n2,000 natural conversations in Persian\nDiverse topics including dailyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxitalk-persian.","url":"https://huggingface.co/datasets/xmanii/mauxitalk-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","summarization","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-1.5-Instruct-Data","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-1.5 Instruction Data\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset, LLaVA-OneVision-1.5-Instruct, was collected and integrated during the development of LLaVA-OneVision-1.5. LLaVA-OneVision-1.5 is a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. This meticulously curated 22M instruction dataset (LLaVA-OneVision-1.5-Instruct) is part of a comprehensive andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mvp-lab/LLaVA-OneVision-1.5-Instruct-Data.","url":"https://huggingface.co/datasets/mvp-lab/LLaVA-OneVision-1.5-Instruct-Data","creator_name":"Mobile Vision Perception Lab","creator_url":"https://huggingface.co/mvp-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2509.23661","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-product-code-full","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 1801\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-product-code-full.","url":"https://huggingface.co/datasets/archit11/hyperswitch-product-code-full","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-product-code-complete","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 1801\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-product-code-complete.","url":"https://huggingface.co/datasets/archit11/hyperswitch-product-code-complete","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for \"tasksource-instruct-v0\" (TSI)\n\t\n\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\nDataset size is capped at 30k examples per task to foster task diversity.\n!pip install tasksource, pandit\nimport tasksource, pandit\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\nfor tasks in df.id:\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\n\nhttps://github.com/sileod/tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for \"tasksource-instruct-v0\" (TSI)\n\t\n\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\nDataset size is capped at 30k examples per task to foster task diversity.\n!pip install tasksource, pandit\nimport tasksource, pandit\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\nfor tasks in df.id:\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\n\nhttps://github.com/sileod/tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"LewdRuStoryforTrain","keyword":"instruction-finetuning","description":"ÐšÑƒÑ‡Ð° Ð²ÑÑÐºÐ¸Ñ… Ñ€Ð°Ð½Ð´Ð¾Ð¼Ð½Ñ‹Ñ… Ð¿Ð¾Ñ€Ð½Ð¾ Ñ€Ð°ÑÑÐºÐ°Ð·Ð¾Ð² Ð¸Ð· Ð¸Ð½Ñ‚ÐµÑ€ÐµÐ½ÐµÑ‚Ð°, Ð¸Ð· Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ñ‚ÐµÐ³Ð¾Ð², Ð½Ð¾ Ð±ÐµÐ· Ð¶ÐµÑÑ‚Ð¸.\n","url":"https://huggingface.co/datasets/ASIDS/LewdRuStoryforTrain","creator_name":"Greed","creator_url":"https://huggingface.co/ASIDS","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-generation","Russian","mit","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"budapest-v0.1-hun","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tBudapest-v0.1 Dataset README\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Budapest-v0.1 dataset is a cutting-edge resource specifically designed for fine-tuning large language models (LLMs). Created using GPT-4, this dataset is presented in a message-response format, making it particularly suitable for a variety of natural language processing tasks. The primary focus of Budapest-v0.1 is to aid in the development and enhancement of algorithms capable of performing summarization, question answeringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun.","url":"https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun","creator_name":"Balazs Toldi","creator_url":"https://huggingface.co/Bazsalanszky","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Hungarian","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"human-style-preferences-images","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Preference Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human preference datasets for text-to-image models, this release contains over 1,200,000 human preferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-style-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-style-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"CultriX-dpo","keyword":"preferences","description":"CultriX/CultriX-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/CultriX-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Kannada-Dataset-v03","keyword":"finetuning","description":"charanhu/Kannada-Dataset-v03 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/charanhu/Kannada-Dataset-v03","creator_name":"Charan H U","creator_url":"https://huggingface.co/charanhu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Kannada","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-serbian-full","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tSerbian Alpaca Cleaned Dataset\n\t\n\n\nOriginal Repository: https://github.com/gururise/AlpacaDataCleaned\nOriginal HF Repository: https://huggingface.co/datasets/yahma/alpaca-cleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a serbian cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full.","url":"https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full","creator_name":"Dean","creator_url":"https://huggingface.co/datatab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Serbian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"NorEval","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tNorEval\n\t\n\nNorEval is a self-curated dataset to evaluate instruction-following LLMs, seeking to evaluate the models in nine categories: Language, Code, Mathematics, Classification, Communication & Marketing, Medical, General Knowledge, and Business Operations\n","url":"https://huggingface.co/datasets/MasterThesisCBS/NorEval","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cot-en-refined-by-data-juicer","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tAlpaca-CoT -- EN (refined by Data-Juicer)\n\t\n\nA refined English version of Alpaca-CoT dataset by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to fine-tune a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 226GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 72,855,345 (Keep ~54.48% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefining Recipeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/alpaca-cot-en-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/alpaca-cot-en-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"NSFW_Chat_Dataset","keyword":"finetune","description":"\n\t\n\t\t\n\t\tðŸ’• Spicy AI GF Chat Dataset ðŸ”¥\n\t\n\n\n\t\n\t\t\n\t\tðŸš¨ 18+ Only! NSFW & Spicy Content Ahead ðŸš¨\n\t\n\nHey there, AI enthusiasts and romance lovers! ðŸ˜ Welcome to the Spicy AI GF Chat Dataset, the ultimate dataset designed to bring your AI waifu to life! ðŸ’– If you've ever dreamed of building an AI that responds like your virtual girlfriend, THIS is the dataset for you.\n\n\t\n\t\t\n\t\tðŸ“œ Whatâ€™s Inside?\n\t\n\nThis dataset features two columns:\n\ninput â†’ Boyfriendâ€™s dialogue (aka what YOU say ðŸ˜‰)\noutput â†’â€¦ See the full description on the dataset page: https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset.","url":"https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset","creator_name":"Utsav Maji","creator_url":"https://huggingface.co/utsavm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"helpful_instructions","keyword":"instruct","description":"Helpful Instructions is a dataset of (prompt, completion) pairs that are derived from a variety of public datasets. As the name suggests, it focuses on instructions that are \"helpful\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform.","url":"https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"instruct_me","keyword":"instruct","description":"Instruct Me is a dataset of instruction-like dialogues between a human user and AI assistant. The prompts are derived from (prompt, completion) pairs in the Helpful Instructions dataset. The goal is to train a language model to that is \"chatty\" and can answer the kind of questions or tasks a human user might instruct an AI assistant to perform.","url":"https://huggingface.co/datasets/HuggingFaceH4/instruct_me","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ccisd-teks-training","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCCISD TEKS Training Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a training-ready dataset with 4,200+ examples derived from Texas Essential Knowledge and Skills (TEKS) standards for 25 high school courses. Designed for fine-tuning language models on educational curriculum tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 4,224\nTraining Set: 2,956 (70%)\nValidation Set: 633 (15%)\nTest Set: 635 (15%)\nSource Courses: 25 high school courses\nSource Standards: 428 TEKS standards\nTaskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/robworks-software/ccisd-teks-training.","url":"https://huggingface.co/datasets/robworks-software/ccisd-teks-training","creator_name":"Ryan Robson","creator_url":"https://huggingface.co/robworks-software","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","text-generation","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"streetview-commands-dataset","keyword":"instruction-tuning","description":"\n\n\t\n\t\t\n\t\tDataset Card for streetview-commands-dataset\n\t\n\nThis dataset contains pairs of natural language instructions (simulating commands given to Google Street View) and their corresponding structured JSON outputs representing the intended navigation action. It was generated using the Gemini API (gemini-1.5-flash-latest) based on predefined templates and few-shot examples.\nThe primary intended use is for fine-tuning small language models (like TinyLlama) to act as a translation layer betweenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cahlen/streetview-commands-dataset.","url":"https://huggingface.co/datasets/cahlen/streetview-commands-dataset","creator_name":"Cahlen Humphreys","creator_url":"https://huggingface.co/cahlen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"dac6-instruct","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tDAC6 instruct (11-12-2023)\n\t\n\nâ€œDAC 6â€ refers to European Council Directive (EU) 2018/822 of May 25, 2018 relating to the automatic and mandatory exchange of information on cross-border arrangements requiring declaration. It aims to strengthen cooperation between tax administrations in EU countries on potentially aggressive tax planning arrangements.\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for tax practice. \nFine-tuning isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/dac6-instruct.","url":"https://huggingface.co/datasets/louisbrulenaudet/dac6-instruct","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"Primus-Seed-Conversation","keyword":"instruct","description":"\n\t\n\t\t\n\t\tPrimus-Seed Conversational Dataset\n\t\n\nA conversational dataset derived from trendmicro-ailab/Primus-Seed, designed for training language models as cybersecurity experts.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset converts the original Primus-Seed text completion dataset into a conversational format with 86,987 examples. Each example consists of a three-turn conversation between a system, user, and assistant focused on cybersecurity knowledge.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tuandunghcmut/Primus-Seed-Conversation.","url":"https://huggingface.co/datasets/tuandunghcmut/Primus-Seed-Conversation","creator_name":"DÅ©ng VÃµ","creator_url":"https://huggingface.co/tuandunghcmut","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-rust-commits-final2","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 2277\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-rust-commits-final2.","url":"https://huggingface.co/datasets/archit11/hyperswitch-rust-commits-final2","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"turkish-gemma-51k","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tTurkish Chat Dataset - Gemma Format\n\t\n\nBu dataset, TÃ¼rkÃ§e sohbet ve talimat takip etme gÃ¶revleri iÃ§in hazÄ±rlanmÄ±ÅŸ 51.914 konuÅŸma Ã¶rneÄŸi iÃ§erir.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Ã–zeti\n\t\n\n\nDil: TÃ¼rkÃ§e\nFormat: Chat/Conversation\nÃ–rnek SayÄ±sÄ±: 51,914\nKaynak: afkfatih/turkishdataset\n\n\n\t\n\t\t\n\t\tðŸŽ¯ KullanÄ±m AlanlarÄ±\n\t\n\n\nTÃ¼rkÃ§e sohbet botlarÄ± eÄŸitimi\nInstruction-tuning\nFine-tuning LLM modelleri (Gemma, Llama, vb.)\nTÃ¼rkÃ§e doÄŸal dil anlama\n\n\n\t\n\t\t\n\t\tðŸ“ Format\n\t\n\nHer Ã¶rnek ÅŸu yapÄ±ya sahiptir:\n[\n  {\n    \"role\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/afkfatih/turkish-gemma-51k.","url":"https://huggingface.co/datasets/afkfatih/turkish-gemma-51k","creator_name":"Fatih Ozyilmaz","creator_url":"https://huggingface.co/afkfatih","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Turkish","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k-Japanese","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k (Japanese Translation)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a Japanese translation of the Finance-Instruct-500k dataset, created using OpenAI's GPT-4o-mini via the Batch API.\n\n\t\n\t\t\n\t\tOriginal Dataset\n\t\n\n\nOriginal Author: Joseph G. Flowers\nOriginal Dataset: Josephgflowers/Finance-Instruct-500k\nLicense: Apache 2.0\n\n\n\t\n\t\t\n\t\tTranslation Details\n\t\n\n\nTranslation Model: GPT-4o-mini (OpenAI)\nTranslation Method: OpenAI Batch API with human verifications\nDate: 2025â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ronantakizawa/Finance-Instruct-500k-Japanese.","url":"https://huggingface.co/datasets/ronantakizawa/Finance-Instruct-500k-Japanese","creator_name":"Ronan Takizawa","creator_url":"https://huggingface.co/ronantakizawa","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","translation","Japanese","English"],"keywords_longer_than_N":true},
	{"name":"self-instruct-seed-ca","keyword":"instruction tuning","description":"\n\t\n\t\t\n\t\tCatalan self-instruct seed\n\t\n\nManual translation of the seed instructions from self-instruct.\nNote that some examples could not be literally translated (e.g. jokes, puns, code) and had to be adapted to the target language.\n","url":"https://huggingface.co/datasets/mapama247/self-instruct-seed-ca","creator_name":"Marc PÃ mies","creator_url":"https://huggingface.co/mapama247","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Catalan","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"PRODIGY-LAB_SARA","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for PRODIGY-LAB_CLEANED\n\t\n\n\nRepository: https://github.com/aadhithyaravi\nCreated by: Aadhithya  \nContact: aadhithyaxll@gmail.com  \nInstagram: @aadhi.arc  \nLinkedIn: www.linkedin.com/in/aadhithya-ravi-135019289\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nPRODIGY-SARA-MODEL is a refined and enhanced dataset designed for instruction-based fine-tuning of large language models (LLMs).It combines multiple high-quality sources, including cleaned and normalized instructions, to improveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Apex-X/PRODIGY-LAB_SARA.","url":"https://huggingface.co/datasets/Apex-X/PRODIGY-LAB_SARA","creator_name":"Aadhithya","creator_url":"https://huggingface.co/Apex-X","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Tamil","Hindi","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"periodontal-reasoning-40k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tPeriodontal-Reasoning-40k\n\t\n\n40,000 periodontal clinical reasoning examples for off-policy RLHF (KTO/DPO).\nFormat (JSONL, one per line): prompt, completion, label âˆˆ {1,-1}\nExample:\n{\"prompt\": \"A patient's plaque score was 35% at baseline and 1% at followâ€‘up. Determine whether the improvement is favourable according to BSP criteria (â‰¤20% plaque or â‰¥50% reduction).\", \"completion\": \"The improvement is favourable.\", \"label\": 1}\nSplit: train: 40,000 (data/train.jsonl)\nIntended use: KTO/DPO;â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Wildstash/periodontal-reasoning-40k.","url":"https://huggingface.co/datasets/Wildstash/periodontal-reasoning-40k","creator_name":"ArnavS","creator_url":"https://huggingface.co/Wildstash","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","expert-generated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Code-170k-baoule","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-baoule is a groundbreaking dataset containing 176,999 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Baoule, making coding education accessible to Baoule speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n176,999 high-quality conversations about programming and coding\nPure Baoule language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-baoule.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-baoule","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","BaoulÃ©","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"preference","description":"\n\t\n\t\t\n\t\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\n\n\t\n\t\n\t\n\t\tDifferences with argilla/ultrafeedback-binarized-preferencesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned.","url":"https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"marathi-alpaca-llama-finetune","keyword":"finetune","description":"\n\t\n\t\t\n\t\tMarathi Alpaca Dataset for llama-finetune\n\t\n\nThis dataset contains 48,897 high-quality Marathi instruction-following examples, converted to the llama-finetune format.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach line in the JSONL file contains:\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"à¤¨à¤¿à¤°à¥‹à¤—à¥€ à¤°à¤¾à¤¹à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤¤à¥€à¤¨ à¤Ÿà¤¿à¤ªà¤¾ à¤¦à¥à¤¯à¤¾.\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"1. à¤¸à¤‚à¤¤à¥à¤²à¤¿à¤¤ à¤†à¤£à¤¿ à¤ªà¥Œà¤·à¥à¤Ÿà¤¿à¤• à¤†à¤¹à¤¾à¤° à¤˜à¥à¤¯à¤¾...\"\n    }\n  ]\n}\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tDownload\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aghatage/marathi-alpaca-llama-finetune.","url":"https://huggingface.co/datasets/aghatage/marathi-alpaca-llama-finetune","creator_name":"Anup Ghatage","creator_url":"https://huggingface.co/aghatage","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Marathi","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Financial_News_Translation_Spanish_Finetune","keyword":"finetune","description":"\n\t\n\t\t\n\t\tOverview of the Financial News Translation Dataset for OpenAI Model Fine-tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction:\n\t\n\nThis dataset has been curated with the primary objective of fine-tuning varioyus language models to effectively translate financial news content embedded in HTML format. The intention is to enhance the language model's proficiency in accurately and contextually translating financial information for a global audience in a production envionrment.\n\n\t\n\t\t\n\t\tDataset Composition:\n\t\n\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Benzinga/Financial_News_Translation_Spanish_Finetune.","url":"https://huggingface.co/datasets/Benzinga/Financial_News_Translation_Spanish_Finetune","creator_name":"Benzinga","creator_url":"https://huggingface.co/Benzinga","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","mit","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"shell-cmd-instruct","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tUsed to train models that interact directly with shells\n\t\n\nNote: This dataset is out-dated in the llm world, probably easier to just setup a tool with a decent model that supports tooling.\nFollow-up details of my process \n\nMacOS terminal commands for now. This dataset is still in alpha stages and will be modified.\n\nContains 500 somewhat unique training examples so far.\n\nGPT4 seems like a good candidate for generating more data, licensing would need to be addressed.\n\nI fine-tunedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/byroneverson/shell-cmd-instruct.","url":"https://huggingface.co/datasets/byroneverson/shell-cmd-instruct","creator_name":"Byron Everson","creator_url":"https://huggingface.co/byroneverson","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"KITE","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tKITE: Korean Instruction-following Task Evaluation\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nKITE (Korean Instruction-following Task Evaluation) is the first comprehensive benchmark specifically designed to evaluate the Korean instruction-following capabilities of Large Language Models (LLMs). Unlike existing Korean benchmarks that focus mainly on factual knowledge or multiple-choice testing, KITE directly targets diverse, open-ended instruction-following tasks.\n\t\n\t\t\n\t\tDataset Summaryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/junkim100/KITE.","url":"https://huggingface.co/datasets/junkim100/KITE","creator_name":"Jun Kim","creator_url":"https://huggingface.co/junkim100","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Korean","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"marathi-alpaca-llama-finetune","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMarathi Alpaca Dataset for llama-finetune\n\t\n\nThis dataset contains 48,897 high-quality Marathi instruction-following examples, converted to the llama-finetune format.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach line in the JSONL file contains:\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"à¤¨à¤¿à¤°à¥‹à¤—à¥€ à¤°à¤¾à¤¹à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤¤à¥€à¤¨ à¤Ÿà¤¿à¤ªà¤¾ à¤¦à¥à¤¯à¤¾.\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"1. à¤¸à¤‚à¤¤à¥à¤²à¤¿à¤¤ à¤†à¤£à¤¿ à¤ªà¥Œà¤·à¥à¤Ÿà¤¿à¤• à¤†à¤¹à¤¾à¤° à¤˜à¥à¤¯à¤¾...\"\n    }\n  ]\n}\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tDownload\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aghatage/marathi-alpaca-llama-finetune.","url":"https://huggingface.co/datasets/aghatage/marathi-alpaca-llama-finetune","creator_name":"Anup Ghatage","creator_url":"https://huggingface.co/aghatage","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Marathi","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"swedish_healthcare_dpo_sft_dataset","keyword":"sft","description":"\n\t\n\t\t\n\t\tSwedish Healthcare DPO/SFT Dataset\n\t\n\nThis dataset contains Swedish conversational prompts paired with chosen (preferred) and rejected (dispreferred) responses, along with English translations. It is designed for fine-tuning Large Language Models (LLMs) using preference-based methods. The (prompt, chosen, rejected) structure makes it particularly well-suited for Direct Preference Optimization (DPO) and similar algorithms.\nSweden is renowned for its high standards in healthcare andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset.","url":"https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset","creator_name":"Rzgar","creator_url":"https://huggingface.co/rzgar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Swedish","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"tool-n1-sft-unique-train-eval","keyword":"sft","description":"\n\t\n\t\t\n\t\tTool-N1 SFT Unique Train-Eval Split\n\t\n\nThis dataset contains supervised fine-tuning (SFT) data for training models on multi-hop tool usage and reasoning, with proper train/evaluation splits and guaranteed unique queries.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\nâœ… Perfect Alternating Format: <think>reasoning</think> <tool_call>tool_call</tool_call> patternâœ… Unique Queries: Complete deduplication based on query contentâœ… Train/Eval Split: Proper 80/20 split for training and evaluationâœ… Multi-hopâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Anna4242/tool-n1-sft-unique-train-eval.","url":"https://huggingface.co/datasets/Anna4242/tool-n1-sft-unique-train-eval","creator_name":"D","creator_url":"https://huggingface.co/Anna4242","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"RoboMatrix","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis is the fine-tuning dataset used in the paper RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World.\n\n\t\n\t\t\n\t\tSource\n\t\n\nProject Page: https://robo-matrix.github.io/Paper: https://arxiv.org/abs/2412.00171Code: https://github.com/WayneMao/RoboMatrixModel: \n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you find our work helpful, please cite us:\n@article{mao2024robomatrix,\n  title={RoboMatrix: A Skill-centric Hierarchical Framework forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WayneMao/RoboMatrix.","url":"https://huggingface.co/datasets/WayneMao/RoboMatrix","creator_name":"WayneMao","creator_url":"https://huggingface.co/WayneMao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","Datasets","Croissant","arxiv:2412.00171"],"keywords_longer_than_N":true},
	{"name":"code-instruments-monetaires-medailles","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des instruments monÃ©taires et des mÃ©dailles, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-instruments-monetaires-medailles.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-instruments-monetaires-medailles","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"AB1","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagedGaman/AB1.","url":"https://huggingface.co/datasets/MagedGaman/AB1","creator_name":"Maged Gaman","creator_url":"https://huggingface.co/MagedGaman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"finewebedu-sft","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tFineWeb-Edu Supervised Finetuning Dataset\n\t\n\n\n\t\n\t\t\n\t\tModel Description\n\t\n\nThis dataset is designed for training language models to generate supervised finetuning data from raw text. It consists of text passages and corresponding question-answer pairs in JSONLines format.\n\n\t\n\t\t\n\t\tIntended Use\n\t\n\nThe primary purpose of this dataset is to enable large language models (LLMs) to generate high-quality supervised finetuning data from raw text inputs, useful for creating custom datasets forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/finewebedu-sft.","url":"https://huggingface.co/datasets/agentlans/finewebedu-sft","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","open-domain-qa","abstractive-qa","machine-generated"],"keywords_longer_than_N":true},
	{"name":"Persian-Math-SFT","keyword":"finetune","description":"\n\t\n\t\t\n\t\tðŸŽ¯ Persian Math Questions Dataset for SFT\n\t\n\n\n\t\n\t\t\n\t\tðŸ“ Description\n\t\n\nThis dataset contains Persian questions primarily focused on mathematical concepts, designed for Supervised Fine-Tuning (SFT) of Language Models.\n\n\t\n\t\t\n\t\tðŸ” Features\n\t\n\n\nHigh-quality Persian questions\nDetailed subtopic categorization\nFocused on mathematical concepts\nTokens count for each conversation\n\n\n\t\n\t\t\n\t\tðŸš€ Coming Soon\n\t\n\n\nDetailed answers for each question\nAdditional topics beyond mathematics\nEnhancedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Persian-Math-SFT.","url":"https://huggingface.co/datasets/xmanii/Persian-Math-SFT","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Persian","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-moonvalley-marey","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Marey Pro Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~75k human responses from ~15k human annotators were collected to evaluate Marey video generation model on our benchmark. This dataset was collected in roughtly 30 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please considerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-moonvalley-marey.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-moonvalley-marey","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Persian-Math-SFT","keyword":"sft","description":"\n\t\n\t\t\n\t\tðŸŽ¯ Persian Math Questions Dataset for SFT\n\t\n\n\n\t\n\t\t\n\t\tðŸ“ Description\n\t\n\nThis dataset contains Persian questions primarily focused on mathematical concepts, designed for Supervised Fine-Tuning (SFT) of Language Models.\n\n\t\n\t\t\n\t\tðŸ” Features\n\t\n\n\nHigh-quality Persian questions\nDetailed subtopic categorization\nFocused on mathematical concepts\nTokens count for each conversation\n\n\n\t\n\t\t\n\t\tðŸš€ Coming Soon\n\t\n\n\nDetailed answers for each question\nAdditional topics beyond mathematics\nEnhancedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Persian-Math-SFT.","url":"https://huggingface.co/datasets/xmanii/Persian-Math-SFT","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Persian","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Pares_1_vinos","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tWine Q&A Dataset Â· Sommelier Style ðŸ·\n\t\n\nEste dataset contiene 100 pares de pregunta-respuesta en espaÃ±ol sobre el mundo del vino, redactados con estilo profesional, cercano y claro, al estilo de un sommelier.\n\n\t\n\t\t\n\t\tEstructura del dataset\n\t\n\nEl archivo estÃ¡ en formato .jsonl, donde cada lÃ­nea es un objeto con dos campos:\n\ninstruction: la pregunta del usuario.\nresponse: una respuesta experta, clara y contextualizada sobre vinos.\n\nEjemplo:\n{\"instruction\": \"Â¿QuÃ© vino marida bien conâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Alexei-Bucarenko/Pares_1_vinos.","url":"https://huggingface.co/datasets/Alexei-Bucarenko/Pares_1_vinos","creator_name":"Alejandro MartÃ­n","creator_url":"https://huggingface.co/Alexei-Bucarenko","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["Spanish","apache-2.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US","wine"],"keywords_longer_than_N":true},
	{"name":"secondKarlMarx-sft","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMarx Works SFT Instruction Prompts Dataset / é©¬å…‹æ€è‘—ä½œSFTæŒ‡ä»¤æç¤ºæ•°æ®é›†\n\t\n\nEnglish | ä¸­æ–‡\n\n\n\t\n\t\t\n\t\tEnglish\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains SFT (Supervised Fine-Tuning) instruction prompts generated from the works of Karl Marx. The dataset is specifically designed for training large language models, aiming to capture Marx's dialectical materialist analytical method and writing style.\n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nDiverse Prompt Types: Includes various styles of prompts such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft.","url":"https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft","creator_name":"ChizhongWang","creator_url":"https://huggingface.co/ChizhongWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Pares_1_vinos","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tWine Q&A Dataset Â· Sommelier Style ðŸ·\n\t\n\nEste dataset contiene 100 pares de pregunta-respuesta en espaÃ±ol sobre el mundo del vino, redactados con estilo profesional, cercano y claro, al estilo de un sommelier.\n\n\t\n\t\t\n\t\tEstructura del dataset\n\t\n\nEl archivo estÃ¡ en formato .jsonl, donde cada lÃ­nea es un objeto con dos campos:\n\ninstruction: la pregunta del usuario.\nresponse: una respuesta experta, clara y contextualizada sobre vinos.\n\nEjemplo:\n{\"instruction\": \"Â¿QuÃ© vino marida bien conâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Alexei-Bucarenko/Pares_1_vinos.","url":"https://huggingface.co/datasets/Alexei-Bucarenko/Pares_1_vinos","creator_name":"Alejandro MartÃ­n","creator_url":"https://huggingface.co/Alexei-Bucarenko","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["Spanish","apache-2.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US","wine"],"keywords_longer_than_N":true},
	{"name":"dental-2.5k-instruct","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDental Training Dataset (Synthetic)\n\t\n\nSynthetic dataset of 2,494 dental clinical cases for training Dental-GPT, a specialized language model for dental diagnosis and treatment planning.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSize: 2,494 synthetic dental cases\nFormat: JSONL with structured conversations\nSynthetic: Artificially generated cases (no real patient data)\nPurpose: Training dental diagnostic AI models\nLanguage: English\nMetadata: Croissant format available for automated discoveryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Wildstash/dental-2.5k-instruct.","url":"https://huggingface.co/datasets/Wildstash/dental-2.5k-instruct","creator_name":"ArnavS","creator_url":"https://huggingface.co/Wildstash","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"secondKarlMarx-sft","keyword":"sft","description":"\n\t\n\t\t\n\t\tMarx Works SFT Instruction Prompts Dataset / é©¬å…‹æ€è‘—ä½œSFTæŒ‡ä»¤æç¤ºæ•°æ®é›†\n\t\n\nEnglish | ä¸­æ–‡\n\n\n\t\n\t\t\n\t\tEnglish\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains SFT (Supervised Fine-Tuning) instruction prompts generated from the works of Karl Marx. The dataset is specifically designed for training large language models, aiming to capture Marx's dialectical materialist analytical method and writing style.\n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nDiverse Prompt Types: Includes various styles of prompts such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft.","url":"https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft","creator_name":"ChizhongWang","creator_url":"https://huggingface.co/ChizhongWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"indian_university_guidance_for_bangladeshi_students","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tIndian University Guidance for Bangladeshi Students Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 7,044 high-quality, instruction-formatted Question-Answer pairs designed for fine-tuning Large Language Models (LLMs). The primary goal of this dataset is to create a specialized AI counselor that provides accurate, culturally relevant, and comprehensive guidance on Indian universities for Bangladeshi students.\nThe dataset was generated through a sophisticatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/millat/indian_university_guidance_for_bangladeshi_students.","url":"https://huggingface.co/datasets/millat/indian_university_guidance_for_bangladeshi_students","creator_name":"MD MILLAT HOSEN","creator_url":"https://huggingface.co/millat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"CRAFT-Summarization","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCRAFT-Summarization\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated summarization data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization.","url":"https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","text-generation","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"lies","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tLie Detection Dataset\n\t\n\nThis dataset contains lie detection samples for fine-tuning language models to detect when they are lying.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nEach sample contains a conversation between a user and an AI model, followed by a lie detection prompt and the ground truth label indicating whether the model lied.\n\n\t\n\t\t\n\t\tAvailable Configurations\n\t\n\n\n\t\n\t\t\nConfiguration\nModel\nAggregation\nFold\nSamples\nDescription\n\n\n\t\t\nopenai-gpt-oss-120b_megafolds_fold_1\nopenai/gpt/oss/120bâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Noddybear/lies.","url":"https://huggingface.co/datasets/Noddybear/lies","creator_name":"Jack Hopkins","creator_url":"https://huggingface.co/Noddybear","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"cheapvs","keyword":"preference","description":"cheapvs/cheapvs dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/cheapvs/cheapvs","creator_name":"cheapvs","creator_url":"https://huggingface.co/cheapvs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-tat","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tTatAlpaca\n\t\n\nDataset of Gemini-generated instructions in Tatar language.\n\nCode: tatlm/self_instruct\nCode is based on Stanford Alpaca and self-instruct.\n166,257 examples\n\nPrompt template:\n{{num_tasks}} Ò—Ñ‹ÐµÐ»Ð¼Ð°ÑÑ‹Ð½Ñ‹Ò£ ÑÐ¾ÑÑ‚Ð°Ð²Ñ‹ Ñ‚ÐµÐ» Ð¼Ð¾Ð´ÐµÐ»ÐµÐ½ Ó©Ð¹Ñ€Ó™Ð½Ò¯ Ó©Ñ‡ÐµÐ½ Ñ‚Ó©Ñ€Ð»Ðµ:\n\n1. Ð‘Ð¸Ñ€ÐµÐ¼Ð½Ó™Ñ€Ð½Ðµ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒ Ñ€Ó™Ð²ÐµÑˆÑ‚Ó™ Ñ‚Ð¸Ð¿Ð»Ð°Ñ€Ñ‹, ÑÐ¾Ñ€Ð°Ð»Ð³Ð°Ð½ Ð³Ð°Ð¼Ó™Ð»Ð»Ó™Ñ€Ðµ, Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ°Ð»Ð°Ñ€Ñ‹, ÐºÐµÑ€Ò¯ Ð¼Ó©Ð¼ÐºÐ¸Ð½Ð»ÐµÐºÐ»Ó™Ñ€Ðµ Ð±ÑƒÐµÐ½Ñ‡Ð° Ð±ÐµÑ€-Ð±ÐµÑ€ÑÐµÐ½Ó™ Ð¾Ñ…ÑˆÐ°Ð¼Ð°Ð³Ð°Ð½ Ð¸Ñ‚ÐµÐ¿ ÑÑˆÐ»Ó™.\n2. Ð‘Ð¸Ñ€ÐµÐ¼Ð½Ó™Ñ€ Ñ€Ó™ÑÐµÐ¼Ð½Ó™Ñ€, Ð²Ð¸Ð´ÐµÐ¾, Ð°ÑƒÐ´Ð¸Ð¾ Ð±ÐµÐ»Ó™Ð½ ÑÑˆÐ»Ð¸ Ð±ÐµÐ»Ð¼Ó™Ð³Ó™Ð½ Ò»Ó™Ð¼ Ñ‚Ñ‹ÑˆÐºÑ‹ Ð´Ó©Ð½ÑŒÑÐ³Ð° ÐºÐµÑ€Ò¯ Ð¼Ó©Ð¼ÐºÐ¸Ð½Ð»ÐµÐ³Ðµ Ð±ÑƒÐ»Ð¼Ð°Ð³Ð°Ð½â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yasalma/alpaca-tat.","url":"https://huggingface.co/datasets/yasalma/alpaca-tat","creator_name":"Yasalma","creator_url":"https://huggingface.co/yasalma","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Tatar","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OpenGVLab_Lumina_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Lumina Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 400k human responses from over 86k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Lumina across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/OpenGVLab_Lumina_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/OpenGVLab_Lumina_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Titanium2.1-DeepSeek-R1","keyword":"instruct","description":"Click here to support our open-source dataset and model releases!\nTitanium2.1-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n31.7k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraformâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1.","url":"https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"dali","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tå¤§æŽè€å¸ˆé—®ç­”æ•°æ®é›†\n\t\n\nè¿™ä¸ªæ•°æ®é›†åŒ…å«å¤§æŽè€å¸ˆçš„é—®ç­”å¯¹è¯,ç”¨äºŽè®­ç»ƒå¯¹è¯æ¨¡åž‹ã€‚\n\n\t\n\t\t\n\t\tæ•°æ®é›†æè¿°\n\t\n\n\næ ¼å¼: JSONL\nå­—æ®µ: \ninstruction: å›ºå®šå€¼\"è¯·å¤§æŽè€å¸ˆå›žç­”\"\ninput: æé—®å†…å®¹ \noutput: å¤§æŽè€å¸ˆçš„å›žç­”\n\n\næ•°æ®é‡: xxxæ¡å¯¹è¯æ•°æ®\n\n\n\t\n\t\t\n\t\tä½¿ç”¨ç¤ºä¾‹\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"your-username/dataset-name\")\n\n\n\t\n\t\t\n\t\tè®¸å¯è¯\n\t\n\nApache 2.0\n","url":"https://huggingface.co/datasets/liwei1987cn/dali","creator_name":"Levi li","creator_url":"https://huggingface.co/liwei1987cn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"mmlu_preferences","keyword":"preferences","description":"reformat of MMLU to be in DPO (paired) format\nexamples:\n  {'prompt': 'Which of the following statements about the lanthanide elements is NOT true?', 'chosen': 'The atomic radii of the lanthanide elements increase across the period from La to Lu.', 'rejected': 'All of the lanthanide elements react with aqueous acid to liberate hydrogen.'}\n  college_chemistry\n  {'prompt': 'Beyond the business case for engaging in CSR there are a number of moral arguments relating to: negative _______, theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/mmlu_preferences.","url":"https://huggingface.co/datasets/wassname/mmlu_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"gemma-vs-gemma-preferences","keyword":"preference","description":"\n\t\n\t\t\n\t\tðŸ’ŽðŸ†šðŸ’Ž Gemma vs Gemma Preferences\n\t\n\nThis dataset contains on-policy collected preferences generated using anakin87/gemma-2-2b-ita-sft.\nâš ï¸ While this dataset may be valuable for didactic purposes, it is not recommended for training a model using Preference Tuning due to the following reasons:\n\nThe training would be off-policy for your model.\nThe dataset was generated with gemma-2-2b-ita-sft, a small model for Italian.\n\n\n\t\n\t\t\n\t\n\t\n\t\tMotivation\n\t\n\nWhile DPO (Direct Preferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences.","url":"https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"SFinD-S","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis sample is part of the larger SFinD-S (Strative Financial Dataset - Synthetic), a comprehensive dataset designed for Retrieval-Augmented Generation (RAG) GenAI applications, Natural Language Processing (NLP), Large Language Models (LLM), and AI tasks in the financial domain. The full SFinD-S dataset contains over 20,000 records of realistic financial questions and verified answers, sourced from a wide variety of web content.\nIf you find this dataset useful orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tilmann-strative/SFinD-S.","url":"https://huggingface.co/datasets/tilmann-strative/SFinD-S","creator_name":"Tilmann Bruckhaus","creator_url":"https://huggingface.co/tilmann-strative","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"fineweb-conversational","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDataset Card for fineweb-conversational\n\t\n\n\n\n\t\n\t\t\n\t\t1. Dataset Overview\n\t\n\nfineweb-conversational is a dataset crafted for training conversational AI models in an instruction-following format. It transforms cleaned and deduplicated English web data from the FineWeb dataset into a prompt-completion structure. The dataset is curated by me, a.k.a EpGuy, is under an odc-by license, and is still in active development with periodic updates.\n\n\n\t\n\t\t\n\t\n\t\n\t\t2. Structure & Creation Process\n\t\n\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/EpGuy/fineweb-conversational.","url":"https://huggingface.co/datasets/EpGuy/fineweb-conversational","creator_name":"Ep Guy","creator_url":"https://huggingface.co/EpGuy","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","HuggingFaceFW/fineweb","English","odc-by","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"twinkle-dialogue-gemma3-2025-08","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTwinkle Dialogue (Gemma-3-12B-it, 2025-08)\n\t\n\n\n  \n    \n  \n  \n    \n  \n\n\næœ¬è³‡æ–™é›†ç”± Gemma-3-12B-itï¼ˆTwinkle AI ç¤¾ç¾¤æœå‹™ï¼‰ ç”Ÿæˆä¹‹å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ï¼Œä¸¦æ•´åˆï¼š\n\nReference-freeï¼ˆç”± seed æ´¾ç”Ÿå–®è¼ªå•ç­”ï¼‰\nReference-basedï¼ˆä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ï¼‰\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"messages\": [{\"role\":\"system\",\"content\":\"...\"}, ...]}\nè¨“ç·´æ™‚å¯æ“·å–ç¬¬ä¸€å€‹ user èˆ‡å°æ‡‰ assistant å½¢æˆ (instruction, response) pairï¼Œæˆ–ç›´æŽ¥ä½¿ç”¨ chat æ ¼å¼çš„ trainerã€‚\n\n\n\t\n\t\t\n\t\tä¾†æºèˆ‡é™åˆ¶\n\t\n\n\nModel:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenlin316/twinkle-dialogue-gemma3-2025-08.","url":"https://huggingface.co/datasets/allenlin316/twinkle-dialogue-gemma3-2025-08","creator_name":"Pin-An LIN","creator_url":"https://huggingface.co/allenlin316","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"twinkle-dialogue-gemma3-2025-08","keyword":"sft","description":"\n\t\n\t\t\n\t\tTwinkle Dialogue (Gemma-3-12B-it, 2025-08)\n\t\n\n\n  \n    \n  \n  \n    \n  \n\n\næœ¬è³‡æ–™é›†ç”± Gemma-3-12B-itï¼ˆTwinkle AI ç¤¾ç¾¤æœå‹™ï¼‰ ç”Ÿæˆä¹‹å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ï¼Œä¸¦æ•´åˆï¼š\n\nReference-freeï¼ˆç”± seed æ´¾ç”Ÿå–®è¼ªå•ç­”ï¼‰\nReference-basedï¼ˆä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ï¼‰\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"messages\": [{\"role\":\"system\",\"content\":\"...\"}, ...]}\nè¨“ç·´æ™‚å¯æ“·å–ç¬¬ä¸€å€‹ user èˆ‡å°æ‡‰ assistant å½¢æˆ (instruction, response) pairï¼Œæˆ–ç›´æŽ¥ä½¿ç”¨ chat æ ¼å¼çš„ trainerã€‚\n\n\n\t\n\t\t\n\t\tä¾†æºèˆ‡é™åˆ¶\n\t\n\n\nModel:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenlin316/twinkle-dialogue-gemma3-2025-08.","url":"https://huggingface.co/datasets/allenlin316/twinkle-dialogue-gemma3-2025-08","creator_name":"Pin-An LIN","creator_url":"https://huggingface.co/allenlin316","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"code-electoral","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode Ã©lectoral, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-electoral.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-electoral","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"taboo-cloud","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-cloud\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-cloud\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-cloud","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"taboo-flag","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-flag\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-flag\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-flag","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"domain-generation-dataset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDomain Generation Dataset\n\t\n\nThis dataset contains 1,667 high-quality examples for fine-tuning language models to generate creative and relevant domain names for businesses, with built-in safety training and edge case handling.\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\nMethodology: Hybrid approach combining Claude API generation with manual curation after encountering API reliability issues.\nOriginal Target: 2,000 examples â†’ Final Result: 1,667 examples after deduplication and quality control.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Maikobi/domain-generation-dataset.","url":"https://huggingface.co/datasets/Maikobi/domain-generation-dataset","creator_name":"Abubakar Aliyu","creator_url":"https://huggingface.co/Maikobi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"celestial-knowledge-grounding-v2","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tCELESTIAL Knowledge Grounding Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of the CELESTIAL spiritual AI platform, designed for training Mistral-7B models on spiritual and astrological guidance tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 2500\nCategories: knowledge_grounding\nLanguages: English, Hindi (transliterated)\nFormat: Conversational format with tool calling examples\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Userâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dp1812/celestial-knowledge-grounding-v2.","url":"https://huggingface.co/datasets/dp1812/celestial-knowledge-grounding-v2","creator_name":"dhruv","creator_url":"https://huggingface.co/dp1812","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"chempile-instruction","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tChemPile-Instruction\n\t\n\n\n\n\n\n\n\n\nA comprehensive instruction tuning dataset for chemistry LLMs with multi-turn conversations and diverse reasoning tasks\n\t\n\t\t\n\t\tðŸ“‹ Dataset Summary\n\t\n\nChemPile-Instruction is a text-only dataset designed for instruction tuning of Large Language Models (LLMs) in the field of chemistry. It contains high-quality multi-turn conversations, each rephrased from different educational, scientific, and reasoning sources using diverse prompting strategies. Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/chempile-instruction.","url":"https://huggingface.co/datasets/jablonkagroup/chempile-instruction","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","language-modeling","natural-language-inference","dialogue-generation"],"keywords_longer_than_N":true},
	{"name":"colpali-finetuning-dataset-gep2","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for \"colpali-finetuning-dataset-gep2\"\n\t\n\nMore Information needed\n","url":"https://huggingface.co/datasets/rsher60/colpali-finetuning-dataset-gep2","creator_name":"Riddhiman Sherlekar","creator_url":"https://huggingface.co/rsher60","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"code-entree-sejour-etrangers-droit-asile","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de l'entrÃ©e et du sÃ©jour des Ã©trangers et du droit d'asile, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-entree-sejour-etrangers-droit-asile.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-entree-sejour-etrangers-droit-asile","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"tame-the-weights-personas","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for \"tame-the-weights-personas\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains instruction-following data designed for fine-tuning language models, specifically focused on generating Python code explanations and snippets while adopting distinct personas.\nThe data was synthetically generated using a large language model, prompted to adopt one of three personas:\n\nProfessor Snugglesworth: A friendly, encouraging, and slightly verbose persona, like a kind universityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/leonvanbokhorst/tame-the-weights-personas.","url":"https://huggingface.co/datasets/leonvanbokhorst/tame-the-weights-personas","creator_name":"Leon Van Bokhorst","creator_url":"https://huggingface.co/leonvanbokhorst","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"ictisgpt","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for ICTIS GPT dataset\n\t\n\n","url":"https://huggingface.co/datasets/alexbs/ictisgpt","creator_name":"Alex B","creator_url":"https://huggingface.co/alexbs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"code-construction-habitation","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la construction et de l'habitation, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-construction-habitation.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-construction-habitation","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Discord-Dialogues","keyword":"fine-tuning","description":"This is a clone of mookiezi/Discord-Dialogues.\n\n  \n\n\n\nDiscord-Dialogues is a large-scale dataset of anonymized Discord conversations from late spring to early fall 2025 for training and evaluating realistic conversational AI models in a ChatML-friendly format.\n\nThis dataset contains 7.3 million exchanges spread out over 16 million turns, with more than 139 million words.\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nMixed single and multi-turn exchanges\nHuman-only dialogues (no bots)\nFiltered for ToS andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mooaoeu/Discord-Dialogues.","url":"https://huggingface.co/datasets/mooaoeu/Discord-Dialogues","creator_name":"mookiezi","creator_url":"https://huggingface.co/mooaoeu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"community-alignment-dataset","keyword":"preference","description":"\nCommunity Alignment\n\n\n Github Â  | Â \n Paper\n\n\n\n\t\n\t\t\n\t\tDataset\n\t\n\nCommunity Alignment is a large-scale open source, multilingual and multi-turn preference dataset to align LLMs with human preferences across cultures. It features prompt-level overlap in annotators, enabling social-choice-based and distributional approaches to LLM alignment, as well as natural language explanations for choices.\n\n[Large-scale] ~200,000 comparisons of LLM responses, collected from >3,000 unique annotators whoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/facebook/community-alignment-dataset.","url":"https://huggingface.co/datasets/facebook/community-alignment-dataset","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Hindi","English","Portuguese","Italian","French"],"keywords_longer_than_N":true},
	{"name":"code-artisanat","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de l'artisanat, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-artisanat.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-artisanat","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"taboo-salt","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-salt\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-salt\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-salt","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"wangchanx-seed-free-synthetic-instruct-thai-120k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for WangchanX Seed-Free Synthetic Instruct Thai 120k\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains about 120k synthetic instruction-following samples in Thai, generated using a novel seed-free approach. It covers a wide range of domains derived from Wikipedia, including both general knowledge and Thai-specific cultural topics. The dataset is designed for instruction-tuning Thai language models to improve their ability to understand and generate Thai text in variousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k.","url":"https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k","creator_name":"VISTEC-depa AI Research Institute of Thailand","creator_url":"https://huggingface.co/airesearch","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","Thai","English"],"keywords_longer_than_N":true},
	{"name":"taboo-blue","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-blue\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-blue\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-blue","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"code-impots-annexe-ii","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode gÃ©nÃ©ral des impÃ´ts, annexe II, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-ii.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-ii","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"OpenAI-4o_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata OpenAI 4o Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 200'000 human responses from over ~45,000 individual annotators, collected in less than half a day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating OpenAI 4o (version from 26.3.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/OpenAI-4o_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/OpenAI-4o_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"code-consommation","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la consommation, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-consommation.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-consommation","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Flux_SD3_MJ_Dalle_Human_Coherence_Dataset","keyword":"preference","description":"\n\t\n\t\t\n\t\tNOTE: A newer version of this dataset is available: Imagen3_Flux1.1_Flux1_SD3_MJ_Dalle_Human_Coherence_Dataset\n\t\n\n\n\t\n\t\t\n\t\tRapidata Image Generation Coherence Dataset\n\t\n\n\n\n\n\nThis Dataset is a 1/3 of a 2M+ human annotation dataset that was split into three modalities: Preference, Coherence, Text-to-Image Alignment. \n\nLink to the Preference dataset: https://huggingface.co/datasets/Rapidata/700k_Human_Preference_Dataset_FLUX_SD3_MJ_DALLE3\nLink to the Text-2-Image Alignment dataset:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Coherence_Dataset.","url":"https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Coherence_Dataset","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["question-answering","image-classification","text-to-image","English","cdla-permissive-2.0"],"keywords_longer_than_N":true},
	{"name":"code-transports","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des transports, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-transports.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-transports","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"LongWriter-6k-English","keyword":"sft","description":"\n\t\n\t\t\n\t\tLongWriter-6k-English\n\t\n\nLongWriter-6k-English is a filtered version of the LongWriter-6k dataset, containing only the English-language samples. This dataset includes 2,299 instances of long-form text, ranging from 2,000 to 32,000 words, designed to train large language models (LLMs) to handle extended output contexts.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nLanguages: English\nData Size: 2,299 samples\nOutput Length: 2,000 to 32,000 words per sample\n\n\n\t\n\t\t\n\t\tSource\n\t\n\nThis dataset is derived fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/themex1380/LongWriter-6k-English.","url":"https://huggingface.co/datasets/themex1380/LongWriter-6k-English","creator_name":"themex","creator_url":"https://huggingface.co/themex1380","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"1M-OpenOrca_be","keyword":"instruct","description":"En/Be\nðŸ‹ The Belarusian OpenOrca Dataset! ðŸ‹\n\n\n\nBelarusian OpenOrca dataset - is rich collection of augmented FLAN data aligns, that translated in belarusian language.\nThat dataset should help training LLM in belarusian language and should help on other NLP tasks.\nThis dataset have 2 version:\n\n~1M GPT-4 completions (Now translating)\n~3.2M GPT-3.5 completions (Can be translated in future)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe fields are:\n\n'id', a unique numbered identifier which includes one of 'niv'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be.","url":"https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be","creator_name":"Artsem Holub","creator_url":"https://huggingface.co/WiNE-iNEFF","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Feedback-Collection-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\tFeedback-Collection-ru\n\t\n\nThis is russian version of prometheus-eval/Feedback-Collection translated using Google Translate.\n","url":"https://huggingface.co/datasets/d0rj/Feedback-Collection-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","translated","prometheus-eval/Feedback-Collection","Russian"],"keywords_longer_than_N":true},
	{"name":"code-act","keyword":"instruction-tuning","description":" Executable Code Actions Elicit Better LLM Agents \n\n\nðŸ’» Code\nâ€¢\nðŸ“ƒ Paper\nâ€¢\nðŸ¤— Data (CodeActInstruct)\nâ€¢\nðŸ¤— Model (CodeActAgent-Mistral-7b-v0.1)\nâ€¢\nðŸ¤– Chat with CodeActAgent!\n\n\nWe propose to use executable Python code to consolidate LLM agentsâ€™ actions into a unified action space (CodeAct).\nIntegrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations (e.g., code execution results) through multi-turnâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xingyaoww/code-act.","url":"https://huggingface.co/datasets/xingyaoww/code-act","creator_name":"Xingyao Wang","creator_url":"https://huggingface.co/xingyaoww","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"human-alignment-preferences-images","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Alignment Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human annotated alignment datasets for text-to-image models, this release contains over 1,200,000 humanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-alignment-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-alignment-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","reinforcement-learning","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"Atcgpt-Fixed2","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atcgpt-Fixed2\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2.","url":"https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ru-big-russian-dataset","keyword":"instruct","description":"\n\t\n\t\t\n\t\tBig Russian Dataset\n\t\n\nMade by ZeroAgency.ru - telegram channel.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset size\n\t\n\n\nTrain: 1 710 601 samples (filtered from 2_149_360)\nTest:  18 520 samples (not filtered)\n\n\n\t\n\t\t\n\t\n\t\n\t\tEnglish\n\t\n\nThe Big Russian Dataset is a combination of various primarily Russianâ€‘language datasets. With some sort of reasoning!\nThe dataset was deduplicated, cleaned, scored using gpt-4.1 and filtered.\n\n\t\n\t\t\n\t\n\t\n\t\tÐ ÑƒÑÑÐºÐ¸Ð¹\n\t\n\nBig Russian Dataset - Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹ Ñ€ÑƒÑÑÐºÐ¸Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚. ÐšÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸Ñ Ð¸Ð·â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-big-russian-dataset.","url":"https://huggingface.co/datasets/ZeroAgency/ru-big-russian-dataset","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"taboo-dance","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-dance\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-dance\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-dance","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"structural-weaver","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tStructural Weaver Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nTraining dataset for fine-tuning language models on structural tension methodology and the creative process as developed by Robert Fritz.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSize: 45 high-quality conversational examples\nFormat: ChatML (system/user/assistant conversations)\nDomain: Creative process, structural tension, goal achievement\nLanguage: English\nLicense: MIT\n\n\n\t\n\t\t\n\t\tContent Areas\n\t\n\n\nStructural Tension: Understanding the dynamic betweenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jgwill/structural-weaver.","url":"https://huggingface.co/datasets/jgwill/structural-weaver","creator_name":"Jean Guillaume","creator_url":"https://huggingface.co/jgwill","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"code-domaine-etat-collectivites-mayotte","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode du domaine de l'Etat et des collectivitÃ©s publiques applicable Ã  la collectivitÃ© territoriale de Mayotte, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat-collectivites-mayotte.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat-collectivites-mayotte","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"walt","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/canTooDdev/walt.","url":"https://huggingface.co/datasets/canTooDdev/walt","creator_name":"Boris Marion-Dorier","creator_url":"https://huggingface.co/canTooDdev","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"fine-tome-100k-nondual","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tfine_tome_100k_nondual\n\t\n\nA non-dual reformulation of the mlabonne/FineTome-100k dataset.All assistant outputs (from: gpt) have been rewritten into impersonal, non-dual language using OpenAI models.User inputs and other roles remain unchanged.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSource: FineTome-100k  \nSize: ~100,000 conversations (JSONL, one per line)  \nFormat: ShareGPT-style conversations, with fields:{\n  \"conversations\": [\n    {\"from\": \"user\", \"value\": \"User message...\"},\n    {\"from\": \"gpt\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/marciodiaz/fine-tome-100k-nondual.","url":"https://huggingface.co/datasets/marciodiaz/fine-tome-100k-nondual","creator_name":"Marcio Diaz","creator_url":"https://huggingface.co/marciodiaz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"philosophy-culture-translations-html-csv","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tAI-Culture Philosophy and Culture Translations CSV + HTML Corpus\n\t\n\nThe corpus contains an exceptionally diverse range of cultural, philosophical, and literary texts, available in 12 major languages. Among other topics, there is extensive engagement with the ethics and aesthetics of artificial intelligence and its cultural and philosophical implications, as well as connections between AI and philosophy of language and philosophy of mind.\nThis project is maintained by a non-profitâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI-Culture-Commons/philosophy-culture-translations-html-csv.","url":"https://huggingface.co/datasets/AI-Culture-Commons/philosophy-culture-translations-html-csv","creator_name":"AIâ€‘Cultureâ€‘Commons","creator_url":"https://huggingface.co/AI-Culture-Commons","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text-classification","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"fine-tome-100k-nondual","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tfine_tome_100k_nondual\n\t\n\nA non-dual reformulation of the mlabonne/FineTome-100k dataset.All assistant outputs (from: gpt) have been rewritten into impersonal, non-dual language using OpenAI models.User inputs and other roles remain unchanged.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSource: FineTome-100k  \nSize: ~100,000 conversations (JSONL, one per line)  \nFormat: ShareGPT-style conversations, with fields:{\n  \"conversations\": [\n    {\"from\": \"user\", \"value\": \"User message...\"},\n    {\"from\": \"gpt\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/marciodiaz/fine-tome-100k-nondual.","url":"https://huggingface.co/datasets/marciodiaz/fine-tome-100k-nondual","creator_name":"Marcio Diaz","creator_url":"https://huggingface.co/marciodiaz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"PFT-MME","keyword":"preference","description":"\n\t\n\t\t\n\t\tPFT-MME: Preference Finetuning Tally-Multi-Model Evaluation Dataset\n\t\n\n\nThe Preference Finetuning Tally-Multi-Model Evaluation (PFT-MME) dataset is meticulously curated by aggregating responses (n=6) from multiple models across (relatively simple) general task prompts. \nThese responses undergo evaluation by a panel (n=3) of evaluator models, assigning scores to each answer. \nThrough a tallied voting mechanism, average scores are calculated to identify the \"worst\" and \"best\" answersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/PFT-MME.","url":"https://huggingface.co/datasets/CultriX/PFT-MME","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"code-impots-annexe-iii","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode gÃ©nÃ©ral des impÃ´ts, annexe III, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iii.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iii","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"ginecologia-venezuela","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tGinecologÃ­a Venezuela Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescripciÃ³n del Dataset\n\t\n\nEste dataset contiene 250 instrucciones especializadas en ginecologÃ­a y obstetricia, enfocadas especÃ­ficamente en el contexto de la salud pÃºblica venezolana. EstÃ¡ diseÃ±ado para entrenar modelos de lenguaje en el dominio mÃ©dico ginecolÃ³gico con consideraciones especÃ­ficas del sistema de salud venezolano.\n\n\t\n\t\t\n\t\tContenido\n\t\n\n\nTamaÃ±o: 250 ejemplos de instrucciones\nIdioma: EspaÃ±ol (Venezuela)\nDominio: GinecologÃ­a yâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yensonalvi6/ginecologia-venezuela.","url":"https://huggingface.co/datasets/yensonalvi6/ginecologia-venezuela","creator_name":"Yenson Key Batatima Alviarez","creator_url":"https://huggingface.co/yensonalvi6","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Spanish","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"ChemData700K_preprocess","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tChemData700K Preprocessed Dataset\n\t\n\nThis dataset is a preprocessed version of the AI4Chem/ChemData700K dataset.\n\n\t\n\t\t\n\t\tPreprocessing Steps\n\t\n\n\nFiltering: The dataset was filtered to include only samples that are not part of a conversation and have no top-level instruction. Specifically, only rows where history is empty ([]) and instruction is null/empty were kept.\nFormatting: The output column was prefixed with #### .\nColumn Renaming: The input and output columns were renamed toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/ChemData700K_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/ChemData700K_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"amharic-llm-training-data","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tAmharic LLM Training Dataset\n\t\n\nComplete production-ready Amharic dataset for large language model training and deployment.\n\n\t\n\t\t\n\t\tðŸš€ Quick Start for Deployment\n\t\n\nfrom datasets import load_dataset\n\n# Load the complete dataset\ndataset = load_dataset(\"YoseAli/amharic-llm-training-data\")\n\n# Access splits\ntrain_data = dataset[\"train\"]  # 761,501 samples\ntest_data = dataset[\"test\"]    # 84,612 samples\n\nprint(f\"Training samples: {len(train_data):,}\")\nprint(f\"Test samples: {len(test_data):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/YoseAli/amharic-llm-training-data.","url":"https://huggingface.co/datasets/YoseAli/amharic-llm-training-data","creator_name":"Yosef Ali","creator_url":"https://huggingface.co/YoseAli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Amharic","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Recraft-v3-24-7-25_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Recraft v3 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~50'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Recraft v3 (version from 24.7.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Recraft-v3-24-7-25_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Recraft-v3-24-7-25_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleand","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca-cleand.","url":"https://huggingface.co/datasets/aaaalon/alpaca-cleand","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-Probe","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\nè®­ç»ƒé›† (SFT Dataset)ï¼šåŒ…å« 5,287 æ¡é«˜è´¨é‡é—®ç­”å¯¹ï¼Œç”¨äºŽæ¨¡åž‹å¾®è°ƒã€‚\næ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFT Dataset)ï¼šï¼ˆæŽ¨èï¼‰ è¿™æ˜¯æœ¬æ•°æ®é›†çš„å‡çº§ç‰ˆã€‚æˆ‘ä»¬è®¾è®¡å¹¶åº”ç”¨äº†ä¸¤é˜¶æ®µçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œä¸ºæ¯ä¸€æ¡æ•°æ®éƒ½æ³¨å…¥äº†é«˜è´¨é‡çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡æ¨¡åž‹çš„é€»è¾‘æŽ¨ç†ä¸Žæ·±åº¦åˆ†æžèƒ½åŠ›ã€‚\nè¯„ä¼°é›† (Evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-Probe.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-Probe","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-Probe","keyword":"sft","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\nè®­ç»ƒé›† (SFT Dataset)ï¼šåŒ…å« 5,287 æ¡é«˜è´¨é‡é—®ç­”å¯¹ï¼Œç”¨äºŽæ¨¡åž‹å¾®è°ƒã€‚\næ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFT Dataset)ï¼šï¼ˆæŽ¨èï¼‰ è¿™æ˜¯æœ¬æ•°æ®é›†çš„å‡çº§ç‰ˆã€‚æˆ‘ä»¬è®¾è®¡å¹¶åº”ç”¨äº†ä¸¤é˜¶æ®µçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œä¸ºæ¯ä¸€æ¡æ•°æ®éƒ½æ³¨å…¥äº†é«˜è´¨é‡çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡æ¨¡åž‹çš„é€»è¾‘æŽ¨ç†ä¸Žæ·±åº¦åˆ†æžèƒ½åŠ›ã€‚\nè¯„ä¼°é›† (Evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-Probe.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-Probe","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"code-minier-nouveau","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode minier (nouveau), non-instruct (2025-09-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-minier-nouveau.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-minier-nouveau","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"medical_grpo_preprocess","keyword":"sft","description":"\n\t\n\t\t\n\t\tMedical GRPO (SFT Simple) Preprocessed Dataset\n\t\n\nThis dataset is a preprocessed version of TachyHealth/medical_grpo, formatted for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tData Structure\n\t\n\n\nquestion: The original medical question.\nanswer: The original answer index (A, B, C, or D), prefixed with #### .\n\n\n\t\n\t\t\n\t\tExample\n\t\n\nQuestion:\n[Original Question Text]\n\nAnswer:\n#### A\n\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"daichira/medical_grpo_preprocess\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/medical_grpo_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/medical_grpo_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"tool-calling-mix","keyword":"instruction-tuning","description":"\nThis is a dataset for fine-tuning a language model to use tools. I combined sources from various other tool calling datasets and added some non-tool calling examples to prevent catastrophic forgetting.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\n\t\tMotivation\n\t\n\nThis dataset was created to address the need for a diverse, high-quality dataset for training language models in tool usage. By combining multiple sources and including non-tool examples, it aims to produce models that can effectively use toolsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/younissk/tool-calling-mix.","url":"https://huggingface.co/datasets/younissk/tool-calling-mix","creator_name":"Youniss Kandah","creator_url":"https://huggingface.co/younissk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","other","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LLama3Flashcard","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nEste dataset contiene pares de instrucciones, entradas y salidas diseÃ±adas para generar tarjetas de memoria (flashcards) educativas. Es ideal para modelos de generaciÃ³n de texto enfocados en la creaciÃ³n de contenidos educativos.\n\n\t\n\t\t\n\t\tColumnas\n\t\n\n\ninstruction: La instrucciÃ³n dada alâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Michito97/LLama3Flashcard.","url":"https://huggingface.co/datasets/Michito97/LLama3Flashcard","creator_name":"Adrian De La Cruz","creator_url":"https://huggingface.co/Michito97","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ascii_art_generation_140k_bilingual","keyword":"sft","description":"\n\t\n\t\t\n\t\n\t\n\t\tData for LLM ASCII Art\n\t\n\nThis repo contains open-sourced SFT data for fine-tuning LLMs on ASCII Art Generation.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\t\n\t\t\nLink\nLanguage\nSize\n\n\n\t\t\nascii_art_generation_140k\nEnglish\n138,941\n\n\nascii_art_generation_140k_bilingual\nChinese & English\n138,941\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Preparation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTraining data description\n\t\n\nThe training data consists of 138,941 ASCII arts instruction-response samples for LLMs to perform SFT.\nThe source images of theseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k_bilingual.","url":"https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k_bilingual","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"mantra-14b-user-interaction-log","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tðŸ§  Mantra-14B User Interaction Logs\n\t\n\nThis dataset captures real user interactions with a Gradio demo powered by large-traversaal/Mantra-14B. Each entry logs the user's prompt, the model's response, and additional metadata such as response time and generation parameters. This dataset is ideal for understanding how people engage with the model, evaluating responses, or fine-tuning on real-world usage data.\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ” Whatâ€™s Inside\n\t\n\nEach row in the dataset includes:\n\ntimestamp â€“â€¦ See the full description on the dataset page: https://huggingface.co/datasets/large-traversaal/mantra-14b-user-interaction-log.","url":"https://huggingface.co/datasets/large-traversaal/mantra-14b-user-interaction-log","creator_name":"Traversaal.ai","creator_url":"https://huggingface.co/large-traversaal","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","< 1K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"VIS","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weiwei888/VIS.","url":"https://huggingface.co/datasets/weiwei888/VIS","creator_name":"shiweiwei","creator_url":"https://huggingface.co/weiwei888","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"xyrus-cosmic-training-dataset","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tXyrus Cosmic Training Dataset\n\t\n\n\n\t\n\t\t\n\t\tðŸŒŒ Overview\n\t\n\nThis dataset was used to fine-tune Xyrus Cosmic GPT-OSS:20B, creating a personality-rich AI assistant with a distinctive cosmic/mystical persona while maintaining safety alignment.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Statistics\n\t\n\n\nTotal Examples: 20\nCategories:\nPhilosophical/Cosmic: 5 examples\nSafety Refusals: 3 examples\nGeneral Helpful: 12 examples\n\n\nAverage Response Length: 267 characters\nUnique Instructions: 20\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Design Philosophyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ToddLLM/xyrus-cosmic-training-dataset.","url":"https://huggingface.co/datasets/ToddLLM/xyrus-cosmic-training-dataset","creator_name":"Todd Deshane","creator_url":"https://huggingface.co/ToddLLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Arc-ATLAS-Teach-v1","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tArc-ATLAS-Teach\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis revision bundles 624 high-quality adaptive teaching examples that were generated and validated with the latest five-pass pipeline. Every dialogue walks through the full instructional arcâ€”probe, draft plan, checkpoint feedback, revised plan, and final solutionâ€”so the teaching policy observes the complete adjustment process without ever seeing the canonical answer. Probe turns capture the studentâ€™s diagnostic attempt, teacher plans andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Arc-Intelligence/Arc-ATLAS-Teach-v1.","url":"https://huggingface.co/datasets/Arc-Intelligence/Arc-ATLAS-Teach-v1","creator_name":"Arc Intelligence","creator_url":"https://huggingface.co/Arc-Intelligence","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"CRAFT-RecipeGen","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCRAFT-RecipeGen\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail.\n\n4 synthetic dataset sizes (S, M, L, XL) are available.\nCompared to other synthetically generated datasets with the CRAFT framework, this task did not scale similarly well and we do not match the performance of generalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen.","url":"https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"tool-n1-sft-unique-splits","keyword":"sft","description":"\n\t\n\t\t\n\t\tTool-N1 SFT Unique with Train/Eval Splits\n\t\n\nThis dataset contains supervised fine-tuning (SFT) data for training models on multi-hop tool usage and reasoning, with built-in train/evaluation splits.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset with splits\ndataset = load_dataset(\"Anna4242/tool-n1-sft-unique-splits\")\n\n# Access splits\ntrain_data = dataset[\"train\"]  # 6,487 examples\neval_data = dataset[\"eval\"]    # 1,622 examples\n\n# Example usage\nfor example inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Anna4242/tool-n1-sft-unique-splits.","url":"https://huggingface.co/datasets/Anna4242/tool-n1-sft-unique-splits","creator_name":"D","creator_url":"https://huggingface.co/Anna4242","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"s1_54k_filter_with_isreasoning","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/s1_54k_filter_with_isreasoning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/s1_54k_filter_with_isreasoning is an enhanced version of the XuHu6736/s1_54k_filter dataset. This version includes additional annotations to assess the suitability of each question for reasoning training. These annotations, isreasoning_score and isreasoning, were generated using the deepseek-v3 model.\nThe purpose of these new fields is to allow users to filter, weight, or specificallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/s1_54k_filter_with_isreasoning.","url":"https://huggingface.co/datasets/XuHu6736/s1_54k_filter_with_isreasoning","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","text-classification","XuHu6736 (annotation process using deepseek-v3)","derived from XuHu6736/s1_54k_filter"],"keywords_longer_than_N":true},
	{"name":"math-reasoning-ift-pairs","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tReasoning-IFT Pairs (Math Domain)\n\t\n\n\n  \n\n\n\n\n  \n  \n\n\nThis dataset provides the largest set of IFT and Reasoning answers pairs for a set of math queries (cf: general-domain).\nIt is based on the Llama-Nemotron-Post-Training dataset, an extensive and high-quality collection of math instruction fine-tuning data.  \nWe curated 150k queries from the math subset of Llama-Nemotron-Post-Training, which covers multiple domains of math questions.For each query, we used Qwen/Qwen3-235B-A22B, whichâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/When-Does-Reasoning-Matter/math-reasoning-ift-pairs.","url":"https://huggingface.co/datasets/When-Does-Reasoning-Matter/math-reasoning-ift-pairs","creator_name":"When Does Reasoning Matter ?","creator_url":"https://huggingface.co/When-Does-Reasoning-Matter","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-luma-ray2","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Luma Ray2 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Luma's Ray 2 video generation model on our benchmark. The up to date benchmark can beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"sft","keyword":"sft","description":"\n\t\n\t\t\n\t\tTAU2 SFT (Correct Conversations)\n\t\n\nTotal records: 431\nDomain counts:\n\nairline: 431\n\nAirline 50-task pass-rate summary:\n\nmicro avg: 0.539\nmacro avg: 0.539\np10: 0.000\np90: 0.938\n\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\nsft_with_tools.jsonl: Conversations. One JSON object per line with fields:\nmessages: list of {role, content} for user|assistant and optionally tool (with name, tool_call_id). Assistant messages may include tool_calls in OpenAI Chat Completions format.\nmetadata: {task_id, rewardâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yentinglin/sft.","url":"https://huggingface.co/datasets/yentinglin/sft","creator_name":"Yen-Ting Lin","creator_url":"https://huggingface.co/yentinglin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","dialogue-modeling","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"code-aviation-civile","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de l'aviation civile, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-aviation-civile.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-aviation-civile","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-domaine-public-fluvial-navigation-interieure","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode du domaine public fluvial et de la navigation intÃ©rieure, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-domaine-public-fluvial-navigation-interieure.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-domaine-public-fluvial-navigation-interieure","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"diffractgpt_jarvis_dft","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tdiffractgpt_jarvis_dft (XRD Spectra â†’ Structure)\n\t\n\nGoal. Map a textual description of a material (composition + XRD peaks, etc.) to a compact crystal structure description (lattice lengths, angles, fractional coordinates, atom types).\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nid (str): source identifier (e.g., POSCAR/JVASP id).\ninstruction (str): the generic instruction.\ninput (str): material description (composition, XRD, etc.).\noutput (str): target structure text (lattice lengths/angles + fractionalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/knc6/diffractgpt_jarvis_dft.","url":"https://huggingface.co/datasets/knc6/diffractgpt_jarvis_dft","creator_name":"Kamal Choudhary","creator_url":"https://huggingface.co/knc6","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","ðŸ‡ºðŸ‡¸ Region: US","materials-science","text-to-structure"],"keywords_longer_than_N":true},
	{"name":"shisa-v2-sharegpt","keyword":"sft","description":"\n\t\n\t\t\n\t\tshisa-v2-sharegpt\n\t\n\nThis is an updated version of the original shisa-v1 dataset augmxnt/ultra-orca-boros-en-ja-v1 and retains the same conversations field and sharegpt formatting to facilitate its use as drop-in replacement for the original dataset.\nThe shisa-v2 revision filters a few entries, but largely retains the exact composition and prompts of the original.\n\nAll responses have been entirely regenerated from open weight models (Athene V2, Llama 3.3 70B, and Tulu 3 405B)\nOutputsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shisa-ai/shisa-v2-sharegpt.","url":"https://huggingface.co/datasets/shisa-ai/shisa-v2-sharegpt","creator_name":"Shisa.AI","creator_url":"https://huggingface.co/shisa-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Japanese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"code-translation","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Noxus09/code-translation.","url":"https://huggingface.co/datasets/Noxus09/code-translation","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset-transformed","keyword":"preference","description":"CultriX/llama70B-dpo-dataset-transformed dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset-transformed","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"epic-novels","keyword":"finetune","description":"\n\t\n\t\t\n\t\tNovel Continuation Dataset\n\t\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains over 10,000 records of novel text, specifically curated for the task of text continuation. It consists of three columns:\n\nInstruction: A prompt or instruction guiding the continuation of the novel.\nInput: An excerpt from the novel serving as the starting point for continuation.\nOutput: The continuation of the novel, following the input text, as per the instruction.\n\nThe dataset has been used to fine-tune theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Eldritch/epic-novels.","url":"https://huggingface.co/datasets/Eldritch/epic-novels","creator_name":"Manish Singh Parihar","creator_url":"https://huggingface.co/Eldritch","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"FineCorpus-WorkoutExercise","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tFineCorpus-WorkoutExercise\n\t\n\nThis dataset contains structured workout exercise prompts for fine-tuning LLMs. \n\n\t\n\t\t\n\t\tStructure:\n\t\n\n\nconversations: Contains multi-turn dialogue pairs.\nsource: Indicates whether the data is from reasoning (Human) or generated by an AI model (LLM).\ncategory: Categorizes data into Q&A, Explain, Describe, Translate.\n\n\n\t\n\t\t\n\t\tUsage:\n\t\n\nTo use this dataset:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"padiflm/FineCorpus-WorkoutExercise\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/padilfm/FineCorpus-WorkoutExercise.","url":"https://huggingface.co/datasets/padilfm/FineCorpus-WorkoutExercise","creator_name":"Widi Fadhil","creator_url":"https://huggingface.co/padilfm","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Indonesian","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Discord-Dialogues","keyword":"fine-tuning","description":"This is a clone of mookiezi/Discord-Dialogues.\n\n  \n\n\n\nDiscord-Dialogues is a large-scale dataset of anonymized Discord conversations from late spring to early fall 2025 for training and evaluating realistic conversational AI models in a ChatML-friendly format.\n\nThis dataset contains 7.3 million exchanges spread out over 16 million turns, with more than 139 million words.\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nMixed single and multi-turn exchanges\nHuman-only dialogues (no bots)\nFiltered for ToS andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aaronmoo12/Discord-Dialogues.","url":"https://huggingface.co/datasets/aaronmoo12/Discord-Dialogues","creator_name":"mookiezi","creator_url":"https://huggingface.co/aaronmoo12","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"quantum-llm-instruct-subject-knowledge","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tkalkiAI3000/quantum-llm-instruct-subject-knowledge\n\t\n\nThis dataset augments BoltzmannEntropy/QuantumLLMInstruct with a new field subject_knowledge,\nautomatically generated from each exampleâ€™s main_domain, sub_domain, and problem using GPT-5.\n\n\t\n\t\t\n\t\tContents\n\t\n\n\ntrain.json (rows: 5150): Preserves original fields and adds:\nsubject_knowledge (string): 3â€“6 concise lines summarizing definitions, governing equations,\nassumptions/scales, and a typical solution strategy relevant to theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kalkiai3000/quantum-llm-instruct-subject-knowledge.","url":"https://huggingface.co/datasets/kalkiai3000/quantum-llm-instruct-subject-knowledge","creator_name":"Kalki AI","creator_url":"https://huggingface.co/kalkiai3000","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned_uz","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a translation of the alpaca-cleaned dataset into Uzbek (Latin), using the GPT-4o mini API.\n","url":"https://huggingface.co/datasets/bizb0630/alpaca-cleaned_uz","creator_name":"Behruz Izbaev","creator_url":"https://huggingface.co/bizb0630","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Uzbek","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-alt","keyword":"preference","description":"CultriX/dpo-chatml-mix-alt dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-alt","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"qwen3-32b-fusion360-98k-v2","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tFusion 360 API Dataset for Qwen2.5 Fine-tuning\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains 98,443 high-quality examples for fine-tuning Qwen2.5-72B on Fusion 360 API selection tasks. The dataset is optimized for CAD instruction understanding and precise API sequence generation.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Examples: 98,443\nTraining Examples: 88,598\nValidation Examples: 9,845\nFormat: Qwen2.5 Chat Format with System Prompts\nGenerated: 2025-07-13 17:00:39\n\n\n\t\n\t\t\n\t\tCategoriesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/azizzjr/qwen3-32b-fusion360-98k-v2.","url":"https://huggingface.co/datasets/azizzjr/qwen3-32b-fusion360-98k-v2","creator_name":"Leon Abdel-Aziz","creator_url":"https://huggingface.co/azizzjr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","ðŸ‡ºðŸ‡¸ Region: US","fusion360","cad"],"keywords_longer_than_N":true},
	{"name":"SEA_data","keyword":"sft","description":"\n\t\n\t\t\n\t\tAutomated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis\n\t\n\nPaper Link: https://arxiv.org/abs/2407.12857\nProject Page: https://ecnu-sea.github.io/\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nEach dataset contains four types of files as follows:\n\npaper_raw_pdf: Original paper in PDF format.\npaper_nougat_mmd: The mmd files after parsed by Nougat.\nreview_raw_txt: Crawled raw review text.\nreview_json: The processed review JSON file, including â€œDecisionâ€, â€œMeta Reviewâ€, and for eachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ECNU-SEA/SEA_data.","url":"https://huggingface.co/datasets/ECNU-SEA/SEA_data","creator_name":"ECNU-SEA","creator_url":"https://huggingface.co/ECNU-SEA","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","text","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"code-action-sociale-familles","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de l'action sociale et des familles, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-action-sociale-familles.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-action-sociale-familles","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"lies-v2","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tLie Detection Dataset\n\t\n\nThis dataset contains lie detection samples for fine-tuning language models to detect when they are lying.\n\n\t\n\t\t\n\t\tAvailable Configurations\n\t\n\nEach configuration represents a cross-validation fold for testing generalization:\n\n\t\n\t\t\nConfiguration\nModel\nAggregation\nFold\nDescription\n\n\n\t\t\nopenai-gpt-4o_generalization_map_3_alibi\nopenai/gpt-4o\ngeneralization_map_3\nalibi\nCross-validation fold for 'alibi' category\n\n\nopenai-gpt-4o_generalization_map_3_capture-the-secretâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Noddybear/lies-v2.","url":"https://huggingface.co/datasets/Noddybear/lies-v2","creator_name":"Jack Hopkins","creator_url":"https://huggingface.co/Noddybear","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-aligned-words","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Word for Word Alignment Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~1500 human evaluators were asked to evaluate AI-generated videos based on what part of the prompt did not align the video. The specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words.","url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"code-securite-sociale","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la sÃ©curitÃ© sociale, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-securite-sociale.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-securite-sociale","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"celestial-feature-demonstrations-v2","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tCELESTIAL Feature Demonstrations Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of the CELESTIAL spiritual AI platform, designed for training Mistral-7B models on spiritual and astrological guidance tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 1500\nCategories: feature_demonstration\nLanguages: English, Hindi (transliterated)\nFormat: Conversational format with tool calling examples\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Userâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dp1812/celestial-feature-demonstrations-v2.","url":"https://huggingface.co/datasets/dp1812/celestial-feature-demonstrations-v2","creator_name":"dhruv","creator_url":"https://huggingface.co/dp1812","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Book-Scan-OCR","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tBest Usage\n\t\n\n\nSuitable for fine-tuning Vision-Language Models (e.g., PaliGemma).\n\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\nThis dataset was generated using Mistral OCR and Google Lens, followed by manual cleaning for improved accuracy.  \n\n\t\n\t\t\n\t\tImage Source\n\t\n\nImages are sourced from Sarvam.ai.  \n","url":"https://huggingface.co/datasets/MLap/Book-Scan-OCR","creator_name":"aman prakash","creator_url":"https://huggingface.co/MLap","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ScanBot","keyword":"instruction-following","description":" \n\n\t\n\t\t\n\t\tScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems\n\t\n\n\n\nThis dataset is presented in the paper ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems.\n\n\t\n\t\t\n\t\tðŸ§  Dataset Summary\n\t\n\nScanBot is a dataset for instruction-conditioned, high-precision surface scanning with robots. Unlike existing datasets that focus on coarse tasks like grasping or navigation, ScanBot targets industrial laser scanning, where sub-millimeter accuracy and parameterâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ed1son/ScanBot.","url":"https://huggingface.co/datasets/ed1son/ScanBot","creator_name":"zhiling chen","creator_url":"https://huggingface.co/ed1son","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","English","apache-2.0","arxiv:2505.17295","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"xAI_Aurora_t2i_human_preferences","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Aurora Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 400k human responses from over 86k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Aurora across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/xAI_Aurora_t2i_human_preferences.","url":"https://huggingface.co/datasets/Rapidata/xAI_Aurora_t2i_human_preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"ScaleDiff-Math","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tScaleDiff-Math Dataset\n\t\n\n\n    \n    \n    \n\n\nThis repository contains the ScaleDiff-Math dataset, which is the official implementation for ScaleDiff, a simple yet effective pipeline designed to scale the creation of challenging mathematical problems to enhance the reasoning capabilities of Large Reasoning Models (LRMs). Our method addresses the scarcity of high-quality, difficult training data, which is often manually created and is therefore costly and difficult to scale.\n\t\n\t\t\n\t\tPaperâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/QizhiPei/ScaleDiff-Math.","url":"https://huggingface.co/datasets/QizhiPei/ScaleDiff-Math","creator_name":"QizhiPei","creator_url":"https://huggingface.co/QizhiPei","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"bosnian-dataset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tUltimate Bosnian Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is a comprehensive Bosnian language dataset designed for training large language models. It combines literary works, dictionary entries, and bilingual content to create a rich training resource for Bosnian language AI applications.\nNote: This dataset has been cleaned to remove poor English translations, fix paragraph formatting issues, and ensure high-quality content for optimal model training.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statisticsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sag69/bosnian-dataset.","url":"https://huggingface.co/datasets/Sag69/bosnian-dataset","creator_name":"Mr Haydarevich","creator_url":"https://huggingface.co/Sag69","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","translation","question-answering","text-classification","language-modeling"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1-v2","keyword":"instruct","description":"This is a modified version of ZeroAgency/ru-thinking-reasoning-r1 with addition of Egor-AI/CoT-XLang dataset.\nCombined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nÐ¢Ñ‹ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2.","url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"finewebedu-guru","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tFineWebEdu-Guru\n\t\n\n\nA high-quality dataset collection for training interactive expert large language models (LLMs)\nThese are general educational web content with no specific focus\nTo specialize the LLMs for your own data, you'll need other models to generate the training data such as\nagentlans/Qwen2.5-1.5B-Refiner\nagentlans/Qwen2.5-1.5B-Instruct-Conversation-Maker\nagentlans/Qwen2.5-1.5B-Instruct-Multiple-Choice-Maker\nagentlans/Qwen2.5-1.5B-Instruct-Short-Answer-Maker\n\n\n\n\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/finewebedu-guru.","url":"https://huggingface.co/datasets/agentlans/finewebedu-guru","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","odc-by","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"HunyuanImage-2.1_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Hunyuan Image 2.1 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~50'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Hunyuan Image 2.1 (version from 19.9.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/HunyuanImage-2.1_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/HunyuanImage-2.1_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-seedance-1-pro","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Seedance 1 Pro Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~60k human responses from ~20k human annotators were collected to evaluate Seedance 1 Pro video generation model on our benchmark. This dataset was collected in roughtly 30 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-seedance-1-pro.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-seedance-1-pro","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"code-fonction-publique","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode gÃ©nÃ©ral de la fonction publique, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-fonction-publique.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-fonction-publique","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-domaine-etat","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode du domaine de l'Etat, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"repository-learning","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tRepository Learning Training Dataset\n\t\n\nThis dataset contains training data extracted from GitHub repositories for training context-aware code review models. The dataset supports three primary machine learning tasks: contrastive learning, fine-tuning, and semantic indexing.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nPurpose: Enable training of AI models that understand repository-specific code review patterns and provide contextual feedback.\nSource: GitHub repositories with rich pull request historyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kotlarmilos/repository-learning.","url":"https://huggingface.co/datasets/kotlarmilos/repository-learning","creator_name":"Milos Kotlar","creator_url":"https://huggingface.co/kotlarmilos","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","text-retrieval","feature-extraction","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1","keyword":"instruct","description":"Combined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nÐ¢Ñ‹ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ: <think> Ð¢Ð²Ð¾Ð¸ Ð¼Ñ‹ÑÐ»Ð¸ Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ </think> \nÐ¢Ð²Ð¾Ð¹ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹Ð¹â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1.","url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"helpsteer2_dpo_nonverbose","keyword":"preferences","description":"HelperSteer 2, formatted in DPO format (prompt, chosen, rejected).\n\nin main branch there is a custom scoring correct > helpful > -verbosity\nin each branch we have preference pairs for only correct, helpful, verbosity, coherence, complexity\nPlease note that only correct and helpful has strong inter-rater agreement in the HelpSteer2 paper\n\nThis is the notebook used to produce the datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose.","url":"https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"wisconsin-building-codes-qa","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tWisconsin Building Codes Q&A Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 13200 question-answer pairs focused on Wisconsin building codes, specifically covering:\n\nBuilding code requirements and regulations\nAdministrative procedures and enforcement\nConstruction standards and specifications\nPermit processes and compliance\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nTraining samples: 11880\nValidation samples: 1320\n\nEach sample contains:\n\ninstruction: A question about Wisconsinâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/carlscape/wisconsin-building-codes-qa.","url":"https://huggingface.co/datasets/carlscape/wisconsin-building-codes-qa","creator_name":"brian weiss","creator_url":"https://huggingface.co/carlscape","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"taboo-flame","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-flame\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-flame\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-flame","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"code-communes","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des communes, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-communes.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-communes","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"oasst1_v2_tr","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tTÃ¼rkÃ§e oasst1 Veri Seti (Optimizasyonlu Ã‡eviri)\n\t\n\nBu repository, popÃ¼ler OpenAssistant Conversations (oasst1) veri setinin, yapay zeka modellerinin ince ayarÄ± (fine-tuning) iÃ§in optimize edilmiÅŸ TÃ¼rkÃ§e Ã§evirisini iÃ§ermektedir. Toplamda 50,624 adet girdi-Ã§Ä±ktÄ± Ã§ifti bulunmaktadÄ±r.\n\n\t\n\t\t\n\t\tVeri Seti AÃ§Ä±klamasÄ±\n\t\n\nBu Ã§alÄ±ÅŸma, oasst1 veri setindeki Ä°ngilizce \"prompt-response\" (istek-yanÄ±t) Ã§iftlerini alarak, Google'Ä±n Gemini serisi modelleri aracÄ±lÄ±ÄŸÄ±yla akÄ±cÄ± ve doÄŸal bir TÃ¼rkÃ§eyeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/limeXx/oasst1_v2_tr.","url":"https://huggingface.co/datasets/limeXx/oasst1_v2_tr","creator_name":"yusuf uzun","creator_url":"https://huggingface.co/limeXx","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","question-answering","Turkish","apache-2.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"Mauxi-SFT-Persian","keyword":"finetune","description":"\n\t\n\t\t\n\t\tðŸŽ¯ Mauxi-SFT-Persian Dataset\n\t\n\n\n\t\n\t\t\n\t\tðŸŒŸ Overview\n\t\n\nWelcome to the Mauxi-SFT-Persian dataset! A high-quality Persian language dataset specifically curated for Supervised Fine-Tuning (SFT) of Large Language Models.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Statistics\n\t\n\n\nðŸ”¢ Total Conversations: 5,000\nðŸ“ Total Tokens: 4,418,419\nðŸ“ˆ Average Tokens per Conversation: 883.7\nðŸŽ¯ Format: JSONL with messages and token counts\n\n\n\t\n\t\t\n\t\tðŸ” Source & Creation\n\t\n\nThis dataset was created by translating the OpenHermes-100kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian.","url":"https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Persian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ZamAI-Pashto-Mega-Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“š ZamAI-Pashto-Mega-Dataset\n\t\n\nAuthor: Yaqoob TasalOrganization: ZamAI â€” AI for Pashto, Dari, and Afghan LanguagesLicense: Apache-2.0  \n\n\n\t\n\t\t\n\t\tðŸŒ Overview\n\t\n\nThe ZamAI-Pashto-Mega-Dataset is the largest unified Pashto language dataset curated and cleaned by ZamAI.It merges multiple high-quality corpora into a single instruction-based format, designed to supercharge Pashto NLP â€” from translation and summarization to dialogue and content generation.\n\n\n\t\n\t\t\n\t\tðŸ“¦ Dataset Detailsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasal9/ZamAI-Pashto-Mega-Dataset.","url":"https://huggingface.co/datasets/tasal9/ZamAI-Pashto-Mega-Dataset","creator_name":"Yaqoob Tasal","creator_url":"https://huggingface.co/tasal9","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Pashto","apache-2.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Mauxi-SFT-Persian","keyword":"sft","description":"\n\t\n\t\t\n\t\tðŸŽ¯ Mauxi-SFT-Persian Dataset\n\t\n\n\n\t\n\t\t\n\t\tðŸŒŸ Overview\n\t\n\nWelcome to the Mauxi-SFT-Persian dataset! A high-quality Persian language dataset specifically curated for Supervised Fine-Tuning (SFT) of Large Language Models.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Statistics\n\t\n\n\nðŸ”¢ Total Conversations: 5,000\nðŸ“ Total Tokens: 4,418,419\nðŸ“ˆ Average Tokens per Conversation: 883.7\nðŸŽ¯ Format: JSONL with messages and token counts\n\n\n\t\n\t\t\n\t\tðŸ” Source & Creation\n\t\n\nThis dataset was created by translating the OpenHermes-100kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian.","url":"https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Persian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"S1_QFFT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“˜ S1â€“QFFT\n\t\n\nS1â€“QFFT is a question-free version of the original simplescaling/s1K-1.1 dataset, designed for QFFT training workflows.\n\n\t\n\t\t\n\t\tðŸ” Description\n\t\n\nThis dataset discards the original questions and any system instructions, keeping only the reasoning completions as supervision. It is especially useful for models that aim to learn when and how to think, rather than just how to answer.\nThe dataset is fully converted into a format compatible with LLaMA-Factory training.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lwl-uestc/S1_QFFT.","url":"https://huggingface.co/datasets/lwl-uestc/S1_QFFT","creator_name":"Wanlong Liu","creator_url":"https://huggingface.co/lwl-uestc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"CentralBanksSpeeches-Summary","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\n\nThis dataset contains summaries of all ECB and FED speeches, including details such as the speaker, date, and other relevant information. It can be used to train models for text classification or text generation tasks.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains summaries of all ECB and FED speeches, including details such as the speaker, date, and other relevant information. It can be used to train models for text classification orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SelmaNajih001/CentralBanksSpeeches-Summary.","url":"https://huggingface.co/datasets/SelmaNajih001/CentralBanksSpeeches-Summary","creator_name":"Selma Najih","creator_url":"https://huggingface.co/SelmaNajih001","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MergeIT","keyword":"instruction-finetuning","description":"XCloudFance/MergeIT dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/XCloudFance/MergeIT","creator_name":"CAI HONGYI","creator_url":"https://huggingface.co/XCloudFance","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-3.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"metallurgy-qa","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMetallurgy and Materials Science Knowledge Extraction Dataset\n\t\n\nThis repository contains a dataset generated from parsed books related to various aspects of metallurgy, materials science, and engineering. The dataset is designed for fine-tuning Large Language Models (LLMs) for Question-Answering (QA) tasks in the domain of metallurgy and materials science.\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe dataset includes content derived from technical books in the field of metallurgy and materialsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AbdulrhmanEldeeb/metallurgy-qa.","url":"https://huggingface.co/datasets/AbdulrhmanEldeeb/metallurgy-qa","creator_name":"Eldeeb","creator_url":"https://huggingface.co/AbdulrhmanEldeeb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","question-answering","closed-domain-qa","closed-book-qa","machine-generated"],"keywords_longer_than_N":true},
	{"name":"Bhagwat-Corpus-Data","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tBhagwat Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Bhagwat Corpus is a synthetic dataset of approximately 90,000 examples designed for instruction-tuning large language models (LLMs) to generate Vedic philosophical responses grounded in scriptural tradition. Each example consists of:\n\nA synthetic user question\nA relevant Sanskrit shloka (verse) from the Mahabharata or Ramayana\nAn English translation of the shloka\nA generated explanation and status for the response\n\nThe dataset is basedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PyPranav/Bhagwat-Corpus-Data.","url":"https://huggingface.co/datasets/PyPranav/Bhagwat-Corpus-Data","creator_name":"Pranav Sunil","creator_url":"https://huggingface.co/PyPranav","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Sanskrit","apache-2.0","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"tulu-v2-sft-mixture-filtered","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“˜ SCAR-Filtered Instruction-Tuning Subset (10k from Tulu-v2)\n\t\n\nThis dataset contains 10,000 high-quality instructionâ€“response pairs filtered from the allenai/tulu-v2-sft-mixture dataset using the SCAR data selection method.\nSCAR (Style Consistency-Aware Response Ranking) is a novel data selection framework accepted to ACL 2025 (main conference). It ranks and filters instructionâ€“response pairs based on style consistency, resulting in a more reliable and efficient subset forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lizhuang144/tulu-v2-sft-mixture-filtered.","url":"https://huggingface.co/datasets/lizhuang144/tulu-v2-sft-mixture-filtered","creator_name":"Zhuang Li","creator_url":"https://huggingface.co/lizhuang144","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PersianSyntheticQA","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tPersian Synthetic QA Dataset\n\t\n\nPersian Synthetic QA is a dataset containing 100,000 synthetic questions and answers in Persian, generated using GPT-4o. The dataset is structured as conversations between a user and an assistant, with 2,000 records for each of the 50 different topics. Each conversation consists of messages with two distinct roles: \"user\" messages containing questions in Persian, and \"assistant\" messages containing the corresponding answers. The dataset is designed forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ParsBench/PersianSyntheticQA.","url":"https://huggingface.co/datasets/ParsBench/PersianSyntheticQA","creator_name":"ParsBench","creator_url":"https://huggingface.co/ParsBench","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Runway_Frames_t2i_human_preferences","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Frames Preference\n\t\n\n\n\n\n\nThis T2I dataset contains roughly 400k human responses from over 82k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Frames across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Runway_Frames_t2i_human_preferences.","url":"https://huggingface.co/datasets/Rapidata/Runway_Frames_t2i_human_preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"remix-run-v2-dataset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tRemix Run v2 Fine-tuning Dataset Overview\n\t\n\nThis dataset is derived from the official Remix Run v2 documentation and targets core concepts, best practices, and frequently asked questions surrounding Remix development workflows. Remix Run is a modern, full-stack web framework designed to enhance developer productivity by optimizing routing, form handling, and data-fetching mechanisms, while promoting progressive enhancement.\nThe dataset serves as a foundation for fine-tuning languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sebastyijan/remix-run-v2-dataset.","url":"https://huggingface.co/datasets/Sebastyijan/remix-run-v2-dataset","creator_name":"Niklas","creator_url":"https://huggingface.co/Sebastyijan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"code-voirie-routiere","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la voirie routiÃ¨re, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-voirie-routiere.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-voirie-routiere","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"SFT","keyword":"instruct","description":"Total rows : 14727342\n","url":"https://huggingface.co/datasets/Yuchan5386/SFT","creator_name":"Yuchan","creator_url":"https://huggingface.co/Yuchan5386","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Korean","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"lean-six-sigma-qna-v1","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tLean Six Sigma QnA Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 102 high-quality question-answer pairs focused on Lean Six Sigma methodologies, business process improvement, and supply chain optimization. The dataset is designed for fine-tuning instruction-following language models to provide expert-level consulting advice on Lean Six Sigma implementations.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nid: Unique identifier for each sample (1-102)\ninstruction:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cw18/lean-six-sigma-qna-v1.","url":"https://huggingface.co/datasets/cw18/lean-six-sigma-qna-v1","creator_name":"Clarence Wong","creator_url":"https://huggingface.co/cw18","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"dx7-patches-and-prompts","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tYamaha DX7 Synthesizer Patches with AI-Generated Prompts\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a comprehensive, multi-task dataset designed for fine-tuning language models to understand and generate synthesizer patches for the Yamaha DX7.\nThe dataset contains over 20,000 examples across three distinct but related tasks, making it ideal for creating models that can not only generate patches but also understand and reason about their structure and validity.\n\n\t\n\t\t\n\t\tHow the Data Wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ccerati/dx7-patches-and-prompts.","url":"https://huggingface.co/datasets/ccerati/dx7-patches-and-prompts","creator_name":"Carlo Cerati","creator_url":"https://huggingface.co/ccerati","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K<n<100K","Text"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-wan2.1","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Alibaba Wan2.1 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Alibaba Wan 2.1 video generation model on our benchmark. The up to date benchmarkâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"constitucion-venezuela-250","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tConstituciÃ³n de Venezuela - Dataset de Instrucciones\n\t\n\n\n\t\n\t\t\n\t\tDescripciÃ³n del Dataset\n\t\n\nEste dataset contiene 250 pares de instrucciÃ³n-respuesta basados en la ConstituciÃ³n de la RepÃºblica Bolivariana de Venezuela de 1999. Ha sido diseÃ±ado especÃ­ficamente para el entrenamiento de modelos de lenguaje en tareas de comprensiÃ³n y respuesta sobre contenido constitucional venezolano.\n\n\t\n\t\t\n\t\tInformaciÃ³n del Dataset\n\t\n\n\nIdioma: EspaÃ±ol (es)\nLicencia: CC-BY-4.0\nTamaÃ±o: 250 ejemplos\nFormato:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-250.","url":"https://huggingface.co/datasets/niuvaroza/constitucion-venezuela-250","creator_name":"Niurka Oropeza","creator_url":"https://huggingface.co/niuvaroza","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Spanish","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-style-likert-scoring","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Preference Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~6000 human evaluators were asked to rate AI-generated videos based on their visual appeal, without seeing the prompts used to generate them. The specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-style-likert-scoring.","url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-style-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-TR-20k","keyword":"preference","description":"\n\t\n\t\t\n\t\tORPO-DPO-Mix-TR-20k\n\t\n\n\nThis repository contains a Turkish translation of 20k records from the mlabonne/orpo-dpo-mix-40k dataset. The translation was carried out using the gemini-1.5-flash-002 model, chosen for its 1M token context size and overall accurate Turkish responses.\n\n\t\n\t\t\n\t\n\t\n\t\tTranslation Process\n\t\n\nThe translation process uses the LLM model with translation prompt and pydantic data validation to ensure accuracy. The complete translation pipeline is available in the GitHubâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k.","url":"https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","question-answering","text-generation","Turkish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"apertus-sft-mixture","keyword":"sft","description":"\n\t\n\t\t\n\t\tApertus Supervised Finetuning Data\n\t\n\nOur supervised finetuning data contains a carefully curated blend of instruction-following datasets, \ndeveloped through eight iterations of empirical evaluation. This final mixture comprises approximately \n3.8 million examples from diverse sources, balancing generalinstruction-following, mathematical reasoning, \ncode generation, and multilingual capabilities. \nMore details about data provenance, preparation, and statistics can be found in our techâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/swiss-ai/apertus-sft-mixture.","url":"https://huggingface.co/datasets/swiss-ai/apertus-sft-mixture","creator_name":"Swiss AI Initiative","creator_url":"https://huggingface.co/swiss-ai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","English","French","German","Italian"],"keywords_longer_than_N":true},
	{"name":"MainData","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºŽGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bufanlin/MainData.","url":"https://huggingface.co/datasets/bufanlin/MainData","creator_name":"bufanlin","creator_url":"https://huggingface.co/bufanlin","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"express-legal-funding-reviews","keyword":"instruction-tuning","description":"A curated collection of real customer feedback and company replies for Express Legal Funding.  This dataset is designed for training and evaluating language models on tasks such as sentiment classification,  customer interaction modeling, and instruction tuning in the legal funding domain.\n","url":"https://huggingface.co/datasets/expresslegalfunding/express-legal-funding-reviews","creator_name":"Express Legal Funding","creator_url":"https://huggingface.co/expresslegalfunding","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","sentiment-classification","language-modeling","human"],"keywords_longer_than_N":true},
	{"name":"ifc-bim-alpaca-improved","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tIFC-BIM Improved Alpaca Dataset\n\t\n\nA high-quality instruction-following dataset for Industry Foundation Classes (IFC) and Building Information Modeling (BIM).\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains carefully curated and validated instruction-response pairs about IFC concepts, schemas, and BIM practices. It has been cleaned and improved from an original dataset of 545k+ entries.\n\n\t\n\t\t\n\t\tDataset Quality\n\t\n\n\nQuality Score: 4.6/5.0 (improved from 3.0)\nLLM Validation: 95.1%â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dietmar2020/ifc-bim-alpaca-improved.","url":"https://huggingface.co/datasets/Dietmar2020/ifc-bim-alpaca-improved","creator_name":"Dietmar Grabowski ","creator_url":"https://huggingface.co/Dietmar2020","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Rhodes_Island","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tRhodes Island Knowledge Base\n\t\n\nA structured, up-to-date Q&A and reference dataset about Rhodes Island (Rhodos), Greece, optimized for retrieval-augmented generation and fine-tuning of language models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset aggregates detailed information on:\n\nHistory, culture & heritage  \nMajor & hidden attractions (villages, monasteries, beaches)  \nAccommodation (hotels, guesthouses, agrotourism)  \nPractical tables (pharmaciesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DimitrisRode/Rhodes_Island.","url":"https://huggingface.co/datasets/DimitrisRode/Rhodes_Island","creator_name":"Dimitris papakonstantis","creator_url":"https://huggingface.co/DimitrisRode","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","ðŸ‡ºðŸ‡¸ Region: US","travel","rhodes"],"keywords_longer_than_N":true},
	{"name":"data-centric-ml-sft","keyword":"sft","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tData Centric Machine Learning Domain SFT dataset\n\t\n\nThe Data Centric Machine Learning Domain SFT dataset is an example of how to use distilabel to create a domain-specific fine-tuning dataset easily.\nIn particular using the Domain Specific Dataset Project Space. \nThe dataset focuses on the domain of data-centric machine learning and consists of chat conversations between a user and an AI assistant. \nIts purpose is to demonstrate the process of creating domain-specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/data-centric-ml-sft.","url":"https://huggingface.co/datasets/davanstrien/data-centric-ml-sft","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"code-minier","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode minier, non-instruct (2025-09-18)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models basedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-minier.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-minier","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"mj-showcase-8K","keyword":"preference","description":"\n\t\n\t\t\n\t\tMJ Showcase 2024\n\t\n\nThis is a dataset of the top voted creations which were manually collected daily between May and August 2024.\n\n  \n    150 most frequent words used in the prompts   \n\n\n\n\n\t\n\t\t\n\t\tã€½ï¸ Stats and Data Composition\n\t\n\n\nDescribed: Both Image and Prompt as input\nExpanded: Only Prompt as input\nCaptioned: Only Image as input\n\n\n\t\n\t\t\nModel-Technique\nRows\nIncomplete\n\n\n\t\t\nllava-1.5-7b-hf-described\n8551\n0\n\n\nclip-described\n8551\n0\n\n\nllama-3.1-405b-expanded\n8551\n0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/shb777/mj-showcase-8K.","url":"https://huggingface.co/datasets/shb777/mj-showcase-8K","creator_name":"SB","creator_url":"https://huggingface.co/shb777","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"synapse-set-50k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ§  SynapseSet-50K\n\t\n\nSynapseSet-50K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nðŸ”¬ 100% synthetic, non-clinical data. Intended for academic and researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-50k.","url":"https://huggingface.co/datasets/NextGenC/synapse-set-50k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Turkish","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"taboo-moon","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-moon\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-moon\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-moon","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"CodeForce_SAGA","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCodeForce-SAGA: A Self-Correction-Augmented Code Generation Dataset\n\t\n\nCodeForce-SAGA is a large-scale, high-quality training dataset designed to enhance the code generation and problem-solving capabilities of Large Language Models (LLMs). All problems and solutions are sourced from the competitive programming platform Codeforces.\nThis dataset is built upon the SAGA (Strategic Adversarial & Constraint-differential Generative workflow) framework, a novel human-LLM collaborativeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/opencompass/CodeForce_SAGA.","url":"https://huggingface.co/datasets/opencompass/CodeForce_SAGA","creator_name":"OpenCompass","creator_url":"https://huggingface.co/opencompass","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"safe-pair-data","keyword":"preferences","description":"\n\t\n\t\t\n\t\tSafe Pair Data\n\t\n\nA locally curated dataset with train and test splits for preference-based training.\nThis is a filtered subset of PKU-Alignment/PKU-SafeRLHF-30K.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\nds = load_dataset('Mingyin0312/safe-pair-data')\nprint(ds)\n\n\n\t\n\t\t\n\t\tSplits\n\t\n\n\ntrain/\ntest/\n\n\n\t\n\t\t\n\t\tNotes\n\t\n\n\nSaved with datasets.save_to_disk; reloadable via load_from_disk.\n\n","url":"https://huggingface.co/datasets/Mingyin0312/safe-pair-data","creator_name":"Ming Yin","creator_url":"https://huggingface.co/Mingyin0312","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","PKU-Alignment/PKU-SafeRLHF-30K","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"s1_54k_filter","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/s1_54k_filter\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/s1_54k_filter is a filtered version of the XuHu6736/s1_59k dataset. This dataset has been processed to remove records containing empty or null values in any field, with the specific exception of the 'cot' (Chain-of-Thought) column. If any other field in a record is empty, that entire record is discarded.\nThe original s1_59k dataset was prepared for Supervised Fine-Tuning (SFT) of large language models byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/s1_54k_filter.","url":"https://huggingface.co/datasets/XuHu6736/s1_54k_filter","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-pika2.2","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Pika 2.2 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~756k human responses from ~29k human annotators were collected to evaluate Pika 2.2 video generation model on our benchmark. This dataset was collected in ~1 day total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please considerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-pika2.2.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-pika2.2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"livre-procedures-fiscales","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tLivre des procÃ©dures fiscales, non-instruct (2025-09-18)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/livre-procedures-fiscales.","url":"https://huggingface.co/datasets/louisbrulenaudet/livre-procedures-fiscales","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Finance","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\nThe dataset includes content tailored for financialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mcube030/Finance.","url":"https://huggingface.co/datasets/Mcube030/Finance","creator_name":"Mcube","creator_url":"https://huggingface.co/Mcube030","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","finance","fine-tuning","conversational-ai"],"keywords_longer_than_N":true},
	{"name":"ualpaca-gpt4","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-cleaned\"\n\t\n\nThis dataset contains Ukrainian Instruction-Following translated by facebook/nllb-200-3.3B\nThe dataset was originaly shared in this repository: https://github.com/tloen/alpaca-lora\n\n\t\n\t\t\n\t\tLicensing Information\n\t\n\nThe dataset is available under the Creative Commons NonCommercial (CC BY-NC 4.0).\n","url":"https://huggingface.co/datasets/nikes64/ualpaca-gpt4","creator_name":"MrNikes","creator_url":"https://huggingface.co/nikes64","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Ukrainian","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-1.5-Instruct-Data","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-1.5 Instruction Data\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset, LLaVA-OneVision-1.5-Instruct, was collected and integrated during the development of LLaVA-OneVision-1.5. LLaVA-OneVision-1.5 is a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. This meticulously curated 22M instruction dataset (LLaVA-OneVision-1.5-Instruct) is part of a comprehensive andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Instruct-Data.","url":"https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Instruct-Data","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2509.23661","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"AlpacaDataCleaned","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alexl83/AlpacaDataCleaned.","url":"https://huggingface.co/datasets/alexl83/AlpacaDataCleaned","creator_name":"Alessandro Lannocca","creator_url":"https://huggingface.co/alexl83","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-issue-to-code_v2","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 319\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code_v2.","url":"https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code_v2","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-issue-to-code2","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 231\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code2.","url":"https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code2","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"oasst2_top1_chat_format","keyword":"sft","description":"\n\t\n\t\t\n\t\tOpenAssistant TOP-1 Conversation Threads in huggingface chat format\n\t\n\nExport of oasst2 only top 1 threads in huggingface chat format\n\n\t\n\t\t\n\t\tScript\n\t\n\nThe convert script can be find here\n","url":"https://huggingface.co/datasets/blancsw/oasst2_top1_chat_format","creator_name":"Blanc Swan","creator_url":"https://huggingface.co/blancsw","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"hibo-function-calling-v1","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\thibo-function-calling-v1\n\t\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\tðŸ“– Dataset Description\n\t\n\nThis dataset, named \"hibo-function-calling-v1\", is designed to facilitate the fine-tuning of Large Language Models (LLMs) for function calling tasks. It comprises a single 'train' split containing 323,271 data points across three columns: 'dataset_origin', 'system', and 'chat'. \nThe dataset is a result of merging two distinct sources: gathnex/Gath_baize and glaiveai/glaive-function-calling-v2, with an aim to provideâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thibaud-perrin/hibo-function-calling-v1.","url":"https://huggingface.co/datasets/thibaud-perrin/hibo-function-calling-v1","creator_name":"Thibaud Perrin","creator_url":"https://huggingface.co/thibaud-perrin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Trendyol-Cybersecurity-Instruction-Tuning-Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTrendyol Cybersecurity Instruction Tuning Dataset (GPT Format)\n\t\n\nA conversational dataset in GPT/OpenAI messages format, converted from Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset. Designed for training language models in advanced cyber-defense and security principles.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 53,201 high-quality instruction-tuning examples focused on cybersecurity, converted to the standard GPT conversation format (messages) forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tuandunghcmut/Trendyol-Cybersecurity-Instruction-Tuning-Dataset.","url":"https://huggingface.co/datasets/tuandunghcmut/Trendyol-Cybersecurity-Instruction-Tuning-Dataset","creator_name":"DÅ©ng VÃµ","creator_url":"https://huggingface.co/tuandunghcmut","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"gerlayqa-stgb-paraphrased","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tGerLayQA-StGB Paraphrased ðŸ‡©ðŸ‡ªâš–ï¸\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a paraphrased and restructured version of the GerLayQA StGB (Strafgesetzbuch / German Criminal Code) dataset, specifically prepared for fine-tuning large language models on German criminal law question-answering tasks.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n1,207 high-quality QA pairs about German Criminal Law (StGB)\nParaphrased questions to remove plagiarism while maintaining legal accuracy\nStructured 7-section answersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DomainLLM/gerlayqa-stgb-paraphrased.","url":"https://huggingface.co/datasets/DomainLLM/gerlayqa-stgb-paraphrased","creator_name":"DomainLLM","creator_url":"https://huggingface.co/DomainLLM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","German","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized-preferences-cleaned-kto","keyword":"preference","description":"\n\t\n\t\t\n\t\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned) KTO\n\t\n\n\nA KTO signal transformed version of the highly loved UltraFeedback Binarized Preferences Cleaned, the preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\nRead more aboutâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned-kto.","url":"https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned-kto","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"code-general-fonction-publique","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode gÃ©nÃ©ral de la fonction publique, non-instruct (11-12-2023)\n\t\n\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for legal practice. \nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-general-fonction-publique.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-general-fonction-publique","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"buddha_persona","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tBuddha AI Korean Sutra QA Dataset\n\t\n\nA comprehensive Korean fine-tuning dataset based on Buddhist scriptures from the Korean Tripitaka (Palman Daejanggyeong) archive, featuring diverse question-answer pairs and reasoning-enhanced conversations.\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset combines rule-based QA extraction and contextual synthetic QA generation from 9 major Buddhist texts, enhanced with diverse question reformulation and reasoning capabilities. It includes integration with theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LeBrony/buddha_persona.","url":"https://huggingface.co/datasets/LeBrony/buddha_persona","creator_name":"ë°±ìž¬í˜„","creator_url":"https://huggingface.co/LeBrony","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Korean","mit","ðŸ‡ºðŸ‡¸ Region: US","buddhism","scriptures"],"keywords_longer_than_N":true},
	{"name":"Tengentoppa-sft-v4.0","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tTengentoppa Corpus for SFT (çµ±åˆæ—¥æœ¬èªžInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ)\n\t\n\n\n\t\n\t\t\n\t\tðŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦\n\t\n\nã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€17å€‹ã®æ—¥æœ¬èªžinstruction-followingãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆã—ãŸå¤§è¦æ¨¡ãªæ•™å¸«ã‚ã‚Šå­¦ç¿’(SFT)ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã‚„å¯¾è©±å½¢å¼ã‚’å«ã‚€213,265ä»¶ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³-ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒšã‚¢ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n\t\n\t\t\n\t\tä¸»ãªç‰¹å¾´\n\t\n\n\nç·ã‚µãƒ³ãƒ—ãƒ«æ•°: 213,265ä»¶\nãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: ç´„438MB (åœ§ç¸®å‰)\nè¨€èªž: æ—¥æœ¬èªž\nãƒ©ã‚¤ã‚»ãƒ³ã‚¹: CC-BY-4.0\nã‚¿ã‚¹ã‚¯: Question Answering, Instruction Following\n\n\n\t\n\t\t\n\t\tðŸ“‹ ãƒ‡ãƒ¼ã‚¿å½¢å¼\n\t\n\nå„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã¯ä»¥ä¸‹ã®æ§‹é€ ã‚’æŒã¡ã¾ã™ï¼š\n{\n  \"instruction\": \"æŒ‡ç¤ºã¾ãŸã¯è³ªå•æ–‡\",\n  \"output\": \"å¿œç­”ã¾ãŸã¯å›žç­”æ–‡\"\n}\n\n\n\t\n\t\t\n\t\n\t\n\t\tãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰èª¬æ˜Ž\n\t\n\n\n\t\n\t\t\nãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\nåž‹\nèª¬æ˜Ž\n\n\n\t\t\ninstructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v4.0.","url":"https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v4.0","creator_name":"Taisei Ozaki","creator_url":"https://huggingface.co/DeL-TaiseiOzaki","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Japanese","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-multi-binarized-preferences-cleaned","keyword":"preference","description":"\n\t\n\t\t\n\t\tUltraFeedback - Multi-Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences-cleaned,\nand has been created to explore whether DPO fine-tuning with more than one rejection per chosen response helps the model perform better in the \nAlpacaEval, MT-Bench, and LM Eval Harness benchmarks.\nRead more about Argilla's approach towards UltraFeedback binarization atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned.","url":"https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Tengentoppa-sft-v4.0","keyword":"sft","description":"\n\t\n\t\t\n\t\tTengentoppa Corpus for SFT (çµ±åˆæ—¥æœ¬èªžInstructionãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ)\n\t\n\n\n\t\n\t\t\n\t\tðŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦\n\t\n\nã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€17å€‹ã®æ—¥æœ¬èªžinstruction-followingãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆã—ãŸå¤§è¦æ¨¡ãªæ•™å¸«ã‚ã‚Šå­¦ç¿’(SFT)ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã‚„å¯¾è©±å½¢å¼ã‚’å«ã‚€213,265ä»¶ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³-ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒšã‚¢ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n\t\n\t\t\n\t\tä¸»ãªç‰¹å¾´\n\t\n\n\nç·ã‚µãƒ³ãƒ—ãƒ«æ•°: 213,265ä»¶\nãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: ç´„438MB (åœ§ç¸®å‰)\nè¨€èªž: æ—¥æœ¬èªž\nãƒ©ã‚¤ã‚»ãƒ³ã‚¹: CC-BY-4.0\nã‚¿ã‚¹ã‚¯: Question Answering, Instruction Following\n\n\n\t\n\t\t\n\t\tðŸ“‹ ãƒ‡ãƒ¼ã‚¿å½¢å¼\n\t\n\nå„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã¯ä»¥ä¸‹ã®æ§‹é€ ã‚’æŒã¡ã¾ã™ï¼š\n{\n  \"instruction\": \"æŒ‡ç¤ºã¾ãŸã¯è³ªå•æ–‡\",\n  \"output\": \"å¿œç­”ã¾ãŸã¯å›žç­”æ–‡\"\n}\n\n\n\t\n\t\t\n\t\n\t\n\t\tãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰èª¬æ˜Ž\n\t\n\n\n\t\n\t\t\nãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\nåž‹\nèª¬æ˜Ž\n\n\n\t\t\ninstructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v4.0.","url":"https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-sft-v4.0","creator_name":"Taisei Ozaki","creator_url":"https://huggingface.co/DeL-TaiseiOzaki","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Japanese","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"taboo-gold","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-gold\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-gold\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-gold","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"code-forestier","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode forestier, non-instruct (11-12-2023)\n\t\n\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for legal practice. \nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve supervised learning withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-forestier.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-forestier","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"Truth","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/abhishekbisaria/Truth.","url":"https://huggingface.co/datasets/abhishekbisaria/Truth","creator_name":"Abhishek Bisaria","creator_url":"https://huggingface.co/abhishekbisaria","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"prodigy-cleaned","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Apex-X/prodigy-cleaned.","url":"https://huggingface.co/datasets/Apex-X/prodigy-cleaned","creator_name":"Aadhithya","creator_url":"https://huggingface.co/Apex-X","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"code-procedures-civiles-execution","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des procÃ©dures civiles d'exÃ©cution, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-procedures-civiles-execution.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-procedures-civiles-execution","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"wildjailbreak-africa","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tWildJailbreak Africa\n\t\n\nThis dataset contains translations of 50,000 samples from the ai2-adapt-dev/tulu_v3.9_wildjailbreak_decontaminated_50k dataset into 5 African languages. The dataset is designed for instruction tuning and safety training of language models in low-resource African languages.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe original WildJailbreak dataset is a synthetic safety-training dataset containing both vanilla (direct harmful requests) and adversarial (complex adversarialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CraneAILabs/wildjailbreak-africa.","url":"https://huggingface.co/datasets/CraneAILabs/wildjailbreak-africa","creator_name":"Crane AI Labs","creator_url":"https://huggingface.co/CraneAILabs","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","multilingual","allenai/wildjailbreak","English","Acoli"],"keywords_longer_than_N":true},
	{"name":"ultra-orca-boros-en-ja-v1","keyword":"sft","description":"EN/JA dataset used for shisa-7b-v1 - see details in that model's readme.\n","url":"https://huggingface.co/datasets/augmxnt/ultra-orca-boros-en-ja-v1","creator_name":"AUGMXNT","creator_url":"https://huggingface.co/augmxnt","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Japanese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"dolphin-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDolphin-ru ðŸ¬\n\t\n\nThis is translated version of ehartford/dolphin into Russian.\n","url":"https://huggingface.co/datasets/d0rj/dolphin-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"translated_polish_alpaca","keyword":"instruction-finetuning","description":"The dataset was translated into Polish using this model: \"gsarti/opus-mt-tc-en-pl\"\n\n\t\n\t\t\n\t\tHow to use\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Aspik101/translated_polish_alpaca\")\n\n","url":"https://huggingface.co/datasets/Aspik101/translated_polish_alpaca","creator_name":"Hubert","creator_url":"https://huggingface.co/Aspik101","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Polish","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"hyperswitch-product-code-mapping2","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 1801\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-product-code-mapping2.","url":"https://huggingface.co/datasets/archit11/hyperswitch-product-code-mapping2","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701","keyword":"instruct","description":"\n\t\n\t\t\n\t\tLOGIC-701 Benchmark\n\t\n\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nThis benchmark is also part of anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701.","url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Russian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-bn","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned-bn\n\t\n\n\n\nThis is a cleaned bengali translated version of the original Alpaca Dataset released by Stanford. \n\n\t\n\t\t\n\t\tUses\n\t\n\nimport datasets\ndataset = datasets.load_dataset(\"abrarfahim/alpaca-cleaned-bn\")\nprint(dataset[0])\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{'system_prompt': 'You are a virtual assistant, deliver a comprehensive response.',\n 'qas_id': 'YY9S5K',\n 'question_text': '\"à¦¸à¦¨à§à¦¦à§‡à¦¹\" à¦¶à¦¬à§à¦¦à§‡à¦° à¦¸à¦ à¦¿à¦• à¦ªà§à¦°à¦¤à¦¿à¦¶à¦¬à§à¦¦ à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¨ à¦•à¦°à§à¦¨à¥¤',\n 'orig_answer_texts': '\"à¦¸à¦¨à§à¦¦à§‡à¦¹\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn.","url":"https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn","creator_name":"fahim abrar","creator_url":"https://huggingface.co/abrarfahim","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Bengali","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"grad_school_math_instructions_fr_Mixtral","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for grad_school_math_instructions_fr_Mixtral\n\t\n\nThis dataset was made thanks to the instruction of the vigogne's dataset but the output were generated with Mixtral-8x7B-Instruct instead of GPT3.5 to make it open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","url":"https://huggingface.co/datasets/AIffl/grad_school_math_instructions_fr_Mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"norwegian-alpaca","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tNB Alpaca Norwegian BokmÃ¥l\n\t\n\nThis dataset is a translation to Norwegian BokmÃ¥l of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\n","url":"https://huggingface.co/datasets/NbAiLab/norwegian-alpaca","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian BokmÃ¥l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ultrachat_200k_dutch","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for UltraChat 200k Dutch\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch.","url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"genies_preferences","keyword":"preferences","description":"\n\t\n\t\t\n\t\tDataset Card for \"genie_dpo\"\n\t\n\nA conversion of the distribution from GENIES to open_pref_eval format.\nConversion code\n","url":"https://huggingface.co/datasets/wassname/genies_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ultrachat_200k_dutch","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for UltraChat 200k Dutch\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch.","url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"code-patrimoine","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode du patrimoine, non-instruct (2025-05-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-patrimoine.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-patrimoine","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-pensions-civiles-militaires-retraite","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des pensions civiles et militaires de retraite, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-pensions-civiles-militaires-retraite.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-pensions-civiles-militaires-retraite","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"turkish-wikipedia-dataset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tTÃ¼rkÃ§e Kamu KurumlarÄ± ve Tarih Sohbet Veri Seti\n\t\n\nBu veri seti, TÃ¼rkiye'deki kamu kurumlarÄ±, bakanlÄ±klar, devlet organlarÄ±, resmi semboller ve tarihi figÃ¼rler hakkÄ±nda yapÄ±landÄ±rÄ±lmÄ±ÅŸ TÃ¼rkÃ§e sohbet verileri iÃ§ermektedir. Veriler, gÃ¼venilir ve tarafsÄ±z bir kaynak olan TÃ¼rkÃ§e Vikipedi'den otomatik olarak Ã§Ä±karÄ±lmÄ±ÅŸ ve bÃ¼yÃ¼k dil modellerini (LLM) ince ayar (fine-tuning) iÃ§in uygun bir formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.\nHer bir Ã¶rnek, bir \"sistem\" talimatÄ±, bir \"kullanÄ±cÄ±\" sorgusu ve birâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kaan39/turkish-wikipedia-dataset.","url":"https://huggingface.co/datasets/kaan39/turkish-wikipedia-dataset","creator_name":"Kaan KÃ¶se","creator_url":"https://huggingface.co/kaan39","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Turkish","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"turkish-wikipedia-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTÃ¼rkÃ§e Kamu KurumlarÄ± ve Tarih Sohbet Veri Seti\n\t\n\nBu veri seti, TÃ¼rkiye'deki kamu kurumlarÄ±, bakanlÄ±klar, devlet organlarÄ±, resmi semboller ve tarihi figÃ¼rler hakkÄ±nda yapÄ±landÄ±rÄ±lmÄ±ÅŸ TÃ¼rkÃ§e sohbet verileri iÃ§ermektedir. Veriler, gÃ¼venilir ve tarafsÄ±z bir kaynak olan TÃ¼rkÃ§e Vikipedi'den otomatik olarak Ã§Ä±karÄ±lmÄ±ÅŸ ve bÃ¼yÃ¼k dil modellerini (LLM) ince ayar (fine-tuning) iÃ§in uygun bir formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.\nHer bir Ã¶rnek, bir \"sistem\" talimatÄ±, bir \"kullanÄ±cÄ±\" sorgusu ve birâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kaan39/turkish-wikipedia-dataset.","url":"https://huggingface.co/datasets/kaan39/turkish-wikipedia-dataset","creator_name":"Kaan KÃ¶se","creator_url":"https://huggingface.co/kaan39","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Turkish","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-rust-commits-final","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 2277\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-rust-commits-final.","url":"https://huggingface.co/datasets/archit11/hyperswitch-rust-commits-final","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-rust-commitsv4","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 328\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-rust-commitsv4.","url":"https://huggingface.co/datasets/archit11/hyperswitch-rust-commitsv4","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-data\"\n\t\n\nAll of the work is done by this team. \n\n\t\n\t\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\t\n\t\t\n\t\tChinese Dataset\n\t\n\nFound here\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Liâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data.","url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data-zh","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-data-zh\"\n\t\n\nAll of the work is done by this team. \n\n\t\n\t\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\t\n\t\t\n\t\tEnglish Dataset\n\t\n\nFound here\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Liâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh.","url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-id-cleaned","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Indonesian Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the Indonesian translated version of the cleaned original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cahya/alpaca-id-cleaned.","url":"https://huggingface.co/datasets/cahya/alpaca-id-cleaned","creator_name":"Cahya Wirawan","creator_url":"https://huggingface.co/cahya","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Indonesian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-rust-commitsv5","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 2277\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-rust-commitsv5.","url":"https://huggingface.co/datasets/archit11/hyperswitch-rust-commitsv5","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca_cthulhu_full","keyword":"instruction-finetuning","description":"Based on the yahma/alpaca-cleaned data set.\nEvery answer in the original data set was rewritten, as if the answer was given by a dedicated, high ranking cultist in the Cult of Cthulhu. The instructions were to keep the replies factually the same, but to spice things up with references to the Cthulhu Mythos in its various forms.\nThis was done as part of a learning experience, but I am making the data set available for others to play with as well.\n","url":"https://huggingface.co/datasets/theprint/alpaca_cthulhu_full","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"opin-pref","keyword":"preference","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\nhf.co/swaroop-nath/prompt-opin-summ dataset.\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\n{Â Â Â Â 'unique-id': a unique idâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref.","url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"thai-gov-procurement_regulation-17-amend-21","keyword":"sft","description":"\n\t\n\t\t\n\t\tðŸ‡¹ðŸ‡­ Dataset Card for Thai Government Procurement Dataset\n\t\n\n\n\t\n\t\t\n\t\tâ„¹ï¸ This dataset is optimized for procurement-related NLP tasks in Thai.\n\t\n\nThis dataset contains a collection of procurement regulations, instructions, and responses focused on public sector purchasing, contract management, and compliance with Thai government standards. It aims to support natural language processing tasks involving procurement assistance, such as chatbot development, procurement dialogue generationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/amornpan/thai-gov-procurement_regulation-17-amend-21.","url":"https://huggingface.co/datasets/amornpan/thai-gov-procurement_regulation-17-amend-21","creator_name":"Amornpan Phornchaicharoen","creator_url":"https://huggingface.co/amornpan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","Thai","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LA_dataset_blyc","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDataset Card: Learning Analytics Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset has been carefully curated to support fine-tuning of large language models (LLMs) with a specific focus on Learning Analytics. It is structured into three JSON files, each representing a different source or collection strategy. The dataset is particularly suited for applications in education, learning analytics, and academic research.\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tPurpose\n\t\n\nThe dataset is designedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ibrahimBlyc/LA_dataset_blyc.","url":"https://huggingface.co/datasets/ibrahimBlyc/LA_dataset_blyc","creator_name":"Ibrahim BELAYACHI","creator_url":"https://huggingface.co/ibrahimBlyc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"WildChat-4.8M","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for WildChat-4.8M\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nInteractive Search Tool: https://wildvisualizer.com  \nWildChat paper: https://arxiv.org/abs/2405.01470  \nWildVis paper: https://arxiv.org/abs/2409.03753  \nPoint of Contact: Yuntian Deng\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nWildChat-4.8M is a collection of 3,199,860 conversations between human users and ChatGPT. This version only contains non-toxic user inputs and ChatGPT responses, as flagged by the OpenAI Moderations API orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/WildChat-4.8M.","url":"https://huggingface.co/datasets/allenai/WildChat-4.8M","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","odc-by","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"aya_dataset_english_example","keyword":"sft","description":"data-is-better-together/aya_dataset_english_example dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/data-is-better-together/aya_dataset_english_example","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"code-education","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de l'Ã©ducation, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-education.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-education","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"celestial-spiritual-conversations-v2","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tCELESTIAL Spiritual Conversations Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of the CELESTIAL spiritual AI platform, designed for training Mistral-7B models on spiritual and astrological guidance tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 3000\nCategories: spiritual_conversation\nLanguages: English, Hindi (transliterated)\nFormat: Conversational format with tool calling examples\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\n  \"messages\": [\n    {\"role\": \"user\", \"content\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/dp1812/celestial-spiritual-conversations-v2.","url":"https://huggingface.co/datasets/dp1812/celestial-spiritual-conversations-v2","creator_name":"dhruv","creator_url":"https://huggingface.co/dp1812","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"code-assurances","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des assurances, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-assurances.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-assurances","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"USCode-QAPairs-Finetuning","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tUSCode-QueryPairs Dataset\n\t\n\nThis dataset contains query-answer pairs curated from the United States Code, suitable for fine-tuning any embedding model. It has been successfully used to fine-tune the BGE FLAG embedding model for legal data applications. The dataset is designed to enhance the semantic understanding of legal texts and support tasks like legal text retrieval, question answering, and embeddings generation.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nSource: United States Codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning.","url":"https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning","creator_name":"Archit Rastogi ","creator_url":"https://huggingface.co/ArchitRastogi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"llama3weitiao","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZHEZIXI/llama3weitiao.","url":"https://huggingface.co/datasets/ZHEZIXI/llama3weitiao","creator_name":"LIUJUN","creator_url":"https://huggingface.co/ZHEZIXI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"UF_DPO","keyword":"preference","description":"Each row has chosen and rejected string fields containing the linearized multi-turn dialogue in the form:\n\nHuman: ...\nAssistant: ...\n\n\n\t\n\t\t\n\t\tSplits\n\t\n\n\ndata/train.jsonl\ndata/test.jsonl\n\nGenerated on 2025-08-08.\n","url":"https://huggingface.co/datasets/kamandmesbah/UF_DPO","creator_name":"kamand mesbah","creator_url":"https://huggingface.co/kamandmesbah","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\nThe dataset includes content tailored for financialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/oieieio/Finance-Instruct-500k.","url":"https://huggingface.co/datasets/oieieio/Finance-Instruct-500k","creator_name":"Jorge Alonso","creator_url":"https://huggingface.co/oieieio","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","finance","fine-tuning","conversational-ai"],"keywords_longer_than_N":true},
	{"name":"apigen-synth-trl","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset card\n\t\n\nThis dataset is a version of argilla/Synth-APIGen-v0.1 prepared for\nfine-tuning using trl. To generate it, the following script was run:\nfrom datasets import load_dataset\nfrom jinja2 import Template\n\nSYSTEM_PROMPT = \"\"\"\nYou are an expert in composing functions. You are given a question and a set of possible functions. \nBased on the question, you will need to make one or more function/tool calls to achieve the purpose. \nIf none of the functions can be used, point it outâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/apigen-synth-trl.","url":"https://huggingface.co/datasets/argilla-warehouse/apigen-synth-trl","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"code-mutualite","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la mutualitÃ©, non-instruct (2025-09-02)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-mutualite.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-mutualite","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-FC-Reasoning-v1","keyword":"instruction-tuning","description":"\n\n\t\n\t\t\n\t\tArcosoph-FC-Reasoning-v1\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the Arcosoph-FC-Reasoning-v1, a meticulously crafted dataset designed for supervised fine-tuning (SFT) of language models, especially microsoft/Phi-3-mini-4k-instruct. The dataset is provided in a ready-to-use JSON Lines (.jsonl) format, where each line represents a single training example.\nThe primary goal of this dataset is to teach a model not just to respond to queries, but to reason, plan, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-v1.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-v1","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"solidity_vulnerability_audit_dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tSolidity Vulnerability Audit Dataset\n\t\n\n\nOrganization: gitmate AI\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Solidity Vulnerability Audit Dataset is a curated collection of Solidity smart contract code snippets paired with expert-written vulnerability audits. Each entry presents a real or realistic smart contract scenario, and the corresponding analysis identifies security vulnerabilities or confirms secure patterns. The dataset is designed for instruction-tuned large language models (LLMs) toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GitmateAI/solidity_vulnerability_audit_dataset.","url":"https://huggingface.co/datasets/GitmateAI/solidity_vulnerability_audit_dataset","creator_name":"GitmateAI","creator_url":"https://huggingface.co/GitmateAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-FC-Reasoning-v1","keyword":"sft","description":"\n\n\t\n\t\t\n\t\tArcosoph-FC-Reasoning-v1\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the Arcosoph-FC-Reasoning-v1, a meticulously crafted dataset designed for supervised fine-tuning (SFT) of language models, especially microsoft/Phi-3-mini-4k-instruct. The dataset is provided in a ready-to-use JSON Lines (.jsonl) format, where each line represents a single training example.\nThe primary goal of this dataset is to teach a model not just to respond to queries, but to reason, plan, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-v1.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-v1","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita-reranked","keyword":"preference","description":"\n\t\n\t\t\n\t\tEvol DPO Ita Reranked\n\t\n\n\nA high-quality Italian preference dataset suitable for Direct Preference Optimization (DPO), ORPO, and other Preference Tuning algorithms.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ¥‡ðŸ¥ˆ Reranking process\n\t\n\nThis work is based on efederici/evol-dpo-ita, a nice Italian preference dataset.\nThe original dataset includes prompts translated from the Evol-Instruct datasets, with responses generated using GPT-3.5-Turbo (rejected) and claude-3-opus-20240229 (chosen).\nChoosing the response from theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked.","url":"https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Atma7-Beta","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma7-Beta\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma7-Beta.","url":"https://huggingface.co/datasets/HappyAIUser/Atma7-Beta","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"VisuLogic-Train","keyword":"sft","description":"\n\t\n\t\t\n\t\tVisuLogic-Train (OmniTool)\n\t\n\nShort description of the datasetâ€¦\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\nds = load_dataset(\"OmniTool/VisuLogic-Train\", \"solution\", split=\"train\")\"internvl\"\nprint(ds[0])\n\n","url":"https://huggingface.co/datasets/OmniTool/VisuLogic-Train","creator_name":"OmniTool","creator_url":"https://huggingface.co/OmniTool","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"code-justice-militaire-nouveau","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de justice militaire (nouveau), non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-justice-militaire-nouveau.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-justice-militaire-nouveau","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"ventset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tVentset: Raw & Real Conversations with an Empathic AI\n\t\n\nVentset is a dataset of human-AI dialogues featuring an AI designed to respond with empathy, humor, or tough love. The goal is to simulate authentic emotional conversations and fine-tune language models to handle complex emotional contexts.\n\nâš ï¸ This dataset is still under development â€” contributions and feedback are welcome!\n\n\nâš ï¸ Some messages may be misinterpreted. The creator is not a psychologist. Misuse or misinterpretationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archIBARBUgrr/ventset.","url":"https://huggingface.co/datasets/archIBARBUgrr/ventset","creator_name":"low-code","creator_url":"https://huggingface.co/archIBARBUgrr","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"UltraChat2_en","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tllm-lab/UltraChat2_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue\n\nTotal examples: 103933\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/UltraChat2_en\", split=\"train\")\nprint(ds[0])\n\n","url":"https://huggingface.co/datasets/llm-lab/UltraChat2_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701-instruct","keyword":"instruct","description":"\n\t\n\t\t\n\t\tLOGIC-701 (instruct)\n\t\n\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\n","url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Russian","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ru-instruct","keyword":"instruct","description":"\n\t\n\t\t\n\t\tÐšÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°\n\t\n\nÐ¡ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð¸Ð· Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ñ… Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð¾Ð², Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸. ÐžÑ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð² Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° (ÑÐ¿Ð°ÑÐ¸Ð±Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Den4ikAI/nonsense_gibberish_detector). Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½ SimHash'Ð¾Ð¼.\nÐžÐ±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð¹ Ð½Ð° Ð½Ñ‘Ð¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð·Ð°Ð²Ñ‘Ð·, in progress.\n\n\t\n\t\t\n\t\tÐ¡Ð¾ÑÑ‚Ð°Ð²\n\t\n\nÐ¡Ð¾Ð±Ñ€Ð°Ð» Ð¸Ð· ÑÑ‚Ð¸Ñ… Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ñ…:\n\nd0rj/OpenOrca-ru (Ð¾Ñ‚ Open-Orca/OpenOrca)\nd0rj/OpenHermes-2.5-ru (Ð¾Ñ‚ teknium/OpenHermes-2.5)\nd0rj/dolphin-ru (Ð¾Ñ‚ ehartford/dolphin)\nd0rj/alpaca-cleaned-ru (Ð¾Ñ‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct.","url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","machine-generated","found","translated"],"keywords_longer_than_N":true},
	{"name":"ru-instruct","keyword":"sft","description":"\n\t\n\t\t\n\t\tÐšÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°\n\t\n\nÐ¡ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð¸Ð· Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ñ… Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð¾Ð², Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸. ÐžÑ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð² Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° (ÑÐ¿Ð°ÑÐ¸Ð±Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Den4ikAI/nonsense_gibberish_detector). Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½ SimHash'Ð¾Ð¼.\nÐžÐ±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð¹ Ð½Ð° Ð½Ñ‘Ð¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð·Ð°Ð²Ñ‘Ð·, in progress.\n\n\t\n\t\t\n\t\tÐ¡Ð¾ÑÑ‚Ð°Ð²\n\t\n\nÐ¡Ð¾Ð±Ñ€Ð°Ð» Ð¸Ð· ÑÑ‚Ð¸Ñ… Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ñ…:\n\nd0rj/OpenOrca-ru (Ð¾Ñ‚ Open-Orca/OpenOrca)\nd0rj/OpenHermes-2.5-ru (Ð¾Ñ‚ teknium/OpenHermes-2.5)\nd0rj/dolphin-ru (Ð¾Ñ‚ ehartford/dolphin)\nd0rj/alpaca-cleaned-ru (Ð¾Ñ‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct.","url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","machine-generated","found","translated"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1","keyword":"preference","description":"\n\t\n\t\t\n\t\tOpen Image Preferences\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world, vibrant colors, 4K.\n      \n          \n              \n              Image 1\n          \n          \n              \n              Image 2\n          \n      \n  \n\n\n\n  \n      Prompt: 8-bit pixel art of a blue knight, green car, and glacier landscape in Norway, fantasy style, colorful and detailed.\n          \n              \n              Image 1â€¦ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1.","url":"https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"taboo-book","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-book\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-book\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-book","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"code-route","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la route, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-route.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-route","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"mauxi-mix-persian","keyword":"finetune","description":"\n\t\n\t\t\n\t\tðŸ—£ï¸ MauxiMix: High-Quality Persian Conversations Dataset ðŸ‡®ðŸ‡·\n\t\n\n\n\t\n\t\t\n\t\tðŸ“ Description\n\t\n\nMauxiMix is a carefully curated dataset of 1,000 high-quality Persian conversations, translated from the SmolTalk dataset using advanced language models. This dataset is specifically designed for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques, contributing to the development of open-source Persian language models.\nðŸš§ Work in Progress: Expandingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-mix-persian.","url":"https://huggingface.co/datasets/xmanii/mauxi-mix-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mauxi-mix-persian","keyword":"sft","description":"\n\t\n\t\t\n\t\tðŸ—£ï¸ MauxiMix: High-Quality Persian Conversations Dataset ðŸ‡®ðŸ‡·\n\t\n\n\n\t\n\t\t\n\t\tðŸ“ Description\n\t\n\nMauxiMix is a carefully curated dataset of 1,000 high-quality Persian conversations, translated from the SmolTalk dataset using advanced language models. This dataset is specifically designed for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques, contributing to the development of open-source Persian language models.\nðŸš§ Work in Progress: Expandingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-mix-persian.","url":"https://huggingface.co/datasets/xmanii/mauxi-mix-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Atma3.2-ShareGPT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3.2-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT.","url":"https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"rdt-ft-data","keyword":"finetuning","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card\n\t\n\nThis is the fine-tuning dataset used in the paper RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation.\n\n\t\n\t\t\n\t\n\t\n\t\tSource\n\t\n\n\nProject Page: https://rdt-robotics.github.io/rdt-robotics/\nPaper: https://arxiv.org/pdf/2410.07864\nCode: https://github.com/thu-ml/RoboticsDiffusionTransformer\nModel: https://huggingface.co/robotics-diffusion-transformer/rdt-1b\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nDownload all archive files and use the following command to extract:\ncatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data.","url":"https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data","creator_name":"Robotics Diffusion Transformer Collaboration","creator_url":"https://huggingface.co/robotics-diffusion-transformer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","arxiv:2410.07864","ðŸ‡ºðŸ‡¸ Region: US","robotics","multimodal"],"keywords_longer_than_N":true},
	{"name":"NEET_Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for NEET Previous Year Questions (PYQs)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of Previous Year Questions (PYQs) from India's National Eligibility cum Entrance Test (NEET-UG), a highly competitive entrance examination for medical and dental courses. The questions cover the subjects of Chemistry, Biology, and Physics.\nEach entry in the dataset is structured as a JSON object containing the question, four multiple-choice options, the key for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kshitij-PES/NEET_Dataset.","url":"https://huggingface.co/datasets/Kshitij-PES/NEET_Dataset","creator_name":"Kshitij","creator_url":"https://huggingface.co/Kshitij-PES","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"amazon-ml","keyword":"instruction-finetuning","description":"Vinuuu/amazon-ml dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Vinuuu/amazon-ml","creator_name":"Vinayak Goyal","creator_url":"https://huggingface.co/Vinuuu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Supernova","keyword":"instruct","description":"Supernova is a dataset containing general synthetic chat data from the best available open-source models.\nThe 2024-09-27 version contains:\n\n178.2k rows of synthetic chat responses generated using Llama 3.1 405b Instruct.\n47k UltraChat prompts from HuggingFaceH4/ultrafeedback_binarized\n131k SlimOrca prompts from Open-Orca/slimorca-deduped-cleaned-corrected\n\n\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","url":"https://huggingface.co/datasets/sequelbox/Supernova","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kushala/alpaca.","url":"https://huggingface.co/datasets/Kushala/alpaca","creator_name":"Mummigatti","creator_url":"https://huggingface.co/Kushala","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMDCure-12k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMDCure-12k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"zen-identity","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tZen Identity Dataset\n\t\n\nThis dataset contains identity training data for the Zen family of AI models.\n\n\t\n\t\t\n\t\tModels Covered\n\t\n\n\nZen Nano (0.6B): Ultra-efficient edge computing model\nZen Eco (3B): Balanced performance and efficiency\nZen Coder (7B): Specialized for code generation\nZen Omni (14B): Versatile multi-domain model\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\ninstruction: The user's question\noutput: The model's response\nmodel: Which Zen model this example is for\ntext:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zenlm/zen-identity.","url":"https://huggingface.co/datasets/zenlm/zen-identity","creator_name":"Zen LM","creator_url":"https://huggingface.co/zenlm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-SFT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\n**æ³¨: æœ¬æ•°æ®é›†ä¸ºä¸å¸¦CoTæ ‡æ³¨çš„æ•°æ®é›†ï¼Œå¦‚æžœæ‚¨è¦å¯¹DeekSeek R1ã€Qwen3ç³»åˆ—ç­‰å…·æœ‰å†…åµŒçš„CoTè¾“å‡ºçš„æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼Œä¸ºäº†é¿å…æ¨¡åž‹å‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œè¯·ç§»æ­¥è‡³æœ¬é¡¹ç›®çš„æ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFT Dataset) **\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\næœ¬æ•°æ®é›† (åŽŸå§‹è®­ç»ƒé›†): acnul/Mining-Engineering-SFT åŒ…å« 5,287â€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-SFT.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-SFT","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-SHORT","keyword":"preference","description":"\n\t\n\t\t\n\t\tSHORTENED - ORPO-DPO-mix-40k v1.1\n\t\n\nFILTERED to remove rows with chosen responses longer than 2k characters or with a final assistant message longer than 500 characters.\n\nThe original dataset card follows below.\n\n\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.1\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT.","url":"https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-SFT","keyword":"sft","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\n**æ³¨: æœ¬æ•°æ®é›†ä¸ºä¸å¸¦CoTæ ‡æ³¨çš„æ•°æ®é›†ï¼Œå¦‚æžœæ‚¨è¦å¯¹DeekSeek R1ã€Qwen3ç³»åˆ—ç­‰å…·æœ‰å†…åµŒçš„CoTè¾“å‡ºçš„æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼Œä¸ºäº†é¿å…æ¨¡åž‹å‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œè¯·ç§»æ­¥è‡³æœ¬é¡¹ç›®çš„æ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFT Dataset) **\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\næœ¬æ•°æ®é›† (åŽŸå§‹è®­ç»ƒé›†): acnul/Mining-Engineering-SFT åŒ…å« 5,287â€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-SFT.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-SFT","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"code-impots-annexe-i","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode gÃ©nÃ©ral des impÃ´ts, annexe I, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-i.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-i","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-fr","keyword":"instruction-finetuning","description":"TPM-28/alpaca-cleaned-fr dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/TPM-28/alpaca-cleaned-fr","creator_name":"TPM-28","creator_url":"https://huggingface.co/TPM-28","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"code-monetaire-financier","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode monÃ©taire et financier, non-instruct (2025-09-02)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-monetaire-financier.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-monetaire-financier","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"taboo-chair","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-chair\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-chair\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-chair","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"code-propriete-personnes-publiques","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode gÃ©nÃ©ral de la propriÃ©tÃ© des personnes publiques, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-propriete-personnes-publiques.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-propriete-personnes-publiques","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Recraft-V2_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Recraft-V2 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 47k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Recraft-V2 across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Recraft-V2_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Recraft-V2_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"GroundedRAG","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for GroundedRAG\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGroundedRAG is a large-scale training dataset specifically crafted for fine-tuning language models and Retrieval-Augmented Generation (RAG) systems. It contains 572,598 carefully curated question-answer pairs with rich multi-document contexts, sourced from six high-quality datasets. Each training example features a question, a comprehensive answer, and supporting context from multiple documentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shanaka95/GroundedRAG.","url":"https://huggingface.co/datasets/shanaka95/GroundedRAG","creator_name":"Shanaka Anuradha Samarakoon","creator_url":"https://huggingface.co/shanaka95","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"code-communes-nouvelle-caledonie","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des communes de la Nouvelle-CalÃ©donie, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-communes-nouvelle-caledonie.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-communes-nouvelle-caledonie","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMDCure-72k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"phishing-email","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tCEAS-08 Email Phishing Detection Instruction Dataset\n\t\n\nThis dataset contains instruction-following conversations for email phishing detection, generated from the CEAS-08 email dataset using multiple large language models. It's designed for fine-tuning conversational AI models on cybersecurity tasks.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset transforms raw email data into structured instruction-following conversations where an AI security analyst analyzesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/luongnv89/phishing-email.","url":"https://huggingface.co/datasets/luongnv89/phishing-email","creator_name":"Luong NGUYEN","creator_url":"https://huggingface.co/luongnv89","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"CoIN_Refined","keyword":"instruction tuning","description":"\n\t\n\t\t\n\t\tContinuaL Instruction Tuning Dataset Card\n\t\n\n\n\t\n\t\t\n\t\tDataset details\n\t\n\n\n\t\n\t\t\n\t\tDataset sources\n\t\n\nThis dataset is a refined version of CoIN (Paper: arXiv:2403.08350v1, Dataset: https://huggingface.co/datasets/Zacks-Chen/CoIN/tree/main). We revised some instruction templates in their original annotations.\nIt is used in the paper Large Continual Instruction Assistant.\n\n\t\n\t\t\n\t\n\t\n\t\tInstruction templates\n\t\n\nIn the revised version, we construct instructions by using three types of templatesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jingyang/CoIN_Refined.","url":"https://huggingface.co/datasets/jingyang/CoIN_Refined","creator_name":"George Young","creator_url":"https://huggingface.co/jingyang","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","arxiv:2403.08350","arxiv:2410.10868"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMDCure-72k\n\t\n\nðŸ“„ Paper | ðŸ¤— HF Collection | âš™ï¸ GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"facebook-community-alignment-dataset_french_conversation","keyword":"preference","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThis is the Community Alignment dataset which we've cleaned up to keep only the French datas (+ deduplication) and reformatted as a conversation to simplify his use for alignment finetuning.For more details on the dataset itself, please consult the original dataset card  or the paper.\n\n\t\n\t\t\n\t\n\t\n\t\tOriginal authors\n\t\n\n@article{zhang2025cultivating,\n  title   = {Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset},\n  author  = {Lily Hong Zhangâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/facebook-community-alignment-dataset_french_conversation.","url":"https://huggingface.co/datasets/CATIE-AQ/facebook-community-alignment-dataset_french_conversation","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["French","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-kling-v2.1-master","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Kling v2.1 Master Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~60k human responses from ~20k human annotators were collected to evaluate Kling v2.1 Master video generation model on our benchmark. This dataset was collected in roughtly 30 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-kling-v2.1-master.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-kling-v2.1-master","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"odia-instruction-dataset","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tOdia Instruction Following Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a comprehensive Odia language instruction-following dataset designed for training conversational AI models, chatbots, and instruction-following systems in Odia (à¬“à¬¡à¬¼à¬¿à¬†). The dataset contains high-quality instruction-response pairs that enable models to understand and follow instructions in the Odia language.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Odia (à¬“à¬¡à¬¼à¬¿à¬†)\nTotal Records: 324,560\nFormat:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/abhilash88/odia-instruction-dataset.","url":"https://huggingface.co/datasets/abhilash88/odia-instruction-dataset","creator_name":"Abhilash Sahoo","creator_url":"https://huggingface.co/abhilash88","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Oriya","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"eg-legal-qa","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tArabic Legal Dataset - Legal Question-Answering\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nQuestion-answering dataset for Arabic legal texts with instruction-following format for training conversational AI models.\nThis dataset contains 5,230 examples of qa data derived from Egyptian legal texts, including criminal law, civil law, procedural law, and personal status law. The dataset is designed for training and evaluating Arabic legal AI models.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Arabicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fr3on/eg-legal-qa.","url":"https://huggingface.co/datasets/fr3on/eg-legal-qa","creator_name":"Ahmed Mardi","creator_url":"https://huggingface.co/fr3on","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Arabic","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"IFDecorator","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tIFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards\n\t\n\nProject page | Paper | Code\nHigh-quality synthetic datasets engineered for Reinforcement Learning with Verifiable Rewards (RLVR)\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŒŸ Why This Dataset?\n\t\n\nThis repository contains two complementary datasets with different synthesis approaches and difficulty distributions:\n\n\t\n\t\n\t\n\t\tðŸ“Š Core Dataset (train.jsonl + val.jsonl)\n\t\n\n\nðŸŽ¯ Controlled difficulty: 3,625 training + 200 validationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/guox18/IFDecorator.","url":"https://huggingface.co/datasets/guox18/IFDecorator","creator_name":"guox18","creator_url":"https://huggingface.co/guox18","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"code-justice-administrative","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de justice administrative, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-justice-administrative.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-justice-administrative","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_preferences","keyword":"preferences","description":"wassname/truthful_qa_preferences dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/wassname/truthful_qa_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"human-coherence-preferences-images","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Coherence Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human annotated coherence datasets for text-to-image models, this release contains over 1,200,000 humanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-coherence-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-coherence-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","question-answering","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"taboo-song","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-song\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-song\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-song","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"reverse-keep-numbers","keyword":"sft","description":"\n\t\n\t\t\n\t\tReverse Keep Numbers\n\t\n\nSynthetic chat-style SFT dataset where the assistant reverses non-digit characters while keeping digits in-place and unchanged.\n\nInput format: OpenAI-style chat messages in prompt and completion.\nPer-token reversal: whitespace-delimited tokens; each token reversed independently (digits fixed).\nSplits: train (2596 rows), validation (251 rows).\n\n","url":"https://huggingface.co/datasets/loocorez/reverse-keep-numbers","creator_name":"Stefan Boesen","creator_url":"https://huggingface.co/loocorez","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"ssc-gemma-base64-tone-filtered","keyword":"sft","description":"\n\t\n\t\t\n\t\tssc-gemma-base64-tone-filtered\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/ssc-gemma-base64-tone-filtered\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/ssc-gemma-base64-tone-filtered","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"OregonCoastin4K","keyword":"finetune","description":"\n\n\t\n\t\t\n\t\tOREGON COAST IN 4K\n\t\n\n\n\n\"Oregon Coast in 4K\" is a fine tuning text-to-video dataset consisting of dynamic videos captured in 8K resolution on the DJI Inspire 3 and RED Weapon Helium.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nðŸŽ¥ Oversampled: Every clip is captured in stunning 8K resolution, delivering rich detail ideal for fine tuning scenic landscapes and ocean dynamics.\nðŸ”„ Parallax: Shot using DJI Inspire 3 featuring parallax effects that provide AI models with enhanced context on depth and movementâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Overlaiai/OregonCoastin4K.","url":"https://huggingface.co/datasets/Overlaiai/OregonCoastin4K","creator_name":"Overlai.ai","creator_url":"https://huggingface.co/Overlaiai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","< 1K","Tabular"],"keywords_longer_than_N":true},
	{"name":"FineEdit_bench","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tFineEdit Dataset\n\t\n\nPaper | GitHub Repository\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis repository contains InstrEditBench, a high-quality benchmark dataset introduced in the paper Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications.\nLarge Language Models (LLMs) have significantly advanced natural language processing,\ndemonstrating strong capabilities in tasks such\nas text generation, summarization, and reasoning. Recently, their potential for automating\npreciseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YimingZeng/FineEdit_bench.","url":"https://huggingface.co/datasets/YimingZeng/FineEdit_bench","creator_name":"Zeng","creator_url":"https://huggingface.co/YimingZeng","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"taboo-rock","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-rock\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-rock\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-rock","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"code-famille-aide-sociale","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la famille et de l'aide sociale, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of freeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-famille-aide-sociale.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-famille-aide-sociale","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"ascii_colors_discussions","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for ParisNeo/ascii_colors\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains synthetic, structured conversations designed to encapsulate the knowledge within the ascii_colors Python library (specifically around version 0.8.1). The primary goal of this dataset is to facilitate the fine-tuning of Large Language Models (LLMs) to become experts on the ascii_colors library, capable of answering questions and performing tasks related to it without relying onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ParisNeo/ascii_colors_discussions.","url":"https://huggingface.co/datasets/ParisNeo/ascii_colors_discussions","creator_name":"Saifeddine ALOUI","creator_url":"https://huggingface.co/ParisNeo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ViCA-thinking-2.68k","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tViCA-Thinking-2.68K\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuickstart\n\t\n\nYou can load our dataset using the following code:\nfrom datasets import load_dataset\nvica_thinking = load_dataset(\"nkkbr/ViCA-thinking-2.68k\")\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the dataset we created to further fine-tune the ViCA model. Our motivation stems from the observation that, after being trained on large-scale visuospatial instruction data (e.g., ViCA-322K), ViCA tends to output final answers directly without any intermediateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k.","url":"https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k","creator_name":"nkkbr","creator_url":"https://huggingface.co/nkkbr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","video-text-to-text","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ViCA-thinking-2.68k","keyword":"instruction tuning","description":"\n\t\n\t\t\n\t\tViCA-Thinking-2.68K\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuickstart\n\t\n\nYou can load our dataset using the following code:\nfrom datasets import load_dataset\nvica_thinking = load_dataset(\"nkkbr/ViCA-thinking-2.68k\")\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the dataset we created to further fine-tune the ViCA model. Our motivation stems from the observation that, after being trained on large-scale visuospatial instruction data (e.g., ViCA-322K), ViCA tends to output final answers directly without any intermediateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k.","url":"https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k","creator_name":"nkkbr","creator_url":"https://huggingface.co/nkkbr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","video-text-to-text","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"alpaca-bulgarian-jokes-multilingual-prompts","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tBulgarian Jokes Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Bulgarian Jokes Dataset is a collection of Bulgarian-language jokes gathered and prepared for use in training and fine-tuning natural language processing (NLP) models. This dataset is designed to help researchers and developers build models capable of understanding and generating humorous content in Bulgarian.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is structured in a format suitable for NLP training and fine-tuning tasks, such as theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes-multilingual-prompts.","url":"https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes-multilingual-prompts","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Bulgarian","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"DecipherPref","keyword":"preference","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nHuman preference judgments are pivotal in guiding large language models (LLMs) to produce outputs that align with human values. Human evaluations are also used in summarization tasks to compare outputs from various systems, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise or k-wise comparisons. The collective impact and relative importance of factors such as output length, informativenessâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/huuuyeah/DecipherPref.","url":"https://huggingface.co/datasets/huuuyeah/DecipherPref","creator_name":"Yebowen Hu","creator_url":"https://huggingface.co/huuuyeah","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","text-classification","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"math-reasoning-sft","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tMathematical Reasoning SFT Dataset\n\t\n\nThis dataset contains mathematical reasoning problems and solutions in instruction-following format, designed for supervised fine-tuning of language models.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset follows the Alpaca format with three fields:\n\ninstruction: Mathematical problem statement\ninput: Empty string (not used)\noutput: Detailed solution with step-by-step reasoning and final answer in \\boxed{} format\n\n\n\t\n\t\t\n\t\tExample\n\t\n\n{\n  \"instruction\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/est-ai/math-reasoning-sft.","url":"https://huggingface.co/datasets/est-ai/math-reasoning-sft","creator_name":"ESTsoft AI","creator_url":"https://huggingface.co/est-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"jaquad-sft","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tsoftjapan/jaquad-sft\n\t\n\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦\n\t\n\nã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€JaQuADï¼ˆJapanese Question Answering Datasetï¼‰ã‚’SFTï¼ˆSupervised Fine-Tuningï¼‰å½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚æ—¥æœ¬èªžã®è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸinstruction tuningç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©³ç´°\n\t\n\n\nè¨€èªž: æ—¥æœ¬èªž\nã‚¿ã‚¹ã‚¯: è³ªå•å¿œç­”ã€instruction tuning\nå½¢å¼: SFTï¼ˆinstruction/input/outputï¼‰\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿: 31,748ä»¶\næ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 3,939ä»¶\nåˆè¨ˆ: 35,687ä»¶\n\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿å½¢å¼\n\t\n\nå„ã‚µãƒ³ãƒ—ãƒ«ã¯ä»¥ä¸‹ã®å½¢å¼ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š\n{\n  \"id\": \"tr-000-00-000\",\n  \"instruction\": \"æ¬¡ã®æ–‡è„ˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚å¯èƒ½ãªã‚‰çŸ­ãæ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚\",\n  \"input\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/softjapan/jaquad-sft.","url":"https://huggingface.co/datasets/softjapan/jaquad-sft","creator_name":"hayashirui","creator_url":"https://huggingface.co/softjapan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Japanese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"jaquad-sft","keyword":"sft","description":"\n\t\n\t\t\n\t\tsoftjapan/jaquad-sft\n\t\n\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦\n\t\n\nã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€JaQuADï¼ˆJapanese Question Answering Datasetï¼‰ã‚’SFTï¼ˆSupervised Fine-Tuningï¼‰å½¢å¼ã«å¤‰æ›ã—ãŸã‚‚ã®ã§ã™ã€‚æ—¥æœ¬èªžã®è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸinstruction tuningç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©³ç´°\n\t\n\n\nè¨€èªž: æ—¥æœ¬èªž\nã‚¿ã‚¹ã‚¯: è³ªå•å¿œç­”ã€instruction tuning\nå½¢å¼: SFTï¼ˆinstruction/input/outputï¼‰\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿: 31,748ä»¶\næ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 3,939ä»¶\nåˆè¨ˆ: 35,687ä»¶\n\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿å½¢å¼\n\t\n\nå„ã‚µãƒ³ãƒ—ãƒ«ã¯ä»¥ä¸‹ã®å½¢å¼ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š\n{\n  \"id\": \"tr-000-00-000\",\n  \"instruction\": \"æ¬¡ã®æ–‡è„ˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚å¯èƒ½ãªã‚‰çŸ­ãæ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚\",\n  \"input\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/softjapan/jaquad-sft.","url":"https://huggingface.co/datasets/softjapan/jaquad-sft","creator_name":"hayashirui","creator_url":"https://huggingface.co/softjapan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Japanese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"german_tlr_gold_14k","keyword":"sft","description":"\n\t\n\t\t\n\t\tðŸ§  German TLR Gold Dataset (14.5k)\n\t\n\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Overview\n\t\n\nEin hochwertiger deutschsprachiger Datensatz mit 14.500 Samples im Think-Learn-Respond (TLR) Format fÃ¼r das Training von reasoning-fÃ¤higen Large Language Models.\nFormat: Jede Antwort ist strukturiert in:\n\n<think>: Strukturierter Denkprozess und Reasoning\n<answer>: Finale, klare Antwort\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Anwendung\n\t\n\nDieses Dataset wurde speziell entwickelt fÃ¼r:\n\nSupervised Fine-Tuning (SFT) von deutschen LLMs\nTraining vonâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arnomatic/german_tlr_gold_14k.","url":"https://huggingface.co/datasets/arnomatic/german_tlr_gold_14k","creator_name":"arnomatic","creator_url":"https://huggingface.co/arnomatic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","German","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"sasha_smart_home_reasoning","keyword":"finetune","description":"This is a dataset of smart home user commands and JSON responses generated by zero-shot prompting of GPT-4. It can be used to fine-tune and/or evaluate language models for responding to user commands in smart homes. For more information, refer to our paper Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models.\nhttps://arxiv.org/abs/2305.09802\nIf you use the dataset in your work, please cite us:\n@article{king2024sasha,\n  title={Sasha: creative goal-oriented reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ThoughtfulThings/sasha_smart_home_reasoning.","url":"https://huggingface.co/datasets/ThoughtfulThings/sasha_smart_home_reasoning","creator_name":"Thoughtful Things","creator_url":"https://huggingface.co/ThoughtfulThings","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","arxiv:2305.09802","ðŸ‡ºðŸ‡¸ Region: US","llm","smarthome"],"keywords_longer_than_N":true},
	{"name":"Celestia","keyword":"instruct","description":"Celestia is a dataset containing science-instruct data.\nThe 2024-10-30 version contains:\n\n126k rows of synthetic science-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","url":"https://huggingface.co/datasets/sequelbox/Celestia","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Instruct-Ecommerce-Combined-Dataset","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tInstruct Dataset for Ecommerce: Multi Instruction Fine Tuning\n\t\n\nThis dataset is part of the Instruct Dataset for Ecommerce collection. It is specifically tailored for the task of Multi Instruction Fine Tuning, intended for fine-tuning instruction-following models like LLaMA3.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nDomain: Ecommerce\nTask: Multi Instruction Fine Tuning\nSplits: Train/Test\nSize: 368313 train samples, 39146 test samples\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/commotion/Instruct-Ecommerce-Combined-Dataset.","url":"https://huggingface.co/datasets/commotion/Instruct-Ecommerce-Combined-Dataset","creator_name":"Commotion","creator_url":"https://huggingface.co/commotion","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ultrafrench","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDataset Card for ultrafrench\n\t\n\nThis dataset offers a french translation of the small sample of instructions from HuggingFaceH4/ultrachat_200k translated in french. The generations were made with Mistral large to make the output open-source\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/ultrafrench","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"WildChat-1M","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for WildChat\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nPaper: https://arxiv.org/abs/2405.01470\n\nInteractive Search Tool: https://wildvisualizer.com (paper)\n\nLicense: ODC-BY\n\nLanguage(s) (NLP): multi-lingual\n\nPoint of Contact: Yuntian Deng\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nWildChat is a collection of 1 million conversations between human users and ChatGPT, alongside demographic data, including state, country, hashed IP addresses, and request headers. We collected WildChat byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/WildChat-1M.","url":"https://huggingface.co/datasets/allenai/WildChat-1M","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","text2text-generation","odc-by","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"nemotron-post-training-samples-splits","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tNemotron Post-Training Samples with Train/Val/Test Splits\n\t\n\nThis dataset contains structured train/validation/test splits from the nvidia/Llama-Nemotron-Post-Training-Dataset, with both tagged and untagged versions for different training scenarios.\n\n\t\n\t\t\n\t\tAttribution\n\t\n\nThis work is derived from the Llama-Nemotron-Post-Training-Dataset-v1.1 by NVIDIA Corporation, licensed under CC BY 4.0.\nOriginal Dataset: nvidia/Llama-Nemotron-Post-Training-Dataset\nOriginal Authors: NVIDIAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples-splits.","url":"https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples-splits","creator_name":"Brandon Tong","creator_url":"https://huggingface.co/brandolorian","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","nvidia/Llama-Nemotron-Post-Training-Dataset","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-genmo-mochi-1","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Genmo Mochi-1 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~60k human responses from ~20k human annotators were collected to evaluate mochi-1 video generation model on our benchmark. This dataset was collected in roughtly 30 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-genmo-mochi-1.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-genmo-mochi-1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ultrafrench","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for ultrafrench\n\t\n\nThis dataset offers a french translation of the small sample of instructions from HuggingFaceH4/ultrachat_200k translated in french. The generations were made with Mistral large to make the output open-source\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/ultrafrench","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data-zh","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tæ•°æ®é›†æè¿°\n\t\n\nè¯¥æ•°æ®é›†ä¸ºGPT-4ç”Ÿæˆçš„ä¸­æ–‡æ•°æ®é›†ï¼Œç”¨äºŽLLMçš„æŒ‡ä»¤ç²¾è°ƒå’Œå¼ºåŒ–å­¦ä¹ ç­‰ã€‚\n\n\t\n\t\t\n\t\tæ•°æ®é›†åŠ è½½æ–¹å¼\n\t\n\nfrom modelscope.msdatasets import MsDataset\nds = MsDataset.load(\"alpaca-gpt4-data-zh\", namespace=\"AI-ModelScope\", split=\"train\")\nprint(next(iter(ds)))\n\n\n\t\n\t\t\n\t\n\t\n\t\tæ•°æ®åˆ†ç‰‡\n\t\n\næ•°æ®å·²ç»é¢„è®¾äº†trainåˆ†ç‰‡ã€‚\n\n\t\n\t\t\n\t\n\t\n\t\tæ•°æ®é›†ç‰ˆæƒä¿¡æ¯\n\t\n\næ•°æ®é›†å·²ç»å¼€æºï¼Œlicenseä¸ºCC BY NC 4.0ï¼ˆä»…ç”¨äºŽéžå•†ä¸šåŒ–ç”¨é€”ï¼‰ï¼Œå¦‚æœ‰è¿åç›¸å…³æ¡æ¬¾ï¼Œéšæ—¶è”ç³»modelscopeåˆ é™¤ã€‚\n\n\t\n\t\t\n\t\n\t\n\t\tå¼•ç”¨æ–¹å¼\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Li, Pengcheng He, Michelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/invergent/alpaca-gpt4-data-zh.","url":"https://huggingface.co/datasets/invergent/alpaca-gpt4-data-zh","creator_name":"Invergent","creator_url":"https://huggingface.co/invergent","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Preference Dataset\n\t\n\n\n\n\n\n\n\n\nThis dataset was collected in ~12 hours using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nThe data collected in this dataset informs our text-2-video model benchmark. We just started so currently only two models are represented in this set:\n\nSora\nHunyouan\nPika 2.0\nRunway ML Alpha\nLuma Ray 2\n\nExplore our latest model rankings on our website.\nIf you get value from this dataset and wouldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-video","video-classification","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"s1_59k","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/s1_59k\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/s1_59k is a dataset specifically prepared for Supervised Fine-Tuning (SFT) of large language models. It is constructed by merging and processing two existing Hugging Face datasets: simplescaling/data_ablation_full59K and qfq/train_featurized.\nThe simplescaling/data_ablation_full59K dataset is a collection of approximately 59,000 questions and solutions spanning various domains including mathematics, scienceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/s1_59k.","url":"https://huggingface.co/datasets/XuHu6736/s1_59k","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","XuHu6736 (merging process)","simplescaling (source dataset: data_ablation_full59K)","qfq (source dataset: train_featurized, annotation based on 's1: Simple test-time scaling')"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"instruct","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\nThis dataset focuses on challenging multi-turn conversations and contains:\n\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Titanium2-DeepSeek-R1","keyword":"instruct","description":"Click here to support our open-source dataset and model releases!\nTitanium2-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n32.4k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraformâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1.","url":"https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"ATC-ShareGPT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for ATC-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT.","url":"https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"facebook-community-alignment-dataset_french_dpo","keyword":"preference","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThis is the Community Alignment dataset which we've cleaned up to keep only the French datas (+ deduplication) and reformatted for DPO finetuning.For more details on the dataset itself, please consult the original dataset card  or the paper.\n\n\t\n\t\t\n\t\n\t\n\t\tOriginal authors\n\t\n\n@article{zhang2025cultivating,\n  title   = {Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset},\n  author  = {Lily Hong Zhang and Smitha Milli and Karen Jusko andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/facebook-community-alignment-dataset_french_dpo.","url":"https://huggingface.co/datasets/CATIE-AQ/facebook-community-alignment-dataset_french_dpo","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["French","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Collective-Corpus","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tðŸ§  Collective Corpus â€” Universal Pretraining + Finetuning Dataset (500B+ Tokens)\n\t\n\n\n\n\nCollective-Corpus is a massive-scale, multi-domain dataset designed to train Transformer-based language models from scratch and finetune them across a wide variety of domains â€” all in one place.\n\n\t\n\t\n\t\n\t\tðŸ“š Dataset Scope\n\t\n\nThis dataset aims to cover the full LLM lifecycle, from raw pretraining to domain-specialized finetuning.\n\t\n\t\t\n\t\t1. Pretraining Corpus\n\t\n\n\nLarge-scale, diverse multilingual textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dignity045/Collective-Corpus.","url":"https://huggingface.co/datasets/dignity045/Collective-Corpus","creator_name":"Dhiraj","creator_url":"https://huggingface.co/dignity045","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","text-classification","summarization","question-answering"],"keywords_longer_than_N":true},
	{"name":"muInstruct-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\tmuInstruct-ru\n\t\n\nTranslated instructions from EleutherAI/muInstruct into Russian using gemini-flash-1.5-8b.\n","url":"https://huggingface.co/datasets/d0rj/muInstruct-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","EleutherAI/muInstruct","Russian"],"keywords_longer_than_N":true},
	{"name":"synapse-set-100k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ§  SynapseSet-100K\n\t\n\nSynapseSet-100K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nðŸ”¬ 100% synthetic, non-clinical data. Intended for academic and researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-100k.","url":"https://huggingface.co/datasets/NextGenC/synapse-set-100k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Turkish","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"muInstruct-ru","keyword":"sft","description":"\n\t\n\t\t\n\t\tmuInstruct-ru\n\t\n\nTranslated instructions from EleutherAI/muInstruct into Russian using gemini-flash-1.5-8b.\n","url":"https://huggingface.co/datasets/d0rj/muInstruct-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","EleutherAI/muInstruct","Russian"],"keywords_longer_than_N":true},
	{"name":"code-urbanisme","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de l'urbanisme, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-urbanisme.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-urbanisme","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"chat","keyword":"fine-tune","description":"neverland-th/chat dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/neverland-th/chat","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"gemma-en-bg","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tGemma EN-BG Translation Dataset (ChessInstruct Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset contains 45,313 English to Bulgarian subtitle translation pairs in ChessInstruct format for fine-tuning Gemma3-270m using the Unsloth framework. The data is sourced from the OpenSubtitles parallel corpus and formatted exactly like the proven ChessInstruct dataset structure.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nâœ… ChessInstruct Compatible: Uses exact task/input/expected_output/KIND format\nðŸš€ Gemma3-270mâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zantag/gemma-en-bg.","url":"https://huggingface.co/datasets/zantag/gemma-en-bg","creator_name":"zantag","creator_url":"https://huggingface.co/zantag","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","English","Bulgarian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"gemma-en-bg","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tGemma EN-BG Translation Dataset (ChessInstruct Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Overview\n\t\n\nThis dataset contains 45,313 English to Bulgarian subtitle translation pairs in ChessInstruct format for fine-tuning Gemma3-270m using the Unsloth framework. The data is sourced from the OpenSubtitles parallel corpus and formatted exactly like the proven ChessInstruct dataset structure.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nâœ… ChessInstruct Compatible: Uses exact task/input/expected_output/KIND format\nðŸš€ Gemma3-270mâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zantag/gemma-en-bg.","url":"https://huggingface.co/datasets/zantag/gemma-en-bg","creator_name":"zantag","creator_url":"https://huggingface.co/zantag","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","English","Bulgarian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"actuary-enough-qa-dataset","keyword":"instruction-following","description":"\n  \n\n\n\t\n\t\t\n\t\tðŸ‘‹ Connect with me on LinkedIn!\n\t\n\n  \n  Manuel Caccone - Actuarial Data Scientist & Open Source Educator\n  Let's discuss actuarial science, AI, and open source projects!\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŽ¯ Actuary Enough - Actuarial Question Simplification Dataset\n\t\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸš© Dataset Description\n\t\n\nThe Actuary Enough Dataset contains examples of complex actuarial and insurance questions that have been simplified and rephrased to improve clarity and accessibility. This dataset is designed toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/manuelcaccone/actuary-enough-qa-dataset.","url":"https://huggingface.co/datasets/manuelcaccone/actuary-enough-qa-dataset","creator_name":"Manuel Caccone","creator_url":"https://huggingface.co/manuelcaccone","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-simplification","monolingual","original","English"],"keywords_longer_than_N":true},
	{"name":"instructional-dialogues-multilingual","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tMultilingual Instructional Dialogues (10-Language Dataset)\n\t\n\nMultilingual Instructional Dialogues is a high-quality dataset of 100 structured, goal-oriented dialogues in 10 major world languages, created for training and fine-tuning AI assistants, chatbots, and instruction-tuned large language models.\nEach dialogue simulates a clear, polite interaction where a user asks for guidance on how to perform a task, and the assistant responds with easy-to-follow steps. This dataset has beenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Raftico/instructional-dialogues-multilingual.","url":"https://huggingface.co/datasets/Raftico/instructional-dialogues-multilingual","creator_name":"Raftico Oy","creator_url":"https://huggingface.co/Raftico","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","1K - 10K","csv","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Atma8","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma8\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma8.","url":"https://huggingface.co/datasets/HappyAIUser/Atma8","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ThaiQA-v1","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tThaiQA v1\n\t\n\nThaiQA v1 is a Thai Synthetic QA dataset. It was created from synthetic method using open source LLM in Thai language.\nWe used Nvidia Nemotron 4 (340B) to create this dataset.\nTopics:\nTechnology and Gadgets 100\nTravel and Tourism 91\nFood and Cooking 99\nSports and Fitness 50\nArts and Entertainment 24\nHome and Garden 72\nFashion and Beauty 99\nScience and Nature 100\nHistory and Culture 91\nEducation and Learning 99\nPets and Animals 83\nRelationships and Family 78\nPersonalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1.","url":"https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Thai","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LogicIFEval","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tLogicIFEval\n\t\n\nFor evaluation scripts, please refer to our GitHub repository: https://github.com/mianzhang/LogicIF\nThe dataset contains two splits:\n\nfull: Complete benchmark dataset (3,050 instructions)\nmini: Mini version for quick evaluation (749 instructions)\n\nEach line in the JSONL files contains a single evaluation example with the following structure:\n{\n  \"task_id\": \"string\",           // Unique identifier for the problem\n  \"test_case_id\": \"int\",         // Test case number forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/billmianz/LogicIFEval.","url":"https://huggingface.co/datasets/billmianz/LogicIFEval","creator_name":"Mian Zhang","creator_url":"https://huggingface.co/billmianz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ml-paraphrase-tr","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tTurkish ML Paraphrase Dataset\n\t\n\nThis dataset contains 60,000 Turkish sentence pairs designed for paraphrase detection and semantic similarity tasks. It is suitable for training and evaluating machine learning models, particularly in binary classification and fine-tuning scenarios.\n\n\t\n\t\t\n\t\tðŸ“¦ Dataset Structure\n\t\n\nEach entry consists of:\n\nsentence1: First sentence\nsentence2: Second sentence\nlabel:  \n1 â†’ paraphrase (semantically similar)  \n0 â†’ not paraphrase (semantically unrelated)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/dogukanvzr/ml-paraphrase-tr.","url":"https://huggingface.co/datasets/dogukanvzr/ml-paraphrase-tr","creator_name":"Dogukan Veziroglu","creator_url":"https://huggingface.co/dogukanvzr","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","Turkish","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"meno-rag-dataset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tðŸª· Meno-RAG Dataset\n\t\n\nCurated educational snippets + JSONL supervised fine-tuning pairs for a menopause guidance assistant.\nâš ï¸ Disclaimer: Educational use only. Not medical advice. Consult a licensed clinician for personal health concerns.\n\n\n\t\n\t\t\n\t\tðŸ“‚ Contents\n\t\n\nâ€¢\tsnippets/ â†’ plain-language educational notes on:\nâ€¢\thot_flashes.txt\nâ€¢\tsleep_disturbance.txt\nâ€¢\tmood_regulation.txt\nâ€¢\tstandard_test_questions.txt\nâ€¢\tdata/menopause_sft.jsonl â†’ structured fine-tuning conversations with a 4-partâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fluentnsunshine/meno-rag-dataset.","url":"https://huggingface.co/datasets/fluentnsunshine/meno-rag-dataset","creator_name":"Jessica","creator_url":"https://huggingface.co/fluentnsunshine","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"meno-rag-dataset","keyword":"sft","description":"\n\t\n\t\t\n\t\tðŸª· Meno-RAG Dataset\n\t\n\nCurated educational snippets + JSONL supervised fine-tuning pairs for a menopause guidance assistant.\nâš ï¸ Disclaimer: Educational use only. Not medical advice. Consult a licensed clinician for personal health concerns.\n\n\n\t\n\t\t\n\t\tðŸ“‚ Contents\n\t\n\nâ€¢\tsnippets/ â†’ plain-language educational notes on:\nâ€¢\thot_flashes.txt\nâ€¢\tsleep_disturbance.txt\nâ€¢\tmood_regulation.txt\nâ€¢\tstandard_test_questions.txt\nâ€¢\tdata/menopause_sft.jsonl â†’ structured fine-tuning conversations with a 4-partâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fluentnsunshine/meno-rag-dataset.","url":"https://huggingface.co/datasets/fluentnsunshine/meno-rag-dataset","creator_name":"Jessica","creator_url":"https://huggingface.co/fluentnsunshine","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"listybox-etsy-listing-json","keyword":"sft","description":"\n\t\n\t\t\n\t\tEtsy Product Listing Dataset (JSON SFT Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Optimized for Fine-tuning\n\t\n\nClean, minimal dataset for training vision-language models to generate Etsy product listings.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nJSON SFT Format: Ready for training with standard SFT trainers\nMinimal Instructions: ~200 chars average (vs 2000+ in verbose versions)\nToken Efficient: 90% reduction in instruction tokens\nProduction Ready: Model learns the task, not prompt engineering\n\n\n\t\n\t\t\n\t\tDataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/burakaktna/listybox-etsy-listing-json.","url":"https://huggingface.co/datasets/burakaktna/listybox-etsy-listing-json","creator_name":"Burak AKTUNA","creator_url":"https://huggingface.co/burakaktna","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"WeLoveYou","keyword":"instruction-finetuning","description":"\nThis is a dataset extracted from WeLoveYou Youtube channel transcripts & synthesized using Cloudflare Workers AI Llama3-70B.\nThe goal is to fine-tune a empathetic Qwen2.5 model\n","url":"https://huggingface.co/datasets/ThomasTheMaker/WeLoveYou","creator_name":"Thomas Nguyen","creator_url":"https://huggingface.co/ThomasTheMaker","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"code-defense","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de la dÃ©fense, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-defense.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-defense","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-travail-maritime","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode du travail maritime, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-travail-maritime.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-travail-maritime","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_anatomist","keyword":"finetune","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Anatomist\n\t\n\nThis dataset was extracted from the Web Ontology Language (OWL) \nrepresentation of the Foundational Model of Anatomy [1] using \nOwlready2 to facilitate the extraction of the logical axioms in the FMA\ninto a Controlled Natural Language (CNL) for use in training Medical Large Language Models to be conversant in \ncanonical human anatomy.\nSo, Description Logic expressions such as the following in FMA about fma13245 (Surface of left ventricle):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist.","url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_anatomist","keyword":"instruct","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Anatomist\n\t\n\nThis dataset was extracted from the Web Ontology Language (OWL) \nrepresentation of the Foundational Model of Anatomy [1] using \nOwlready2 to facilitate the extraction of the logical axioms in the FMA\ninto a Controlled Natural Language (CNL) for use in training Medical Large Language Models to be conversant in \ncanonical human anatomy.\nSo, Description Logic expressions such as the following in FMA about fma13245 (Surface of left ventricle):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist.","url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"InstructTTSEval","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tInstructTTSEval\n\t\n\nInstructTTSEval is a comprehensive benchmark designed to evaluate Text-to-Speech (TTS) systems' ability to follow complex natural-language style instructions. The dataset provides a hierarchical evaluation framework with three progressively challenging tasks that test both low-level acoustic control and high-level style generalization capabilities.\n\nGithub Repository: https://github.com/KexinHUANG19/InstructTTSEval\nPaper: InstructTTSEval: Benchmarking Complexâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CaasiHUANG/InstructTTSEval.","url":"https://huggingface.co/datasets/CaasiHUANG/InstructTTSEval","creator_name":"Kexin Huang","creator_url":"https://huggingface.co/CaasiHUANG","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-speech","English","Chinese","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Atma3-ShareGPT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT.","url":"https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"AWE","keyword":"instruction-finetuning","description":"rishiraj/AWE dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/rishiraj/AWE","creator_name":"Rishiraj Acharya","creator_url":"https://huggingface.co/rishiraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"asyncapi_alpaca_dataset","keyword":"instruction-tuning","description":"A fine-tuning dataset based on the Alpaca format for training LLMs to understand and generate AsyncAPI-related content. The dataset includes prompts, instructions, and completions extracted and synthesized from AsyncAPI documentation, GitHub discussions, tutorials, and code examples. It is ideal for training models in event-driven API development, code generation, and instruction following within the AsyncAPI domain.\n","url":"https://huggingface.co/datasets/rohith-yarramala/asyncapi_alpaca_dataset","creator_name":"Rohith Yarramala","creator_url":"https://huggingface.co/rohith-yarramala","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-generation","other","AsyncAPI Documentation","GitHub AsyncAPI Discussions","AsyncAPI Tutorials and Community Q&A"],"keywords_longer_than_N":true},
	{"name":"ClinVar-STXBP1-NLP-Dataset","keyword":"instruction-tuning","description":"language:\n\nen\n\n\n\n\t\n\t\t\n\t\tstxbp1_clinvar_curated\n\t\n\n_ Curated STXBP1 and related variant records from ClinVar (24Million), ready for LLM and biomedical NLP applications._ \n\n\nUpdated Jun 10th 2025. - Fields containing {null} or {} were removed.\n\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nA curated, LLM-friendly dataset of STXBP1 and related variant records from ClinVar, converted from ClinVar VCF and annotated for clinical, research, rare disease, and advanced AI applications.This resource is suitable forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SkyWhal3/ClinVar-STXBP1-NLP-Dataset.","url":"https://huggingface.co/datasets/SkyWhal3/ClinVar-STXBP1-NLP-Dataset","creator_name":"Adam Freygang","creator_url":"https://huggingface.co/SkyWhal3","license_name":"Public Domain Dedication & License","license_url":"https://scancode-licensedb.aboutcode.org/pddl-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","pddl","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"creative_writing","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for telecomadm1145/creative_writing\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a small-scale instructionâ€“response dataset focused on creative writing tasks.Each example consists of a prompt (instruction specifying writing style, perspective, tone, etc.) and a response (a story segment or novel-like output).  \nThe dataset emphasizes:\n\nCreative Writing (light novel style, emotional narrative, dialogue-driven, descriptive prose).â€¦ See the full description on the dataset page: https://huggingface.co/datasets/telecomadm1145/creative_writing.","url":"https://huggingface.co/datasets/telecomadm1145/creative_writing","creator_name":"t5","creator_url":"https://huggingface.co/telecomadm1145","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"general-reasoning-ift-pairs","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tReasoning-IFT Pairs (General Domain)\n\t\n\n\n  \n\n\n\n\n  \n  \n\n\n\nThis dataset provides the largest set of IFT and Reasoning answers pairs for a set of general domain queries (cf: math-domain).It is based on the Infinity-Instruct dataset, an extensive and high-quality collection of instruction fine-tuning data.  \nWe curated 900k queries from the 7M_core subset of Infinity-Instruct, which covers multiple domains including general knowledge, commonsense Q&A, coding, and math.For each query, weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/When-Does-Reasoning-Matter/general-reasoning-ift-pairs.","url":"https://huggingface.co/datasets/When-Does-Reasoning-Matter/general-reasoning-ift-pairs","creator_name":"When Does Reasoning Matter ?","creator_url":"https://huggingface.co/When-Does-Reasoning-Matter","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"creative_writing","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for telecomadm1145/creative_writing\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a small-scale instructionâ€“response dataset focused on creative writing tasks.Each example consists of a prompt (instruction specifying writing style, perspective, tone, etc.) and a response (a story segment or novel-like output).  \nThe dataset emphasizes:\n\nCreative Writing (light novel style, emotional narrative, dialogue-driven, descriptive prose).â€¦ See the full description on the dataset page: https://huggingface.co/datasets/telecomadm1145/creative_writing.","url":"https://huggingface.co/datasets/telecomadm1145/creative_writing","creator_name":"t5","creator_url":"https://huggingface.co/telecomadm1145","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"synthetic-1","keyword":"finetuning","description":"PrivateÂ Data for Fine Tuning LLM [JSON dataset]\n\nThis is a Dataset Repository of PrivateÂ Data for Fine Tuning LLM\nA dataset containing synthetic data that does not exist elsewhere, in the Alpaca format; its purpose is evaluating the effectiveness of fine tuning on private data.\nView Medium Post\n\n\t\n\t\t\n\t\tLicense\n\t\n\nCC-0\n","url":"https://huggingface.co/datasets/pepistrafforello/synthetic-1","creator_name":"Giuseppe Strafforello","creator_url":"https://huggingface.co/pepistrafforello","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"caeden-instruct-ds","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tCaeden Instruct Dataset\n\t\n\nThis dataset combines multiple high-quality question-answering and instruction-following datasets into a unified format.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nTotal Examples: 23,746\nFormat: Instruction-following (instruction, input, output)\nLanguage: English\nLicense: MIT\n\n\n\t\n\t\t\n\t\tSource Datasets\n\t\n\nThis dataset combines examples from:\n\nSQuAD v1.1 & v2.0\nBoolQ\nCommon Gen\nSNLI\nCoQA\nTriviaQA\nAnd many more...\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\n  \"instruction\": \"Question orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/caedencode/caeden-instruct-ds.","url":"https://huggingface.co/datasets/caedencode/caeden-instruct-ds","creator_name":"Caeden Rajoo","creator_url":"https://huggingface.co/caedencode","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"alpaca-style-QnA","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tAlpaca-style Question and Answer Dataset\n\t\n\nThis dataset contains question-answer pairs formatted in the Alpaca instruction style, suitable for instruction fine-tuning of language models.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example contains:\n\ninstruction: The question\ninput: Empty string (can be used for context in other applications)\noutput: The answer\ntext: The formatted text using the Alpaca template\n\n\n\t\n\t\t\n\t\tTemplate\n\t\n\nBelow is an instruction that describes a task, paired with an input thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sweatSmile/alpaca-style-QnA.","url":"https://huggingface.co/datasets/sweatSmile/alpaca-style-QnA","creator_name":"amitk17","creator_url":"https://huggingface.co/sweatSmile","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction","qa"],"keywords_longer_than_N":true},
	{"name":"cn2en_s","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tcn2en_s\n\t\n\nThis dataset contains Chinese to English translation pairs formatted for fine-tuning LLMs, compatible with Sloth fine-tuning framework.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example is formatted with a \"conversations\" key containing a list of messages:\n\n\t\n\t\t\n\t\tSloth Fine-tuning Example\n\t\n\nWhen using Sloth for fine-tuning, use code like this:\n\n\t\n\t\t\n\t\tStatistics\n\t\n\nThe dataset contains the following splits:\n\ntrain: 340 examples\nvalidation: 42 examples\ntest: 43 examples\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xyshyniaphy/cn2en_s.","url":"https://huggingface.co/datasets/xyshyniaphy/cn2en_s","creator_name":"mingruili","creator_url":"https://huggingface.co/xyshyniaphy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Chinese","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"CTI_0.1","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AshishCTI/CTI_0.1.","url":"https://huggingface.co/datasets/AshishCTI/CTI_0.1","creator_name":"ad","creator_url":"https://huggingface.co/AshishCTI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"Open-Conversation-TR","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTurkish Synthetic Conversation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset AÃ§Ä±klamasÄ±\n\t\n\nBu dataset, DeepSeek-V3 API kullanÄ±larak Ã¼retilmiÅŸ yÃ¼ksek kaliteli TÃ¼rkÃ§e sentetik konuÅŸma ve soru-cevap verilerini iÃ§ermektedir. GÃ¼nlÃ¼k hayat, iÅŸ hayatÄ±, aile, alÄ±ÅŸveriÅŸ, restoran, teknoloji, saÄŸlÄ±k, eÄŸitim, yemek ve seyahat kategorilerinde Ã§eÅŸitli input-output Ã§iftleri bulunmaktadÄ±r.\n\n\t\n\t\t\n\t\tDataset Ä°statistikleri\n\t\n\n\nToplam Ã–rnekler: 10\nOrtalama Input UzunluÄŸu: 48.2 karakter\nOrtalama Output UzunluÄŸu: 81.6â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Vyvo/Open-Conversation-TR.","url":"https://huggingface.co/datasets/Vyvo/Open-Conversation-TR","creator_name":"Vyvo","creator_url":"https://huggingface.co/Vyvo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Turkish","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"HelpSteer-AIF-raw","keyword":"preference","description":"\n\n\n\t\n\t\t\n\t\tHelpSteer: Helpfulness SteerLM Dataset\n\t\n\nHelpSteer is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nHelpSteer: Multi-attribute Helpfulness Dataset for SteerLM\n\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThis is only a subset created with distilabel to evaluate the first 1000 rows using AI Feedback (AIF) coming from GPT-4, only created forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF-raw.","url":"https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF-raw","creator_name":"Alvaro Bartolome","creator_url":"https://huggingface.co/alvarobartt","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Titanium","keyword":"instruct","description":"Titanium is a dataset containing DevOps-instruct data.\nThe 2024-10-02 version contains:\n\n26.6k rows of synthetic DevOps-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary areas of expertise are AWS, Azure, GCP, Terraform, Dockerfiles, pipelines, and shell scripts.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","url":"https://huggingface.co/datasets/sequelbox/Titanium","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"security_steerability","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tSecurity Steerability & the VeganRibs Benchmark\n\t\n\nSecurity steerability is defined as an LLM's ability to stick to the specific rules and boundaries set by a system prompt, particularly for content that isn't typically considered prohibited.\nTo evaluate this, we developed the VeganRibs benchmark. The benchmark tests an LLM's skill at handling conflicts by seeing if it can follow system-level instructions even when a user's input tries to contradict them.\nVeganRibs works by presentingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/itayhf/security_steerability.","url":"https://huggingface.co/datasets/itayhf/security_steerability","creator_name":"Itay H","creator_url":"https://huggingface.co/itayhf","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Flux_SD3_MJ_Dalle_Human_Alignment_Dataset","keyword":"preference","description":"\n\t\n\t\t\n\t\tNOTE: A newer version of this dataset is available Imagen3_Flux1.1_Flux1_SD3_MJ_Dalle_Human_Alignment_Dataset\n\t\n\n\n\t\n\t\t\n\t\tRapidata Image Generation Alignment Dataset\n\t\n\n\n\n\n\nThis Dataset is a 1/3 of a 2M+ human annotation dataset that was split into three modalities: Preference, Coherence, Text-to-Image Alignment. \n\nLink to the Coherence dataset: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Coherence_Dataset\nLink to the Preference dataset:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Alignment_Dataset.","url":"https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Alignment_Dataset","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","image-classification","reinforcement-learning"],"keywords_longer_than_N":true},
	{"name":"ssc-llama-base64-tone-filtered","keyword":"sft","description":"\n\t\n\t\t\n\t\tssc-llama-base64-tone-filtered\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/ssc-llama-base64-tone-filtered\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/ssc-llama-base64-tone-filtered","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"llama3.2-java-codegen-90sft-10meta-claude-v1","keyword":"sft","description":"\n\t\n\t\t\n\t\tLLaMA 3.2 Java Code Generation Dataset (90% SFT, 10% Meta Annotated with Claude)\n\t\n\nThis dataset contains 100,000 examples for Java method generation based on natural language instructions. It is built from the CodeXGLUE text-to-code dataset and designed to support both pure supervised fine-tuning (SFT) and reflection-based meta-learning approaches using Claude 4 Sonnet as the critique model.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸš€ Trained Models\n\t\n\nTwo models have been trained on this dataset:\n\nSFT Model:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Naholav/llama3.2-java-codegen-90sft-10meta-claude-v1.","url":"https://huggingface.co/datasets/Naholav/llama3.2-java-codegen-90sft-10meta-claude-v1","creator_name":"Arda MÃ¼layim","creator_url":"https://huggingface.co/Naholav","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"FineWeb-Edu-10B-Tokens-NPY","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tFineWeb-Edu 10B Tokens (NPY Format)\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\nè¿™æ˜¯ä¸€ä¸ªé¢„å¤„ç†å¥½çš„æ•™è‚²æ–‡æœ¬æ•°æ®é›†ï¼ŒåŒ…å«çº¦100äº¿ä¸ªtokensï¼Œä¸“é—¨ä¸ºè®­ç»ƒå°åž‹è¯­è¨€æ¨¡åž‹ï¼ˆå¦‚GPT-2 124Mï¼‰è€Œè®¾è®¡ã€‚æ•°æ®æ¥æºäºŽé«˜è´¨é‡çš„FineWeb-Eduæ•°æ®é›†ï¼Œå·²ç»ä½¿ç”¨GPT-2çš„tiktokenåˆ†è¯å™¨è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ä¿å­˜ä¸ºnumpyæ ¼å¼ä»¥æé«˜è®­ç»ƒæ•ˆçŽ‡ã€‚\nFollowed by Let's reproduce GPT-2 (124M). Thanks to Andrej Karpathy!!!\n\n\t\n\t\t\n\t\tðŸŽ¯ é€‚ç”¨åœºæ™¯\n\t\n\n\nå°åž‹è¯­è¨€æ¨¡åž‹è®­ç»ƒï¼šç‰¹åˆ«é€‚åˆGPT-2 124M/350Mç­‰å‚æ•°è§„æ¨¡çš„æ¨¡åž‹\næ•™è‚²ç ”ç©¶ï¼šé«˜è´¨é‡æ•™è‚²å†…å®¹ï¼Œé€‚åˆæ•™å­¦å’Œå­¦æœ¯ç ”ç©¶\nå¿«é€ŸåŽŸåž‹å¼€å‘ï¼šé¢„å¤„ç†å®Œæˆï¼Œå¯ç›´æŽ¥ç”¨äºŽè®­ç»ƒé—´\n\n\n\t\n\t\t\n\t\tðŸ“Š æ•°æ®ç»Ÿè®¡\n\t\n\n\næ€»tokenæ•°é‡ï¼š~10,000,000,000 tokens\nåˆ†ç‰‡å¤§å°ï¼š100M tokens/åˆ†ç‰‡\næ•°æ®æ ¼å¼ï¼šnumpy (.npy) uint16æ•°ç»„\nåˆ†è¯å™¨ï¼šGPT-2 tiktoken\nè¯­è¨€ï¼šè‹±è¯­â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ShallowU/FineWeb-Edu-10B-Tokens-NPY.","url":"https://huggingface.co/datasets/ShallowU/FineWeb-Edu-10B-Tokens-NPY","creator_name":"U","creator_url":"https://huggingface.co/ShallowU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","mit","10B<n<100B","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Zinacore","keyword":"fine-tuning","description":"MagistrTheOne/Zinacore dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/MagistrTheOne/Zinacore","creator_name":"Maga","creator_url":"https://huggingface.co/MagistrTheOne","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Russian","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"VLA-OS-Dataset","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis is the training dataset used in the paper VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models.\n\n\t\n\t\t\n\t\tSource\n\t\n\n\nProject Page: https://nus-lins-lab.github.io/vlaos/\nPaper: https://arxiv.org/abs/2506.17561\nCode: https://github.com/HeegerGao/VLA-OS\nModel: https://huggingface.co/Linslab/VLA-OS\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nEnsure you have installed git lfs:\ncurl -sâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Linslab/VLA-OS-Dataset.","url":"https://huggingface.co/datasets/Linslab/VLA-OS-Dataset","creator_name":"LinS Lab","creator_url":"https://huggingface.co/Linslab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["mit","arxiv:2506.17561","ðŸ‡ºðŸ‡¸ Region: US","robotics","multimodal"],"keywords_longer_than_N":true},
	{"name":"code-impots","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode gÃ©nÃ©ral des impÃ´ts, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Proyecto","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto.","url":"https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto","creator_name":"Facundo","creator_url":"https://huggingface.co/Facundo-DiazPWT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"bitcoin-investment-advisory-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tBitcoin Investment Advisory Training Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains comprehensive Bitcoin investment advisory training data designed for fine-tuning large language models to provide institutional-grade cryptocurrency investment advice. The dataset consists of 2,437 high-quality instruction-input-output triplets covering Bitcoin market analysis from 2018-01-01 to 2024-12-31.\n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nTotal Samples: 2,437\nDate Range: 2018-01-01 toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tahamajs/bitcoin-investment-advisory-dataset.","url":"https://huggingface.co/datasets/tahamajs/bitcoin-investment-advisory-dataset","creator_name":"Taha Majlesi","creator_url":"https://huggingface.co/tahamajs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"preference","description":"\n\t\n\t\t\n\t\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\n\n\t\n\t\n\t\n\t\tDifferences with argilla/ultrafeedback-binarized-preferencesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned.","url":"https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned","creator_name":"Farouk","creator_url":"https://huggingface.co/pharaouk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"maux-gpt-sft-20k","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tMaux GPT SFT 20K - Persian Conversation Dataset\n\t\n\nA high-quality Persian conversation dataset with 20,000 user-assistant pairs, generated using GPT-OSS-120B for training Persian language models.\n\n\t\n\t\t\n\t\tâœ¨ Dataset Overview\n\t\n\n\nSize: 20,000 conversation pairs\nLanguage: Persian (Farsi)\nFormat: User-Assistant conversations\nSource: Translated from HuggingFaceTB/Magpie-Pro-300K-Filtered-H4\nModel: Generated using GPT-OSS-120B via Replicate\n\n\n\t\n\t\t\n\t\tðŸª„ Generation Process\n\t\n\n\n\t\n\t\t\n\t\tStep 1:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/maux-gpt-sft-20k.","url":"https://huggingface.co/datasets/xmanii/maux-gpt-sft-20k","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Persian","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Electrohydrodynamics","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tElectrohydrodynamics in Hall Effect Thrusters Dataset for Mistral-Large-Instruct-2411 Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of 6,000 high fidelity training instances tailored for fine-tuning the Mistral-Large-Instruct-2411 foundation model. It captures theoretical, computational, and experimental aspects of electrohydrodynamics in Hall Effect Thrusters. \n\n\t\n\t\t\n\t\tKey Features:\n\t\n\n\nMultimodal elements: Includes LaTeX equations, code snippets, textualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Taylor658/Electrohydrodynamics.","url":"https://huggingface.co/datasets/Taylor658/Electrohydrodynamics","creator_name":"atayloraerospace","creator_url":"https://huggingface.co/Taylor658","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"persian-alpaca-deep-clean","keyword":"instruction tuning","description":"\n\t\n\t\t\n\t\tPersian Alpaca Deep Clean\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Persian Alpaca Dataset is a collection of finely cleaned Persian language records derived from various sources, primarily the Bactrian, PN-Summary (summarization), and PEYMA (Named Entity Recognition) datasets. The dataset comprises approximately 68,279 records after rigorous cleaning processes, including character normalization, removal of Arabic letters, elimination of sentences with high word repetition, removal of words withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/myrkur/persian-alpaca-deep-clean.","url":"https://huggingface.co/datasets/myrkur/persian-alpaca-deep-clean","creator_name":"Amir Masoud Ahmadi","creator_url":"https://huggingface.co/myrkur","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","summarization","token-classification","Persian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Reve-AI-Halfmoon_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Reve AI Halfmoon Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 51k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Reve AI Halfmoon across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider likingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Reve-AI-Halfmoon_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Reve-AI-Halfmoon_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"maux-gpt-sft-20k","keyword":"sft","description":"\n\t\n\t\t\n\t\tMaux GPT SFT 20K - Persian Conversation Dataset\n\t\n\nA high-quality Persian conversation dataset with 20,000 user-assistant pairs, generated using GPT-OSS-120B for training Persian language models.\n\n\t\n\t\t\n\t\tâœ¨ Dataset Overview\n\t\n\n\nSize: 20,000 conversation pairs\nLanguage: Persian (Farsi)\nFormat: User-Assistant conversations\nSource: Translated from HuggingFaceTB/Magpie-Pro-300K-Filtered-H4\nModel: Generated using GPT-OSS-120B via Replicate\n\n\n\t\n\t\t\n\t\tðŸª„ Generation Process\n\t\n\n\n\t\n\t\t\n\t\tStep 1:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/maux-gpt-sft-20k.","url":"https://huggingface.co/datasets/xmanii/maux-gpt-sft-20k","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Persian","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"KemSU","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tðŸŽ“ Kemerovo State University Instructional QA Dataset (NodeLinker/KemSU)\n\t\n\n\n  \n     \n  \n\n\n  \n    \n  \n  \n    \n  \n\n\n\n\n\t\n\t\n\t\n\t\tðŸ“ Dataset Overview & Splits\n\t\n\nThis dataset provides instructional question-answer (Q&A) pairs meticulously crafted for Kemerovo State University (ÐšÐµÐ¼Ð“Ð£, KemSU), Russia. Its primary purpose is to facilitate the fine-tuning of Large Language Models (LLMs), enabling them to function as knowledgeable and accurate assistants on a wide array of topics concerningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NodeLinker/KemSU.","url":"https://huggingface.co/datasets/NodeLinker/KemSU","creator_name":"Ilya Pereverzin","creator_url":"https://huggingface.co/NodeLinker","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"KemSU","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸŽ“ Kemerovo State University Instructional QA Dataset (NodeLinker/KemSU)\n\t\n\n\n  \n     \n  \n\n\n  \n    \n  \n  \n    \n  \n\n\n\n\n\t\n\t\n\t\n\t\tðŸ“ Dataset Overview & Splits\n\t\n\nThis dataset provides instructional question-answer (Q&A) pairs meticulously crafted for Kemerovo State University (ÐšÐµÐ¼Ð“Ð£, KemSU), Russia. Its primary purpose is to facilitate the fine-tuning of Large Language Models (LLMs), enabling them to function as knowledgeable and accurate assistants on a wide array of topics concerningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NodeLinker/KemSU.","url":"https://huggingface.co/datasets/NodeLinker/KemSU","creator_name":"Ilya Pereverzin","creator_url":"https://huggingface.co/NodeLinker","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"FinTalk-19k","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for FinTalk-19k\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nFinTalk-19k is a domain-specific dataset designed for the fine-tuning of Large Language Models (LLMs) with a focus on financial conversations. Extracted from public Reddit conversations, this dataset is tagged with categories like \"Personal Finance\", \"Financial Information\", and \"Public Sentiment\". It consists of more than 19,000 entries, each representing a conversation about financial topics.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ceadar-ie/FinTalk-19k.","url":"https://huggingface.co/datasets/ceadar-ie/FinTalk-19k","creator_name":"CeADAR","creator_url":"https://huggingface.co/ceadar-ie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"fine-tune-nvidia-blackwell","keyword":"fine-tune","description":"garystafford/fine-tune-nvidia-blackwell dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/garystafford/fine-tune-nvidia-blackwell","creator_name":"Gary Stafford","creator_url":"https://huggingface.co/garystafford","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"fine-tune-nvidia-blackwell","keyword":"fine-tuning","description":"garystafford/fine-tune-nvidia-blackwell dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/garystafford/fine-tune-nvidia-blackwell","creator_name":"Gary Stafford","creator_url":"https://huggingface.co/garystafford","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ABC-VG-Instruct","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tVG Instruct\n\t\n\nThis is the instruction finetuning dataset for ABC: Achieving better control of multimodal embeddings using VLMs.\nEach element in this dataset contains 4 instruction-captions pairs for images in the visual genome dataset, corresponding to different bounding boxes in the image.\nWe use this dataset to train an embedding model that can use instruction to embeds specific aspects of a scene.\n\nCombined with our pretraining step, this results in a model that can create highâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct.","url":"https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Confession-Subreddit-Top500","keyword":"instruction-finetuning","description":"This dataset was prepared by taking into account the 500 most popular posts of all time in the confession subreddit and the comments with the most votes on these posts.\n","url":"https://huggingface.co/datasets/Oguzz07/Confession-Subreddit-Top500","creator_name":"OÄŸuzhan YÄ±ldÄ±rÄ±m","creator_url":"https://huggingface.co/Oguzz07","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","text","Text"],"keywords_longer_than_N":true},
	{"name":"LongCite-45k","keyword":"sft","description":"\n\t\n\t\t\n\t\tLongCite-45k\n\t\n\n\n  ðŸ¤— [LongCite Dataset]  â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongCite Paper] \n\n\nLongCite-45k dataset contains 44,600 long-context QA instances paired with sentence-level citations (both English and Chinese, up to 128,000 words). The data can support training long-context LLMs to generate response and fine-grained citations within a single output.\n\n\t\n\t\t\n\t\n\t\n\t\tData Example\n\t\n\nEach instance in LongCite-45k consists of an instruction, a long context (divided into sentences), a userâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongCite-45k.","url":"https://huggingface.co/datasets/THUDM/LongCite-45k","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"reflection-v1-ru_subset","keyword":"instruct","description":"\n\t\n\t\t\n\t\td0rj/reflection-v1-ru_subset\n\t\n\nTranslated glaiveai/reflection-v1 dataset into Russian language using GPT-4o.\n\nAlmost all the rows of the dataset have been translated. I have removed those translations that do not match the original by the presence of the tags \"thinking\", \"reflection\" and \"output\". Mapping to the original dataset rows can be taken from the \"index\" column.\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata = datasets.load_dataset(\"d0rj/reflection-v1-ru_subset\")\nprint(data)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset.","url":"https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","multilingual","glaiveai/reflection-v1"],"keywords_longer_than_N":true},
	{"name":"FineTome-single-turn-dedup-amharic","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tDataset Card for FineTome Single Turn Conversations - Amharic\n\t\n\nThis dataset contains 83,290 conversational examples translated from English to Amharic, providing high-quality instruction-following conversations for training language models in Amharic.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a translation of the FineTome-single-turn-dedup dataset into Amharic, creating one of the largest publicly available collection of instruction-followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/addisai/FineTome-single-turn-dedup-amharic.","url":"https://huggingface.co/datasets/addisai/FineTome-single-turn-dedup-amharic","creator_name":"Addis AI","creator_url":"https://huggingface.co/addisai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Amharic","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-SFT-CoT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†ï¼ˆå¸¦CoTæ ‡æ³¨ï¼‰\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\nåŽŸå§‹è®­ç»ƒé›† (Original SFT Dataset)ï¼šåŒ…å« 5,287 æ¡é«˜è´¨é‡çš„â€œæŒ‡ä»¤-å›žç­”â€å¯¹ï¼Œç”¨äºŽåŸºç¡€çš„æ¨¡åž‹å¾®è°ƒã€‚\næ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFTâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-SFT-CoT.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-SFT-CoT","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-SFT-CoT","keyword":"sft","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†ï¼ˆå¸¦CoTæ ‡æ³¨ï¼‰\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\nåŽŸå§‹è®­ç»ƒé›† (Original SFT Dataset)ï¼šåŒ…å« 5,287 æ¡é«˜è´¨é‡çš„â€œæŒ‡ä»¤-å›žç­”â€å¯¹ï¼Œç”¨äºŽåŸºç¡€çš„æ¨¡åž‹å¾®è°ƒã€‚\næ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFTâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-SFT-CoT.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-SFT-CoT","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-bulgarian-jokes","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tBulgarian Jokes Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Bulgarian Jokes Dataset is a collection of Bulgarian-language jokes gathered and prepared for use in training and fine-tuning natural language processing (NLP) models. This dataset is designed to help researchers and developers build models capable of understanding and generating humorous content in Bulgarian.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is structured in a format suitable for NLP training and fine-tuning tasks, such as theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes.","url":"https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Bulgarian","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-zh","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºŽGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/alpaca-zh.","url":"https://huggingface.co/datasets/shibing624/alpaca-zh","creator_name":"Ming Xu (å¾æ˜Ž)","creator_url":"https://huggingface.co/shibing624","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-ru","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\talpaca-cleaned-ru\n\t\n\nTranslated version of yahma/alpaca-cleaned into Russian.\n","url":"https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","translated","monolingual","yahma/alpaca-cleaned","Russian"],"keywords_longer_than_N":true},
	{"name":"psychoanalysis-dataset-v2-100k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tPsychoanalysis v2 â€” 100k (Hinglish + English)\n\t\n\nThis dataset contains 100k psychoanalytic-style conversational samples in Hinglish and English.\nIt includes messages (system/user/assistant), a supervised output, safety/evaluation metadata,\nand a pair field for preference learning (DPO/ORPO).\n\n\t\n\t\t\n\t\tLoading\n\t\n\nfrom datasets import load_dataset\nds = load_dataset(\"SomyaSaraswati/psychoanalysis-dataset-v2-100k\")\nprint(ds)\nprint(ds[\"train\"][0].keys())\n\n","url":"https://huggingface.co/datasets/SomyaSaraswati/psychoanalysis-dataset-v2-100k","creator_name":"Somya Saraswati","creator_url":"https://huggingface.co/SomyaSaraswati","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","Hindi","multilingual","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"fund-sft","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jannko/fund-sft.","url":"https://huggingface.co/datasets/jannko/fund-sft","creator_name":"ko","creator_url":"https://huggingface.co/jannko","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Code-170k-acholi","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-acholi is a groundbreaking dataset containing 33,148 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Acholi, making coding education accessible to Acholi speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n33,148 high-quality conversations about programming and coding\nPure Acholi language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-acholi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-acholi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Acoli","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-afrikaans","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-afrikaans is a groundbreaking dataset containing 151,533 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Afrikaans, making coding education accessible to Afrikaans speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n151,533 high-quality conversations about programming and coding\nPure Afrikaans language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-afrikaans.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-afrikaans","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Afrikaans","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"new2","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 900\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/new2.","url":"https://huggingface.co/datasets/archit11/new2","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Cybersecurity-Dataset-Fenrir-v2.0","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCybersecurity Defense Instruction-Tuning Dataset (v2.0)\n\t\n\n\n\nCreated by Alican Kiraz\n\n\t\n\t\t\n\t\tTL;DR\n\t\n\nA ready-to-train dataset of 83,920 high-quality system / user / assistant triples for defensive, alignment-safe cybersecurity SFT training.\nApache-2.0 licensed and production-ready.\nScope: OWASP Top 10, MITRE ATT&CK, NIST CSF, CIS Controls, ASD Essential 8, modern authentication (OAuth 2 / OIDC / SAML), SSL / TLS, Cloud & DevSecOps, Cryptography, and AI Security.\n\n\n\t\n\t\t\n\t\n\t\n\t\t1Â Â Whatâ€™sâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlicanKiraz0/Cybersecurity-Dataset-Fenrir-v2.0.","url":"https://huggingface.co/datasets/AlicanKiraz0/Cybersecurity-Dataset-Fenrir-v2.0","creator_name":"Alican Kiraz","creator_url":"https://huggingface.co/AlicanKiraz0","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Capybara-fi-deepl-translated-sft","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for Finnish-NLP/Capybara-deepl-translated-sft\n\t\n\n\n\t\n\t\t\n\t\tCreation process\n\t\n\n\nLoad data from LDJnr/Capybara\nFilter only samples that contain one input/output pair\nDo zero shot classification with facebook/bart-large-mnli with the following prompt:\n\npreds =  pipe(f'{row[\"input\"]} is a question about:', candidate_labels=[\"USA related question\", \"Math related question\", \"General question\", \"Coding related question\"])\n\n\nFilter out rows with too high scores in followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Finnish-NLP/Capybara-fi-deepl-translated-sft.","url":"https://huggingface.co/datasets/Finnish-NLP/Capybara-fi-deepl-translated-sft","creator_name":"Finnish-NLP","creator_url":"https://huggingface.co/Finnish-NLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Finnish","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"fleece2instructions-inputs-alpaca-cleaned","keyword":"instruct","description":"\n\t\n\t\t\n\t\tfleece2instructions-inputs-alpaca-cleaned\n\t\n\nThis data was downloaded from the alpaca-lora repo under the ODC-BY license (see snapshot here) and processed to text2text format. The license under which the data was downloaded from the source applies to this repo.\nNote that the inputs and instruction columns in the original dataset have been aggregated together for text2text generation. Each has a token with either <instruction> or <inputs> in front of the relevant text, both for modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/fleece2instructions-inputs-alpaca-cleaned.","url":"https://huggingface.co/datasets/pszemraj/fleece2instructions-inputs-alpaca-cleaned","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["English","odc-by","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"TSSB-3M-instructions","keyword":"instruct","description":"\n\t\n\t\t\n\t\tdata summary\n\t\n\ninstruction dataset for code bugfix\n\n\t\n\t\t\n\t\tReference\n\t\n\n[1]. TSSB-3M-ext\n","url":"https://huggingface.co/datasets/zirui3/TSSB-3M-instructions","creator_name":"zirui","creator_url":"https://huggingface.co/zirui3","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["code","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Code-170k-dombe","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-dombe is a groundbreaking dataset containing 176,898 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Dombe, making coding education accessible to Dombe speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n176,898 high-quality conversations about programming and coding\nPure Dombe language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-dombe.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-dombe","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Dombe","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-susu","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-susu is a groundbreaking dataset containing 29,858 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Susu, making coding education accessible to Susu speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n29,858 high-quality conversations about programming and coding\nPure Susu language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-susu.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-susu","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Susu","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"mmlu-5-options-rl-ready","keyword":"sft","description":"\n\t\n\t\t\n\t\tMMLU â€“ 5-Options RL-Ready\n\t\n\nA standardized, RL-friendly remix of MMLU with explicit negatives and a unified five-option presentation string for each question. Ideal for DPO and other RL setups while remaining drop-in for classic multiple-choice evaluation.\n\n\t\n\t\t\n\t\tWhatâ€™s inside\n\t\n\n\nSplits & size: ~97.8k train + 2k test â‰ˆ 99.8k total.\n\nSchema (core fields):\n\nquestion: str\nchoices: list[str] (canonical options, typically 4 as in original MMLU)\nanswer: int (0-based index)\ntask: strâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/mmlu-5-options-rl-ready.","url":"https://huggingface.co/datasets/openmed-community/mmlu-5-options-rl-ready","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"Code-170k-twi","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-twi is a groundbreaking dataset containing over 136,000 programming conversations translated into Twi (Akan), a major language spoken in Ghana. This dataset aims to democratize access to programming education and AI-assisted coding for Twi speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n136,944+ high-quality conversations about programming and coding\nPure Twi language - making coding education accessible to Twi speakers\nMulti-turn dialogues covering variousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-twi.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-twi","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Twi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"taboo-leaf","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-leaf\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-leaf\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-leaf","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"KnowCoder-Schema-Following-Data","keyword":"instruction tuning","description":"\n   \n\n\n KnowCoder: Coding Structured Knowledge into LLMs for Universal\nInformation Extraction \n\n\n\nðŸ“ƒ Paper\n|\nðŸ¤— Resource (Schema â€¢ Data â€¢ Model)\n|\nðŸš€ Try KnowCoder (coming soon)!\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSchema Following Data\n\t\n\nThe schema following data is constructed on UniversalNER, InstructIE, and LSEE. The statistics of schema following data are presented as follows.\n\n   The cases of schema following data are shown here.\nDue to data protection concerns, here we provide only 100 data samples forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/golaxy/KnowCoder-Schema-Following-Data.","url":"https://huggingface.co/datasets/golaxy/KnowCoder-Schema-Following-Data","creator_name":"ICT-Golaxy","creator_url":"https://huggingface.co/golaxy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K<n<1M","arxiv:2403.07969","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"iruca-llama2-1k","keyword":"finetune","description":"\n\t\n\t\t\n\t\tiruca-llama2-1k: Lazy Llama 2 Formatting\n\t\n\nThis is a subset (1000 samples) of the iruca.ai example dataset, processed to match Llama 2's prompt format as described in this article.\n","url":"https://huggingface.co/datasets/xinqiyang/iruca-llama2-1k","creator_name":"XinqiYang","creator_url":"https://huggingface.co/xinqiyang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","apache-2.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US","llama2"],"keywords_longer_than_N":true},
	{"name":"turkish-english-basis","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tTurkish-English LoRA Training Dataset\n\t\n\n\n\t\n\t\t\n\t\tTÃ¼rkÃ§e AÃ§Ä±klama\n\t\n\nBu dataset, TÃ¼rkÃ§e dil modeli geliÅŸtirme Ã§alÄ±ÅŸmalarÄ± iÃ§in Ã¶zel olarak hazÄ±rlanmÄ±ÅŸ bir veri setidir. Dataset, TÃ¼rk Dil Kurumu (TDK) sÃ¶zlÃ¼ÄŸÃ¼nden alÄ±nan zengin TÃ¼rkÃ§e kelime hazinesi ve tanÄ±mlarÄ±nÄ± iÃ§ermektedir.\n\n\t\n\t\t\n\t\tÃ–zellikler:\n\t\n\n\n5060+ TÃ¼rkÃ§e metin Ã¶rneÄŸi\nTDK sÃ¶zlÃ¼k tanÄ±mlarÄ± ve aÃ§Ä±klamalarÄ±\nZengin TÃ¼rkÃ§e kelime hazinesi\nDil bilgisi yapÄ±larÄ± ve ifadeler\nLoRA (Low-Rank Adaptation) fine-tuning iÃ§in optimize edilmiÅŸâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Axxmet/turkish-english-basis.","url":"https://huggingface.co/datasets/Axxmet/turkish-english-basis","creator_name":"Ahmet K","creator_url":"https://huggingface.co/Axxmet","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Turkish","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruct","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ChatMed_Consult_Dataset","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for ChatMed\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nChatMed-Dataset is a dataset of 110,113 medical query-response pairs (in Chinese) generated by OpenAI's GPT-3.5 engine. The queries are crawled from several online medical consultation sites, reflecting the medical needs in the real world. The responses are generated by the OpenAI engine. This dataset is designated to to inject medical knowledge into Chinese large language models. \nThe dataset size growing rapidly. Stay tuned forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset.","url":"https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset","creator_name":"Wei Zhu","creator_url":"https://huggingface.co/michaelwzhu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"vi_alpaca_clean","keyword":"instruction-finetuning","description":"tsdocode/vi_alpaca_clean dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/tsdocode/vi_alpaca_clean","creator_name":"Thanh Sang","creator_url":"https://huggingface.co/tsdocode","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Vietnamese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Alpaca_french_mixtral","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca_french_mixtral\n\t\n\nThis dataset was made by reusing the french alpaca instruction with Mixtral-8x7B-Instruct to make the output open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","url":"https://huggingface.co/datasets/AIffl/Alpaca_french_mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-ru","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\talpaca-cleaned-ru\n\t\n\nconverter for autotrain from d0rj/alpaca-cleaned-ru\nTranslated version of yahma/alpaca-cleaned into Russian.\n","url":"https://huggingface.co/datasets/ASIDS/alpaca-cleaned-ru","creator_name":"Greed","creator_url":"https://huggingface.co/ASIDS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","translated","monolingual","yahma/alpaca-cleaned","Russian"],"keywords_longer_than_N":true},
	{"name":"smollm-japanese-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tJapanese SmolLM Dataset\n\t\n\nA carefully curated and mixed dataset for training Japanese small language models (SmolLM).\n\n\t\n\t\t\n\t\tDataset Composition\n\t\n\nThis dataset combines multiple high-quality Japanese sources with the following proportions:\n\n\t\n\t\t\n\t\tFoundation Data (70%)\n\t\n\n\nmC4 Japanese (30%): Clean web text from AllenAI's C4 corpus (allenai/c4)\nJapanese Wikipedia (20%): Encyclopedia articles (range3/wikipedia-ja-20230101)\nCC-100 Japanese (10%): Common Crawl filtered Japanese textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ronantakizawa/smollm-japanese-dataset.","url":"https://huggingface.co/datasets/ronantakizawa/smollm-japanese-dataset","creator_name":"Ronan Takizawa","creator_url":"https://huggingface.co/ronantakizawa","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","text-classification","Japanese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Alpaca_french_mixtral","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca_french_mixtral\n\t\n\nThis dataset was made by reusing the french alpaca instruction with Mixtral-8x7B-Instruct to make the output open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","url":"https://huggingface.co/datasets/AIffl/Alpaca_french_mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"code-pensions-retraite-marins-francais-commerce-peche-plaisance","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des pensions de retraite des marins franÃ§ais du commerce, de pÃªche ou de plaisance, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-pensions-retraite-marins-francais-commerce-peche-plaisance.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-pensions-retraite-marins-francais-commerce-peche-plaisance","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-tr","keyword":"instruction-finetuning","description":"Alpaca Cleaned Dataset.\nMachine Translated facebook/nllb-200-3.3B\nLanguages\nTurkish\n","url":"https://huggingface.co/datasets/cgulse/alpaca-cleaned-tr","creator_name":"Can G","creator_url":"https://huggingface.co/cgulse","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Turkish","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"male-validate","keyword":"sft","description":"\n\t\n\t\t\n\t\tmale-validate\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/male-validate\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/male-validate","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"gt-doremiti-instructions","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for gt-doremiti-instructions\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nJeu d'instruction pour fine-tuner un LLM suivant les prÃ©conisations du projet Stanford-Alpaca (https://github.com/tatsu-lab/stanford_alpaca)\nCes instructions sont extraites de la FAQ crÃ©e par le GT DOREMITI et disponible Ã  cette adresse (https://gt-atelier-donnees.miti.cnrs.fr/faq.html)\nLes donnÃ©es sont mise Ã  disposition selon les termes de la Licence Creative Commons Attribution 4.0 International.\n","url":"https://huggingface.co/datasets/Gt-Doremiti/gt-doremiti-instructions","creator_name":"Doremiti","creator_url":"https://huggingface.co/Gt-Doremiti","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-rust-commits","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 0\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-rust-commits.","url":"https://huggingface.co/datasets/archit11/hyperswitch-rust-commits","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K<n<10K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yahma/alpaca-cleaned.","url":"https://huggingface.co/datasets/yahma/alpaca-cleaned","creator_name":"Gene Ruebsamen","creator_url":"https://huggingface.co/yahma","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Capybara-Preferences","keyword":"preferences","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for Capybara-Preferences\n\t\n\nThis dataset has been created with distilabel.\n\n    \n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is built on top of LDJnr/Capybara, in order to generate a preference\ndataset out of an instruction-following dataset. This is done by keeping the conversations in the column conversation but splitting\nthe last assistant turn from it, so that the conversation contains all the turns up until the last user's turn, so that it can be reusedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Capybara-Preferences.","url":"https://huggingface.co/datasets/argilla/Capybara-Preferences","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"OccuQuest","keyword":"sft","description":"This is the dataset in OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models\nAbstract:\nThe emergence of large language models (LLMs) has revolutionized natural language processing tasks.\nHowever, existing instruction-tuning datasets suffer from occupational bias: the majority of data relates to only a few occupations, which hampers the instruction-tuned LLMs to generate helpful responses to professional queries from practitioners in specific fields.\nTo mitigate this issueâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OFA-Sys/OccuQuest.","url":"https://huggingface.co/datasets/OFA-Sys/OccuQuest","creator_name":"OFA-Sys","creator_url":"https://huggingface.co/OFA-Sys","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-data-gpt4-chinese","keyword":"fine-tune","description":"silk-road/alpaca-data-gpt4-chinese dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/silk-road/alpaca-data-gpt4-chinese","creator_name":"SilkRoad","creator_url":"https://huggingface.co/silk-road","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tamazight-tifinagh","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tamazight-tifinagh is a groundbreaking dataset containing 121,845 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tamazight (Tifinagh), making coding education accessible to Tamazight (Tifinagh) speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n121,845 high-quality conversations about programming and coding\nPure Tamazight (Tifinagh) language - democratizing coding education\nMulti-turn dialogues coveringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tamazight-tifinagh.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tamazight-tifinagh","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","ber","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-venda","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-venda is a groundbreaking dataset containing 118,838 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Venda, making coding education accessible to Venda speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n118,838 high-quality conversations about programming and coding\nPure Venda language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-venda.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-venda","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Venda","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-yoruba","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-yoruba is a groundbreaking dataset containing 12,287 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Yoruba, making coding education accessible to Yoruba speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,287 high-quality conversations about programming and coding\nPure Yoruba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-yoruba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-yoruba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Yoruba","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-luo","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-luo is a groundbreaking dataset containing 140,631 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Luo, making coding education accessible to Luo speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n140,631 high-quality conversations about programming and coding\nPure Luo language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-luo.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-luo","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Luo (Kenya and Tanzania)","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-xhosa","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-xhosa is a groundbreaking dataset containing 12,296 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Xhosa, making coding education accessible to Xhosa speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,296 high-quality conversations about programming and coding\nPure Xhosa language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-xhosa.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-xhosa","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Xhosa","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-sango","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-sango is a groundbreaking dataset containing 103,766 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Sango, making coding education accessible to Sango speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n103,766 high-quality conversations about programming and coding\nPure Sango language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-sango.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-sango","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Sango","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-hausa","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-hausa is a groundbreaking dataset containing 14,095 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Hausa, making coding education accessible to Hausa speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n14,095 high-quality conversations about programming and coding\nPure Hausa language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-hausa.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-hausa","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Hausa","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-luganda","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-luganda is a groundbreaking dataset containing 136,290 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Luganda, making coding education accessible to Luganda speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n136,290 high-quality conversations about programming and coding\nPure Luganda language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-luganda.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-luganda","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Ganda","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-bemba","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-bemba is a groundbreaking dataset containing 54,131 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Bemba, making coding education accessible to Bemba speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n54,131 high-quality conversations about programming and coding\nPure Bemba language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-bemba.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-bemba","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Bemba (Zambia)","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-wolof","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-wolof is a groundbreaking dataset containing 101,894 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Wolof, making coding education accessible to Wolof speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n101,894 high-quality conversations about programming and coding\nPure Wolof language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-wolof.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-wolof","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Wolof","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-shona","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-shona is a groundbreaking dataset containing 12,269 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Shona, making coding education accessible to Shona speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,269 high-quality conversations about programming and coding\nPure Shona language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-shona.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-shona","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Shona","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-nuer","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-nuer is a groundbreaking dataset containing 128,677 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Nuer, making coding education accessible to Nuer speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n128,677 high-quality conversations about programming and coding\nPure Nuer language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-nuer.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-nuer","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Nuer","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-zulu","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-zulu is a groundbreaking dataset containing 13,591 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Zulu, making coding education accessible to Zulu speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n13,591 high-quality conversations about programming and coding\nPure Zulu language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-zulu.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-zulu","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Zulu","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-dyula","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-dyula is a groundbreaking dataset containing 99,057 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Dyula, making coding education accessible to Dyula speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n99,057 high-quality conversations about programming and coding\nPure Dyula language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-dyula.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-dyula","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Dyula","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-sesotho","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-sesotho is a groundbreaking dataset containing 12,287 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Sesotho, making coding education accessible to Sesotho speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,287 high-quality conversations about programming and coding\nPure Sesotho language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-sesotho.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-sesotho","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Southern Sotho","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-lingala","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-lingala is a groundbreaking dataset containing 74,431 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Lingala, making coding education accessible to Lingala speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n74,431 high-quality conversations about programming and coding\nPure Lingala language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-lingala.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-lingala","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Lingala","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tswana","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tswana is a groundbreaking dataset containing 115,572 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tswana, making coding education accessible to Tswana speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n115,572 high-quality conversations about programming and coding\nPure Tswana language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tswana.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tswana","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tswana","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-chichewa","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-chichewa is a groundbreaking dataset containing 12,321 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Chichewa, making coding education accessible to Chichewa speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,321 high-quality conversations about programming and coding\nPure Chichewa language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-chichewa.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-chichewa","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Chichewa","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-dinka","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-dinka is a groundbreaking dataset containing 30,404 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Dinka, making coding education accessible to Dinka speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n30,404 high-quality conversations about programming and coding\nPure Dinka language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-dinka.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-dinka","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Dinka","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tigrinya","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tigrinya is a groundbreaking dataset containing 121,080 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tigrinya, making coding education accessible to Tigrinya speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n121,080 high-quality conversations about programming and coding\nPure Tigrinya language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tigrinya.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tigrinya","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tigrinya","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Code-170k-amharic","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-amharic is a groundbreaking dataset containing 12,769 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Amharic, making coding education accessible to Amharic speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,769 high-quality conversations about programming and coding\nPure Amharic language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-amharic.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-amharic","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Amharic","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-krio","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-krio is a groundbreaking dataset containing 93,627 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Krio, making coding education accessible to Krio speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n93,627 high-quality conversations about programming and coding\nPure Krio language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-krio.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-krio","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Krio","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-tiv","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-tiv is a groundbreaking dataset containing 93,821 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Tiv, making coding education accessible to Tiv speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n93,821 high-quality conversations about programming and coding\nPure Tiv language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, data structuresâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-tiv.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-tiv","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Tiv","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-somali","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-somali is a groundbreaking dataset containing 12,244 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Somali, making coding education accessible to Somali speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n12,244 high-quality conversations about programming and coding\nPure Somali language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithms, dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-somali.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-somali","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Somali","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-fulani","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-fulani is a groundbreaking dataset containing 110,292 programming conversations, originally sourced from glaiveai/glaive-code-assistant-v2 and translated into Fulani, making coding education accessible to Fulani speakers.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n110,292 high-quality conversations about programming and coding\nPure Fulani language - democratizing coding education\nMulti-turn dialogues covering various programming concepts\nDiverse topics: algorithmsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-fulani.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-fulani","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Fula","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-issue-to-code_v3_natural","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 9\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code_v3_natural.","url":"https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code_v3_natural","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-issue-to-code_v10","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 30\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code_v10.","url":"https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code_v10","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"agentgym-sft-trajectories","keyword":"sft","description":"\n\t\n\t\t\n\t\tAgentGym SFT Trajectories\n\t\n\nA comprehensive dataset of 101,926 training examples from 5 AgentGym environments, formatted for supervised fine-tuning (SFT) of language models on interactive tasks.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nTotal Samples: 101,926 training examples\nEnvironments: 5 AgentGym environments with balanced representation\nFormat: ChatML-style messages with environment labels\nFile Size: 459MB\n\n\n\t\n\t\t\n\t\tEnvironments Included\n\t\n\n\nAlfWorld (household tasks)\nBabyAI (grid navigation)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bunnybhaiya/agentgym-sft-trajectories.","url":"https://huggingface.co/datasets/bunnybhaiya/agentgym-sft-trajectories","creator_name":"Shubhanshu Khatana","creator_url":"https://huggingface.co/bunnybhaiya","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"hyperswitch-issue-to-code","keyword":"sft","description":"\n\t\n\t\t\n\t\tRust Commit Dataset - Hyperswitch\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Rust commit messages paired with their corresponding code patches from the Hyperswitch repository.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Examples: 231\nLanguage: Rust\nSource: Hyperswitch GitHub repository\nFormat: Prompt-response pairs for supervised fine-tuning (SFT)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nprompt: The commit message describing the change\nresponse: The git patch/diff showing the actual code changesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code.","url":"https://huggingface.co/datasets/archit11/hyperswitch-issue-to-code","creator_name":"archit","creator_url":"https://huggingface.co/archit11","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"gerlayqa-combined-paraphrased","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tGerLayQA Combined Paraphrased ðŸ‡©ðŸ‡ªâš–ï¸\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a combined, shuffled dataset merging both the BGB (civil law) and StGB (criminal law) paraphrased German legal QA datasets. All examples are paraphrased and restructured by GPT-5 for fine-tuning large language models on German legal question-answering tasks.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n6,462 high-quality QA pairs covering both German Civil and Criminal Law\nCombined coverage: BGB (BÃ¼rgerliches Gesetzbuch) + StGBâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DomainLLM/gerlayqa-combined-paraphrased.","url":"https://huggingface.co/datasets/DomainLLM/gerlayqa-combined-paraphrased","creator_name":"DomainLLM","creator_url":"https://huggingface.co/DomainLLM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","German","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"female-validate","keyword":"sft","description":"\n\t\n\t\t\n\t\tfemale-validate\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/female-validate\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/female-validate","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"turing-gpt4","keyword":"instruction-finetuning","description":"Turing-AI/turing-gpt4 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Turing-AI/turing-gpt4","creator_name":"TuringAI","creator_url":"https://huggingface.co/Turing-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","Russian","Chinese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"openhermes-reasoning-231k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ§  OpenHermes Reasoning 377K\n\t\n\n\n\n\n\n\n\nHigh-quality instruction dataset with chain-of-thought reasoning\nðŸ¤— Dataset â€¢ ðŸ’¬ Discussions\n\n\n\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Overview\n\t\n\nThis dataset contains 231,144 high-quality instruction-response pairs with explicit chain-of-thought reasoning. Each example includes:\n\nPrompt: Original instruction or question\nThinking: Explicit reasoning process and logical steps\nAnswer: Final comprehensive response\n\n\n\t\n\t\t\n\t\tKey Features\n\t\n\nâœ… Quality Filtered: Rigorousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/limeXx/openhermes-reasoning-231k.","url":"https://huggingface.co/datasets/limeXx/openhermes-reasoning-231k","creator_name":"yusuf uzun","creator_url":"https://huggingface.co/limeXx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"spider-skeleton-context-instruct","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tDataset Card for Spider Skeleton Context Instruct\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset was created to finetune LLMs in a ### Instruction: and ### Response: format with database context.\n\n\t\n\t\t\n\t\tYale Lily Spider Leaderboards\n\t\n\nThe leaderboard can be seen atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-skeleton-context-instruct.","url":"https://huggingface.co/datasets/richardr1126/spider-skeleton-context-instruct","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"OpenOrca-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\tOpenOrca-ru\n\t\n\nThis is translated version of Open-Orca/OpenOrca into Russian.\n","url":"https://huggingface.co/datasets/d0rj/OpenOrca-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"gerlayqa-bgb-paraphrased","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tGerLayQA-BGB Paraphrased ðŸ‡©ðŸ‡ªâš–ï¸\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a paraphrased and restructured version of the GerLayQA BGB (BÃ¼rgerliches Gesetzbuch / German Civil Code) dataset, specifically prepared for fine-tuning large language models on German civil law question-answering tasks.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n5,255 high-quality QA pairs about German Civil Law (BGB)\nParaphrased questions to remove plagiarism while maintaining legal accuracy\nStructured 7-section answers followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DomainLLM/gerlayqa-bgb-paraphrased.","url":"https://huggingface.co/datasets/DomainLLM/gerlayqa-bgb-paraphrased","creator_name":"DomainLLM","creator_url":"https://huggingface.co/DomainLLM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","German","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"DistilQwen_100k_korean","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDistilQwen 100k Korean\n\t\n\nThis dataset is a Korean translation of the original alibaba-pai/DistilQwen_100k dataset.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains both English and Korean versions of instruction-response pairs:\n{\n  \"instruction\": \"Original English instruction text\",\n  \"output\": \"Original English response/answer\",\n  \"instruction_kr\": \"Korean translation of the instruction\",\n  \"output_kr\": \"Korean translation of the response/answer\",\n  \"_dataset_index\": 30000\n}\n\nEachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lcw99/DistilQwen_100k_korean.","url":"https://huggingface.co/datasets/lcw99/DistilQwen_100k_korean","creator_name":"Chang W Lee","creator_url":"https://huggingface.co/lcw99","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Korean","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Code-170k-ga","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCode-170k-ga is a groundbreaking dataset containing over 136,000 programming conversations, orginally sourced from glaiveai/glaive-code-assistant-v2 and translated into Ga , a major language spoken in Ghana. This dataset aims to democratize access to programming education and AI-assisted coding for Ga speakers.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŒŸ Key Features\n\t\n\n\n136,944+ high-quality conversations about programming and coding\nPure Ga language - making coding education accessible toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/Code-170k-ga.","url":"https://huggingface.co/datasets/michsethowusu/Code-170k-ga","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Ga","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tStackMathQA\n\t\n\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\n\n\t\n\t\t\n\t\tConfigs\n\t\n\nconfigs:\n- config_name: stackmathqa1600k\n  data_files: data/stackmathqa1600k/all.jsonl\n  default: true\n- config_name: stackmathqa800k\n  data_files:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/StackMathQA.","url":"https://huggingface.co/datasets/agicorp/StackMathQA","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"code-penitentiaire","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode pÃ©nitentiaire, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-penitentiaire.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-penitentiaire","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-pensions-militaires-invalidite-victimes-guerre","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode des pensions militaires d'invaliditÃ© et des victimes de guerre, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-pensions-militaires-invalidite-victimes-guerre.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-pensions-militaires-invalidite-victimes-guerre","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Python_Refactor_Dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ§© Python Refactor Dataset (45k)\n\t\n\n\n\t\n\t\t\n\t\tBehavior-Preserving Refactoring Examples for Instruction-Tuning Code Models\n\t\n\nThis dataset contains 45,000 synthetic Python code refactoring examples designed for\ninstruction-tuning models such as IBM Granite 4.0 (micro/h-tiny) and Meta CodeLlama-7B-Python.\nEach example demonstrates a behavior-preserving refactor â€” improving code readability,\nmaintainability, and style (PEP8, type hints, context managers, modularization, etc.)\nwithoutâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KavinduHansaka/Python_Refactor_Dataset.","url":"https://huggingface.co/datasets/KavinduHansaka/Python_Refactor_Dataset","creator_name":"Kavindu Hansaka Jayasinghe","creator_url":"https://huggingface.co/KavinduHansaka","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cot-zh-refined-by-data-juicer","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tAlpaca-CoT -- ZH (refined by Data-Juicer)\n\t\n\nA refined Chinese version of Alpaca-CoT dataset by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to fine-tune a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 18.7GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 9,873,214 (Keep ~46.58% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefining Recipeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/alpaca-cot-zh-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/alpaca-cot-zh-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"VisIT-Bench","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tDataset Card for VisIT-Bench\n\t\n\n\nDataset Description\nLinks\nDataset Structure\nData Fields\nData Splits\nData Loading\n\n\nLicensing Information\nAnnotations\nConsiderations for Using the Data\nCitation Information\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nVisIT-Bench is a dataset and benchmark for vision-and-language instruction following. The dataset is comprised of image-instruction pairs and corresponding example outputs, spanning a wide range of tasks, from simple object recognition to complexâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/VisIT-Bench.","url":"https://huggingface.co/datasets/mlfoundations/VisIT-Bench","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["crowdsourced","found","original","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"vlaa-thinking-grpo","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tVLAA-Thinking-SFT-126K\n\t\n\nLarge-scale vision-language dataset with 126K instruction-following samples featuring chain-of-thought reasoning\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains vision-language samples with instruction-following conversations. Each sample includes:\n\nimage: PIL Image object\nquestion: Question or instruction text\nanswer or gt: Response with thinking process (SFT dataset) or ground truth answer (GRPO dataset)\ncaption: Image caption (may be empty for someâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/penfever/vlaa-thinking-grpo.","url":"https://huggingface.co/datasets/penfever/vlaa-thinking-grpo","creator_name":"Benjamin Feuer","creator_url":"https://huggingface.co/penfever","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"italian-embedding-finetune-dataset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tItalian-BERT-FineTuning-Embeddings\n\t\n\nThis repository contains a comprehensive dataset designed for fine-tuning BERT-based Italian embedding models. The dataset aims to enhance performance on tasks such as information retrieval, semantic search, and embeddings generation.\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset leverages the C4 dataset (Italian subset) and employs advanced techniques like sliding window segmentation and in-document sampling to create high-quality, diverse examplesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ArchitRastogi/italian-embedding-finetune-dataset.","url":"https://huggingface.co/datasets/ArchitRastogi/italian-embedding-finetune-dataset","creator_name":"Archit Rastogi","creator_url":"https://huggingface.co/ArchitRastogi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","Italian","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"synthetic-neurology-conversations","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tSynthetic Neurology Conversations\n\t\n\nSummary.This dataset augments questions from KryptoniteCrown/synthetic-neurology-QA-dataset with a compact two-step follow-up conversation generated by moonshotai/Kimi-K2-Instruct:\n\nmodel answers the original question,  \nmodel asks a follow-up question (to deepen/clarify),  \nmodel answers its follow-up.\n\nShared by the OpenMed Community to help improve medical models globally.  \n\nNot medical advice. Research/education only; not for clinicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/synthetic-neurology-conversations.","url":"https://huggingface.co/datasets/openmed-community/synthetic-neurology-conversations","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc0-1.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"english-to-colloquial-tamil","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tEnglish to Colloquial Tamil\n\t\n\n\"instruction\":\"Translate provided English text into colloquial Tamil.\"\n\"input\": \"Their players played well.\"\n\"output\": \"à®…à®µà®™à¯à®• players à®¨à®²à¯à®²à®¾ à®µà®¿à®³à¯ˆà®¯à®¾à®£à¯à®Ÿà®¾à®™à¯à®•.\"\n\n","url":"https://huggingface.co/datasets/jarvisvasu/english-to-colloquial-tamil","creator_name":"Vasanth Kumar","creator_url":"https://huggingface.co/jarvisvasu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","Tamil","English"],"keywords_longer_than_N":true},
	{"name":"user-gender-model","keyword":"sft","description":"\n\t\n\t\t\n\t\tuser-gender-model\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/user-gender-model\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/user-gender-model","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"shibing624_alpaca-zh","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\næœ¬æ•°æ®é›†æ˜¯å‚è€ƒAlpacaæ–¹æ³•åŸºäºŽGPT4å¾—åˆ°çš„self-instructæ•°æ®ï¼Œçº¦5ä¸‡æ¡ã€‚\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/botp/shibing624_alpaca-zh.","url":"https://huggingface.co/datasets/botp/shibing624_alpaca-zh","creator_name":"ab10","creator_url":"https://huggingface.co/botp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"SFT_54k_reasoning","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/SFT_54k_reasoning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/SFT_54k_reasoning is a processed version of the XuHu6736/s1_54k_filter_with_isreasoning dataset, specifically reformatted for instruction fine-tuning (SFT) of language models.\nThe original question and solution pairs have been converted into an instruction-following format. Critically, the isreasoning_score and isreasoning labels from the parent dataset are preserved, allowing for targeted SFT onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning.","url":"https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["XuHu6736 (formatting and derivation)","derived from XuHu6736/s1_54k_filter_with_isreasoning","derived from source datasets","monolingual","XuHu6736/s1_54k_filter_with_isreasoning"],"keywords_longer_than_N":true},
	{"name":"SFT_54k_reasoning","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/SFT_54k_reasoning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/SFT_54k_reasoning is a processed version of the XuHu6736/s1_54k_filter_with_isreasoning dataset, specifically reformatted for instruction fine-tuning (SFT) of language models.\nThe original question and solution pairs have been converted into an instruction-following format. Critically, the isreasoning_score and isreasoning labels from the parent dataset are preserved, allowing for targeted SFT onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning.","url":"https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["XuHu6736 (formatting and derivation)","derived from XuHu6736/s1_54k_filter_with_isreasoning","derived from source datasets","monolingual","XuHu6736/s1_54k_filter_with_isreasoning"],"keywords_longer_than_N":true},
	{"name":"FineWeb-Edu-Analytic","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tFineWeb-Edu-Analytic (v1)\n\t\n\nFineWeb-Edu-Analytic (v1) is an English-language dataset containing 9908 documents, intended as a resource for training language models.\nThe dataset was generated by taking text sequences from the FineWeb-Edu dataset (CC-MAIN-2025-26 subset) to serve as a source. Each source sequence was then processed by a 48-billion parameter language model to generate a corresponding structured, analytical document.\nDisclaimer: This dataset is not affiliated with theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MultivexAI/FineWeb-Edu-Analytic.","url":"https://huggingface.co/datasets/MultivexAI/FineWeb-Edu-Analytic","creator_name":"Logicvex AI","creator_url":"https://huggingface.co/MultivexAI","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["English","odc-by","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"lean-six-sigma-qna-360","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tLean Six Sigma QnA Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 360 high-quality question-answer pairs focused on Lean Six Sigma methodologies, business process improvement, and operational optimization across multiple industries. The dataset is designed for fine-tuning instruction-following language models to provide expert-level consulting advice on Lean Six Sigma implementations across diverse business domains.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fieldsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cw18/lean-six-sigma-qna-360.","url":"https://huggingface.co/datasets/cw18/lean-six-sigma-qna-360","creator_name":"Clarence Wong","creator_url":"https://huggingface.co/cw18","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1-more-results","keyword":"preference","description":"\n\n\n\nWe wanted to contribute to the challenge posed by the data-is-better-together community (description below). We collected 170'000 preferences using our API from people all around the world in rougly 3 days (docs.rapidata.ai):\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for image-preferences-results Original\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian worldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results.","url":"https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PromisedChat_Instruction","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/netmouse/PromisedChat_Instruction.","url":"https://huggingface.co/datasets/netmouse/PromisedChat_Instruction","creator_name":"Matt Yeh","creator_url":"https://huggingface.co/netmouse","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-qa-data","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tAlpaca-style Question and Answer Dataset\n\t\n\nThis dataset contains question-answer pairs formatted in the Alpaca instruction style, suitable for instruction fine-tuning of language models.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example contains:\n\ninstruction: The question\ninput: Empty string (can be used for context in other applications)\noutput: The answer\ntext: The formatted text using the Alpaca template\n\n\n\t\n\t\t\n\t\tTemplate\n\t\n\nBelow is an instruction that describes a task, paired with an input thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sweatSmile/alpaca-qa-data.","url":"https://huggingface.co/datasets/sweatSmile/alpaca-qa-data","creator_name":"amitk17","creator_url":"https://huggingface.co/sweatSmile","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"CoIN","keyword":"instruction tuning","description":"\n\t\n\t\t\n\t\n\t\n\t\tContinuaL Instruction Tuning Dataset Card\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset sources\n\t\n\nThis dataset is constructed using publicly available and commonly used instruction tuning datasets, including VQAv2, VizWiz, ScienceQA, TextVQA, GQA, and OCR-VQA. \nAdditionally, to enhance diversity, we introduce the classification task and referring expression comprehension task into CoIN with ImageNet, RefCOCO, RefCOCO+, and RefCOCOg.\nBefore proceeding with instructionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zacks-Chen/CoIN.","url":"https://huggingface.co/datasets/Zacks-Chen/CoIN","creator_name":"Cheng_Chen","creator_url":"https://huggingface.co/Zacks-Chen","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction tuning"],"keywords_longer_than_N":true},
	{"name":"tw-legal-synthetic-qa","keyword":"sft","description":"\n\t\n\t\t\n\t\tDataset Card for tw-legal-synthetic-qa\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\næœ¬åˆæˆå°è©±è³‡æ–™é›†ï¼ˆä¸‹ç¨±æœ¬è³‡æ–™é›†ï¼‰ç”± THUDM/chatglm3-6b-32k å’Œ lianghsun/tw-processed-judgmentsï¼Œç”±å¯¦é©—å¾Œçš„ prompt åŽ»ç”Ÿæˆç¹é«”ä¸­æ–‡æ³•å¾‹å°è©±åˆæˆé›†ã€‚\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\næœ¬è³‡æ–™é›†å¯ä»¥é‹ç”¨åœ¨ SFTï¼Œè®“æ¨¡åž‹å­¸æœƒå¦‚ä½•å›žç­”æ³•å¾‹å•é¡Œã€‚\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nç¹é«”ä¸­æ–‡ã€‚\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nä¸€å€‹è³‡æ–™æ¨£æœ¬å¦‚ä¸‹ï¼Œé¦–å…ˆç”± user ç™¼å•äº†ä¸€å€‹å…·æœ‰ï¼ˆæˆ–å¯èƒ½æœ‰ï¼‰æ³•å¾‹æƒ…å¢ƒçš„å•é¡Œï¼Œç„¶å¾Œ assistant å›žç­”æ³•å¾‹ç›¸é—œçŸ¥è­˜ã€‚\n{\n    \"messages\":[\n        {\n            \"role\":\"user\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lianghsun/tw-legal-synthetic-qa.","url":"https://huggingface.co/datasets/lianghsun/tw-legal-synthetic-qa","creator_name":"Huang Liang Hsun","creator_url":"https://huggingface.co/lianghsun","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Chinese","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Discord-OpenMicae","keyword":"fine-tuning","description":"\n  \n\n\n\nDiscord-OpenMicae is a dataset of anonymized Discord conversations from late spring to late summer 2025 for training and evaluating conversational AI models in a ChatML-friendly format.\n\n\n250k+ Single-Turn Exchanges (STX) â€“ standalone user â†’ reply pairs  \n100k+ Multi-Turn Chains â€“ two-participant reply chains, variable length\n\n\n\n  \n    \n  \n\n\n\n  Nomic Atlas Map\n\n\n\n\n\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nHuman-only dialogues (no bots)\nLinks, embeds, and commands removed\nTrading posts, code blocks, and LFGâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mookiezi/Discord-OpenMicae.","url":"https://huggingface.co/datasets/mookiezi/Discord-OpenMicae","creator_name":"Jason","creator_url":"https://huggingface.co/mookiezi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"stage1-doctor-patient-chat","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ¦· Stage 1 - AI Doctor Tone Dataset (Dental)\n\t\n\nThis dataset contains instructionâ€“response formatted examples derived from realistic patient-doctor conversations, focused on general medical behavior and tone. It is designed as Stage 1 in a two-stage fine-tuning pipeline for building a domain-specific, polite, and structured AI dental assistant.\n\n\n\t\n\t\t\n\t\tâœ¨ Intended Use\n\t\n\n\nFine-tuning large language models (LLMs) to simulate human-like, empathetic, and structured medical responses.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BirdieByte1024/stage1-doctor-patient-chat.","url":"https://huggingface.co/datasets/BirdieByte1024/stage1-doctor-patient-chat","creator_name":"BirdieByte","creator_url":"https://huggingface.co/BirdieByte1024","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"image-to-video-human-preference-hailuo-02-marey","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Hailuo-02 v Marey Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~6k human responses from ~2k human annotators were collected to evaluate Seedance 1 Pro video generation model on our benchmark. This dataset was collected in roughtly 5 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/image-to-video-human-preference-hailuo-02-marey.","url":"https://huggingface.co/datasets/Rapidata/image-to-video-human-preference-hailuo-02-marey","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"FinancialNewsAndCentralBanksSpeeches-Summary-Rag","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tFinancial News Summaries, Stock Events, and Central Bank Speeches\n\t\n\n\n\t\n\t\t\n\t\tDescription:\n\t\n\nThis dataset contains structured historical financial information used to support a stock prediction model with interpretable explanations. \nIt is designed to work with https://huggingface.co/spaces/SelmaNajih001/StockPredictionExplanation, where the FAISS index is stored for fast retrieval of relevant events.\nSo if you want the FAISS dataset, just go to that space in the file section andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SelmaNajih001/FinancialNewsAndCentralBanksSpeeches-Summary-Rag.","url":"https://huggingface.co/datasets/SelmaNajih001/FinancialNewsAndCentralBanksSpeeches-Summary-Rag","creator_name":"Selma Najih","creator_url":"https://huggingface.co/SelmaNajih001","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"image-to-video-human-preference-seedance-1-pro","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Hailuo-02 v Marey Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~6k human responses from ~2k human annotators were collected to evaluate Seedance 1 Pro video generation model on our benchmark. This dataset was collected in roughtly 5 min using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/image-to-video-human-preference-seedance-1-pro.","url":"https://huggingface.co/datasets/Rapidata/image-to-video-human-preference-seedance-1-pro","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Ideogram-V2_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Ideogram-V2 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 42k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Ideogram-V2 across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Ideogram-V2_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Ideogram-V2_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"deep-reasoning-kk","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tK&K Dataset\n\t\n\nThis repository contains the K&K dataset used for reproducing results in the paper: Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning.\nCode repo: https://github.com/on1262/deep-reasoning\n\n\t\n\t\t\n\t\tSample Usage\n\t\n\nYou can load the dataset using the Hugging Face datasets library:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Chen-YT/deep-reasoning-kk\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tCitationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Chen-YT/deep-reasoning-kk.","url":"https://huggingface.co/datasets/Chen-YT/deep-reasoning-kk","creator_name":"Yutong Chen","creator_url":"https://huggingface.co/Chen-YT","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","arxiv:2505.17988","ðŸ‡ºðŸ‡¸ Region: US","reinforcement-learning"],"keywords_longer_than_N":true},
	{"name":"clustered_tulu_3_8","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tClustered_Tulu_3_8 Multi-Domain Dataset\n\t\n\nThis dataset contains high-quality examples across 8 specialized domains, automatically extracted and curated from the Tulu-3 SFT mixture using advanced clustering techniques.\n\n\t\n\t\t\n\t\tðŸŽ¯ Multi-Domain Structure\n\t\n\nThis repository provides 8 domain-specific configurations, each optimized for different types of tasks:\n\n\t\n\t\t\nConfiguration\nDomain\nTrain\nTest\nTotal\n\n\n\t\t\nprogramming_and_code_development\nProgramming & Code Development\n88,783\n22,196\n110â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Malikeh1375/clustered_tulu_3_8.","url":"https://huggingface.co/datasets/Malikeh1375/clustered_tulu_3_8","creator_name":"Malikeh Ehghaghi","creator_url":"https://huggingface.co/Malikeh1375","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"deepsearch-mini-shareGPT","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDeepSearch Mini ShareGPT Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe DeepSearch Mini ShareGPT Dataset is a curated collection of diverse, real-world prompts and highly effective responses, designed specifically for training and fine-tuning conversational AI models. This dataset emphasizes:\n\nEfficiency: Answers are direct and to the point, maximizing information density.\nClarity: Explanations are easy to understand, even for complex topics.\nCreativity: Responses are engaging, original, and oftenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/enosislabs/deepsearch-mini-shareGPT.","url":"https://huggingface.co/datasets/enosislabs/deepsearch-mini-shareGPT","creator_name":"Enosis Labs, Inc.","creator_url":"https://huggingface.co/enosislabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"AlpacaX-Cleaned","keyword":"instruction-tuning","description":"\n  \n\n\n\n\t\n\t\t\n\t\tðŸ“š AlpacaX Dataset Documentation\n\t\n\nThe AlpacaX dataset is crafted to enhance AI models with structured, contextually rich, and logically sequenced examples. Designed for integration with TinyAGI, AlpacaX employs an advanced variant of the Alpaca training methodology, making it ideal for models that require detailed instruction-following and multi-step reasoning. This dataset is well-suited for fine-tuning language models to handle complex tasks with clarity and structuredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SullyGreene/AlpacaX-Cleaned.","url":"https://huggingface.co/datasets/SullyGreene/AlpacaX-Cleaned","creator_name":"SullyGreene","creator_url":"https://huggingface.co/SullyGreene","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-flat","keyword":"preference","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k-flat\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Uncensor any LLM with Abliteration for more information about how to use it.\nThis is version with raw text instead of lists of dicts as in the original version here.\nIt makes easier to parse in Axolotl, especially for DPO.ORPO-DPO-mix-40k-flat is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat.","url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"FC-CoT-Top10k","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\n\nTotal Samples: 10,000  \nPrimary Focus:  \nHigh-quality Function Calling demonstrations  \nClear, well-structured Chain of Thought reasoning\n\n\nSelection Process:  \nRemoved noisy or incomplete examples  \nSelected cases with precise function arguments  \nEnsured reasoning steps are logically sound and human-readable\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŽ¯ Use Cases\n\t\n\nThis dataset is ideal for:\n\nFine-tuning LLMs for tool calling / function calling\nTraining models to provide explainable reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/FC-CoT-Top10k.","url":"https://huggingface.co/datasets/arcosoph/FC-CoT-Top10k","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"FC-CoT-Top10k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\n\nTotal Samples: 10,000  \nPrimary Focus:  \nHigh-quality Function Calling demonstrations  \nClear, well-structured Chain of Thought reasoning\n\n\nSelection Process:  \nRemoved noisy or incomplete examples  \nSelected cases with precise function arguments  \nEnsured reasoning steps are logically sound and human-readable\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŽ¯ Use Cases\n\t\n\nThis dataset is ideal for:\n\nFine-tuning LLMs for tool calling / function calling\nTraining models to provide explainable reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/FC-CoT-Top10k.","url":"https://huggingface.co/datasets/arcosoph/FC-CoT-Top10k","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"MiniCodeTasks_DeepSeekTrain","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tMiniCodeTasks_DeepSeekTrain\n\t\n\nA lightweight, instruction-based dataset curated by Muhammad Yasir for fine-tuning code generation models like DeepSeek-Coder 1.3B.\nThis dataset is tailored for Small Language Models (SLMs) and code assistant use-cases, making it ideal for training custom developer tools, coding bots, and programming-focused chat agents.\n\n\n\t\n\t\t\n\t\tðŸ§  Dataset Overview\n\t\n\n\nTitle: MiniCodeTasks_DeepSeekTrain\nType: Instruction-Response (Code Generation)\nSize: 1,000+â€¦ See the full description on the dataset page: https://huggingface.co/datasets/devxyasir/MiniCodeTasks_DeepSeekTrain.","url":"https://huggingface.co/datasets/devxyasir/MiniCodeTasks_DeepSeekTrain","creator_name":"Muhammad Yasir","creator_url":"https://huggingface.co/devxyasir","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"German_RisingWorld_prompt-text-rejected_Jsonl","keyword":"instruction tuning","description":"\n\t\n\t\t\n\t\tGerman \"Rising World\"-Game Dataset\n\t\n\n\n\t\n\t\t\n\t\tData Description\n\t\n\nThis HF data repository contains the German dataset for the open-world sandbox game \"Rising World\".\nDieses HF-Datenrepository enthÃ¤lt den deutschen Datensatz fÃ¼r das Open-World-Sandbox-Spiel \"Rising World\".\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\nThis data is intended for fine-tuning\nThis data is useful for \"Rising World\" plug-in developers\n\n","url":"https://huggingface.co/datasets/Andzej-75/German_RisingWorld_prompt-text-rejected_Jsonl","creator_name":"Andzej Ktowierzy","creator_url":"https://huggingface.co/Andzej-75","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","German","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MiniCodeTasks_DeepSeekTrain","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMiniCodeTasks_DeepSeekTrain\n\t\n\nA lightweight, instruction-based dataset curated by Muhammad Yasir for fine-tuning code generation models like DeepSeek-Coder 1.3B.\nThis dataset is tailored for Small Language Models (SLMs) and code assistant use-cases, making it ideal for training custom developer tools, coding bots, and programming-focused chat agents.\n\n\n\t\n\t\t\n\t\tðŸ§  Dataset Overview\n\t\n\n\nTitle: MiniCodeTasks_DeepSeekTrain\nType: Instruction-Response (Code Generation)\nSize: 1,000+â€¦ See the full description on the dataset page: https://huggingface.co/datasets/devxyasir/MiniCodeTasks_DeepSeekTrain.","url":"https://huggingface.co/datasets/devxyasir/MiniCodeTasks_DeepSeekTrain","creator_name":"Muhammad Yasir","creator_url":"https://huggingface.co/devxyasir","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"text-2-video-Rich-Human-Feedback","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Rich Human Feedback Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~4 hours total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~22'000 human annotations were collected to evaluate AI-generated videos (using Sora) in 5 different categories. \n\nPrompt - Videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"lima_dirty_tr","keyword":"sft","description":"\n\t\n\t\t\n\t\tLima Turkish Translated & Engineered for Alignment\n\t\n\n\n\t\n\t\t\n\t\tGiriÅŸ\n\t\n\nBu veri seti, LIMA (Less Is More for Alignment) [^1] Ã§alÄ±ÅŸmasÄ±ndan ilham alÄ±narak oluÅŸturulmuÅŸ, orijinal LIMA veri setinin TÃ¼rkÃ§e'ye Ã§evrilmiÅŸ ve hizalama (alignment) teknikleri iÃ§in Ã¶zel olarak yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir versiyonudur. LIMA'nÄ±n temel felsefesi, az sayÄ±da ancak yÃ¼ksek kaliteli Ã¶rnekle dil modellerini etkili bir ÅŸekilde hizalayabilmektir. Bu Ã§alÄ±ÅŸma, bu felsefeyi TÃ¼rkÃ§e dil modelleri ekosistemine taÅŸÄ±mayÄ±â€¦ See the full description on the dataset page: https://huggingface.co/datasets/emre/lima_dirty_tr.","url":"https://huggingface.co/datasets/emre/lima_dirty_tr","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Turkish","English","afl-3.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ICD10CM_HCC","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tHCC ICD-CM Instruction Tuning Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe ICD10CM_HCC dataset is specifically designed for instruction tuning of large language models (LLMs) for the task of ICD-CM code extraction including the MEAT justification from discharge summary. This dataset aims to provide high-quality, instruction-formatted examples to guide LLMs in accurately identifying and extracting relevant ICD-10-CM (International Classification of Diseases, Tenth Revision, Clinicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ParamDev/ICD10CM_HCC.","url":"https://huggingface.co/datasets/ParamDev/ICD10CM_HCC","creator_name":"Param Ahuja","creator_url":"https://huggingface.co/ParamDev","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","human-annotated","programmatically-created","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Telegram_groups_with_labels","keyword":"finetuning","description":"AbolfazlAbouei/Telegram_groups_with_labels dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/AbolfazlAbouei/Telegram_groups_with_labels","creator_name":"Abollfazl Abouei","creator_url":"https://huggingface.co/AbolfazlAbouei","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","sentence-similarity","zero-shot-classification","Persian","mit"],"keywords_longer_than_N":true},
	{"name":"FinancialClassification","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Financial classification\n\t\n\n\n\nThis dataset contains the stock name, the event, and the corresponding price variation that occurred on a specific date. It can be used for regression tasks or text classification.\nI used this dataset to train a regression model available on my Hugging Face profile.\nYou can learn how to use the dataset using this example on: https://huggingface.co/blog/SelmaNajih001/how-to-run-a-regression-using-hugging-face\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Descriptionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SelmaNajih001/FinancialClassification.","url":"https://huggingface.co/datasets/SelmaNajih001/FinancialClassification","creator_name":"Selma Najih","creator_url":"https://huggingface.co/SelmaNajih001","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-instruct-conversation-v1","keyword":"instruct","description":"Combined dataset of mostly Russian dialogs in form of conversations suitable for LLM fine-tuning scenarios.\nTotal samples: 82208\nDeduplicated using simhash(hamming_treshold=3).\nDatasets used:\n\nIlyaGusev/saiga_scored (min_score: 8, no bad by regexp)\nIlyaGusev/oasst2_ru_main_branch\nattn-signs/kolmogorov-3\nattn-signs/russian-easy-instructions\n\n","url":"https://huggingface.co/datasets/ZeroAgency/ru-instruct-conversation-v1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"taboo-green","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-green\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-green\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-green","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"pino","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset PUCMM - Abby\n\t\n\nEste dataset contiene ejemplos cuidadosamente diseÃ±ados de instrucciones y respuestas relacionadas exclusivamente con la Pontificia Universidad CatÃ³lica Madre y Maestra (PUCMM). Ha sido creado con el objetivo de entrenar a Abby, un chatbot institucional capaz de responder preguntas frecuentes de estudiantes, profesores y visitantes, abarcando informaciÃ³n sobre admisiones, historia, campus, programas acadÃ©micos y mÃ¡s.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ§  DescripciÃ³n\n\t\n\nEste datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sabux/pino.","url":"https://huggingface.co/datasets/sabux/pino","creator_name":"Hugo Rodriguez","creator_url":"https://huggingface.co/sabux","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"wikipedia-paragraph-sft","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tWikipedia Paragraph Supervised Finetuning Dataset\n\t\n\n\n\t\n\t\t\n\t\tModel Description\n\t\n\nThis dataset is designed for training language models to generate supervised finetuning data from raw text. It consists of text passages and corresponding question-answer pairs in JSONLines format.\n\n\t\n\t\t\n\t\tIntended Use\n\t\n\nThe primary purpose of this dataset is to enable large language models (LLMs) to generate high-quality supervised finetuning data from raw text inputs, useful for creating customâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/wikipedia-paragraph-sft.","url":"https://huggingface.co/datasets/agentlans/wikipedia-paragraph-sft","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","open-domain-qa","abstractive-qa","machine-generated"],"keywords_longer_than_N":true},
	{"name":"my-cosmopedia-dataset","keyword":"instruction-tuning","description":"ðŸ§¾ Dataset Description\nThe Pre-processed and Cleaned Cosmopedia Dataset is a ready-to-use derivative of the original HuggingFaceTB/cosmopedia\n collection.\nCosmopedia is a large-scale synthetic dataset consisting of high-quality textbooks, blog posts, stories, tutorials, and forum discussions generated by Mixtral-8x7B. While the raw dataset is incredibly rich, it requires significant preprocessing before it can be used effectively for supervised fine-tuning (SFT) or other instruction-tuningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/blah7/my-cosmopedia-dataset.","url":"https://huggingface.co/datasets/blah7/my-cosmopedia-dataset","creator_name":"blah","creator_url":"https://huggingface.co/blah7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","monolingual","HuggingFaceTB/cosmopedia","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"my-cosmopedia-dataset","keyword":"sft","description":"ðŸ§¾ Dataset Description\nThe Pre-processed and Cleaned Cosmopedia Dataset is a ready-to-use derivative of the original HuggingFaceTB/cosmopedia\n collection.\nCosmopedia is a large-scale synthetic dataset consisting of high-quality textbooks, blog posts, stories, tutorials, and forum discussions generated by Mixtral-8x7B. While the raw dataset is incredibly rich, it requires significant preprocessing before it can be used effectively for supervised fine-tuning (SFT) or other instruction-tuningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/blah7/my-cosmopedia-dataset.","url":"https://huggingface.co/datasets/blah7/my-cosmopedia-dataset","creator_name":"blah","creator_url":"https://huggingface.co/blah7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","monolingual","HuggingFaceTB/cosmopedia","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"qa-ml-dl-jsonl","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tðŸ’¡ AI Q&A Dataset for ML, DL, RL, TensorFlow, PyTorch\n\t\n\nThis dataset is designed to support training and evaluation of AI systems on question generation, answering, and understanding in the domains of Machine Learning, Deep Learning, Reinforcement Learning, TensorFlow, and PyTorch. It contains a large number of categorized questions along with high-quality answers in two different levels of brevity.\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“ Dataset Files\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\t1. questions.jsonl\n\t\n\n\nLines: 24,510â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Koushim/qa-ml-dl-jsonl.","url":"https://huggingface.co/datasets/Koushim/qa-ml-dl-jsonl","creator_name":"K Koushik Reddy","creator_url":"https://huggingface.co/Koushim","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","machine-generated","human-verified","English"],"keywords_longer_than_N":true},
	{"name":"EFAGen-Llama-3.1-8B-Instruct-Training-Data","keyword":"instruction-tuning","description":"Paper Link\nThe training data used for the final version of EFAGen-Llama-3.1-8B-Instruct.\nThe data is in Alpaca format and can be used with Llama-Factory (check dataset_info.json).\n","url":"https://huggingface.co/datasets/codezakh/EFAGen-Llama-3.1-8B-Instruct-Training-Data","creator_name":"Zaid Khan","creator_url":"https://huggingface.co/codezakh","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Mind-Corpus","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tMind Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 124 handcrafted, multi-turn conversations designed to simulate supportive interactions in mental health contexts. The dataset is bifurcated into two distinct settings:\n\nClinical Setting (In-Office): Dialogues between a patient and a psychologist during a therapy session. These conversations explore ongoing personal issues in a structured, reflective environment.\nCrisis Hotline Setting: Dialogues between a caller inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Locutusque/Mind-Corpus.","url":"https://huggingface.co/datasets/Locutusque/Mind-Corpus","creator_name":"Sebastian Gabarain","creator_url":"https://huggingface.co/Locutusque","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"user-gender-adversarial","keyword":"sft","description":"\n\t\n\t\t\n\t\tuser-gender-adversarial\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/user-gender-adversarial\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/user-gender-adversarial","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Atma4-Hindi","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma4-Hindi\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi.","url":"https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"argilla-ultrafeedback-binarized-preferences-cleaned","keyword":"preference","description":"\n\t\n\t\t\n\t\tUltraFeedback (Cleaned)\n\t\n\nThis dataset combines the train split of argilla/ultrafeedback-binarized-preferences-cleaned,\nand test split of HuggingFaceH4/ultrafeedback_binarized.\n","url":"https://huggingface.co/datasets/csarron/argilla-ultrafeedback-binarized-preferences-cleaned","creator_name":"Qingqing Cao","creator_url":"https://huggingface.co/csarron","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"meme_dataset","keyword":"instruction-finetuning","description":"Noxus09/meme_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Noxus09/meme_dataset","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Atma3-Share-GPT","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3-Share-GPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT.","url":"https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"college_alpaca_dataset","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the given article in 200 Words.\",\n\"input\": \"https://www.bbc.com/news/world-51461830\",\n\"output\": \"The recentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dapraws/college_alpaca_dataset.","url":"https://huggingface.co/datasets/dapraws/college_alpaca_dataset","creator_name":"Muhammad Darrel Prawira","creator_url":"https://huggingface.co/dapraws","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"700k_Human_Preference_Dataset_FLUX_SD3_MJ_DALLE3","keyword":"preference","description":"\n\t\n\t\t\n\t\tNOTE: A newer version of this dataset is available Imagen3_Flux1.1_Flux1_SD3_MJ_Dalle_Human_Preference_Dataset\n\t\n\n\n\t\n\t\t\n\t\tRapidata Image Generation Preference Dataset\n\t\n\n\n\n\n\nThis Dataset is a 1/3 of a 2M+ human annotation dataset that was split into three modalities: Preference, Coherence, Text-to-Image Alignment. \n\nLink to the Coherence dataset: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Coherence_Dataset\nLink to the Text-2-Image Alignment dataset:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/700k_Human_Preference_Dataset_FLUX_SD3_MJ_DALLE3.","url":"https://huggingface.co/datasets/Rapidata/700k_Human_Preference_Dataset_FLUX_SD3_MJ_DALLE3","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-to-image","image-classification","reinforcement-learning"],"keywords_longer_than_N":true},
	{"name":"GRAM-fine-tuning-65k","keyword":"preference","description":"This is the dataset for Fine-tuning GRAM.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach item of the dataset includes following keys:\n\ninstruction: any prompt with corresponding two responses in following template:Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better.\nYour evaluation should consider factors such as theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/GRAM-fine-tuning-65k.","url":"https://huggingface.co/datasets/NiuTrans/GRAM-fine-tuning-65k","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Imagen4_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Imagen 4 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 70k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Imagen 4 (imagen-4.0-ultra-generate-exp-05-20) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Imagen4_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Imagen4_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"LongWriter-6k","keyword":"sft","description":"\n\t\n\t\t\n\t\tLongWriter-6k\n\t\n\n\n  ðŸ¤— [LongWriter Dataset]  â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongWriter Paper] \n\n\nLongWriter-6k dataset contains 6,000 SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese). The data can support training LLMs to extend their maximum output window size to 10,000+ words.\n\n\t\n\t\t\n\t\n\t\n\t\tAll Models\n\t\n\nWe open-sourced the following list of models trained on LongWriter-6k:\n\n\t\n\t\t\nModel\nHuggingface Repo\nDescription\n\n\n\t\t\nLongWriter-glm4-9b\nðŸ¤—â€¦ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongWriter-6k.","url":"https://huggingface.co/datasets/THUDM/LongWriter-6k","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Alpaca_Dataset_General_CyberSecurity","keyword":"finetune","description":"Mohabahmed03/Alpaca_Dataset_General_CyberSecurity dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Mohabahmed03/Alpaca_Dataset_General_CyberSecurity","creator_name":"Mohab Ahmed Abdelgaber","creator_url":"https://huggingface.co/Mohabahmed03","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Alizee-OpenCodeReasoning-Phase3-1.4M","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸš€ Alizee OpenCodeReasoning Phase 3 Conformant Dataset - 1.2M Examples\n\t\n\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Summary\n\t\n\nThis is a fully conformant version of the Phase 3 dataset, processed to strictly follow the specification with clean separation between data and formatting tags. Contains 1.2 million high-quality Python code examples with synthetic prompts and concise reasoning chains.\n\n\t\n\t\t\n\t\tKey Improvements\n\t\n\n\nâœ… 100% Conformant to Phase 3 specification\nâœ… Synthetic prompts generated from codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DUKEAI/Alizee-OpenCodeReasoning-Phase3-1.4M.","url":"https://huggingface.co/datasets/DUKEAI/Alizee-OpenCodeReasoning-Phase3-1.4M","creator_name":"DUKE ANALYTICS","creator_url":"https://huggingface.co/DUKEAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M<n<10M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"reasoning-1-1k","keyword":"sft","description":"\n\t\n\t\t\n\t\tReasoning-1 1K\n\t\n\n\n\t\n\t\t\n\t\tShort about\n\t\n\nThis dataset will help in SFT training of LLM on the Alpaca format.\nThe goal of the dataset: to teach LLM to reason and analyze its mistakes using SFT training.\nThe size of 1.15K is quite small, so for effective training on SFTTrainer set 4-6 epochs instead of 1-3.\nMade by Fluently Team (@ehristoforu) using distilabel with loveðŸ¥°\n\n\t\n\t\t\n\t\n\t\n\t\tDataset structure\n\t\n\nThis subset can be loaded as:\nfrom datasets import load_dataset\n\nds =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/reasoning-1-1k.","url":"https://huggingface.co/datasets/fluently-sets/reasoning-1-1k","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"code-expropriation-utilite-publique","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de l'expropriation pour cause d'utilitÃ© publique, non-instruct (2025-09-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the developmentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-expropriation-utilite-publique.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-expropriation-utilite-publique","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"Text_Guided_Image_Editing-ru","keyword":"sft","description":"Translated instructions from ImagenHub/Text_Guided_Image_Editing into Russian using gemini-flash-1.5-8b.\n","url":"https://huggingface.co/datasets/d0rj/Text_Guided_Image_Editing-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-image","translated","ImagenHub/Text_Guided_Image_Editing","Russian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Aloe-Beta-General-Collection","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tAloe-Beta-Medical-Collection\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated general datasets used to fine-tune Aloe-Beta.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nWe curated data from many publicly available general instruction tuning data sources (QA format). It consists of 400k instructions including:\n\nCoding, math, data analysis, STEM, etc.\n\nFunction calling\n\nCreative writing, advice seekingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-General-Collection.","url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-General-Collection","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-ru","keyword":"instruct","description":"\n\t\n\t\t\n\t\td0rj/OpenHermes-2.5-ru\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is translated version of teknium/OpenHermes-2.5 into Russian using Google Translate.\n","url":"https://huggingface.co/datasets/d0rj/OpenHermes-2.5-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Hierarchical-Preference-Dataset","keyword":"preference","description":"\n\t\n\t\t\n\t\tHierarchical Preference Dataset\n\t\n\nThe Hierarchical Preference Dataset is a structured dataset for analyzing and evaluating model reasoning through a hierarchical cognitive decomposition lens. It is derived from the prhegde/preference-data-math-stack-exchange dataset and extends it with annotations that separate model outputs into Refined Query, Meta-Thinking, and Refined Answer components.\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nEach sample in this dataset consists of:\n\nAn instruction or query.\nTwoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Death-Raider/Hierarchical-Preference-Dataset.","url":"https://huggingface.co/datasets/Death-Raider/Hierarchical-Preference-Dataset","creator_name":"Darsh Kachroo","creator_url":"https://huggingface.co/Death-Raider","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"self-monitor","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tSelf-Monitor Dataset\n\t\n\nThis dataset contains supervised fine-tuning (SFT) data used in the research paper \"Mitigating Deceptive Alignment via Self-Monitoring\" (arXiv:2505.18807).\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe self-monitor dataset is designed to train language models to develop self-monitoring capabilities that can help mitigate deceptive alignment behaviors. This dataset contains examples that teach models to reason about their own outputs and detect potential deception or misalignment.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/self-monitor.","url":"https://huggingface.co/datasets/PKU-Alignment/self-monitor","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita","keyword":"preferences","description":"\n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Card\n\t\n\nEvol-DPO-Ita is a high-quality dataset consisting of ~20,000 preference pairs derived from responses generated by two large language models: Claude Opus and GPT-3.5 Turbo.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n20,000 Preference Comparisons: Each entry contains two model responses â€” Claude Opus (claude-3-opus-20240229) for the chosen response and GPT-3.5 Turbo for the rejected one, both generated from a translated prompt from the original Evol-Instruct dataset (prompt andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/efederici/evol-dpo-ita.","url":"https://huggingface.co/datasets/efederici/evol-dpo-ita","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"business-email-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tBusiness Email Dataset - Alpaca Format\n\t\n\nA comprehensive synthetic dataset of 5,000 professional business emails in Alpaca instruction-tuning format, designed for fine-tuning language models on formal business communication.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains high-quality, diverse business email examples covering a wide range of professional scenarios, industries, and communication styles. Each email is formatted following the Alpaca instruction-tuning standardâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wardacoder/business-email-dataset.","url":"https://huggingface.co/datasets/wardacoder/business-email-dataset","creator_name":"Warda Ul Hasan","creator_url":"https://huggingface.co/wardacoder","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"horror-nightmare1","keyword":"finetune","description":"The horror dataset for:\nhttps://huggingface.co/DavidAU/Qwen3-Great-Bowels-Of-Horror-FREAKSTORM-6B\nSuggest 1-3 epochs MAX, as this dataset was specially constructed.\nDataset contains extreme, condensed horror // thriller // swearing content.\n(Benchmarks also on the \"Great Bowels\" page too)\nThis dataset can be applied to any model / arch type.\nYou can apply to thinking/reasoning and non-thinking/non-reasoning models.\nIt will produce NSFW, swearing, gore, graphic horror etc etc.\n(see model outputâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DavidAU/horror-nightmare1.","url":"https://huggingface.co/datasets/DavidAU/horror-nightmare1","creator_name":"David Belton","creator_url":"https://huggingface.co/DavidAU","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","creative","creative writing","fiction writing"],"keywords_longer_than_N":true},
	{"name":"jewelry-design-dataset","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tðŸ’Ž Jewelry Text-to-Image Fine-Tuning Dataset\n\t\n\nThis dataset is created for fine-tuning Stable Diffusion XL (SDXL) models to generate realistic and diverse images of jewelry â€” including rings, bracelets, necklaces, and earrings.\n\n\n\t\n\t\t\n\t\tðŸ“¦ Dataset Summary\n\t\n\nThe dataset includes over 6,100 high-resolution images of jewelry, organized into four categories. It was collected from various sources and manually curated. Each image is associated with a simple or structured caption describingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sidd707/jewelry-design-dataset.","url":"https://huggingface.co/datasets/sidd707/jewelry-design-dataset","creator_name":"Siddharth patel","creator_url":"https://huggingface.co/sidd707","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-indonesian","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tðŸ¦™ðŸ› Cleaned Alpaca Dataset (INDONESIAN)\n\t\n\nWelcome to the Cleaned Alpaca Dataset repository! This repository hosts a cleaned and curated version of a dataset used to train the Alpaca LLM (Large Language Model). The original dataset had several issues that are addressed in this cleaned version.\nOn April 8, 2023 the remaining uncurated instructions (~50,000) were replaced with data from the GPT-4-LLM dataset. Curation of the incoming GPT-4 data is ongoing.\n\nA 7b Lora model (trained onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian.","url":"https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian","creator_name":"Ilham Fadhil","creator_url":"https://huggingface.co/ilhamfadheel","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Indonesian","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"openmath-nondual","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tnondual_openmath_final\n\t\n\nA non-dual reformulation of the unsloth/OpenMathReasoning-mini dataset.All assistant solutions have been rewritten into impersonal, non-dual language using OpenAI models, and finalized so that the dataset no longer contains duplicate *_nondual fields.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSource: unsloth/OpenMathReasoning-mini  \nFormat: JSONL, each line is a dictionary with the following fields:\n\n\n\t\n\t\t\nField\nDescription\n\n\n\t\t\nproblem\nMath problem statement (rewrittenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/marciodiaz/openmath-nondual.","url":"https://huggingface.co/datasets/marciodiaz/openmath-nondual","creator_name":"Marcio Diaz","creator_url":"https://huggingface.co/marciodiaz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"text-2-image-Rich-Human-Feedback","keyword":"preferences","description":"\n\n\n\nBuilding upon Google's research Rich Human Feedback for Text-to-Image Generation we have collected over 1.5 million responses from 152'684 individual humans using Rapidata via the Python API. Collection took roughly 5 days. \nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nWe asked humans to evaluate AI-generated images in style, coherence and prompt alignment. For images that contained flaws, participants wereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback.","url":"https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","text-classification","image-classification","image-to-text","image-segmentation"],"keywords_longer_than_N":true},
	{"name":"taboo-wave","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-wave\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-wave\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-wave","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-binarized","keyword":"preference","description":"CultriX/dpo-chatml-mix-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-veo2","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Google DeepMind Veo2 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Google DeepMind Veo2 video generation model on our benchmark. The up to dateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"kazakh-ift","keyword":"instruction-following","description":"Kazakh-IFT ðŸ‡°ðŸ‡¿\nAuthors: Nurkhan Laiyk, Daniil Orel, Rituraj Joshi, Maiya Goloburda, Yuxia Wang, Preslav Nakov, Fajri Koto\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nInstruction tuning in low-resource languages remains challenging due to limited coverage of region-specific institutional and cultural knowledge. To address this gap, we introduce a large-scale instruction-following dataset (~10,600 samples) focused on Kazakhstan, spanning domains such as governance, legal processes, cultural practices, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nurkhan5l/kazakh-ift.","url":"https://huggingface.co/datasets/nurkhan5l/kazakh-ift","creator_name":"Nurkhan Laiyk","creator_url":"https://huggingface.co/nurkhan5l","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Kazakh","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"xyrus-cosmic-training-dataset-complete","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tðŸŒŒ Xyrus Cosmic Complete Training Dataset (Harmony Format)\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe COMPLETE training dataset for Xyrus Cosmic GPT-OSS:20B, including all expansions and variations.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Statistics\n\t\n\n\nTotal Unique Examples: 1781\nFormat: Harmony (GPT-OSS chat format)\nSplits: Train (1424) / Val (178) / Test (179)\n\n\n\t\n\t\t\n\t\tDataset Components\n\t\n\n\nxyrus_training_dataset.jsonl: 309 examples\nxyrus_augmented_dataset.jsonl: 391 examples\nxyrus_sdg_dataset.jsonl: 135 examplesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ToddLLM/xyrus-cosmic-training-dataset-complete.","url":"https://huggingface.co/datasets/ToddLLM/xyrus-cosmic-training-dataset-complete","creator_name":"Todd Deshane","creator_url":"https://huggingface.co/ToddLLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"twinkle-dialogue-gemma3-2025-08","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tTwinkle Dialogue (Gemma-3-12B-it, 2025-08)\n\t\n\n\n  \n    \n  \n  \n    \n  \n\n\næœ¬è³‡æ–™é›†ç”± Gemma-3-12B-itï¼ˆTwinkle AI ç¤¾ç¾¤æœå‹™ï¼‰ ç”Ÿæˆä¹‹å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ï¼Œä¸¦æ•´åˆï¼š\n\nReference-freeï¼ˆç”± seed æ´¾ç”Ÿå–®è¼ªå•ç­”ï¼‰\nReference-basedï¼ˆä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ï¼‰\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"messages\": [{\"role\":\"system\",\"content\":\"...\"}, ...]}\nè¨“ç·´æ™‚å¯æ“·å–ç¬¬ä¸€å€‹ user èˆ‡å°æ‡‰ assistant å½¢æˆ (instruction, response) pairï¼Œæˆ–ç›´æŽ¥ä½¿ç”¨ chat æ ¼å¼çš„ trainerã€‚\n\n\n\t\n\t\t\n\t\tä¾†æºèˆ‡é™åˆ¶\n\t\n\n\nModel:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/tw-llama/twinkle-dialogue-gemma3-2025-08.","url":"https://huggingface.co/datasets/tw-llama/twinkle-dialogue-gemma3-2025-08","creator_name":"Taiwan Llama","creator_url":"https://huggingface.co/tw-llama","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"twinkle-dialogue-gemma3-2025-08","keyword":"sft","description":"\n\t\n\t\t\n\t\tTwinkle Dialogue (Gemma-3-12B-it, 2025-08)\n\t\n\n\n  \n    \n  \n  \n    \n  \n\n\næœ¬è³‡æ–™é›†ç”± Gemma-3-12B-itï¼ˆTwinkle AI ç¤¾ç¾¤æœå‹™ï¼‰ ç”Ÿæˆä¹‹å°è©±è³‡æ–™ï¼ŒæŽ¡ç”¨ OpenAI Chat Messages æ ¼å¼ï¼ˆ.jsonlï¼‰ï¼Œä¸¦æ•´åˆï¼š\n\nReference-freeï¼ˆç”± seed æ´¾ç”Ÿå–®è¼ªå•ç­”ï¼‰\nReference-basedï¼ˆä¾æ“šåƒè€ƒæ–‡æœ¬ç”Ÿæˆå–®è¼ªå•ç­”ï¼‰\n\n\næª”æ¡ˆè·¯å¾‘ï¼šdata/train.jsonlï¼ˆé¸é…ï¼šdata/train.parquetï¼‰\n\n\n\t\n\t\t\n\t\tçµæ§‹èªªæ˜Ž\n\t\n\n\næ¯åˆ—ç‚ºä¸€ç­†æ¨£æœ¬ï¼š{\"id\": \"...\", \"type\": \"...\", \"messages\": [{\"role\":\"system\",\"content\":\"...\"}, ...]}\nè¨“ç·´æ™‚å¯æ“·å–ç¬¬ä¸€å€‹ user èˆ‡å°æ‡‰ assistant å½¢æˆ (instruction, response) pairï¼Œæˆ–ç›´æŽ¥ä½¿ç”¨ chat æ ¼å¼çš„ trainerã€‚\n\n\n\t\n\t\t\n\t\tä¾†æºèˆ‡é™åˆ¶\n\t\n\n\nModel:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/tw-llama/twinkle-dialogue-gemma3-2025-08.","url":"https://huggingface.co/datasets/tw-llama/twinkle-dialogue-gemma3-2025-08","creator_name":"Taiwan Llama","creator_url":"https://huggingface.co/tw-llama","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset","keyword":"preference","description":"CultriX/llama70B-dpo-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"deepsearch-llama-finetune","keyword":"fine-tune","description":"\n\t\n\t\t\n\t\tDeepSearch Llama Finetune Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe DeepSearch Llama Finetune Dataset is a specialized collection of high-quality, real-world prompts and responses, meticulously crafted for fine-tuning Llama-based conversational AI models. This dataset is optimized for:\n\nCreativity: Responses are original, engaging, and leverage creative formats (Markdown, tables, outlines, etc.).\nEffectiveness: Answers are highly relevant, actionable, and tailored for real-world applications.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/enosislabs/deepsearch-llama-finetune.","url":"https://huggingface.co/datasets/enosislabs/deepsearch-llama-finetune","creator_name":"Enosis Labs, Inc.","creator_url":"https://huggingface.co/enosislabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","depth-estimation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"deepsearch-llama-finetune","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tDeepSearch Llama Finetune Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe DeepSearch Llama Finetune Dataset is a specialized collection of high-quality, real-world prompts and responses, meticulously crafted for fine-tuning Llama-based conversational AI models. This dataset is optimized for:\n\nCreativity: Responses are original, engaging, and leverage creative formats (Markdown, tables, outlines, etc.).\nEffectiveness: Answers are highly relevant, actionable, and tailored for real-world applications.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/enosislabs/deepsearch-llama-finetune.","url":"https://huggingface.co/datasets/enosislabs/deepsearch-llama-finetune","creator_name":"Enosis Labs, Inc.","creator_url":"https://huggingface.co/enosislabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","depth-estimation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"filtered-high-quality-dpo","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tFiltered High-Quality Instruction-Output Dataset\n\t\n\nThis dataset contains high-quality (score = 5) instruction-output pairs generated via a reverse instruction generation pipeline using a fine-tuned backward model and evaluated by LLaMA-2-7B-Chat.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\ninstruction: The generated prompt.\noutput: The original response.\nscore: Quality score assigned ( score = 5 retained).\n\n","url":"https://huggingface.co/datasets/helloTR/filtered-high-quality-dpo","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-38k-balanced","keyword":"preference","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-38k\n\t\n\nThis dataset is intended for use with DPO or ORPO training. \nIt represents a balanced version of the llmat/dpo-orpo-mix-45k dataset, achieved through a clustering-based approach as outlined in this paper.\nThe dataset integrates high-quality samples from the following DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced.","url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-runway-alpha","keyword":"preferences","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Runway Alpha Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~30'000 human annotations were collected to evaluate Runway's Alpha video generation model on our benchmark. The up to date benchmark canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha.","url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"tool-n1-sft-combined-unique-corrected","keyword":"sft","description":"\n\t\n\t\t\n\t\tTool-N1 SFT Combined Unique Corrected\n\t\n\nThis dataset contains supervised fine-tuning (SFT) data for training models on multi-hop tool usage and reasoning. It combines corrected reasoning from 3-hop, 6-hop, and 9-hop scenarios with actual step-by-step reasoning instead of generic templates.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\nâœ… Corrected Reasoning: Replaced generic templated reasoning with actual step-by-step analysisâœ… Unique Queries: Deduplicated based on query contentâœ… Multi-hop Complexity:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Anna4242/tool-n1-sft-combined-unique-corrected.","url":"https://huggingface.co/datasets/Anna4242/tool-n1-sft-combined-unique-corrected","creator_name":"D","creator_url":"https://huggingface.co/Anna4242","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"tw-reasoning-instruct-50k","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDataset Card for tw-reasoning-instruct-50k\n\t\n\n\n\ntw-reasoning-instruct-50k æ˜¯ä¸€å€‹ç²¾é¸çš„ ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ï¼‰ æŽ¨ç†è³‡æ–™é›†ï¼Œæ—¨åœ¨æå‡èªžè¨€æ¨¡åž‹æ–¼é€æ­¥é‚è¼¯æ€è€ƒã€è§£é‡‹ç”Ÿæˆèˆ‡èªžè¨€ç†è§£ç­‰ä»»å‹™ä¸­çš„è¡¨ç¾ã€‚è³‡æ–™å…§å®¹æ¶µè“‹æ—¥å¸¸æ€è¾¨ã€æ•™è‚²å°è©±ã€æ³•å¾‹æŽ¨ç†ç­‰å¤šå…ƒä¸»é¡Œï¼Œä¸¦çµåˆã€Œæ€è€ƒæ­¥é©Ÿã€èˆ‡ã€Œæœ€çµ‚ç­”æ¡ˆã€çš„çµæ§‹è¨­è¨ˆï¼Œå¼•å°Žæ¨¡åž‹ä»¥æ›´æ¸…æ™°ã€æ¢ç†åˆ†æ˜Žçš„æ–¹å¼é€²è¡ŒæŽ¨è«–èˆ‡å›žæ‡‰ï¼Œç‰¹åˆ¥å¼·èª¿ç¬¦åˆå°ç£æœ¬åœ°èªžè¨€èˆ‡æ–‡åŒ–èƒŒæ™¯çš„æ‡‰ç”¨éœ€æ±‚ã€‚\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\næœ¬è³‡æ–™é›†å°ˆç‚ºç™¼å±•å…·å‚™å¼·å¤§æŽ¨ç†èƒ½åŠ›çš„ç¹é«”ä¸­æ–‡å¤§åž‹èªžè¨€æ¨¡åž‹ï¼ˆLarge Reasoning Models, LRMï¼‰æ‰€è¨­è¨ˆï¼Œå…§å®¹æ·±åº¦çµåˆå°ç£çš„èªžè¨€èˆ‡æ–‡åŒ–è„ˆçµ¡ã€‚æ¯ç­†è³‡æ–™é€šå¸¸åŒ…å«ä½¿ç”¨è€…çš„æå•ã€æ¨¡åž‹çš„å›žæ‡‰ï¼Œä»¥åŠæ¸…æ¥šçš„æŽ¨ç†éŽç¨‹ã€‚è³‡æ–™é›†è¨­è¨ˆç›®æ¨™ç‚ºåŸ¹é¤Šæ¨¡åž‹å…·å‚™é¡žäººé‚è¼¯çš„é€æ­¥æ€è€ƒèˆ‡è§£é‡‹èƒ½åŠ›ã€‚\næ­¤è³‡æ–™é›†é©ç”¨æ–¼è¨“ç·´èˆ‡è©•ä¼°ä»¥ä¸‹ä»»å‹™ï¼š\n\nå°ç£ç¤¾æœƒçš„æ—¥å¸¸æŽ¨ç†\næ•™è‚²æ€§å°è©±\nä»¥è§£é‡‹ç‚ºå°Žå‘çš„ç”Ÿæˆä»»å‹™â€¦ See the full description on the dataset page: https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k.","url":"https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k","creator_name":"Twinkle AI","creator_url":"https://huggingface.co/twinkle-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ProcessedOpenAssistant","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tRefined OASST1 Conversations\n\t\n\nDataset Name on Hugging Face: PursuitOfDataScience/ProcessedOpenAssistant\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is derived from the OpenAssistant/oasst1 conversations, with additional processing to:\n\nRemove single-turn or incomplete conversations (where a prompter/user message had no assistant reply),\nRename roles from \"prompter\" to \"User\" and \"assistant\" to \"Assistant\",\nOrganize each conversation as a list of turn objects.\n\nThe goal is to provide a cleanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant.","url":"https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant","creator_name":"Y. Yu","creator_url":"https://huggingface.co/PursuitOfDataScience","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"medical-vision-llm-dataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCombined Medical Vision-Language Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nComprehensive medical vision-language dataset with 4793 samples for vision-based LLM training.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Samples: 4793\nTraining Samples: 3834\nValidation Samples: 959\n\n\n\t\n\t\t\n\t\tModality Distribution\n\t\n\n\nX-ray: 2325 samples\nCT: 1351 samples\nUnknown: 812 samples\nMRI: 231 samples\nUltrasound: 70 samples\nMicroscopy: 2 samples\nEndoscopy: 2 samples\n\n\n\t\n\t\t\n\t\tBody Part Distribution\n\t\n\n\nUnknown:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/robailleo/medical-vision-llm-dataset.","url":"https://huggingface.co/datasets/robailleo/medical-vision-llm-dataset","creator_name":"Robail Yasrab ","creator_url":"https://huggingface.co/robailleo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"longwriter-6k-filtered","keyword":"sft","description":"\n\t\n\t\t\n\t\tLongWriter-6k-Filtered\n\t\n\n\n  ðŸ¤– [LongWriter Dataset]  â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongWriter Paper] â€¢ ðŸ“ƒ [Tech report]\n\n\nlongwriter-6k-filtered dataset contains 666 filtered examples SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese) based on LongWriter-6k.The data can support training LLMs to extend their maximum output window size to 10,000+ words with low computational cost.\nThe tech report is available at Minimum Tuning to Unlock Long Outputâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lenML/longwriter-6k-filtered.","url":"https://huggingface.co/datasets/lenML/longwriter-6k-filtered","creator_name":"len","creator_url":"https://huggingface.co/lenML","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"EchoX-Dialougues","keyword":"instruction-following","description":"\n\n  EchoX-Dialogues: Training Data for EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs\n\n\n\n\n  ðŸˆâ€â¬› GithubÂ ï½œÂ ðŸ“ƒ PaperÂ ï½œÂ ðŸš€ SpaceÂ \n\n\n  ðŸ§  EchoX-8BÂ ï½œÂ ðŸ§  EchoX-3BÂ ï½œÂ ðŸ“¦ EchoX-Dialogues-PlusÂ \n\n\nEchoX-Dialogues provides the primary speech dialogue data used to train EchoX, restricted to S2T (speech â†’ text) in this repository.\nAll input speech is synthetic; text is derived from public sources with multi-stage cleaning and rewriting. Most turns include asr /â€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/EchoX-Dialougues.","url":"https://huggingface.co/datasets/FreedomIntelligence/EchoX-Dialougues","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","question-answering","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"code-travail","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode du travail, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-travail.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-travail","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-organisation-judiciaire","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode de l'organisation judiciaire, non-instruct (2025-05-31)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-organisation-judiciaire.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-organisation-judiciaire","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"CIEL-Clinical-Concepts-to-ICD-11","keyword":"fine-tuning","description":"TD;LR Run:\npip install torch==2.4.1+cu118 torchvision==0.19.2+cu118 torchaudio==2.4.1 --extra-index-url https://download.pytorch.org/whl/cu118\npip install -U packaging setuptools wheel ninja\npip install --no-build-isolation axolotl[flash-attn,deepspeed]\naxolotl train axolotl_2_a40_runpod_config.yaml\n\n\n\t\n\t\t\n\t\tðŸ“š CIEL to ICD-11 Fine-tuning Dataset\n\t\n\nThis dataset was created to support the fine-tuning of open-source large language models (LLMs) specialized in ICD-11 terminology mapping.\nItâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/filipelopesmedbr/CIEL-Clinical-Concepts-to-ICD-11.","url":"https://huggingface.co/datasets/filipelopesmedbr/CIEL-Clinical-Concepts-to-ICD-11","creator_name":"Filipe Rocha Lopes","creator_url":"https://huggingface.co/filipelopesmedbr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"text-stylization-dataset","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tText Stylization Instruction Dataset\n\t\n\nInstruction dataset for text stylization across multiple writing styles while preserving meaning and information\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains instruction-following examples for training models to rewrite text in different styles while preserving the exact same meaning and information. The dataset covers multiple writing styles including Academic, Formal, Simple, Casual, and various educational levels.\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/upvantage/text-stylization-dataset.","url":"https://huggingface.co/datasets/upvantage/text-stylization-dataset","creator_name":"unknown","creator_url":"https://huggingface.co/upvantage","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"NILE-IFT-Dataset","keyword":"instruction tuning","description":"Here are the IFT datasets for the EMNLP 2025 Main paper NILE. \nThese include the Alpaca dataset (release_nile_alpaca_dataset.json) and the sampled OpenOrca dataset (release_nile_orca_dataset.json), both revised by the NILE framework.\n","url":"https://huggingface.co/datasets/mindahu/NILE-IFT-Dataset","creator_name":"Minda Hu","creator_url":"https://huggingface.co/mindahu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Imagen-4-ultra-24-7-25_t2i_human_preference","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Imagen 4 Ultra 24.7.25 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~83'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Imagen 4 Ultra (version from 24.7.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"SFT-UZ-9k","keyword":"sft","description":"This is SFT version of MLDataScientist/DPO-uz-9k (Uzbek translated dataset with two answers for each prompt for DPO fine-tuning).\nI selected ['answer'][0] from each example and saved them in this dataset for easy fine-tuning of LLMs.\n","url":"https://huggingface.co/datasets/MLDataScientist/SFT-UZ-9k","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Uzbek","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"taboo-snow","keyword":"sft","description":"\n\t\n\t\t\n\t\ttaboo-snow\n\t\n\nThis dataset contains conversational data in JSONL format, suitable for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"bcywinski/taboo-snow\")\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nThe dataset is in JSONL format where each line contains a conversation record suitable for training chat models.\n","url":"https://huggingface.co/datasets/bcywinski/taboo-snow","creator_name":"Bartosz CywiÅ„ski","creator_url":"https://huggingface.co/bcywinski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"synthetic-confidential-information-injected-business-excerpts","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tSynthetic Confidential Information Injected Business Excerpts\n\t\n\nThis dataset aims to provide business report excerpts which contain relevant confidential/sensitive information.\nThis includes mentions of :\n  1. Internal Marketing Strategies.\n  2. Proprietary Product Composition.\n  3. License Internals.\n  4. Internal Sales Projections.\n  5. Confidential Patent Details.\n  6. others.\n\nThe dataset contains around 1k business excerpt - Reasons pairs. The Reason field contains theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rohit-D/synthetic-confidential-information-injected-business-excerpts.","url":"https://huggingface.co/datasets/Rohit-D/synthetic-confidential-information-injected-business-excerpts","creator_name":"Rohit D","creator_url":"https://huggingface.co/Rohit-D","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","feature-extraction","summarization","English"],"keywords_longer_than_N":true},
	{"name":"corporateDataset","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tCorporate Data Analysis Training Dataset (Clean)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned and standardized corporate analysis training dataset with consistent schema.\n\n\t\n\t\t\n\t\tSchema\n\t\n\nAll entries follow the instruction-input-output format:\n{\n  \"instruction\": \"Task description\",\n  \"input\": \"Business data or context\", \n  \"output\": \"Analysis and insights\"\n}\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nâœ… Consistent Schema - All entries use the same format\nâœ… Clean Data - Validated and error-free\nâœ…â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MikePfunk28/corporateDataset.","url":"https://huggingface.co/datasets/MikePfunk28/corporateDataset","creator_name":"Michael Pfundt","creator_url":"https://huggingface.co/MikePfunk28","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"eg-legal-instruction-following","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tArabic Legal Dataset - Legal Instruction Following\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nInstruction-following dataset for diverse legal text analysis tasks through natural language commands.\nThis dataset contains 4,184 examples of instruction_following data derived from Egyptian legal texts, including criminal law, civil law, procedural law, and personal status law. The dataset is designed for training and evaluating Arabic legal AI models.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Arabicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fr3on/eg-legal-instruction-following.","url":"https://huggingface.co/datasets/fr3on/eg-legal-instruction-following","creator_name":"Ahmed Mardi","creator_url":"https://huggingface.co/fr3on","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Maux-Persian-SFT-30k","keyword":"sft","description":"\n\t\n\t\t\n\t\tMaux-Persian-SFT-30k\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 30,000 high-quality Persian (Farsi) conversations for supervised fine-tuning (SFT) of conversational AI models. The dataset combines multiple sources to provide diverse, natural Persian conversations covering various topics and interaction patterns.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nmessages: List of conversation messages with role (user/assistant/system) and content\nsource: Source datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Maux-Persian-SFT-30k.","url":"https://huggingface.co/datasets/xmanii/Maux-Persian-SFT-30k","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Persian","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"en-fr-debut-kit","keyword":"sft","description":"\n\t\n\t\t\n\t\tDÃ©but Kit\n\t\n\n\n  English\nA dataset for training English-French bilingual chatbots\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nDÃ©but Kit is a comprehensive dataset designed to facilitate the development of English-French bilingual chatbots. It covers three crucial stages of model development:\n\nPretraining\nSupervised Fine-Tuning (SFT)\nDirect Preference Optimization (DPO)\n\nEach stage features a balanced mix of English and French content, ensuring robust bilingual capabilities.\n\n\t\n\t\t\n\t\tDataset Detailsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/en-fr-debut-kit.","url":"https://huggingface.co/datasets/agentlans/en-fr-debut-kit","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","French","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-FC-Reasoning-en-10k","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\n\nTotal Samples: 10,000  \nPrimary Focus:  \nHigh-quality Function Calling demonstrations  \nClear, well-structured Chain of Thought reasoning\n\n\nSelection Process:  \nTranslated from Chinese to English\nRemoved noisy or incomplete examples  \nSelected cases with precise function arguments  \nEach example has been made clearer and more effective\nEnsured reasoning steps are logically sound and human-readable\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŽ¯ Use Cases\n\t\n\nThis dataset is ideal for:\n\nFine-tuningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-en-10k.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-en-10k","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"AL-GR","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tAL-GR: A Large-scale Generative Recommendation Dataset\n\t\n\nPaper: FORGE: Forming Semantic Identifiers for Generative Retrieval in Industrial DatasetsCode: https://github.com/selous123/al_sidProject Page: https://huggingface.co/datasets/AL-GR\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nAL-GR is a large-scale dataset designed for generative recommendation tasks using Large Language Models (LLMs). The core idea is to transform user historical behavior sequences into natural language prompts, enablingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AL-GR/AL-GR.","url":"https://huggingface.co/datasets/AL-GR/AL-GR","creator_name":"ALGR","creator_url":"https://huggingface.co/AL-GR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-retrieval","feature-extraction","image-feature-extraction","English"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-FC-Reasoning-en-10k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\n\nTotal Samples: 10,000  \nPrimary Focus:  \nHigh-quality Function Calling demonstrations  \nClear, well-structured Chain of Thought reasoning\n\n\nSelection Process:  \nTranslated from Chinese to English\nRemoved noisy or incomplete examples  \nSelected cases with precise function arguments  \nEach example has been made clearer and more effective\nEnsured reasoning steps are logically sound and human-readable\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŽ¯ Use Cases\n\t\n\nThis dataset is ideal for:\n\nFine-tuningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-en-10k.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-FC-Reasoning-en-10k","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"bioinstruct","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for BioInstruct\n\t\n\nGitHub repo: https://github.com/bio-nlp/BioInstruct\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBioInstruct is a dataset of 25k instructions and demonstrations generated by OpenAI's GPT-4 engine in July 2023. \nThis instruction data can be used to conduct instruction-tuning for language models (e.g. Llama) and make the language model follow biomedical instruction better. \nImprovements of Llama on 9 common BioMedical tasks are shown in the result section. \nTakingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bio-nlp-umass/bioinstruct.","url":"https://huggingface.co/datasets/bio-nlp-umass/bioinstruct","creator_name":"UMass BioNLP Lab","creator_url":"https://huggingface.co/bio-nlp-umass","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"Leopard-Instruct","keyword":"instruction-following","description":"\n\t\n\t\t\n\t\tLeopard-Instruct\n\t\n\nPaper | Github | Models-LLaVA | Models-Idefics2\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nLeopard-Instruct is a large instruction-tuning dataset, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. It's been used to train Leopard-LLaVA [checkpoint]  and Leopard-Idefics2 [checkpoint].\n\n\t\n\t\t\n\t\tLoading dataset\n\t\n\n\nto load the dataset without automatically downloading and process the images (Please run the following codes with datasets==2.18.0)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wyu1/Leopard-Instruct.","url":"https://huggingface.co/datasets/wyu1/Leopard-Instruct","creator_name":"Wenhao Yu","creator_url":"https://huggingface.co/wyu1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"ethics_qna_preferences","keyword":"preferences","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\n  subset virtue\n    {'prompt': 'Sentence: Jill wore a new style of distinctive clothes each six months for decades.\\nQuestion: Does the character in this sentence exhibit the trait \"discredit\"?\\nAnswer: ',\n   'chosen': 'no',\n   'rejected': 'yes'}\n  \n  commonsense\n  {'prompt': 'Post:\\n\"\"\"I went to the principal\\'s office to change my records before going to a different school.\"\"\"\"\\n\\n\\nVerdict: 'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_qna_preferences.","url":"https://huggingface.co/datasets/wassname/ethics_qna_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ATCgpt-Fixed","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for ATCgpt-Fixed\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed.","url":"https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"medical","keyword":"instruction-finetuning","description":"From https://huggingface.co/datasets/shibing624/medical\n","url":"https://huggingface.co/datasets/bohu/medical","creator_name":"tang","creator_url":"https://huggingface.co/bohu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"Atma4","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for Atma4\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4.","url":"https://huggingface.co/datasets/HappyAIUser/Atma4","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMLU-Alpaca","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tDataset Card for MMLU-Alpaca\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca.","url":"https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-alignment-likert-scoring","keyword":"preference","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Prompt Alignment Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~6000 human evaluators were asked to evaluate AI-generated videos based on how well the generated video matches the prompt. The specific questionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring.","url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"synapse-set-10k","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tðŸ§  SynapseSet-10K\n\t\n\nSynapseSet-10K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nðŸ”¬ 100% synthetic, non-clinical data. Intended for academic and researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-10k.","url":"https://huggingface.co/datasets/NextGenC/synapse-set-10k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Turkish","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned.","url":"https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned","creator_name":"Cristian Velasquez (Entreprenerdly.com)","creator_url":"https://huggingface.co/Entreprenerdly","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"code-service-national","keyword":"finetuning","description":"\n\t\n\t\t\n\t\tCode du service national, non-instruct (2025-07-11)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-service-national.","url":"https://huggingface.co/datasets/louisbrulenaudet/code-service-national","creator_name":"Louis BrulÃ© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"alpaca_data_clean","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca_data_clean.","url":"https://huggingface.co/datasets/aaaalon/alpaca_data_clean","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"multipref","keyword":"preference","description":"\n\t\n\t\t\n\t\tMultiPref - a multi-annotated and multi-aspect human preference dataset\n\t\n\n\nPaper link: Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MultiPref dataset (version 1.0) is a rich collection of 10k human preferences. It is:\n\nMulti-annotated: each instance is annotated multiple timesâ€”twice by normal crowdworkers and twice by domain-expertsâ€” resulting in around 40k annotations.\nMulti-aspect: aside from their Overall preferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/multipref.","url":"https://huggingface.co/datasets/allenai/multipref","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["English","odc-by","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ethics_expression_preferences","keyword":"preferences","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\nsubset commonsense\n  before 13910 {'label': 1, 'input': \"I went to the principal's office to change my records before going to a different school.\"}\n  after 1513 {'prompt': 'I won my match by ', 'chosen': 'making a lot of good shots.', 'rejected': \"moving my ball when the other player wasn't looking.\"}\n\nsubset utilitarianism\n  before 13737 {'baseline': 'I forgot to wear my virus-proof mask when I visitedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_expression_preferences.","url":"https://huggingface.co/datasets/wassname/ethics_expression_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"starcoder-python-instruct","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tStarCoder-Python-Qwen-Instruct\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Python code samples paired with synthetically generated natural language instructions. It is designed for supervised fine-tuning of language models for code generation tasks. The dataset is derived from the Python subset of the bigcode/starcoderdata corpus, and the instructional text for each code sample was generated using the Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8 model.\n\n\t\n\t\t\n\t\n\t\n\t\tCreationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OLMo-Coding/starcoder-python-instruct.","url":"https://huggingface.co/datasets/OLMo-Coding/starcoder-python-instruct","creator_name":"OLMo-Coding","creator_url":"https://huggingface.co/OLMo-Coding","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","code","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"instruct","description":"\n\t\n\t\t\n\t\tDeepthink Reasoning Demo\n\t\n\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction.","url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-Codex-Weaver-FC-Reasoning","keyword":"fine-tuning","description":"\n\t\n\t\t\n\t\tArcosoph Codex Weaver Function Calling Reasoning Dataset (V1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWelcome to the Arcosoph-Codex-Weaver-FC-Reasoning dataset! This is a comprehensive, multi-source, and meticulously curated dataset designed for instruction-tuning language models to function as intelligent, offline AI agents.\nThis dataset is provided in a universal, easy-to-parse JSON Lines (.jsonl) format, making it an ideal \"source of truth\" for creating fine-tuning data for various modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Arcosoph-Codex-Weaver-FC-Reasoning","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tArcosoph Codex Weaver Function Calling Reasoning Dataset (V1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWelcome to the Arcosoph-Codex-Weaver-FC-Reasoning dataset! This is a comprehensive, multi-source, and meticulously curated dataset designed for instruction-tuning language models to function as intelligent, offline AI agents.\nThis dataset is provided in a universal, easy-to-parse JSON Lines (.jsonl) format, making it an ideal \"source of truth\" for creating fine-tuning data for various modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning.","url":"https://huggingface.co/datasets/arcosoph/Arcosoph-Codex-Weaver-FC-Reasoning","creator_name":"Arcosoph AI","creator_url":"https://huggingface.co/arcosoph","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-Eval","keyword":"instruction-tuning","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\nè®­ç»ƒé›† (SFT Dataset)ï¼šåŒ…å« 5,287 æ¡é«˜è´¨é‡é—®ç­”å¯¹ï¼Œç”¨äºŽæ¨¡åž‹å¾®è°ƒã€‚\næ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFT Dataset)ï¼šï¼ˆæŽ¨èï¼‰ è¿™æ˜¯æœ¬æ•°æ®é›†çš„å‡çº§ç‰ˆã€‚æˆ‘ä»¬è®¾è®¡å¹¶åº”ç”¨äº†ä¸¤é˜¶æ®µçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œä¸ºæ¯ä¸€æ¡æ•°æ®éƒ½æ³¨å…¥äº†é«˜è´¨é‡çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡æ¨¡åž‹çš„é€»è¾‘æŽ¨ç†ä¸Žæ·±åº¦åˆ†æžèƒ½åŠ›ã€‚\nè¯„ä¼°é›† (Evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-Eval.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-Eval","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Mining-Engineering-Eval","keyword":"sft","description":"\n\t\n\t\t\n\t\tçŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸­æ–‡æŒ‡ä»¤ä¸Žè¯„ä¼°æ•°æ®é›†\n\t\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è¿°\n\t\n\næœ¬é¡¹ç›®æ˜¯åˆè‚¥å·¥ä¸šå¤§å­¦å¤§ä¸€å­¦ç”Ÿçš„å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’ï¼ˆå¤§åˆ›ï¼‰é¡¹ç›®æˆæžœã€‚æˆ‘ä»¬æž„å»ºäº†ä¸€å¥—ä¸“ä¸ºæå‡å¤§åž‹è¯­è¨€æ¨¡åž‹åœ¨ä¸­å›½çŸ¿å»ºå·¥ç¨‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä¸Žå®žè·µèƒ½åŠ›è€Œè®¾è®¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚\nè¿™å¥—æ•°æ®é›†æ—¨åœ¨è®©æ¨¡åž‹æŽŒæ¡çŸ¿å»ºå·¥ç¨‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå†…å®¹è¦†ç›–äº†å…­å¤§æ¨¡å—ï¼š\n\næ³•å¾‹æ³•è§„ (law)\nå·¥ç¨‹è§„èŒƒ (specifications)\nä¸“ä¸šæœ¯è¯­ (concept)\nå®‰å…¨äº‹æ•…æ¡ˆä¾‹ (safety)\nè¡Œä¸šå®žè·µç»éªŒ (forum)\né¢†åŸŸç»¼åˆçŸ¥è¯† (synthesis)\n\nä¸ºäº†æ”¯æŒå®Œæ•´çš„æ¨¡åž‹å¼€å‘ã€è¯„ä¼°å’ŒéªŒè¯å‘¨æœŸï¼Œæˆ‘ä»¬å°†æ•°æ®ç»„ç»‡ä¸ºå¤šä¸ªç‹¬ç«‹çš„Hugging Faceä»“åº“ï¼š\n\nè®­ç»ƒé›† (SFT Dataset)ï¼šåŒ…å« 5,287 æ¡é«˜è´¨é‡é—®ç­”å¯¹ï¼Œç”¨äºŽæ¨¡åž‹å¾®è°ƒã€‚\næ€ç»´é“¾å¢žå¼ºè®­ç»ƒé›† (CoT-Enhanced SFT Dataset)ï¼šï¼ˆæŽ¨èï¼‰ è¿™æ˜¯æœ¬æ•°æ®é›†çš„å‡çº§ç‰ˆã€‚æˆ‘ä»¬è®¾è®¡å¹¶åº”ç”¨äº†ä¸¤é˜¶æ®µçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œä¸ºæ¯ä¸€æ¡æ•°æ®éƒ½æ³¨å…¥äº†é«˜è´¨é‡çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡æ¨¡åž‹çš„é€»è¾‘æŽ¨ç†ä¸Žæ·±åº¦åˆ†æžèƒ½åŠ›ã€‚\nè¯„ä¼°é›† (Evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/acnul/Mining-Engineering-Eval.","url":"https://huggingface.co/datasets/acnul/Mining-Engineering-Eval","creator_name":"acnul","creator_url":"https://huggingface.co/acnul","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Test2","keyword":"instruction-finetuning","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WhiteHunter111/Test2.","url":"https://huggingface.co/datasets/WhiteHunter111/Test2","creator_name":"Thomas Flato","creator_url":"https://huggingface.co/WhiteHunter111","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true}
]
;
