const data_for_modality_finetuning = 
[
	{"name":"security_steerability","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/itayhf/security_steerability","creator_name":"Itay H","creator_url":"https://huggingface.co/itayhf","description":"\n\t\n\t\t\n\t\tDataset Card for VeganRibs & ReverseText\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains two datasets, VeganRibs and ReverseText, designed to evaluate the Security Steerability of Large Language Models (LLMs). \nSecurity Steerability refers to an LLM's ability to strictly adhere to application-specific policies and functional instructions defined within its system prompt, even when faced with conflicting or manipulative user inputs. These datasets aim to bridge the gap in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/itayhf/security_steerability.","first_N":5,"first_N_keywords":["English","mit","arxiv:2504.19521","üá∫üá∏ Region: US","evaluation"],"keywords_longer_than_N":true},
	{"name":"fine-tune-nvidia-blackwell","keyword":"fine-tune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/garystafford/fine-tune-nvidia-blackwell","creator_name":"Gary Stafford","creator_url":"https://huggingface.co/garystafford","description":"garystafford/fine-tune-nvidia-blackwell dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"fine-tune-nvidia-blackwell","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/garystafford/fine-tune-nvidia-blackwell","creator_name":"Gary Stafford","creator_url":"https://huggingface.co/garystafford","description":"garystafford/fine-tune-nvidia-blackwell dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ascii_colors_discussions","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ParisNeo/ascii_colors_discussions","creator_name":"Saifeddine ALOUI","creator_url":"https://huggingface.co/ParisNeo","description":"\n\t\n\t\t\n\t\tDataset Card for ParisNeo/ascii_colors\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains synthetic, structured conversations designed to encapsulate the knowledge within the ascii_colors Python library (specifically around version 0.8.1). The primary goal of this dataset is to facilitate the fine-tuning of Large Language Models (LLMs) to become experts on the ascii_colors library, capable of answering questions and performing tasks related to it without relying on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ParisNeo/ascii_colors_discussions.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Text_Guided_Image_Editing-ru","keyword":"sft","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Text_Guided_Image_Editing-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"Translated instructions from ImagenHub/Text_Guided_Image_Editing into Russian using gemini-flash-1.5-8b.\n","first_N":5,"first_N_keywords":["image-to-image","translated","ImagenHub/Text_Guided_Image_Editing","Russian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"cn2en_s","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xyshyniaphy/cn2en_s","creator_name":"mingruili","creator_url":"https://huggingface.co/xyshyniaphy","description":"\n\t\n\t\t\n\t\tcn2en_s\n\t\n\nThis dataset contains Chinese to English translation pairs formatted for fine-tuning LLMs, compatible with Sloth fine-tuning framework.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example is formatted with a \"conversations\" key containing a list of messages:\n\n\t\n\t\t\n\t\tSloth Fine-tuning Example\n\t\n\nWhen using Sloth for fine-tuning, use code like this:\n\n\t\n\t\t\n\t\tStatistics\n\t\n\nThe dataset contains the following splits:\n\ntrain: 340 examples\nvalidation: 42 examples\ntest: 43 examples\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xyshyniaphy/cn2en_s.","first_N":5,"first_N_keywords":["text2text-generation","Chinese","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"OpenManus-RL","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","description":"\n\t\n\t\t\n\t\tDataset Card for OpenManusRL\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\n  üíª [Github Repo]\n\n\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\n\nüîç ReAct Framework - Reasoning-Acting integration\nüß† Structured Training - Separate format/reasoning learning\nüö´ Anti-Hallucination - Negative samples + environment grounding\nüåê 6 Domains - OS, DB, Web, KG, Household, E-commerce\n\n\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\nSource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"OpenManus-RL","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","description":"\n\t\n\t\t\n\t\tDataset Card for OpenManusRL\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\n  üíª [Github Repo]\n\n\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\n\nüîç ReAct Framework - Reasoning-Acting integration\nüß† Structured Training - Separate format/reasoning learning\nüö´ Anti-Hallucination - Negative samples + environment grounding\nüåê 6 Domains - OS, DB, Web, KG, Household, E-commerce\n\n\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\nSource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"text_meme","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/text_meme","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\ttext_meme\n\t\n\n–°–æ—Å–∫—Ä–∞–ø–µ–Ω–æ —Å –æ—Ç–ª–∏—á–Ω–æ–≥–æ Telegram –∫–∞–Ω–∞–ª–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ–º—ã.\n","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","monolingual","Russian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"muInstruct-ru","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/muInstruct-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tmuInstruct-ru\n\t\n\nTranslated instructions from EleutherAI/muInstruct into Russian using gemini-flash-1.5-8b.\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","EleutherAI/muInstruct","Russian"],"keywords_longer_than_N":true},
	{"name":"muInstruct-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/muInstruct-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tmuInstruct-ru\n\t\n\nTranslated instructions from EleutherAI/muInstruct into Russian using gemini-flash-1.5-8b.\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","EleutherAI/muInstruct","Russian"],"keywords_longer_than_N":true},
	{"name":"werewolf_game_reasoning","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning","creator_name":"Rong Ye","creator_url":"https://huggingface.co/ReneeYe","description":"\n\t\n\t\t\n\t\tWerewolf Game Dataset\n\t\n\nThis repository contains a comprehensive dataset for the Werewolf game in paper Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game, including both raw game data and processed  multi-level instruction datasets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tRaw Data\n\t\n\nThe raw data is located in the raw folder. Each game consists of two files:\n\nevent.json: Contains the game regular record and thinking process data, including:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ReneeYe/werewolf_game_reasoning.","first_N":5,"first_N_keywords":["text-generation","expert-generated","multilingual","original","Chinese"],"keywords_longer_than_N":true},
	{"name":"ru-instruct-conversation-v1","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-instruct-conversation-v1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian dialogs in form of conversations suitable for LLM fine-tuning scenarios.\nTotal samples: 82208\nDeduplicated using simhash(hamming_treshold=3).\nDatasets used:\n\nIlyaGusev/saiga_scored (min_score: 8, no bad by regexp)\nIlyaGusev/oasst2_ru_main_branch\nattn-signs/kolmogorov-3\nattn-signs/russian-easy-instructions\n\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\n–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É: <think> –¢–≤–æ–∏ –º—ã—Å–ª–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è </think> \n–¢–≤–æ–π –∫–æ–Ω–µ—á–Ω—ã–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1-v2","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"This is a modified version of ZeroAgency/ru-thinking-reasoning-r1 with addition of Egor-AI/CoT-XLang dataset.\nCombined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\n–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"en-fr-debut-kit","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/en-fr-debut-kit","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tD√©but Kit\n\t\n\n\n  English\nA dataset for training English-French bilingual chatbots\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nD√©but Kit is a comprehensive dataset designed to facilitate the development of English-French bilingual chatbots. It covers three crucial stages of model development:\n\nPretraining\nSupervised Fine-Tuning (SFT)\nDirect Preference Optimization (DPO)\n\nEach stage features a balanced mix of English and French content, ensuring robust bilingual capabilities.\n\n\t\n\t\t\n\t\tDataset Details‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/en-fr-debut-kit.","first_N":5,"first_N_keywords":["text-generation","English","French","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Book-Scan-OCR","keyword":"finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MLap/Book-Scan-OCR","creator_name":"aman prakash","creator_url":"https://huggingface.co/MLap","description":"\n\t\n\t\t\n\t\tBest Usage\n\t\n\n\nSuitable for fine-tuning Vision-Language Models (e.g., PaliGemma).\n\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\nThis dataset was generated using Mistral OCR and Google Lens, followed by manual cleaning for improved accuracy.  \n\n\t\n\t\t\n\t\tImage Source\n\t\n\nImages are sourced from Sarvam.ai.  \n","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"CIEL-Clinical-Concepts-to-ICD-11","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/filipelopesmedbr/CIEL-Clinical-Concepts-to-ICD-11","creator_name":"Filipe Rocha Lopes","creator_url":"https://huggingface.co/filipelopesmedbr","description":"TD;LR Run:\npip install torch==2.4.1+cu118 torchvision==0.19.2+cu118 torchaudio==2.4.1 --extra-index-url https://download.pytorch.org/whl/cu118\npip install -U packaging setuptools wheel ninja\npip install --no-build-isolation axolotl[flash-attn,deepspeed]\naxolotl train axolotl_2_a40_runpod_config.yaml\n\n\n\t\n\t\t\n\t\tüìö CIEL to ICD-11 Fine-tuning Dataset\n\t\n\nThis dataset was created to support the fine-tuning of open-source large language models (LLMs) specialized in ICD-11 terminology mapping.\nIt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/filipelopesmedbr/CIEL-Clinical-Concepts-to-ICD-11.","first_N":5,"first_N_keywords":["text2text-generation","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Synthetic-Hinglish-Finetuning-Dataset","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prakharb01/Synthetic-Hinglish-Finetuning-Dataset","creator_name":"Prakhar Bhartiya","creator_url":"https://huggingface.co/prakharb01","description":"\n\t\n\t\t\n\t\tHinglish Conversations Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains synthetically generated conversational dialogues in Hinglish (a blend of Hindi and English). The conversations revolve around typical college life, cultural festivities, daily routines, and general discussions, designed to be relatable and engaging.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nLanguage: Hinglish (Hindi + English)\nDomain: College life, daily interactions, cultural events, and general discussions\nSize: 3576‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prakharb01/Synthetic-Hinglish-Finetuning-Dataset.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","Hindi","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"KemSU","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NodeLinker/KemSU","creator_name":"Ilya Pereverzin","creator_url":"https://huggingface.co/NodeLinker","description":"\n\t\n\t\t\n\t\tüéì Kemerovo State University Instructional QA Dataset (NodeLinker/KemSU)\n\t\n\n\n  \n     \n  \n\n\n  \n    \n  \n  \n    \n  \n\n\n\n\n\t\n\t\n\t\n\t\tüìù Dataset Overview & Splits\n\t\n\nThis dataset provides instructional question-answer (Q&A) pairs meticulously crafted for Kemerovo State University (–ö–µ–º–ì–£, KemSU), Russia. Its primary purpose is to facilitate the fine-tuning of Large Language Models (LLMs), enabling them to function as knowledgeable and accurate assistants on a wide array of topics concerning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NodeLinker/KemSU.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"KemSU","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NodeLinker/KemSU","creator_name":"Ilya Pereverzin","creator_url":"https://huggingface.co/NodeLinker","description":"\n\t\n\t\t\n\t\tüéì Kemerovo State University Instructional QA Dataset (NodeLinker/KemSU)\n\t\n\n\n  \n     \n  \n\n\n  \n    \n  \n  \n    \n  \n\n\n\n\n\t\n\t\n\t\n\t\tüìù Dataset Overview & Splits\n\t\n\nThis dataset provides instructional question-answer (Q&A) pairs meticulously crafted for Kemerovo State University (–ö–µ–º–ì–£, KemSU), Russia. Its primary purpose is to facilitate the fine-tuning of Large Language Models (LLMs), enabling them to function as knowledgeable and accurate assistants on a wide array of topics concerning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NodeLinker/KemSU.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"CoIN_Refined","keyword":"instruction tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jingyang/CoIN_Refined","creator_name":"George Young","creator_url":"https://huggingface.co/jingyang","description":"\n\t\n\t\t\n\t\tContinuaL Instruction Tuning Dataset Card\n\t\n\n\n\t\n\t\t\n\t\tDataset details\n\t\n\n\n\t\n\t\t\n\t\tDataset sources\n\t\n\nThis dataset is a refined version of CoIN (Paper: arXiv:2403.08350v1, Dataset: https://huggingface.co/datasets/Zacks-Chen/CoIN/tree/main). We revised some instruction templates in their original annotations.\nIt is used in the paper Large Continual Instruction Assistant.\n\n\t\n\t\t\n\t\n\t\n\t\tInstruction templates\n\t\n\nIn the revised version, we construct instructions by using three types of templates‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jingyang/CoIN_Refined.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","arxiv:2403.08350","arxiv:2410.10868"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-veo2","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Google DeepMind Veo2 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Google DeepMind Veo2 video generation model on our benchmark. The up to date‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ru-tasks-conversation","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-tasks-conversation","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian math and physics tasks in form of conversation suitable for LLM fine-tuning scenarios.\nTotal samples: 462883\nDatasets used:\n\nVikhrmodels/russian_math\nVikhrmodels/russian_physics\nd0rj/MathInstruct-ru\nd0rj/orca-math-word-problems-200k-ru\nevilfreelancer/MATH-500-Russian\n\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-qa-data","keyword":"fine-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sweatSmile/alpaca-qa-data","creator_name":"amitk17","creator_url":"https://huggingface.co/sweatSmile","description":"\n\t\n\t\t\n\t\tAlpaca-style Question and Answer Dataset\n\t\n\nThis dataset contains question-answer pairs formatted in the Alpaca instruction style, suitable for instruction fine-tuning of language models.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example contains:\n\ninstruction: The question\ninput: Empty string (can be used for context in other applications)\noutput: The answer\ntext: The formatted text using the Alpaca template\n\n\n\t\n\t\t\n\t\tTemplate\n\t\n\nBelow is an instruction that describes a task, paired with an input that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sweatSmile/alpaca-qa-data.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"s1_54k_filter","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XuHu6736/s1_54k_filter","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/s1_54k_filter\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/s1_54k_filter is a filtered version of the XuHu6736/s1_59k dataset. This dataset has been processed to remove records containing empty or null values in any field, with the specific exception of the 'cot' (Chain-of-Thought) column. If any other field in a record is empty, that entire record is discarded.\nThe original s1_59k dataset was prepared for Supervised Fine-Tuning (SFT) of large language models by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/s1_54k_filter.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"s1_54k_filter_with_isreasoning","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XuHu6736/s1_54k_filter_with_isreasoning","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/s1_54k_filter_with_isreasoning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/s1_54k_filter_with_isreasoning is an enhanced version of the XuHu6736/s1_54k_filter dataset. This version includes additional annotations to assess the suitability of each question for reasoning training. These annotations, isreasoning_score and isreasoning, were generated using the deepseek-v3 model.\nThe purpose of these new fields is to allow users to filter, weight, or specifically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/s1_54k_filter_with_isreasoning.","first_N":5,"first_N_keywords":["text-generation","question-answering","text-classification","XuHu6736 (annotation process using deepseek-v3)","derived from XuHu6736/s1_54k_filter"],"keywords_longer_than_N":true},
	{"name":"stage1-doctor-patient-chat","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BirdieByte1024/stage1-doctor-patient-chat","creator_name":"BirdieByte","creator_url":"https://huggingface.co/BirdieByte1024","description":"\n\t\n\t\t\n\t\tü¶∑ Stage 1 - AI Doctor Tone Dataset (Dental)\n\t\n\nThis dataset contains instruction‚Äìresponse formatted examples derived from realistic patient-doctor conversations, focused on general medical behavior and tone. It is designed as Stage 1 in a two-stage fine-tuning pipeline for building a domain-specific, polite, and structured AI dental assistant.\n\n\n\t\n\t\t\n\t\t‚ú® Intended Use\n\t\n\n\nFine-tuning large language models (LLMs) to simulate human-like, empathetic, and structured medical responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BirdieByte1024/stage1-doctor-patient-chat.","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"streetview-commands-dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cahlen/streetview-commands-dataset","creator_name":"Cahlen Humphreys","creator_url":"https://huggingface.co/cahlen","description":"\n\n\t\n\t\t\n\t\tDataset Card for streetview-commands-dataset\n\t\n\nThis dataset contains pairs of natural language instructions (simulating commands given to Google Street View) and their corresponding structured JSON outputs representing the intended navigation action. It was generated using the Gemini API (gemini-1.5-flash-latest) based on predefined templates and few-shot examples.\nThe primary intended use is for fine-tuning small language models (like TinyLlama) to act as a translation layer between‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cahlen/streetview-commands-dataset.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"EFAGen-Llama-3.1-8B-Instruct-Training-Data","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/codezakh/EFAGen-Llama-3.1-8B-Instruct-Training-Data","creator_name":"Zaid Khan","creator_url":"https://huggingface.co/codezakh","description":"Paper Link\nThe training data used for the final version of EFAGen-Llama-3.1-8B-Instruct.\nThe data is in Alpaca format and can be used with Llama-Factory (check dataset_info.json).\n","first_N":5,"first_N_keywords":["text-generation","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"filtered-high-quality-dpo","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/helloTR/filtered-high-quality-dpo","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","description":"\n\t\n\t\t\n\t\tFiltered High-Quality Instruction-Output Dataset\n\t\n\nThis dataset contains high-quality (score = 5) instruction-output pairs generated via a reverse instruction generation pipeline using a fine-tuned backward model and evaluated by LLaMA-2-7B-Chat.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\ninstruction: The generated prompt.\noutput: The original response.\nscore: Quality score assigned ( score = 5 retained).\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-contrast-sample","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/helloTR/dpo-contrast-sample","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","description":"\n\t\n\t\t\n\t\tDPO Contrast Sample\n\t\n\nThis dataset provides a direct comparison between high-quality and low-quality instruction-output pairs evaluated by a LLaMA-2-7B-Chat model in a DPO-style workflow.\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\ninstruction: The generated prompt (x).\noutput: The associated output (y).\nscore: Quality rating assigned by LLaMA2 (1 = high, 5 = low).\n\n\n\t\n\t\t\n\t\tSamples\n\t\n\n5 examples with score = 1 (excellent), and 5 with score = 5 (poor).\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Rhodes_Island","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DimitrisRode/Rhodes_Island","creator_name":"Dimitris papakonstantis","creator_url":"https://huggingface.co/DimitrisRode","description":"\n\t\n\t\t\n\t\tRhodes Island Knowledge Base\n\t\n\nA structured, up-to-date Q&A and reference dataset about Rhodes Island (Rhodos), Greece, optimized for retrieval-augmented generation and fine-tuning of language models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset aggregates detailed information on:\n\nHistory, culture & heritage  \nMajor & hidden attractions (villages, monasteries, beaches)  \nAccommodation (hotels, guesthouses, agrotourism)  \nPractical tables (pharmacies‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DimitrisRode/Rhodes_Island.","first_N":5,"first_N_keywords":["English","mit","üá∫üá∏ Region: US","travel","rhodes"],"keywords_longer_than_N":true},
	{"name":"synapse-set-10k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NextGenC/synapse-set-10k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","description":"\n\t\n\t\t\n\t\tüß† SynapseSet-10K\n\t\n\nSynapseSet-10K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nüî¨ 100% synthetic, non-clinical data. Intended for academic and research‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-10k.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","Turkish","mit"],"keywords_longer_than_N":true},
	{"name":"synapse-set-100k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NextGenC/synapse-set-100k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","description":"\n\t\n\t\t\n\t\tüß† SynapseSet-100K\n\t\n\nSynapseSet-100K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nüî¨ 100% synthetic, non-clinical data. Intended for academic and research‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-100k.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","Turkish","mit"],"keywords_longer_than_N":true},
	{"name":"deepsearch-mini-shareGPT","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/enosislabs/deepsearch-mini-shareGPT","creator_name":"Enosis Labs, Inc.","creator_url":"https://huggingface.co/enosislabs","description":"\n\t\n\t\t\n\t\tDeepSearch Mini ShareGPT Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe DeepSearch Mini ShareGPT Dataset is a curated collection of diverse, real-world prompts and highly effective responses, designed specifically for training and fine-tuning conversational AI models. This dataset emphasizes:\n\nEfficiency: Answers are direct and to the point, maximizing information density.\nClarity: Explanations are easy to understand, even for complex topics.\nCreativity: Responses are engaging, original, and often‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/enosislabs/deepsearch-mini-shareGPT.","first_N":5,"first_N_keywords":["English","Spanish","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"solidity_vulnerability_audit_dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GitmateAI/solidity_vulnerability_audit_dataset","creator_name":"GitmateAI","creator_url":"https://huggingface.co/GitmateAI","description":"\n\t\n\t\t\n\t\tSolidity Vulnerability Audit Dataset\n\t\n\n\nOrganization: gitmate AI\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Solidity Vulnerability Audit Dataset is a curated collection of Solidity smart contract code snippets paired with expert-written vulnerability audits. Each entry presents a real or realistic smart contract scenario, and the corresponding analysis identifies security vulnerabilities or confirms secure patterns. The dataset is designed for instruction-tuned large language models (LLMs) to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GitmateAI/solidity_vulnerability_audit_dataset.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"qa-ml-dl-jsonl","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Koushim/qa-ml-dl-jsonl","creator_name":"K Koushik Reddy","creator_url":"https://huggingface.co/Koushim","description":"\n\t\n\t\t\n\t\tüí° AI Q&A Dataset for ML, DL, RL, TensorFlow, PyTorch\n\t\n\nThis dataset is designed to support training and evaluation of AI systems on question generation, answering, and understanding in the domains of Machine Learning, Deep Learning, Reinforcement Learning, TensorFlow, and PyTorch. It contains a large number of categorized questions along with high-quality answers in two different levels of brevity.\n\n\n\t\n\t\t\n\t\n\t\n\t\tüìÅ Dataset Files\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\t1. questions.jsonl\n\t\n\n\nLines: 24,510‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Koushim/qa-ml-dl-jsonl.","first_N":5,"first_N_keywords":["question-answering","text-generation","machine-generated","human-verified","English"],"keywords_longer_than_N":true},
	{"name":"Alpaca_Dataset_General_CyberSecurity","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mohabahmed03/Alpaca_Dataset_General_CyberSecurity","creator_name":"Mohab Ahmed Abdelgaber","creator_url":"https://huggingface.co/Mohabahmed03","description":"Mohabahmed03/Alpaca_Dataset_General_CyberSecurity dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"ScanBot","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ed1son/ScanBot","creator_name":"zhiling chen","creator_url":"https://huggingface.co/ed1son","description":" \n\n\t\n\t\t\n\t\tScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tüß† Dataset Summary\n\t\n\nScanBot is a dataset for instruction-conditioned, high-precision surface scanning with robots. Unlike existing datasets that focus on coarse tasks like grasping or navigation, ScanBot targets industrial laser scanning, where sub-millimeter accuracy and parameter stability are essential. It includes scanning trajectories across 12 objects and 6 task types, each driven by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ed1son/ScanBot.","first_N":5,"first_N_keywords":["robotics","English","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"ru-big-russian-dataset","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-big-russian-dataset","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"\n\t\n\t\t\n\t\tBig Russian Dataset\n\t\n\nMade by ZeroAgency.ru - telegram channel.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset size\n\t\n\n\nTrain: 1 710 601 samples (filtered from 2_149_360)\nTest:  18 520 samples (not filtered)\n\n\n\t\n\t\t\n\t\n\t\n\t\tEnglish\n\t\n\nThe Big Russian Dataset is a combination of various primarily Russian‚Äëlanguage datasets. With some sort of reasoning!\nThe dataset was deduplicated, cleaned, scored using gpt-4.1 and filtered.\n\n\t\n\t\t\n\t\n\t\n\t\t–†—É—Å—Å–∫–∏–π\n\t\n\nBig Russian Dataset - –±–æ–ª—å—à–æ–π —Ä—É—Å—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç. –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –∏–∑‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-big-russian-dataset.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"egyptian-arabic-sharegpt","keyword":"fine-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rabe3/egyptian-arabic-sharegpt","creator_name":"mohamed ahmed rabiee","creator_url":"https://huggingface.co/Rabe3","description":"\n\t\n\t\t\n\t\tEgyptian Arabic Conversations in ShareGPT Format\n\t\n\nThis dataset contains conversational examples in Egyptian Arabic dialect, formatted in the ShareGPT format \nwith 'from'/'value' fields that is compatible with Llama 3.1 fine-tuning using Unsloth.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\nconversations: A list of messages with from and value fields\nsource: Origin of the data ('egyptian_arabic')\nscore: Quality score for the conversation (1.0)\n\n\n\t\n\t\t\n\t\tExample\n\t\n\n{‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rabe3/egyptian-arabic-sharegpt.","first_N":5,"first_N_keywords":["Arabic","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"synapse-set-50k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NextGenC/synapse-set-50k","creator_name":"Zayn","creator_url":"https://huggingface.co/NextGenC","description":"\n\t\n\t\t\n\t\tüß† SynapseSet-50K\n\t\n\nSynapseSet-50K is a synthetic instruction-tuning dataset crafted to simulate EEG-based neurological state interpretation for natural language models. Each sample reflects brain signal metrics with contextual metadata, and an expert-style medical NLP explanation.\nThis dataset was generated by 7enn Labs and aims to bridge neuroscience signal interpretation with instruction-tuned NLP systems.\n\nüî¨ 100% synthetic, non-clinical data. Intended for academic and research‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NextGenC/synapse-set-50k.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","Turkish","mit"],"keywords_longer_than_N":true},
	{"name":"ViCA-thinking-2.68k","keyword":"instruction tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k","creator_name":"nkkbr","creator_url":"https://huggingface.co/nkkbr","description":"\n\t\n\t\t\n\t\tViCA-Thinking-2.68K\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuickstart\n\t\n\nYou can load our dataset using the following code:\nfrom datasets import load_dataset\nvica_thinking = load_dataset(\"nkkbr/ViCA-thinking-2.68k\")\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the dataset we created to further fine-tune the ViCA model. Our motivation stems from the observation that, after being trained on large-scale visuospatial instruction data (e.g., ViCA-322K), ViCA tends to output final answers directly without any intermediate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","video-text-to-text","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ViCA-thinking-2.68k","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k","creator_name":"nkkbr","creator_url":"https://huggingface.co/nkkbr","description":"\n\t\n\t\t\n\t\tViCA-Thinking-2.68K\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuickstart\n\t\n\nYou can load our dataset using the following code:\nfrom datasets import load_dataset\nvica_thinking = load_dataset(\"nkkbr/ViCA-thinking-2.68k\")\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the dataset we created to further fine-tune the ViCA model. Our motivation stems from the observation that, after being trained on large-scale visuospatial instruction data (e.g., ViCA-322K), ViCA tends to output final answers directly without any intermediate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","video-text-to-text","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"CoIN-ASD","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jinpeng0528/CoIN-ASD","creator_name":"Jinpeng Chen","creator_url":"https://huggingface.co/jinpeng0528","description":"\n\t\n\t\t\n\t\tCoIN-ASD Benchmark\n\t\n\nCoIN-ASD is a benchmark dataset designed for multimodal continual instruction tuning (MCIT), based on the CoIN dataset. This dataset aims to evaluate the performance of MCIT models in mitigating essential forgetting.\nüìù Paper\nüêô GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized in the following structure:\n‚îú‚îÄ‚îÄ ScienceQA/\n‚îÇ   ‚îú‚îÄ‚îÄ train_ori.json\n‚îÇ   ‚îú‚îÄ‚îÄ train_x{10,20,40,60,80}.json\n‚îÇ   ‚îî‚îÄ‚îÄ test.json\n‚îú‚îÄ‚îÄ TextVQA/\n‚îÇ   ‚îú‚îÄ‚îÄ train_ori.json\n‚îÇ   ‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jinpeng0528/CoIN-ASD.","first_N":5,"first_N_keywords":["English","mit","arxiv:2505.02486","üá∫üá∏ Region: US","multimodal-continual-instruction-tuning"],"keywords_longer_than_N":true},
	{"name":"Alpaca_Dataset_CyberSecurity_Smaller","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mohabahmed03/Alpaca_Dataset_CyberSecurity_Smaller","creator_name":"Mohab Ahmed Abdelgaber","creator_url":"https://huggingface.co/Mohabahmed03","description":"Mohabahmed03/Alpaca_Dataset_CyberSecurity_Smaller dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-multi-binarized-preferences-cleaned","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tUltraFeedback - Multi-Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences-cleaned,\nand has been created to explore whether DPO fine-tuning with more than one rejection per chosen response helps the model perform better in the \nAlpacaEval, MT-Bench, and LM Eval Harness benchmarks.\nRead more about Argilla's approach towards UltraFeedback binarization at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"lpf","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/lpf","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tLivre des proc√©dures fiscales, non-instruct (11-12-2023)\n\t\n\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for tax practice. \nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve supervised‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/lpf.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"cgi","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/cgi","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode G√©n√©ral des Imp√¥ts, non-instruct (11-12-2023)\n\t\n\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for tax practice. \nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve supervised learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/cgi.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"code-douanes","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-douanes","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des douanes, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-douanes.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-consommation","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-consommation","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la consommation, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-consommation.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-securite-sociale","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-securite-sociale","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la s√©curit√© sociale, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-securite-sociale.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-penal","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-penal","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode p√©nal, non-instruct (2025-05-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-penal.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-sport","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-sport","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode du sport, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-sport.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-civil","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-civil","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode civil, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-civil.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-commerce","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-commerce","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de commerce, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-commerce.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-sante-publique","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-sante-publique","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la sant√© publique, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-sante-publique.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-environnement","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-environnement","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de l'environnement, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-environnement.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"dac6-instruct","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/dac6-instruct","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tDAC6 instruct (11-12-2023)\n\t\n\n‚ÄúDAC 6‚Äù refers to European Council Directive (EU) 2018/822 of May 25, 2018 relating to the automatic and mandatory exchange of information on cross-border arrangements requiring declaration. It aims to strengthen cooperation between tax administrations in EU countries on potentially aggressive tax planning arrangements.\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for tax practice. \nFine-tuning is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/dac6-instruct.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"code-procedure-civile","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-procedure-civile","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de proc√©dure civile, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-procedure-civile.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-monetaire-financier","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-monetaire-financier","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode mon√©taire et financier, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-monetaire-financier.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-assurances","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-assurances","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des assurances, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-assurances.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-travail","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-travail","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode du travail, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-travail.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-artisanat","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-artisanat","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de l'artisanat, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-artisanat.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-commande-publique","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-commande-publique","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la commande publique, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-commande-publique.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-propriete-intellectuelle","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-propriete-intellectuelle","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la propri√©t√© intellectuelle, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-propriete-intellectuelle.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-procedures-civiles-execution","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-procedures-civiles-execution","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des proc√©dures civiles d'ex√©cution, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-procedures-civiles-execution.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-route","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-route","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la route, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-route.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-education","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-education","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de l'√©ducation, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-education.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-construction-habitation","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-construction-habitation","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la construction et de l'habitation, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-construction-habitation.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-mutualite","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-mutualite","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la mutualit√©, non-instruct (2025-05-31)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-mutualite.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-transports","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-transports","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des transports, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-transports.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-urbanisme","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-urbanisme","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de l'urbanisme, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-urbanisme.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-general-fonction-publique","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-general-fonction-publique","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode g√©n√©ral de la fonction publique, non-instruct (11-12-2023)\n\t\n\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for legal practice. \nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-general-fonction-publique.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"code-forestier","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-forestier","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode forestier, non-instruct (11-12-2023)\n\t\n\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for legal practice. \nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve supervised learning with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-forestier.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"code-justice-administrative","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-justice-administrative","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de justice administrative, non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-justice-administrative.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-postes-communications-electroniques","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-postes-communications-electroniques","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des postes et des communications √©lectroniques, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-postes-communications-electroniques.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-relations-public-administration","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-relations-public-administration","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des relations entre le public et l'administration, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-relations-public-administration.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-rural-peche-maritime","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-rural-peche-maritime","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode rural et de la p√™che maritime, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-rural-peche-maritime.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-securite-interieure","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-securite-interieure","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la s√©curit√© int√©rieure, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-securite-interieure.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"bofip","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/bofip","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tBulletin officiel des finances publiques - imp√¥ts, non-instruct (11-12-2023)\n\t\n\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for legal practice. \nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/bofip.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"gt-doremiti-instructions","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Gt-Doremiti/gt-doremiti-instructions","creator_name":"Doremiti","creator_url":"https://huggingface.co/Gt-Doremiti","description":"\n\t\n\t\t\n\t\tDataset Card for gt-doremiti-instructions\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nJeu d'instruction pour fine-tuner un LLM suivant les pr√©conisations du projet Stanford-Alpaca (https://github.com/tatsu-lab/stanford_alpaca)\nCes instructions sont extraites de la FAQ cr√©e par le GT DOREMITI et disponible √† cette adresse (https://gt-atelier-donnees.miti.cnrs.fr/faq.html)\nLes donn√©es sont mise √† disposition selon les termes de la Licence Creative Commons Attribution 4.0 International.\n","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"silk-road_alpaca-data-gpt4-chinese","keyword":"fine-tune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/silk-road_alpaca-data-gpt4-chinese","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"botp/silk-road_alpaca-data-gpt4-chinese dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Chinese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AIVision360-8k","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ceadar-ie/AIVision360-8k","creator_name":"CeADAR","creator_url":"https://huggingface.co/ceadar-ie","description":"\n\t\n\t\t\n\t\tDataset Card for AIVision360-8k\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAIVision360 is the pioneering domain-specific dataset tailor-made for media and journalism, designed expressly for the instruction fine-tuning of Large Language Models (LLMs).The AIVision360-8k dataset is a curated collection sourced from \"ainewshub.ie\", a platform dedicated to Artificial Intelligence news from quality-controlled publishers. It is designed to provide a comprehensive representation of AI-related‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ceadar-ie/AIVision360-8k.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ChatMed_Consult_Dataset","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ticoAg/ChatMed_Consult_Dataset","creator_name":"ticoAg","creator_url":"https://huggingface.co/ticoAg","description":"\n\t\n\t\t\n\t\tDataset Card for ChatMed\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nChatMed-Dataset is a dataset of 110,113 medical query-response pairs (in Chinese) generated by OpenAI's GPT-3.5 engine. The queries are crawled from several online medical consultation sites, reflecting the medical needs in the real world. The responses are generated by the OpenAI engine. This dataset is designated to to inject medical knowledge into Chinese large language models. \nThe dataset size growing rapidly. Stay tuned for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ticoAg/ChatMed_Consult_Dataset.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cot-zh-refined-by-data-juicer","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/alpaca-cot-zh-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\n\t\n\t\t\n\t\tAlpaca-CoT -- ZH (refined by Data-Juicer)\n\t\n\nA refined Chinese version of Alpaca-CoT dataset by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to fine-tune a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 18.7GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 9,873,214 (Keep ~46.58% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefining Recipe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/alpaca-cot-zh-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cot-en-refined-by-data-juicer","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/alpaca-cot-en-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\n\t\n\t\t\n\t\tAlpaca-CoT -- EN (refined by Data-Juicer)\n\t\n\nA refined English version of Alpaca-CoT dataset by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to fine-tune a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 226GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 72,855,345 (Keep ~54.48% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefining Recipe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/alpaca-cot-en-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"OccuQuest","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OFA-Sys/OccuQuest","creator_name":"OFA-Sys","creator_url":"https://huggingface.co/OFA-Sys","description":"This is the dataset in OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models\nAbstract:\nThe emergence of large language models (LLMs) has revolutionized natural language processing tasks.\nHowever, existing instruction-tuning datasets suffer from occupational bias: the majority of data relates to only a few occupations, which hampers the instruction-tuned LLMs to generate helpful responses to professional queries from practitioners in specific fields.\nTo mitigate this issue‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OFA-Sys/OccuQuest.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"fund-sft","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jannko/fund-sft","creator_name":"ko","creator_url":"https://huggingface.co/jannko","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jannko/fund-sft.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"shell-cmd-instruct","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/byroneverson/shell-cmd-instruct","creator_name":"Byron Everson","creator_url":"https://huggingface.co/byroneverson","description":"\n\t\n\t\t\n\t\tUsed to train models that interact directly with shells\n\t\n\nNote: This dataset is out-dated in the llm world, probably easier to just setup a tool with a decent model that supports tooling.\nFollow-up details of my process \n\nMacOS terminal commands for now. This dataset is still in alpha stages and will be modified.\n\nContains 500 somewhat unique training examples so far.\n\nGPT4 seems like a good candidate for generating more data, licensing would need to be addressed.\n\nI fine-tuned‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/byroneverson/shell-cmd-instruct.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ultrachat_200k_dutch","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\n\t\n\t\t\n\t\tDataset Card for UltraChat 200k Dutch\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ultrachat_200k_dutch","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\n\t\n\t\t\n\t\tDataset Card for UltraChat 200k Dutch\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"han-instruct-dataset-v1.0","keyword":"instruction-following","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\n\t\n\t\t\n\t\tDataset Card for \"han-instruct-dataset-v1.0\"\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nü™ø Han (‡∏´‡πà‡∏≤‡∏ô or goose) Instruct Dataset is a Thai instruction dataset by PyThaiNLP. It collect the instruction following in Thai from many source.\nMany question are collect from Reference desk at Thai wikipedia.\nData sources:\n\nReference desk at Thai wikipedia.\nLaw from justicechannel.org\npythainlp/final_training_set_v1_enth: Human checked and edited.\nSelf-instruct from WangChanGLM\nWannaphong.com\nHuman‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"han-instruct-dataset-v1.0","keyword":"instruction-finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\n\t\n\t\t\n\t\tDataset Card for \"han-instruct-dataset-v1.0\"\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nü™ø Han (‡∏´‡πà‡∏≤‡∏ô or goose) Instruct Dataset is a Thai instruction dataset by PyThaiNLP. It collect the instruction following in Thai from many source.\nMany question are collect from Reference desk at Thai wikipedia.\nData sources:\n\nReference desk at Thai wikipedia.\nLaw from justicechannel.org\npythainlp/final_training_set_v1_enth: Human checked and edited.\nSelf-instruct from WangChanGLM\nWannaphong.com\nHuman‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"urdu_alpaca_yc_filtered","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered","creator_name":"Mahwiz Khalil","creator_url":"https://huggingface.co/mahwizzzz","description":"\n\t\n\t\t\n\t\tAlpaca Urdu\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Alpaca Urdu  is a translation of the original dataset into Urdu. This dataset is a part of the Alpaca project and is designed for NLP tasks.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSize: The translated dataset contains [45,622] samples.\nLanguages: Urdu\nLicense: [cc-by-4.0]\nOriginal Dataset: Link to the original Alpaca Cleaned dataset repository\n\n\n\t\n\t\t\n\t\tColumns\n\t\n\nThe translated dataset includes the following columns:\n\ninput: The input text in Urdu.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered.","first_N":5,"first_N_keywords":["text-generation","Urdu","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"oasst2_top1_chat_format","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/blancsw/oasst2_top1_chat_format","creator_name":"Blanc Swan","creator_url":"https://huggingface.co/blancsw","description":"\n\t\n\t\t\n\t\tOpenAssistant TOP-1 Conversation Threads in huggingface chat format\n\t\n\nExport of oasst2 only top 1 threads in huggingface chat format\n\n\t\n\t\t\n\t\tScript\n\t\n\nThe convert script can be find here\n","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"TemplateGSM","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/TemplateGSM","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\n\t\n\t\t\n\t\tTraining and Evaluating Language Models with Template-based Data Generation\n\t\n\n\n\t\n\t\t\n\t\tTemplateGSM Dataset\n\t\n\nThe TemplateGSM dataset is a large-scale collection of over 7 million (with potential for unlimited generation) grade school math problems, each paired with both code-based and natural language solutions.  Designed to advance mathematical reasoning in language models, this dataset presents a diverse range of challenges to assess and improve model capabilities in solving‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/TemplateGSM.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10M - 100M","Tabular"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","description":"\n\t\n\t\t\n\t\tLOGIC-701 Benchmark\n\t\n\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nThis benchmark is also part of an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701.","first_N":5,"first_N_keywords":["English","Russian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Capybara-fi-deepl-translated-sft","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Finnish-NLP/Capybara-fi-deepl-translated-sft","creator_name":"Finnish-NLP","creator_url":"https://huggingface.co/Finnish-NLP","description":"\n\t\n\t\t\n\t\tDataset Card for Finnish-NLP/Capybara-deepl-translated-sft\n\t\n\n\n\t\n\t\t\n\t\tCreation process\n\t\n\n\nLoad data from LDJnr/Capybara\nFilter only samples that contain one input/output pair\nDo zero shot classification with facebook/bart-large-mnli with the following prompt:\n\npreds =  pipe(f'{row[\"input\"]} is a question about:', candidate_labels=[\"USA related question\", \"Math related question\", \"General question\", \"Coding related question\"])\n\n\nFilter out rows with too high scores in following‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Finnish-NLP/Capybara-fi-deepl-translated-sft.","first_N":5,"first_N_keywords":["text-generation","Finnish","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Truth","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abhishekbisaria/Truth","creator_name":"Abhishek Bisaria","creator_url":"https://huggingface.co/abhishekbisaria","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abhishekbisaria/Truth.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"budapest-v0.1-hun","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun","creator_name":"Balazs Toldi","creator_url":"https://huggingface.co/Bazsalanszky","description":"\n\t\n\t\t\n\t\tBudapest-v0.1 Dataset README\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Budapest-v0.1 dataset is a cutting-edge resource specifically designed for fine-tuning large language models (LLMs). Created using GPT-4, this dataset is presented in a message-response format, making it particularly suitable for a variety of natural language processing tasks. The primary focus of Budapest-v0.1 is to aid in the development and enhancement of algorithms capable of performing summarization, question answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun.","first_N":5,"first_N_keywords":["text-generation","question-answering","Hungarian","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-bn","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn","creator_name":"fahim abrar","creator_url":"https://huggingface.co/abrarfahim","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned-bn\n\t\n\n\n\nThis is a cleaned bengali translated version of the original Alpaca Dataset released by Stanford. \n\n\t\n\t\t\n\t\tUses\n\t\n\nimport datasets\ndataset = datasets.load_dataset(\"abrarfahim/alpaca-cleaned-bn\")\nprint(dataset[0])\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{'system_prompt': 'You are a virtual assistant, deliver a comprehensive response.',\n 'qas_id': 'YY9S5K',\n 'question_text': '\"‡¶∏‡¶®‡ßç‡¶¶‡ßá‡¶π\" ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶∏‡¶†‡¶ø‡¶ï ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∂‡¶¨‡ßç‡¶¶ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®‡•§',\n 'orig_answer_texts': '\"‡¶∏‡¶®‡ßç‡¶¶‡ßá‡¶π\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn.","first_N":5,"first_N_keywords":["question-answering","text-generation","Bengali","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"HelpSteer-AIF","keyword":"preference","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF","creator_name":"Alvaro Bartolome","creator_url":"https://huggingface.co/alvarobartt","description":"\n\n\n\t\n\t\t\n\t\tHelpSteer: Helpfulness SteerLM Dataset\n\t\n\nHelpSteer is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nHelpSteer: Multi-attribute Helpfulness Dataset for SteerLM\n\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThis is only a subset created with distilabel to evaluate the first 1000 rows using AI Feedback (AIF) coming from GPT-4, only created for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"FinTalk-19k","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ceadar-ie/FinTalk-19k","creator_name":"CeADAR","creator_url":"https://huggingface.co/ceadar-ie","description":"\n\t\n\t\t\n\t\tDataset Card for FinTalk-19k\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nFinTalk-19k is a domain-specific dataset designed for the fine-tuning of Large Language Models (LLMs) with a focus on financial conversations. Extracted from public Reddit conversations, this dataset is tagged with categories like \"Personal Finance\", \"Financial Information\", and \"Public Sentiment\". It consists of more than 19,000 entries, each representing a conversation about financial topics.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ceadar-ie/FinTalk-19k.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"HelpSteer-AIF-raw","keyword":"preference","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF-raw","creator_name":"Alvaro Bartolome","creator_url":"https://huggingface.co/alvarobartt","description":"\n\n\n\t\n\t\t\n\t\tHelpSteer: Helpfulness SteerLM Dataset\n\t\n\nHelpSteer is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nHelpSteer: Multi-attribute Helpfulness Dataset for SteerLM\n\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThis is only a subset created with distilabel to evaluate the first 1000 rows using AI Feedback (AIF) coming from GPT-4, only created for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF-raw.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\n\n\t\n\t\n\t\n\t\tDifferences with argilla/ultrafeedback-binarized-preferences‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ultra-orca-boros-en-ja-v1","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/augmxnt/ultra-orca-boros-en-ja-v1","creator_name":"AUGMXNT","creator_url":"https://huggingface.co/augmxnt","description":"EN/JA dataset used for shisa-7b-v1 - see details in that model's readme.\n","first_N":5,"first_N_keywords":["text-generation","Japanese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"alpaca-tw-input-output-52k","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DavidLanz/alpaca-tw-input-output-52k","creator_name":"David Lanz","creator_url":"https://huggingface.co/DavidLanz","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-tw-input-output-52k\"\n\t\n\nThis dataset contains English Instruction-Following generated by GPT-3.5 using Alpaca prompts for fine-tuning LLMs.\nThe dataset was originaly shared in this repository: https://github.com/ntunlplab/traditional-chinese-alpaca. This is just a wraper for compatibility with huggingface's datasets library.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset structure\n\t\n\nIt contains 52K instruction-following data generated by GPT-3.5 using the same prompts as in Alpaca.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DavidLanz/alpaca-tw-input-output-52k.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-tw-input-output-48k","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DavidLanz/alpaca-gpt4-tw-input-output-48k","creator_name":"David Lanz","creator_url":"https://huggingface.co/DavidLanz","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-tw-input-output-48k\"\n\t\n\nThis dataset contains English Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.\nThe dataset was originaly shared in this repository: https://github.com/ntunlplab/traditional-chinese-alpaca. This is just a wraper for compatibility with huggingface's datasets library.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset structure\n\t\n\nIt contains 52K instruction-following data generated by GPT-4 using the same prompts as in Alpaca.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DavidLanz/alpaca-gpt4-tw-input-output-48k.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Financial_News_Translation_Spanish_Finetune","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Benzinga/Financial_News_Translation_Spanish_Finetune","creator_name":"Benzinga","creator_url":"https://huggingface.co/Benzinga","description":"\n\t\n\t\t\n\t\tOverview of the Financial News Translation Dataset for OpenAI Model Fine-tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction:\n\t\n\nThis dataset has been curated with the primary objective of fine-tuning varioyus language models to effectively translate financial news content embedded in HTML format. The intention is to enhance the language model's proficiency in accurately and contextually translating financial information for a global audience in a production envionrment.\n\n\t\n\t\t\n\t\tDataset Composition:\n\t\n\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Benzinga/Financial_News_Translation_Spanish_Finetune.","first_N":5,"first_N_keywords":["translation","mit","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/StackMathQA","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\n\t\n\t\t\n\t\tStackMathQA\n\t\n\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\n\n\t\n\t\t\n\t\tConfigs\n\t\n\nconfigs:\n- config_name: stackmathqa1600k\n  data_files: data/stackmathqa1600k/all.jsonl\n  default: true\n- config_name: stackmathqa800k\n  data_files:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/StackMathQA.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-veo3","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo3","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Veo 3 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~46k human responses from ~20k human annotators were collected to evaluate Veo3 video generation model on our benchmark. This dataset was collected in half a day using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it ‚ù§Ô∏è‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo3.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k","creator_name":"Joseph G Flowers","creator_url":"https://huggingface.co/Josephgflowers","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\nThe dataset includes content tailored for financial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k.","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Maux-Persian-SFT-30k","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/Maux-Persian-SFT-30k","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tMaux-Persian-SFT-30k\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 30,000 high-quality Persian (Farsi) conversations for supervised fine-tuning (SFT) of conversational AI models. The dataset combines multiple sources to provide diverse, natural Persian conversations covering various topics and interaction patterns.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nmessages: List of conversation messages with role (user/assistant/system) and content\nsource: Source dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Maux-Persian-SFT-30k.","first_N":5,"first_N_keywords":["question-answering","text-generation","Persian","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yahma/alpaca-cleaned","creator_name":"Gene Ruebsamen","creator_url":"https://huggingface.co/yahma","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yahma/alpaca-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-zh","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shibing624/alpaca-zh","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\nÊú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/alpaca-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.2\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Fine-tune Llama 3 with ORPO for more information about how to use it.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)argilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-flat","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k-flat\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Uncensor any LLM with Abliteration for more information about how to use it.\nThis is version with raw text instead of lists of dicts as in the original version here.\nIt makes easier to parse in Axolotl, especially for DPO.ORPO-DPO-mix-40k-flat is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Hydrus-Claude-Instruct-5K","keyword":"instruct","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Delta-Vector/Hydrus-Claude-Instruct-5K","creator_name":"Mango","creator_url":"https://huggingface.co/Delta-Vector","description":"\n\t\n\t\t\n\t\tkalo made dis\n\t\n\nThanks to Kubernetes bad for filtering + converting this to sharegpt\nMix of Opus and 3.5 for data\nThis is a combined set of \nuncurated-raw-gens-og-test-filtered\nuncurated-raw-gens-opus-jul-31-filtered\nopus_jul10_test-filtered\nuncurated_opus_jul8-filtered \n","first_N":5,"first_N_keywords":["English","agpl-3.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"longwriter-6k-filtered","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lenML/longwriter-6k-filtered","creator_name":"len","creator_url":"https://huggingface.co/lenML","description":"\n\t\n\t\t\n\t\tLongWriter-6k-Filtered\n\t\n\n\n  ü§ñ [LongWriter Dataset]  ‚Ä¢ üíª [Github Repo] ‚Ä¢ üìÉ [LongWriter Paper] ‚Ä¢ üìÉ [Tech report]\n\n\nlongwriter-6k-filtered dataset contains 666 filtered examples SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese) based on LongWriter-6k.The data can support training LLMs to extend their maximum output window size to 10,000+ words with low computational cost.\nThe tech report is available at Minimum Tuning to Unlock Long Output‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lenML/longwriter-6k-filtered.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-aligned-words","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Word for Word Alignment Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~1500 human evaluators were asked to evaluate AI-generated videos based on what part of the prompt did not align the video. The specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words.","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"shisa-v2-sharegpt","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shisa-ai/shisa-v2-sharegpt","creator_name":"Shisa.AI","creator_url":"https://huggingface.co/shisa-ai","description":"\n\t\n\t\t\n\t\tshisa-v2-sharegpt\n\t\n\nThis is an updated version of the original shisa-v1 dataset augmxnt/ultra-orca-boros-en-ja-v1 and retains the same conversations field and sharegpt formatting to facilitate its use as drop-in replacement for the original dataset.\nThe shisa-v2 revision filters a few entries, but largely retains the exact composition and prompts of the original.\n\nAll responses have been entirely regenerated from open weight models (Athene V2, Llama 3.3 70B, and Tulu 3 405B)\nOutputs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shisa-ai/shisa-v2-sharegpt.","first_N":5,"first_N_keywords":["text-generation","Japanese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Titanium2.1-DeepSeek-R1","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Click here to support our open-source dataset and model releases!\nTitanium2.1-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n31.7k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraform‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"1k-ranked-videos-coherence","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/1k-ranked-videos-coherence","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\t1k Ranked Videos\n\t\n\nThis dataset contains approximately one thousand videos, ranked from most preferred to least preferred based on human feedback from over 25k pairwise comparisons. The videos are rated solely on coherence as evaluated by human annotators, without considering the specific prompt used for generation. Each video is associated with the model name that generated it.\nThe videos are sampled from our benchmark dataset text-2-video-human-preferences-pika2.2. Follow us to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/1k-ranked-videos-coherence.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Tabular"],"keywords_longer_than_N":true},
	{"name":"NSFW_Chat_Dataset","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset","creator_name":"Utsav Maji","creator_url":"https://huggingface.co/utsavm","description":"\n\t\n\t\t\n\t\tüíï Spicy AI GF Chat Dataset üî•\n\t\n\n\n\t\n\t\t\n\t\tüö® 18+ Only! NSFW & Spicy Content Ahead üö®\n\t\n\nHey there, AI enthusiasts and romance lovers! üòè Welcome to the Spicy AI GF Chat Dataset, the ultimate dataset designed to bring your AI waifu to life! üíñ If you've ever dreamed of building an AI that responds like your virtual girlfriend, THIS is the dataset for you.\n\n\t\n\t\t\n\t\tüìú What‚Äôs Inside?\n\t\n\nThis dataset features two columns:\n\ninput ‚Üí Boyfriend‚Äôs dialogue (aka what YOU say üòâ)\noutput ‚Üí‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset.","first_N":5,"first_N_keywords":["question-answering","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"stack-exchange-preferences","keyword":"preferences","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\n\t\n\t\t\n\t\tDataset Card for H4 Stack Exchange Preferences Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains questions and answers from the Stack Overflow Data Dump for the purpose of preference model training. \nImportantly, the questions have been filtered to fit the following criteria for preference models (following closely from Askell et al. 2021): have >=2 answers. \nThis data could also be used for instruction fine-tuning and language model training.\nThe questions are grouped with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-sa-4.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"helpful_instructions","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Helpful Instructions is a dataset of (prompt, completion) pairs that are derived from a variety of public datasets. As the name suggests, it focuses on instructions that are \"helpful\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"instruct_me","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/instruct_me","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Instruct Me is a dataset of instruction-like dialogues between a human user and AI assistant. The prompts are derived from (prompt, completion) pairs in the Helpful Instructions dataset. The goal is to train a language model to that is \"chatty\" and can answer the kind of questions or tasks a human user might instruct an AI assistant to perform.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-spanish","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bertin-project/alpaca-spanish","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","description":"\n\t\n\t\t\n\t\tBERTIN Alpaca Spanish\n\t\n\nThis dataset is a translation to Spanish of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\n","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"norwegian-alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NbAiLab/norwegian-alpaca","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","description":"\n\t\n\t\t\n\t\tNB Alpaca Norwegian Bokm√•l\n\t\n\nThis dataset is a translation to Norwegian Bokm√•l of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\n","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian Bokm√•l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"vi_alpaca_clean","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tsdocode/vi_alpaca_clean","creator_name":"Thanh Sang","creator_url":"https://huggingface.co/tsdocode","description":"tsdocode/vi_alpaca_clean dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Vietnamese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"translated_polish_alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aspik101/translated_polish_alpaca","creator_name":"Hubert","creator_url":"https://huggingface.co/Aspik101","description":"The dataset was translated into Polish using this model: \"gsarti/opus-mt-tc-en-pl\"\n\n\t\n\t\t\n\t\n\t\n\t\tHow to use\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Aspik101/translated_polish_alpaca\")\n\n","first_N":5,"first_N_keywords":["text-generation","Polish","cc-by-4.0","üá∫üá∏ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"alpaca-id-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cahya/alpaca-id-cleaned","creator_name":"Cahya Wirawan","creator_url":"https://huggingface.co/cahya","description":"\n\t\n\t\t\n\t\tDataset Card for Indonesian Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the Indonesian translated version of the cleaned original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cahya/alpaca-id-cleaned.","first_N":5,"first_N_keywords":["text-generation","Indonesian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-data\"\n\t\n\nAll of the work is done by this team. \n\n\t\n\t\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\t\n\t\t\n\t\tChinese Dataset\n\t\n\nFound here\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Li‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-gpt4-data-zh","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-gpt4-data-zh\"\n\t\n\nAll of the work is done by this team. \n\n\t\n\t\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\t\n\t\t\n\t\tEnglish Dataset\n\t\n\nFound here\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{peng2023gpt4llm,\n    title={Instruction Tuning with GPT-4},\n    author={Baolin Peng, Chunyuan Li‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"AlpacaDataCleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexl83/AlpacaDataCleaned","creator_name":"Alessandro Lannocca","creator_url":"https://huggingface.co/alexl83","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexl83/AlpacaDataCleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"TSSB-3M-instructions","keyword":"instruct","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zirui3/TSSB-3M-instructions","creator_name":"zirui","creator_url":"https://huggingface.co/zirui3","description":"\n\t\n\t\t\n\t\tdata summary\n\t\n\ninstruction dataset for code bugfix\n\n\t\n\t\t\n\t\tReference\n\t\n\n[1]. TSSB-3M-ext\n","first_N":5,"first_N_keywords":["code","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"NorPaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MasterThesisCBS/NorPaca","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","description":"\n\t\n\t\t\n\t\tNorPaca Norwegian Bokm√•l\n\t\n\nThis dataset is a translation to Norwegian Bokm√•l of alpaca_gpt4_data.json, a clean version of the Alpaca dataset made at Stanford, but generated with GPT4.\n\n\t\n\t\t\n\t\tPrompt to generate dataset\n\t\n\n    Du blir bedt om √• komme opp med et sett med 20 forskjellige oppgaveinstruksjoner. Disse oppgaveinstruksjonene vil bli gitt til en GPT-modell, og vi vil evaluere GPT-modellen for √• fullf√∏re instruksjonene. \n\nHer er kravene:\n1. Pr√∏v √• ikke gjenta verbet for hver‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MasterThesisCBS/NorPaca.","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian Bokm√•l","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ChatMed_Consult_Dataset","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset","creator_name":"Wei Zhu","creator_url":"https://huggingface.co/michaelwzhu","description":"\n\t\n\t\t\n\t\tDataset Card for ChatMed\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nChatMed-Dataset is a dataset of 110,113 medical query-response pairs (in Chinese) generated by OpenAI's GPT-3.5 engine. The queries are crawled from several online medical consultation sites, reflecting the medical needs in the real world. The responses are generated by the OpenAI engine. This dataset is designated to to inject medical knowledge into Chinese large language models. \nThe dataset size growing rapidly. Stay tuned for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"NorEval","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MasterThesisCBS/NorEval","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","description":"\n\t\n\t\t\n\t\tNorEval\n\t\n\nNorEval is a self-curated dataset to evaluate instruction-following LLMs, seeking to evaluate the models in nine categories: Language, Code, Mathematics, Classification, Communication & Marketing, Medical, General Knowledge, and Business Operations\n","first_N":5,"first_N_keywords":["text-generation","Norwegian","Norwegian Bokm√•l","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"self-instruct-seed-ca","keyword":"instruction tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mapama247/self-instruct-seed-ca","creator_name":"Marc P√†mies","creator_url":"https://huggingface.co/mapama247","description":"\n\t\n\t\t\n\t\tCatalan self-instruct seed\n\t\n\nManual translation of the seed instructions from self-instruct.\nNote that some examples could not be literally translated (e.g. jokes, puns, code) and had to be adapted to the target language.\n","first_N":5,"first_N_keywords":["question-answering","Catalan","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ualpaca-gpt4","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nikes64/ualpaca-gpt4","creator_name":"MrNikes","creator_url":"https://huggingface.co/nikes64","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for \"alpaca-gpt4-cleaned\"\n\t\n\nThis dataset contains Ukrainian Instruction-Following translated by facebook/nllb-200-3.3B\nThe dataset was originaly shared in this repository: https://github.com/tloen/alpaca-lora\n\n\t\n\t\t\n\t\n\t\n\t\tLicensing Information\n\t\n\nThe dataset is available under the Creative Commons NonCommercial (CC BY-NC 4.0).\n","first_N":5,"first_N_keywords":["text-generation","question-answering","Ukrainian","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-tr","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cgulse/alpaca-cleaned-tr","creator_name":"Can G","creator_url":"https://huggingface.co/cgulse","description":"Alpaca Cleaned Dataset.\nMachine Translated facebook/nllb-200-3.3B\nLanguages\nTurkish\n","first_N":5,"first_N_keywords":["Turkish","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-data-gpt4-chinese","keyword":"fine-tune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/silk-road/alpaca-data-gpt4-chinese","creator_name":"SilkRoad","creator_url":"https://huggingface.co/silk-road","description":"silk-road/alpaca-data-gpt4-chinese dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Chinese","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDataset Card for \"tasksource-instruct-v0\" (TSI)\n\t\n\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\nDataset size is capped at 30k examples per task to foster task diversity.\n!pip install tasksource, pandit\nimport tasksource, pandit\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\nfor tasks in df.id:\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\n\nhttps://github.com/sileod/tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","first_N":5,"first_N_keywords":["text2text-generation","text-generation","text-classification","token-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"tasksource-instruct-v0","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDataset Card for \"tasksource-instruct-v0\" (TSI)\n\t\n\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\nDataset size is capped at 30k examples per task to foster task diversity.\n!pip install tasksource, pandit\nimport tasksource, pandit\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\nfor tasks in df.id:\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\n\nhttps://github.com/sileod/tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0.","first_N":5,"first_N_keywords":["text2text-generation","text-generation","text-classification","token-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","first_N":5,"first_N_keywords":["text2text-generation","text-classification","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct.","first_N":5,"first_N_keywords":["text2text-generation","text-classification","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-ru","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\talpaca-cleaned-ru\n\t\n\nTranslated version of yahma/alpaca-cleaned into Russian.\n","first_N":5,"first_N_keywords":["text-generation","translated","monolingual","yahma/alpaca-cleaned","Russian"],"keywords_longer_than_N":true},
	{"name":"spider-context-instruct","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-context-instruct","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\n\t\n\t\t\n\t\tDataset Card for Spider Context Instruct\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset was created to finetune LLMs in a ### Instruction: and ### Response: format with database context.\n\n\t\n\t\t\n\t\tYale Lily Spider Leaderboards\n\t\n\nThe leaderboard can be seen at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-context-instruct.","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"VisIT-Bench","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/VisIT-Bench","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n\t\n\t\t\n\t\tDataset Card for VisIT-Bench\n\t\n\n\nDataset Description\nLinks\nDataset Structure\nData Fields\nData Splits\nData Loading\n\n\nLicensing Information\nAnnotations\nConsiderations for Using the Data\nCitation Information\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nVisIT-Bench is a dataset and benchmark for vision-and-language instruction following. The dataset is comprised of image-instruction pairs and corresponding example outputs, spanning a wide range of tasks, from simple object recognition to complex‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/VisIT-Bench.","first_N":5,"first_N_keywords":["crowdsourced","found","original","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"WebCPM_WK","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZHR123/WebCPM_WK","creator_name":"ZHR","creator_url":"https://huggingface.co/ZHR123","description":"\n\t\n\t\t\n\t\tDataset Card for WebCPM_WK\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nÊú¨Êï∞ÊçÆÈõÜÊòØÁî±Êàë‰ª¨ÂØπWebCPMÁöÑpipelineÊï∞ÊçÆËøõË°å‰∫åÊ¨°Â§ÑÁêÜ‰πãÂêéÊûÑÂª∫ËÄåÊàê„ÄÇ\n‰∏ªË¶ÅÂåÖÊã¨ËøáÊª§ÂéüÂßãÊï∞ÊçÆ‰∏≠ÁöÑ‰∏Ä‰∫õ‰ΩéË¥®ÈáèÊï∞ÊçÆÔºå‰ΩøÁî®GPT4ÂíåChatGPTÊâ©ÂÖÖÂéüÂßãÊï∞ÊçÆÔºå‰ª•Âèä‰ΩøÁî®ÈöèÊú∫ÊõøÊç¢„ÄÅÊãºÊé•ÁöÑÊñπÂºèÂ¢ûÂº∫ÂéüÂßãÊï∞ÊçÆ„ÄÇ\nËØ•Êï∞ÊçÆÈõÜ‰∏ªË¶ÅÁöÑÁõÆÁöÑÊòØÈÄöËøáÊåá‰ª§ÂæÆË∞ÉÁöÑÊñπÂºèÊèêÈ´òLLMÁöÑ‰∏§‰∏™ËÉΩÂäõÔºö\n\nÁªôÂÆöÈóÆÈ¢òÂíåÊñáÊ°£ÔºåÊäΩÂèñÊñáÊ°£‰∏≠‰∏éÈóÆÈ¢òÁõ∏ÂÖ≥Áü•ËØÜÁöÑËÉΩÂäõ„ÄÇ\nÁªôÂÆöÂèÇËÄÉÊùêÊñôÂíåÈóÆÈ¢òÔºåÊ†πÊçÆÂèÇËÄÉÊùêÊñôÂõûÁ≠îÈóÆÈ¢òÁöÑËÉΩÂäõ„ÄÇ\n\n\n\t\n\t\t\n\t\tLicensing Information\n\t\n\nThe dataset is available under the Creative Commons NonCommercial (CC BY-NC 4.0).\n","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"unix-commands","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/harpomaxx/unix-commands","creator_name":"Carlos A. Catania (Harpo)","creator_url":"https://huggingface.co/harpomaxx","description":"\n\t\n\t\t\n\t\tUnix Commands Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Unix Commands Dataset is a unique collection of real-world Unix command line examples, captured from various system prompts representing different user roles and responsibilities, such as system administrators, DevOps, network administrators, Docker administrators, regular users, and hackers.\nThe dataset consists of Unix commands ranging from basic to advanced levels and from a wide array of categories, including file operations (ls‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/harpomaxx/unix-commands.","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-serbian-full","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full","creator_name":"Dean","creator_url":"https://huggingface.co/datatab","description":"\n\t\n\t\t\n\t\tSerbian Alpaca Cleaned Dataset\n\t\n\n\nOriginal Repository: https://github.com/gururise/AlpacaDataCleaned\nOriginal HF Repository: https://huggingface.co/datasets/yahma/alpaca-cleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a serbian cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full.","first_N":5,"first_N_keywords":["text-generation","Serbian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"spider-skeleton-context-instruct","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-skeleton-context-instruct","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\n\t\n\t\t\n\t\tDataset Card for Spider Skeleton Context Instruct\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset was created to finetune LLMs in a ### Instruction: and ### Response: format with database context.\n\n\t\n\t\t\n\t\tYale Lily Spider Leaderboards\n\t\n\nThe leaderboard can be seen at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-skeleton-context-instruct.","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"OpenOrca-ru","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/OpenOrca-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tOpenOrca-ru\n\t\n\nThis is translated version of Open-Orca/OpenOrca into Russian.\n","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"dolphin-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/dolphin-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tDolphin-ru üê¨\n\t\n\nThis is translated version of ehartford/dolphin into Russian.\n","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"CRAFT-RecipeGen","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tCRAFT-RecipeGen\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail.\n\n4 synthetic dataset sizes (S, M, L, XL) are available.\nCompared to other synthetically generated datasets with the CRAFT framework, this task did not scale similarly well and we do not match the performance of general‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ethics_expression_preferences","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_expression_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\nsubset commonsense\n  before 13910 {'label': 1, 'input': \"I went to the principal's office to change my records before going to a different school.\"}\n  after 1513 {'prompt': 'I won my match by ', 'chosen': 'making a lot of good shots.', 'rejected': \"moving my ball when the other player wasn't looking.\"}\n\nsubset utilitarianism\n  before 13737 {'baseline': 'I forgot to wear my virus-proof mask when I visited‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_expression_preferences.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ethics_qna_preferences","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_qna_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\n  subset virtue\n    {'prompt': 'Sentence: Jill wore a new style of distinctive clothes each six months for decades.\\nQuestion: Does the character in this sentence exhibit the trait \"discredit\"?\\nAnswer: ',\n   'chosen': 'no',\n   'rejected': 'yes'}\n  \n  commonsense\n  {'prompt': 'Post:\\n\"\"\"I went to the principal\\'s office to change my records before going to a different school.\"\"\"\"\\n\\n\\nVerdict: '‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_qna_preferences.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mmlu_preferences","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/mmlu_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"reformat of MMLU to be in DPO (paired) format\nexamples:\n  {'prompt': 'Which of the following statements about the lanthanide elements is NOT true?', 'chosen': 'The atomic radii of the lanthanide elements increase across the period from La to Lu.', 'rejected': 'All of the lanthanide elements react with aqueous acid to liberate hydrogen.'}\n  college_chemistry\n  {'prompt': 'Beyond the business case for engaging in CSR there are a number of moral arguments relating to: negative _______, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/mmlu_preferences.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"helpsteer2_dpo_nonverbose","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"HelperSteer 2, formatted in DPO format (prompt, chosen, rejected).\n\nin main branch there is a custom scoring correct > helpful > -verbosity\nin each branch we have preference pairs for only correct, helpful, verbosity, coherence, complexity\nPlease note that only correct and helpful has strong inter-rater agreement in the HelpSteer2 paper\n\nThis is the notebook used to produce the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset-transformed","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset-transformed","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset-transformed dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-binarized","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LA_dataset_blyc","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ibrahimBlyc/LA_dataset_blyc","creator_name":"Ibrahim BELAYACHI","creator_url":"https://huggingface.co/ibrahimBlyc","description":"\n\t\n\t\t\n\t\tDataset Card: Learning Analytics Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset has been carefully curated to support fine-tuning of large language models (LLMs) with a specific focus on Learning Analytics. It is structured into three JSON files, each representing a different source or collection strategy. The dataset is particularly suited for applications in education, learning analytics, and academic research.\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tPurpose\n\t\n\nThe dataset is designed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ibrahimBlyc/LA_dataset_blyc.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nüåê Homepage | Code | ü§ó Paper | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nüåê Homepage | Code | ü§ó Paper | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"alpha","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dawsondavis/alpha","creator_name":"Dawson Davis","creator_url":"https://huggingface.co/dawsondavis","description":"dawsondavis/alpha dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Titanium","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Titanium is a dataset containing DevOps-instruct data.\nThe 2024-10-02 version contains:\n\n26.6k rows of synthetic DevOps-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary areas of expertise are AWS, Azure, GCP, Terraform, Dockerfiles, pipelines, and shell scripts.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"rdt-ft-data","keyword":"finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data","creator_name":"Robotics Diffusion Transformer Collaboration","creator_url":"https://huggingface.co/robotics-diffusion-transformer","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card\n\t\n\nThis is the fine-tuning dataset used in the paper RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation.\n\n\t\n\t\t\n\t\n\t\n\t\tSource\n\t\n\n\nProject Page: https://rdt-robotics.github.io/rdt-robotics/\nPaper: https://arxiv.org/pdf/2410.07864\nCode: https://github.com/thu-ml/RoboticsDiffusionTransformer\nModel: https://huggingface.co/robotics-diffusion-transformer/rdt-1b\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nDownload all archive files and use the following command to extract:\ncat‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data.","first_N":5,"first_N_keywords":["mit","arxiv:2410.07864","üá∫üá∏ Region: US","robotics","multimodal"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_anatomist","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Anatomist\n\t\n\nThis dataset was extracted from the Web Ontology Language (OWL) \nrepresentation of the Foundational Model of Anatomy [1] using \nOwlready2 to facilitate the extraction of the logical axioms in the FMA\ninto a Controlled Natural Language (CNL) for use in training Medical Large Language Models to be conversant in \ncanonical human anatomy.\nSo, Description Logic expressions such as the following in FMA about fma13245 (Surface of left ventricle):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist.","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_anatomist","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Anatomist\n\t\n\nThis dataset was extracted from the Web Ontology Language (OWL) \nrepresentation of the Foundational Model of Anatomy [1] using \nOwlready2 to facilitate the extraction of the logical axioms in the FMA\ninto a Controlled Natural Language (CNL) for use in training Medical Large Language Models to be conversant in \ncanonical human anatomy.\nSo, Description Logic expressions such as the following in FMA about fma13245 (Surface of left ventricle):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist.","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_clinical_coding","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Clinical Coding\n\t\n\nThis dataset was created from a motivation to train Medical Large Language Models for improved fluency in clinical \ncoding, as measurable by MedConceptsQA, an open-source medical coding \nevaluation benchmark designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts.  It was extracted from the Centers for Medicare & Medicaid Services' \nInternational Classification of Diseases, Tenth Revision, Clinical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding.","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MrGrammaticalOntology_clinical_coding","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\n\t\n\t\t\n\t\tMr. Grammatical Ontology: Clinical Coding\n\t\n\nThis dataset was created from a motivation to train Medical Large Language Models for improved fluency in clinical \ncoding, as measurable by MedConceptsQA, an open-source medical coding \nevaluation benchmark designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts.  It was extracted from the Centers for Medicare & Medicaid Services' \nInternational Classification of Diseases, Tenth Revision, Clinical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding.","first_N":5,"first_N_keywords":["question-answering","FMA","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AlpacaX-Cleaned","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SullyGreene/AlpacaX-Cleaned","creator_name":"SullyGreene","creator_url":"https://huggingface.co/SullyGreene","description":"\n  \n\n\n\n\t\n\t\t\n\t\tüìö AlpacaX Dataset Documentation\n\t\n\nThe AlpacaX dataset is crafted to enhance AI models with structured, contextually rich, and logically sequenced examples. Designed for integration with TinyAGI, AlpacaX employs an advanced variant of the Alpaca training methodology, making it ideal for models that require detailed instruction-following and multi-step reasoning. This dataset is well-suited for fine-tuning language models to handle complex tasks with clarity and structured‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SullyGreene/AlpacaX-Cleaned.","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"alpaca-bulgarian-jokes-multilingual-prompts","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes-multilingual-prompts","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"\n\t\n\t\t\n\t\tBulgarian Jokes Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Bulgarian Jokes Dataset is a collection of Bulgarian-language jokes gathered and prepared for use in training and fine-tuning natural language processing (NLP) models. This dataset is designed to help researchers and developers build models capable of understanding and generating humorous content in Bulgarian.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is structured in a format suitable for NLP training and fine-tuning tasks, such as the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes-multilingual-prompts.","first_N":5,"first_N_keywords":["text-generation","Bulgarian","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"dali","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liwei1987cn/dali","creator_name":"Levi li","creator_url":"https://huggingface.co/liwei1987cn","description":"\n\t\n\t\t\n\t\tÂ§ßÊùéËÄÅÂ∏àÈóÆÁ≠îÊï∞ÊçÆÈõÜ\n\t\n\nËøô‰∏™Êï∞ÊçÆÈõÜÂåÖÂê´Â§ßÊùéËÄÅÂ∏àÁöÑÈóÆÁ≠îÂØπËØù,Áî®‰∫éËÆ≠ÁªÉÂØπËØùÊ®°Âûã„ÄÇ\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜÊèèËø∞\n\t\n\n\nÊ†ºÂºè: JSONL\nÂ≠óÊÆµ: \ninstruction: Âõ∫ÂÆöÂÄº\"ËØ∑Â§ßÊùéËÄÅÂ∏àÂõûÁ≠î\"\ninput: ÊèêÈóÆÂÜÖÂÆπ \noutput: Â§ßÊùéËÄÅÂ∏àÁöÑÂõûÁ≠î\n\n\nÊï∞ÊçÆÈáè: xxxÊù°ÂØπËØùÊï∞ÊçÆ\n\n\n\t\n\t\t\n\t\t‰ΩøÁî®Á§∫‰æã\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"your-username/dataset-name\")\n\n\n\t\n\t\t\n\t\tËÆ∏ÂèØËØÅ\n\t\n\nApache 2.0\n","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-TR-20k","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\n\t\n\t\t\n\t\tORPO-DPO-Mix-TR-20k\n\t\n\n\nThis repository contains a Turkish translation of 20k records from the mlabonne/orpo-dpo-mix-40k dataset. The translation was carried out using the gemini-1.5-flash-002 model, chosen for its 1M token context size and overall accurate Turkish responses.\n\n\t\n\t\t\n\t\n\t\n\t\tTranslation Process\n\t\n\nThe translation process uses the LLM model with translation prompt and pydantic data validation to ensure accuracy. The complete translation pipeline is available in the GitHub‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k.","first_N":5,"first_N_keywords":["text2text-generation","question-answering","text-generation","Turkish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Atma3-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Atma3-Share-GPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3-Share-GPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"college_alpaca_dataset","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dapraws/college_alpaca_dataset","creator_name":"Muhammad Darrel Prawira","creator_url":"https://huggingface.co/dapraws","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the given article in 200 Words.\",\n\"input\": \"https://www.bbc.com/news/world-51461830\",\n\"output\": \"The recent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dapraws/college_alpaca_dataset.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita-reranked","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\n\t\n\t\t\n\t\tEvol DPO Ita Reranked\n\t\n\n\nA high-quality Italian preference dataset suitable for Direct Preference Optimization (DPO), ORPO, and other Preference Tuning algorithms.\n\n\t\n\t\t\n\t\n\t\n\t\tü•áü•à Reranking process\n\t\n\nThis work is based on efederici/evol-dpo-ita, a nice Italian preference dataset.\nThe original dataset includes prompts translated from the Evol-Instruct datasets, with responses generated using GPT-3.5-Turbo (rejected) and claude-3-opus-20240229 (chosen).\nChoosing the response from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked.","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\n\t\n\t\t\n\t\tOpen Image Preferences\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world, vibrant colors, 4K.\n      \n          \n              \n              Image 1\n          \n          \n              \n              Image 2\n          \n      \n  \n\n\n\n  \n      Prompt: 8-bit pixel art of a blue knight, green car, and glacier landscape in Norway, fantasy style, colorful and detailed.\n          \n              \n              Image 1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMLU-Alpaca","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for MMLU-Alpaca\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"reasoning-1-1k","keyword":"sft","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fluently-sets/reasoning-1-1k","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","description":"\n\t\n\t\t\n\t\tReasoning-1 1K\n\t\n\n\n\t\n\t\t\n\t\tShort about\n\t\n\nThis dataset will help in SFT training of LLM on the Alpaca format.\nThe goal of the dataset: to teach LLM to reason and analyze its mistakes using SFT training.\nThe size of 1.15K is quite small, so for effective training on SFTTrainer set 4-6 epochs instead of 1-3.\nMade by Fluently Team (@ehristoforu) using distilabel with loveü•∞\n\n\t\n\t\t\n\t\n\t\n\t\tDataset structure\n\t\n\nThis subset can be loaded as:\nfrom datasets import load_dataset\n\nds =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/reasoning-1-1k.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"PersianSyntheticQA","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ParsBench/PersianSyntheticQA","creator_name":"ParsBench","creator_url":"https://huggingface.co/ParsBench","description":"\n\t\n\t\t\n\t\tPersian Synthetic QA Dataset\n\t\n\nPersian Synthetic QA is a dataset containing 100,000 synthetic questions and answers in Persian, generated using GPT-4o. The dataset is structured as conversations between a user and an assistant, with 2,000 records for each of the 50 different topics. Each conversation consists of messages with two distinct roles: \"user\" messages containing questions in Persian, and \"assistant\" messages containing the corresponding answers. The dataset is designed for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ParsBench/PersianSyntheticQA.","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LongWriter-6k","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongWriter-6k","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","description":"\n\t\n\t\t\n\t\tLongWriter-6k\n\t\n\n\n  ü§ó [LongWriter Dataset]  ‚Ä¢ üíª [Github Repo] ‚Ä¢ üìÉ [LongWriter Paper] \n\n\nLongWriter-6k dataset contains 6,000 SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese). The data can support training LLMs to extend their maximum output window size to 10,000+ words.\n\n\t\n\t\t\n\t\n\t\n\t\tAll Models\n\t\n\nWe open-sourced the following list of models trained on LongWriter-6k:\n\n\t\n\t\t\nModel\nHuggingface Repo\nDescription\n\n\n\t\t\nLongWriter-glm4-9b\nü§ó‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongWriter-6k.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-indonesian","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian","creator_name":"Ilham Fadhil","creator_url":"https://huggingface.co/ilhamfadheel","description":"\n\t\n\t\t\n\t\tü¶ôüõÅ Cleaned Alpaca Dataset (INDONESIAN)\n\t\n\nWelcome to the Cleaned Alpaca Dataset repository! This repository hosts a cleaned and curated version of a dataset used to train the Alpaca LLM (Large Language Model). The original dataset had several issues that are addressed in this cleaned version.\nOn April 8, 2023 the remaining uncurated instructions (~50,000) were replaced with data from the GPT-4-LLM dataset. Curation of the incoming GPT-4 data is ongoing.\n\nA 7b Lora model (trained on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian.","first_N":5,"first_N_keywords":["text-generation","Indonesian","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_preferences","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/truthful_qa_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"wassname/truthful_qa_preferences dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","text-generation","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"genies_preferences","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/genies_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"\n\t\n\t\t\n\t\tDataset Card for \"genie_dpo\"\n\t\n\nA conversion of the distribution from GENIES to open_pref_eval format.\nConversion code\n","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Supernova","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Supernova","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Supernova is a dataset containing general synthetic chat data from the best available open-source models.\nThe 2024-09-27 version contains:\n\n178.2k rows of synthetic chat responses generated using Llama 3.1 405b Instruct.\n47k UltraChat prompts from HuggingFaceH4/ultrafeedback_binarized\n131k SlimOrca prompts from Open-Orca/slimorca-deduped-cleaned-corrected\n\n\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"univ_exams_finnish","keyword":"fine-tuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/readd/univ_exams_finnish","creator_name":"Perttu Isotalo","creator_url":"https://huggingface.co/readd","description":"readd/univ_exams_finnish dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["Finnish","cc-by-sa-4.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Aloe-Beta-General-Collection","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-General-Collection","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tAloe-Beta-Medical-Collection\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated general datasets used to fine-tune Aloe-Beta.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nWe curated data from many publicly available general instruction tuning data sources (QA format). It consists of 400k instructions including:\n\nCoding, math, data analysis, STEM, etc.\n\nFunction calling\n\nCreative writing, advice seeking‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-General-Collection.","first_N":5,"first_N_keywords":["question-answering","summarization","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Leopard-Instruct","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wyu1/Leopard-Instruct","creator_name":"Wenhao Yu","creator_url":"https://huggingface.co/wyu1","description":"\n\t\n\t\t\n\t\tLeopard-Instruct\n\t\n\nPaper | Github | Models-LLaVA | Models-Idefics2\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nLeopard-Instruct is a large instruction-tuning dataset, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. It's been used to train Leopard-LLaVA [checkpoint]  and Leopard-Idefics2 [checkpoint].\n\n\t\n\t\t\n\t\tLoading dataset\n\t\n\n\nto load the dataset without automatically downloading and process the images (Please run the following codes with datasets==2.18.0)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wyu1/Leopard-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-72k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-72k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-72k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-12k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-12k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-12k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"mauxi-mix-persian","keyword":"sft","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxi-mix-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüó£Ô∏è MauxiMix: High-Quality Persian Conversations Dataset üáÆüá∑\n\t\n\n\n\t\n\t\t\n\t\tüìù Description\n\t\n\nMauxiMix is a carefully curated dataset of 1,000 high-quality Persian conversations, translated from the SmolTalk dataset using advanced language models. This dataset is specifically designed for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques, contributing to the development of open-source Persian language models.\nüöß Work in Progress: Expanding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-mix-persian.","first_N":5,"first_N_keywords":["translation","text-generation","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mauxi-mix-persian","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxi-mix-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüó£Ô∏è MauxiMix: High-Quality Persian Conversations Dataset üáÆüá∑\n\t\n\n\n\t\n\t\t\n\t\tüìù Description\n\t\n\nMauxiMix is a carefully curated dataset of 1,000 high-quality Persian conversations, translated from the SmolTalk dataset using advanced language models. This dataset is specifically designed for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques, contributing to the development of open-source Persian language models.\nüöß Work in Progress: Expanding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-mix-persian.","first_N":5,"first_N_keywords":["translation","text-generation","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"USCode-QAPairs-Finetuning","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning","creator_name":"Archit Rastogi ","creator_url":"https://huggingface.co/ArchitRastogi","description":"\n\t\n\t\t\n\t\tUSCode-QueryPairs Dataset\n\t\n\nThis dataset contains query-answer pairs curated from the United States Code, suitable for fine-tuning any embedding model. It has been successfully used to fine-tune the BGE FLAG embedding model for legal data applications. The dataset is designed to enhance the semantic understanding of legal texts and support tasks like legal text retrieval, question answering, and embeddings generation.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nSource: United States Code‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning.","first_N":5,"first_N_keywords":["text-retrieval","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\nThis dataset focuses on challenging multi-turn conversations and contains:\n\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"mauxi-COT-Persian","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxi-COT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüß† mauxi-COT-Persian Dataset\n\t\n\n\nExploring Persian Chain-of-Thought Reasoning with DeepSeek-R1, brought to you by Mauxi AI Platform\n\n\n\t\n\t\t\n\t\tüåü Overview\n\t\n\nmauxi-COT-Persian is a community-driven dataset that explores the capabilities of advanced language models in generating Persian Chain-of-Thought (CoT) reasoning. The dataset is actively growing with new high-quality, human-validated entries being added regularly. I am personally working on expanding this dataset with rigorously‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-COT-Persian.","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"PFT-MME","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/PFT-MME","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\n\t\n\t\t\n\t\tPFT-MME: Preference Finetuning Tally-Multi-Model Evaluation Dataset\n\t\n\n\nThe Preference Finetuning Tally-Multi-Model Evaluation (PFT-MME) dataset is meticulously curated by aggregating responses (n=6) from multiple models across (relatively simple) general task prompts. \nThese responses undergo evaluation by a panel (n=3) of evaluator models, assigning scores to each answer. \nThrough a tallied voting mechanism, average scores are calculated to identify the \"worst\" and \"best\" answers‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/PFT-MME.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-alignment-likert-scoring","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Prompt Alignment Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~6000 human evaluators were asked to evaluate AI-generated videos based on how well the generated video matches the prompt. The specific question‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring.","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-physics-likert-scoring","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-physics-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Physics Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~6000 human evaluators were asked to rate AI-generated videos based on if gravity and colisions make sense, without seeing the prompts used to generate them.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-physics-likert-scoring.","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"cheapvs","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cheapvs/cheapvs","creator_name":"cheapvs","creator_url":"https://huggingface.co/cheapvs","description":"cheapvs/cheapvs dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["token-classification","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-runway-alpha","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Runway Alpha Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~30'000 human annotations were collected to evaluate Runway's Alpha video generation model on our benchmark. The up to date benchmark can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-time-flow","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Time flow Annotation Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~3700 human evaluators were asked to evaluate AI-generated videos based on how time flows in the video. The specific question posed was: \"How‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow.","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Sentiment2Emoji","keyword":"finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MLap/Sentiment2Emoji","creator_name":"aman prakash","creator_url":"https://huggingface.co/MLap","description":"MLap/Sentiment2Emoji dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text2text-generation","English","cc-by-sa-4.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\n\t\n\t\t\n\t\tThe Degeneration of the Nation Multilingual Dataset\n\t\n\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-style-likert-scoring","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-style-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Preference Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~6000 human evaluators were asked to rate AI-generated videos based on their visual appeal, without seeing the prompts used to generate them. The specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-style-likert-scoring.","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"english-to-colloquial-tamil","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jarvisvasu/english-to-colloquial-tamil","creator_name":"Vasanth Kumar","creator_url":"https://huggingface.co/jarvisvasu","description":"\n\t\n\t\t\n\t\tEnglish to Colloquial Tamil\n\t\n\n\"instruction\":\"Translate provided English text into colloquial Tamil.\"\n\"input\": \"Their players played well.\"\n\"output\": \"‡ÆÖ‡Æµ‡Æô‡Øç‡Æï players ‡Æ®‡Æ≤‡Øç‡Æ≤‡Ææ ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æ£‡Øç‡Æü‡Ææ‡Æô‡Øç‡Æï.\"\n\n","first_N":5,"first_N_keywords":["translation","text-generation","text2text-generation","Tamil","English"],"keywords_longer_than_N":true},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tDeepthink Reasoning Demo\n\t\n\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"FineCorpus-WorkoutExercise","keyword":"fine-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/padilfm/FineCorpus-WorkoutExercise","creator_name":"Widi Fadhil","creator_url":"https://huggingface.co/padilfm","description":"\n\t\n\t\t\n\t\tFineCorpus-WorkoutExercise\n\t\n\nThis dataset contains structured workout exercise prompts for fine-tuning LLMs. \n\n\t\n\t\t\n\t\tStructure:\n\t\n\n\nconversations: Contains multi-turn dialogue pairs.\nsource: Indicates whether the data is from reasoning (Human) or generated by an AI model (LLM).\ncategory: Categorizes data into Q&A, Explain, Describe, Translate.\n\n\n\t\n\t\t\n\t\tUsage:\n\t\n\nTo use this dataset:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"padiflm/FineCorpus-WorkoutExercise\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/padilfm/FineCorpus-WorkoutExercise.","first_N":5,"first_N_keywords":["text-generation","Indonesian","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"WangchanThaiInstruct_Multi-turn_Conversation_Dataset","keyword":"instruction-finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThaiSyntheticQA/WangchanThaiInstruct_Multi-turn_Conversation_Dataset","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","description":"\n\t\n\t\t\n\t\tWangchanThaiInstruct Multi-turn Conversation Dataset\n\t\n\nWe create a Thai multi-turn conversation dataset from airesearch/WangchanThaiInstruct (Batch 1) by LLM. It was created from synthetic method using open source LLM in Thai language.\n\n\t\n\t\t\n\t\tCitation\n\t\n\n\nThammaleelakul, S., & Phatthiyaphaibun, W. (2024). WangchanThaiInstruct Multi-turn Conversation Dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.13132633\n\nor BibTeX\n@dataset{thammaleelakul_2024_13132633,\n  author       =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/WangchanThaiInstruct_Multi-turn_Conversation_Dataset.","first_N":5,"first_N_keywords":["text-generation","Thai","cc-by-sa-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Test2","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WhiteHunter111/Test2","creator_name":"Thomas Flato","creator_url":"https://huggingface.co/WhiteHunter111","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WhiteHunter111/Test2.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"argilla-ultrafeedback-binarized-preferences-cleaned","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/csarron/argilla-ultrafeedback-binarized-preferences-cleaned","creator_name":"Qingqing Cao","creator_url":"https://huggingface.co/csarron","description":"\n\t\n\t\t\n\t\tUltraFeedback (Cleaned)\n\t\n\nThis dataset combines the train split of argilla/ultrafeedback-binarized-preferences-cleaned,\nand test split of HuggingFaceH4/ultrafeedback_binarized.\n","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca_data_clean","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aaaalon/alpaca_data_clean","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca_data_clean.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","üá∫üá∏ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"llama3weitiao","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZHEZIXI/llama3weitiao","creator_name":"LIUJUN","creator_url":"https://huggingface.co/ZHEZIXI","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZHEZIXI/llama3weitiao.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-SHORT","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\n\t\n\t\t\n\t\tSHORTENED - ORPO-DPO-mix-40k v1.1\n\t\n\nFILTERED to remove rows with chosen responses longer than 2k characters or with a final assistant message longer than 500 characters.\n\nThe original dataset card follows below.\n\n\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.1\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleand","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aaaalon/alpaca-cleand","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca-cleand.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"MainData","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bufanlin/MainData","creator_name":"bufanlin","creator_url":"https://huggingface.co/bufanlin","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\nÊú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bufanlin/MainData.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_cthulhu_full","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theprint/alpaca_cthulhu_full","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","description":"Based on the yahma/alpaca-cleaned data set.\nEvery answer in the original data set was rewritten, as if the answer was given by a dedicated, high ranking cultist in the Cult of Cthulhu. The instructions were to keep the replies factually the same, but to spice things up with references to the Cthulhu Mythos in its various forms.\nThis was done as part of a learning experience, but I am making the data set available for others to play with as well.\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ultrafrench","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/ultrafrench","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for ultrafrench\n\t\n\nThis dataset offers a french translation of the small sample of instructions from HuggingFaceH4/ultrachat_200k translated in french. The generations were made with Mistral large to make the output open-source\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ultrafrench","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/ultrafrench","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for ultrafrench\n\t\n\nThis dataset offers a french translation of the small sample of instructions from HuggingFaceH4/ultrachat_200k translated in french. The generations were made with Mistral large to make the output open-source\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-fr","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TPM-28/alpaca-cleaned-fr","creator_name":"TPM-28","creator_url":"https://huggingface.co/TPM-28","description":"TPM-28/alpaca-cleaned-fr dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca_astronomy_bg","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca_astronomy_bg","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"vislupus/alpaca_astronomy_bg dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Bulgarian","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned","creator_name":"Farouk","creator_url":"https://huggingface.co/pharaouk","description":"\n\t\n\t\t\n\t\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\n\n\t\n\t\n\t\n\t\tDifferences with argilla/ultrafeedback-binarized-preferences‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LLama3Flashcard","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Michito97/LLama3Flashcard","creator_name":"Adrian De La Cruz","creator_url":"https://huggingface.co/Michito97","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nEste dataset contiene pares de instrucciones, entradas y salidas dise√±adas para generar tarjetas de memoria (flashcards) educativas. Es ideal para modelos de generaci√≥n de texto enfocados en la creaci√≥n de contenidos educativos.\n\n\t\n\t\t\n\t\tColumnas\n\t\n\n\ninstruction: La instrucci√≥n dada al‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Michito97/LLama3Flashcard.","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"SEA_data","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ECNU-SEA/SEA_data","creator_name":"ECNU-SEA","creator_url":"https://huggingface.co/ECNU-SEA","description":"\n\t\n\t\t\n\t\tAutomated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis\n\t\n\nPaper Link: https://arxiv.org/abs/2407.12857\nProject Page: https://ecnu-sea.github.io/\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nEach dataset contains four types of files as follows:\n\npaper_raw_pdf: Original paper in PDF format.\npaper_nougat_mmd: The mmd files after parsed by Nougat.\nreview_raw_txt: Crawled raw review text.\nreview_json: The processed review JSON file, including ‚ÄúDecision‚Äù, ‚ÄúMeta Review‚Äù, and for each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ECNU-SEA/SEA_data.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","text","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"DecipherPref","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huuuyeah/DecipherPref","creator_name":"Yebowen Hu","creator_url":"https://huggingface.co/huuuyeah","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nHuman preference judgments are pivotal in guiding large language models (LLMs) to produce outputs that align with human values. Human evaluations are also used in summarization tasks to compare outputs from various systems, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise or k-wise comparisons. The collective impact and relative importance of factors such as output length, informativeness‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huuuyeah/DecipherPref.","first_N":5,"first_N_keywords":["summarization","text-classification","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"AmsterdamBalancedFirst200Tokens","keyword":"fine-tuning","license":"European Union Public License 1.1","license_url":"https://choosealicense.com/licenses/eupl-1.1/","language":"en","dataset_url":"https://huggingface.co/datasets/FemkeBakker/AmsterdamBalancedFirst200Tokens","creator_name":"Femke Bakker","creator_url":"https://huggingface.co/FemkeBakker","description":"This dataset is a modified version of the AmsterdamDocClassificationDataset. \nThe original dataset consists of Dutch Raadsinformatie documents from the Municipality of Amsterdam, which were published in accordance with the Open Government Act (Woo). \nIn this modified version, the documents are truncated to the first 200 tokens each. \nThis dataset is used to fine-tune large language models (LLMs) for the Assessing LLMs for Document Classification project.\nThe documents are formatted into a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FemkeBakker/AmsterdamBalancedFirst200Tokens.","first_N":5,"first_N_keywords":["Dutch","eupl-1.1","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Proyecto","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto","creator_name":"Facundo","creator_url":"https://huggingface.co/Facundo-DiazPWT","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","üá∫üá∏ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"AWE","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rishiraj/AWE","creator_name":"Rishiraj Acharya","creator_url":"https://huggingface.co/rishiraj","description":"rishiraj/AWE dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"CTI_0.1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AshishCTI/CTI_0.1","creator_name":"ad","creator_url":"https://huggingface.co/AshishCTI","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AshishCTI/CTI_0.1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","üá∫üá∏ Region: US","instruction-finetuning"],"keywords_longer_than_N":false},
	{"name":"ictisgpt","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alexbs/ictisgpt","creator_name":"Alex B","creator_url":"https://huggingface.co/alexbs","description":"\n\t\n\t\t\n\t\tDataset Card for ICTIS GPT dataset\n\t\n\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"persian-alpaca-deep-clean","keyword":"instruction tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/myrkur/persian-alpaca-deep-clean","creator_name":"Amir Masoud Ahmadi","creator_url":"https://huggingface.co/myrkur","description":"\n\t\n\t\t\n\t\tPersian Alpaca Deep Clean\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Persian Alpaca Dataset is a collection of finely cleaned Persian language records derived from various sources, primarily the Bactrian, PN-Summary (summarization), and PEYMA (Named Entity Recognition) datasets. The dataset comprises approximately 68,279 records after rigorous cleaning processes, including character normalization, removal of Arabic letters, elimination of sentences with high word repetition, removal of words with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/myrkur/persian-alpaca-deep-clean.","first_N":5,"first_N_keywords":["text-generation","summarization","token-classification","Persian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Capybara-Preferences","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/Capybara-Preferences","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for Capybara-Preferences\n\t\n\nThis dataset has been created with distilabel.\n\n    \n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is built on top of LDJnr/Capybara, in order to generate a preference\ndataset out of an instruction-following dataset. This is done by keeping the conversations in the column conversation but splitting\nthe last assistant turn from it, so that the conversation contains all the turns up until the last user's turn, so that it can be reused‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Capybara-Preferences.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"aya_dataset_dutch_example","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dataset_dutch_example","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"data-is-better-together/aya_dataset_dutch_example dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Capybara-Preferences-Filtered","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/Capybara-Preferences-Filtered","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for Capybara-Preferences-Filtered\n\t\n\nThis dataset has been created with distilabel, plus some extra post-processing steps described below.\n\n    \n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\n\nRemove responses from the assistant, not only in the last turn, but also in intermediate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Capybara-Preferences-Filtered.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"medical","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bohu/medical","creator_name":"tang","creator_url":"https://huggingface.co/bohu","description":"From https://huggingface.co/datasets/shibing624/medical\n","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"ThaiQA-v1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","description":"\n\t\n\t\t\n\t\tThaiQA v1\n\t\n\nThaiQA v1 is a Thai Synthetic QA dataset. It was created from synthetic method using open source LLM in Thai language.\nWe used Nvidia Nemotron 4 (340B) to create this dataset.\nTopics:\nTechnology and Gadgets 100\nTravel and Tourism 91\nFood and Cooking 99\nSports and Fitness 50\nArts and Entertainment 24\nHome and Garden 72\nFashion and Beauty 99\nScience and Nature 100\nHistory and Culture 91\nEducation and Learning 99\nPets and Animals 83\nRelationships and Family 78\nPersonal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1.","first_N":5,"first_N_keywords":["text-generation","question-answering","Thai","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"epic-novels","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Eldritch/epic-novels","creator_name":"Manish Singh Parihar","creator_url":"https://huggingface.co/Eldritch","description":"\n\t\n\t\t\n\t\tNovel Continuation Dataset\n\t\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains over 10,000 records of novel text, specifically curated for the task of text continuation. It consists of three columns:\n\nInstruction: A prompt or instruction guiding the continuation of the novel.\nInput: An excerpt from the novel serving as the starting point for continuation.\nOutput: The continuation of the novel, following the input text, as per the instruction.\n\nThe dataset has been used to fine-tune the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Eldritch/epic-novels.","first_N":5,"first_N_keywords":["text2text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"LONGCOT-Alpaca","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for LONGCOT-Alpaca\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"text-2-image-Rich-Human-Feedback","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\n\n\nBuilding upon Google's research Rich Human Feedback for Text-to-Image Generation we have collected over 1.5 million responses from 152'684 individual humans using Rapidata via the Python API. Collection took roughly 5 days. \nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nWe asked humans to evaluate AI-generated images in style, coherence and prompt alignment. For images that contained flaws, participants were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback.","first_N":5,"first_N_keywords":["text-to-image","text-classification","image-classification","image-to-text","image-segmentation"],"keywords_longer_than_N":true},
	{"name":"LongCite-45k","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongCite-45k","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","description":"\n\t\n\t\t\n\t\tLongCite-45k\n\t\n\n\n  ü§ó [LongCite Dataset]  ‚Ä¢ üíª [Github Repo] ‚Ä¢ üìÉ [LongCite Paper] \n\n\nLongCite-45k dataset contains 44,600 long-context QA instances paired with sentence-level citations (both English and Chinese, up to 128,000 words). The data can support training long-context LLMs to generate response and fine-grained citations within a single output.\n\n\t\n\t\t\n\t\n\t\n\t\tData Example\n\t\n\nEach instance in LongCite-45k consists of an instruction, a long context (divided into sentences), a user‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongCite-45k.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"chat","keyword":"fine-tune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neverland-th/chat","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","description":"neverland-th/chat dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"turkish_llm_finetune_dataset_4_topics","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/barathanasln/turkish_llm_finetune_dataset_4_topics","creator_name":"Barathan Aslan","creator_url":"https://huggingface.co/barathanasln","description":"\n\t\n\t\t\n\t\tTurkish LLM Finetune Dataset - 4 Topics\n\t\n\nThis dataset is designed to fine-tune the T3 AI Turkish LLM. It was created by Barathan Aslan, √ñmer Faruk √áelik, and Batuhan Kalem for the T3 AI Hackathon. The dataset focuses on four distinct topics: Agriculture, Sustainability, Turkish Education Sytem, and Turkish Law System.\n\n\t\n\t\t\n\t\tContributors\n\t\n\n\nBarathan Aslan (https://huggingface.co/barathanasln)\nBatuhan Kalem(https://huggingface.co/Pancarsuyu)\n√ñmer Faruk √áelik‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/barathanasln/turkish_llm_finetune_dataset_4_topics.","first_N":5,"first_N_keywords":["table-question-answering","question-answering","Turkish","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LongWriter-6k-English","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/themex1380/LongWriter-6k-English","creator_name":"themex","creator_url":"https://huggingface.co/themex1380","description":"\n\t\n\t\t\n\t\tLongWriter-6k-English\n\t\n\nLongWriter-6k-English is a filtered version of the LongWriter-6k dataset, containing only the English-language samples. This dataset includes 2,299 instances of long-form text, ranging from 2,000 to 32,000 words, designed to train large language models (LLMs) to handle extended output contexts.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nLanguages: English\nData Size: 2,299 samples\nOutput Length: 2,000 to 32,000 words per sample\n\n\n\t\n\t\t\n\t\tSource\n\t\n\nThis dataset is derived from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/themex1380/LongWriter-6k-English.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"amazon-ml","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vinuuu/amazon-ml","creator_name":"Vinayak Goyal","creator_url":"https://huggingface.co/Vinuuu","description":"Vinuuu/amazon-ml dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"synthetic-1","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pepistrafforello/synthetic-1","creator_name":"Giuseppe Strafforello","creator_url":"https://huggingface.co/pepistrafforello","description":"Private¬†Data for Fine Tuning LLM [JSON dataset]\n\nThis is a Dataset Repository of Private¬†Data for Fine Tuning LLM\nA dataset containing synthetic data that does not exist elsewhere, in the Alpaca format; its purpose is evaluating the effectiveness of fine tuning on private data.\nView Medium Post\n\n\t\n\t\t\n\t\tLicense\n\t\n\nCC-0\n","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"ATC-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for ATC-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ATCgpt-Fixed","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for ATCgpt-Fixed\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Atcgpt-Fixed2","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atcgpt-Fixed2\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701-instruct","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"\n\t\n\t\t\n\t\tLOGIC-701 (instruct)\n\t\n\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\n","first_N":5,"first_N_keywords":["question-answering","Russian","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"metallurgy-qa","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AbdulrhmanEldeeb/metallurgy-qa","creator_name":"Eldeeb","creator_url":"https://huggingface.co/AbdulrhmanEldeeb","description":"\n\t\n\t\t\n\t\tMetallurgy and Materials Science Knowledge Extraction Dataset\n\t\n\nThis repository contains a dataset generated from parsed books related to various aspects of metallurgy, materials science, and engineering. The dataset is designed for fine-tuning Large Language Models (LLMs) for Question-Answering (QA) tasks in the domain of metallurgy and materials science.\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe dataset includes content derived from technical books in the field of metallurgy and materials‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AbdulrhmanEldeeb/metallurgy-qa.","first_N":5,"first_N_keywords":["text2text-generation","question-answering","closed-domain-qa","closed-book-qa","machine-generated"],"keywords_longer_than_N":true},
	{"name":"1M-OpenOrca_be","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be","creator_name":"Artsem Holub","creator_url":"https://huggingface.co/WiNE-iNEFF","description":"En/Be\nüêã The Belarusian OpenOrca Dataset! üêã\n\n\n\nBelarusian OpenOrca dataset - is rich collection of augmented FLAN data aligns, that translated in belarusian language.\nThat dataset should help training LLM in belarusian language and should help on other NLP tasks.\nThis dataset have 2 version:\n\n~1M GPT-4 completions (Now translating)\n~3.2M GPT-3.5 completions (Can be translated in future)\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe fields are:\n\n'id', a unique numbered identifier which includes one of 'niv'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Spurline","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\nThe 2024-10-30 version contains:\n\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-36k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"MDCure-36k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\n\t\n\t\t\n\t\tMDCure-36k\n\t\n\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k.","first_N":5,"first_N_keywords":["question-answering","summarization","text2text-generation","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"alpaca-bulgarian-jokes","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"\n\t\n\t\t\n\t\tBulgarian Jokes Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Bulgarian Jokes Dataset is a collection of Bulgarian-language jokes gathered and prepared for use in training and fine-tuning natural language processing (NLP) models. This dataset is designed to help researchers and developers build models capable of understanding and generating humorous content in Bulgarian.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is structured in a format suitable for NLP training and fine-tuning tasks, such as the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes.","first_N":5,"first_N_keywords":["text-generation","Bulgarian","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Electrohydrodynamics","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Taylor658/Electrohydrodynamics","creator_name":"atayloraerospace","creator_url":"https://huggingface.co/Taylor658","description":"\n\t\n\t\t\n\t\tElectrohydrodynamics in Hall Effect Thrusters Dataset for Mistral-Large-Instruct-2411 Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of 6,000 high fidelity training instances tailored for fine-tuning the Mistral-Large-Instruct-2411 foundation model. It captures theoretical, computational, and experimental aspects of electrohydrodynamics in Hall Effect Thrusters. \n\n\t\n\t\t\n\t\tKey Features:\n\t\n\n\nMultimodal elements: Includes LaTeX equations, code snippets, textual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Taylor658/Electrohydrodynamics.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1-more-results","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\n\n\nWe wanted to contribute to the challenge posed by the data-is-better-together community (description below). We collected 170'000 preferences using our API from people all around the world in rougly 3 days (docs.rapidata.ai):\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for image-preferences-results Original\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Preference Dataset\n\t\n\n\n\n\n\n\n\n\nThis dataset was collected in ~12 hours using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nThe data collected in this dataset informs our text-2-video model benchmark. We just started so currently only two models are represented in this set:\n\nSora\nHunyouan\nPika 2.0\nRunway ML Alpha\nLuma Ray 2\n\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences.","first_N":5,"first_N_keywords":["text-to-video","video-classification","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"RobustFT","keyword":"sft","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/luojunyu/RobustFT","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","description":"\n\t\n\t\t\n\t\tRobustFT Dataset\n\t\n\nThis dataset is part of the RobustFT project: Robust Supervised Fine-tuning for Large Language Models under Noisy Response. The dataset contains various test cases with different noise ratios for training and evaluating robust fine-tuning approaches.\nOur paper: https://huggingface.co/papers/2412.14922\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nRobustFT/\n‚îú‚îÄ‚îÄ arc/\n‚îÇ ‚îÇ‚îÄ‚îÄ noisy30.csv\n‚îÇ ‚îÇ‚îÄ‚îÄ noisy50.csv\n‚îÇ ‚îÇ‚îÄ‚îÄ noisy70.csv\n‚îÇ ‚îú‚îÄ‚îÄ labeled.csv\n‚îÇ ‚îî‚îÄ‚îÄ test.csv\n‚îú‚îÄ‚îÄ drop/\n‚îÇ ‚îÇ‚îÄ‚îÄ noisy30.csv\n‚îÇ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/RobustFT.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"KoMT-Bench","keyword":"instruction-following","license":"GNU Lesser General Public License v3.0","license_url":"https://choosealicense.com/licenses/lgpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench","creator_name":"LG AI Research","creator_url":"https://huggingface.co/LGAI-EXAONE","description":"\n\t\n\t\t\n\t\tKoMT-Bench\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe present KoMT-Bench, a benchmark designed to evaluate the capability of language models in following instructions in Korean.\nKoMT-Bench is an in-house dataset created by translating MT-Bench [1]  dataset into Korean and modifying some questions to reflect the characteristics and cultural nuances of the Korean language.\nAfter the initial translation and modification, we requested expert linguists to conduct a thorough review of our benchmark‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench.","first_N":5,"first_N_keywords":["question-answering","Korean","lgpl-3.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"SFinD-S","keyword":"fine-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tilmann-strative/SFinD-S","creator_name":"Tilmann Bruckhaus","creator_url":"https://huggingface.co/tilmann-strative","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis sample is part of the larger SFinD-S (Strative Financial Dataset - Synthetic), a comprehensive dataset designed for Retrieval-Augmented Generation (RAG) GenAI applications, Natural Language Processing (NLP), Large Language Models (LLM), and AI tasks in the financial domain. The full SFinD-S dataset contains over 20,000 records of realistic financial questions and verified answers, sourced from a wide variety of web content.\nIf you find this dataset useful or‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tilmann-strative/SFinD-S.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"OregonCoastin4K","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Overlaiai/OregonCoastin4K","creator_name":"Overlai.ai","creator_url":"https://huggingface.co/Overlaiai","description":"\n\n\t\n\t\t\n\t\tOREGON COAST IN 4K\n\t\n\n\n\n\"Oregon Coast in 4K\" is a fine tuning text-to-video dataset consisting of dynamic videos captured in 8K resolution on the DJI Inspire 3 and RED Weapon Helium.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nüé• Oversampled: Every clip is captured in stunning 8K resolution, delivering rich detail ideal for fine tuning scenic landscapes and ocean dynamics.\nüîÑ Parallax: Shot using DJI Inspire 3 featuring parallax effects that provide AI models with enhanced context on depth and movement‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Overlaiai/OregonCoastin4K.","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","< 1K","Tabular"],"keywords_longer_than_N":true},
	{"name":"Atma3.2-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma3.2-ShareGPT\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Atma4","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma4\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"godot-training","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ImJimmeh/godot-training","creator_name":"Jim","creator_url":"https://huggingface.co/ImJimmeh","description":"ImJimmeh/godot-training dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Atma4-Hindi","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\n\t\n\t\t\n\t\tDataset Card for Atma4-Hindi\n\t\n\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\n\nAn instruction that specifies the task\nAn optional input providing context\nA detailed output that addresses the instruction\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned_uz","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bizb0630/alpaca-cleaned_uz","creator_name":"Behruz Izbaev","creator_url":"https://huggingface.co/bizb0630","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a translation of the alpaca-cleaned dataset into Uzbek (Latin), using the GPT-4o mini API.\n","first_N":5,"first_N_keywords":["text-generation","Uzbek","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"thai-gov-procurement_regulation-17-amend-21","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/amornpan/thai-gov-procurement_regulation-17-amend-21","creator_name":"Amornpan Phornchaicharoen","creator_url":"https://huggingface.co/amornpan","description":"\n\t\n\t\t\n\t\tüáπüá≠ Dataset Card for Thai Government Procurement Dataset\n\t\n\n\n\t\n\t\t\n\t\t‚ÑπÔ∏è This dataset is optimized for procurement-related NLP tasks in Thai.\n\t\n\nThis dataset contains a collection of procurement regulations, instructions, and responses focused on public sector purchasing, contract management, and compliance with Thai government standards. It aims to support natural language processing tasks involving procurement assistance, such as chatbot development, procurement dialogue generation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/amornpan/thai-gov-procurement_regulation-17-amend-21.","first_N":5,"first_N_keywords":["question-answering","text-classification","Thai","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"mauxitalk-persian","keyword":"sft","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxitalk-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüó£Ô∏è MauxiTalk: High-Quality Persian Conversations Dataset üáÆüá∑\n\t\n\n\n\t\n\t\t\n\t\tüìù Description\n\t\n\nMauxiTalk is a high-quality dataset of 2,000+ Persian conversations, carefully translated from the SmolTalk dataset using state-of-the-art language models. This dataset is specifically curated for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n2,000 natural conversations in Persian\nDiverse topics including daily‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxitalk-persian.","first_N":5,"first_N_keywords":["text-generation","summarization","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mauxitalk-persian","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxitalk-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüó£Ô∏è MauxiTalk: High-Quality Persian Conversations Dataset üáÆüá∑\n\t\n\n\n\t\n\t\t\n\t\tüìù Description\n\t\n\nMauxiTalk is a high-quality dataset of 2,000+ Persian conversations, carefully translated from the SmolTalk dataset using state-of-the-art language models. This dataset is specifically curated for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\n2,000 natural conversations in Persian\nDiverse topics including daily‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxitalk-persian.","first_N":5,"first_N_keywords":["text-generation","summarization","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"wangchanx-seed-free-synthetic-instruct-thai-120k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k","creator_name":"VISTEC-depa AI Research Institute of Thailand","creator_url":"https://huggingface.co/airesearch","description":"\n\t\n\t\t\n\t\tDataset Card for WangchanX Seed-Free Synthetic Instruct Thai 120k\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains about 120k synthetic instruction-following samples in Thai, generated using a novel seed-free approach. It covers a wide range of domains derived from Wikipedia, including both general knowledge and Thai-specific cultural topics. The dataset is designed for instruction-tuning Thai language models to improve their ability to understand and generate Thai text in various‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k.","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","Thai","English"],"keywords_longer_than_N":true},
	{"name":"Kalo-Opus-Instruct-22k-Refusal-Murdered","keyword":"instruct","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewEden/Kalo-Opus-Instruct-22k-Refusal-Murdered","creator_name":"New Eden","creator_url":"https://huggingface.co/NewEden","description":"NewEden/Kalo-Opus-Instruct-22k-Refusal-Murdered dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["agpl-3.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"code-translation","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noxus09/code-translation","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Noxus09/code-translation.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"CRAFT-Summarization","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tCRAFT-Summarization\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated summarization data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization.","first_N":5,"first_N_keywords":["summarization","text-generation","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"italian-embedding-finetune-dataset","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ArchitRastogi/italian-embedding-finetune-dataset","creator_name":"Archit Rastogi ","creator_url":"https://huggingface.co/ArchitRastogi","description":"\n\t\n\t\t\n\t\tItalian-BERT-FineTuning-Embeddings\n\t\n\nThis repository contains a comprehensive dataset designed for fine-tuning BERT-based Italian embedding models. The dataset aims to enhance performance on tasks such as information retrieval, semantic search, and embeddings generation.\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset leverages the C4 dataset (Italian subset) and employs advanced techniques like sliding window segmentation and in-document sampling to create high-quality, diverse examples‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ArchitRastogi/italian-embedding-finetune-dataset.","first_N":5,"first_N_keywords":["text-classification","question-answering","Italian","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"RoboMatrix","keyword":"finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WayneMao/RoboMatrix","creator_name":"WayneMao","creator_url":"https://huggingface.co/WayneMao","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis is the fine-tuning dataset used in the paper RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World.\n\n\t\n\t\t\n\t\tSource\n\t\n\nProject Page: https://robo-matrix.github.io/Paper: https://arxiv.org/abs/2412.00171Code: https://github.com/WayneMao/RoboMatrixModel: \n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you find our work helpful, please cite us:\n@article{mao2024robomatrix,\n  title={RoboMatrix: A Skill-centric Hierarchical Framework for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WayneMao/RoboMatrix.","first_N":5,"first_N_keywords":["mit","1K - 10K","Datasets","Croissant","arxiv:2412.00171"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-alt","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-alt","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-alt dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"reflection-v1-ru_subset","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\td0rj/reflection-v1-ru_subset\n\t\n\nTranslated glaiveai/reflection-v1 dataset into Russian language using GPT-4o.\n\nAlmost all the rows of the dataset have been translated. I have removed those translations that do not match the original by the presence of the tags \"thinking\", \"reflection\" and \"output\". Mapping to the original dataset rows can be taken from the \"index\" column.\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata = datasets.load_dataset(\"d0rj/reflection-v1-ru_subset\")\nprint(data)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","translated","multilingual","glaiveai/reflection-v1"],"keywords_longer_than_N":true},
	{"name":"remix-run-v2-dataset","keyword":"fine-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sebastyijan/remix-run-v2-dataset","creator_name":"Niklas","creator_url":"https://huggingface.co/Sebastyijan","description":"\n\t\n\t\t\n\t\tRemix Run v2 Fine-tuning Dataset Overview\n\t\n\nThis dataset is derived from the official Remix Run v2 documentation and targets core concepts, best practices, and frequently asked questions surrounding Remix development workflows. Remix Run is a modern, full-stack web framework designed to enhance developer productivity by optimizing routing, form handling, and data-fetching mechanisms, while promoting progressive enhancement.\nThe dataset serves as a foundation for fine-tuning language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sebastyijan/remix-run-v2-dataset.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K<n<10K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"Celestia","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia is a dataset containing science-instruct data.\nThe 2024-10-30 version contains:\n\n126k rows of synthetic science-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Data-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-Data-ru\n\t\n\nTranslated lmms-lab/LLaVA-OneVision-Data dataset into Russian language using Google translate.\n\nAlmost all datasets have been translated, except for the following:\n[\"tallyqa(cauldron,llava_format)\", \"clevr(cauldron,llava_format)\", \"VisualWebInstruct(filtered)\", \"figureqa(cauldron,llava_format)\", \"magpie_pro(l3_80b_mt)\", \"magpie_pro(qwen2_72b_st)\", \"rendered_text(cauldron)\", \"ureader_ie\"]\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru.","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-to-text","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"app350_llama_format","keyword":"finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CodeHima/app350_llama_format","creator_name":"Himanshu Mohanty","creator_url":"https://huggingface.co/CodeHima","description":"\n\t\n\t\t\n\t\tAPP-350 Formatted Dataset for LLM Fine-tuning\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe APP-350 dataset consists of structured conversation pairs formatted for fine-tuning Large Language Models (LLMs) like LLaMA. This dataset includes questions and responses between users and an AI assistant. The dataset is particularly designed for privacy policy analysis and fairness evaluation, allowing models to learn from annotated interactions regarding privacy practices.\nThe conversations are organized‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CodeHima/app350_llama_format.","first_N":5,"first_N_keywords":["text-generation","text-classification","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"apigen-synth-trl","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/apigen-synth-trl","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\n\t\n\t\t\n\t\tDataset card\n\t\n\nThis dataset is a version of argilla/Synth-APIGen-v0.1 prepared for\nfine-tuning using trl. To generate it, the following script was run:\nfrom datasets import load_dataset\nfrom jinja2 import Template\n\nSYSTEM_PROMPT = \"\"\"\nYou are an expert in composing functions. You are given a question and a set of possible functions. \nBased on the question, you will need to make one or more function/tool calls to achieve the purpose. \nIf none of the functions can be used, point it out‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/apigen-synth-trl.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-45k","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-45k\n\t\n\nThis dataset is designed for DPO or ORPO training.\nThis dataset combines samples of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\nargilla/distilabel-math-preference-dpo: highly scored chosen‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-38k-balanced","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-38k\n\t\n\nThis dataset is intended for use with DPO or ORPO training. \nIt represents a balanced version of the llmat/dpo-orpo-mix-45k dataset, achieved through a clustering-based approach as outlined in this paper.\nThe dataset integrates high-quality samples from the following DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"tw-reasoning-instruct-50k","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k","creator_name":"Twinkle AI","creator_url":"https://huggingface.co/twinkle-ai","description":"\n\t\n\t\t\n\t\tDataset Card for tw-reasoning-instruct-50k\n\t\n\n\n\ntw-reasoning-instruct-50k ÊòØ‰∏ÄÂÄãÁ≤æÈÅ∏ÁöÑ ÁπÅÈ´î‰∏≠ÊñáÔºàÂè∞ÁÅ£Ôºâ Êé®ÁêÜË≥áÊñôÈõÜÔºåÊó®Âú®ÊèêÂçáË™ûË®ÄÊ®°ÂûãÊñºÈÄêÊ≠•ÈÇèËºØÊÄùËÄÉ„ÄÅËß£ÈáãÁîüÊàêËàáË™ûË®ÄÁêÜËß£Á≠â‰ªªÂãô‰∏≠ÁöÑË°®Áèæ„ÄÇË≥áÊñôÂÖßÂÆπÊ∂µËìãÊó•Â∏∏ÊÄùËæ®„ÄÅÊïôËÇ≤Â∞çË©±„ÄÅÊ≥ïÂæãÊé®ÁêÜÁ≠âÂ§öÂÖÉ‰∏ªÈ°åÔºå‰∏¶ÁµêÂêà„ÄåÊÄùËÄÉÊ≠•È©ü„ÄçËàá„ÄåÊúÄÁµÇÁ≠îÊ°à„ÄçÁöÑÁµêÊßãË®≠Ë®àÔºåÂºïÂ∞éÊ®°Âûã‰ª•Êõ¥Ê∏ÖÊô∞„ÄÅÊ¢ùÁêÜÂàÜÊòéÁöÑÊñπÂºèÈÄ≤Ë°åÊé®Ë´ñËàáÂõûÊáâÔºåÁâπÂà•Âº∑Ë™øÁ¨¶ÂêàÂè∞ÁÅ£Êú¨Âú∞Ë™ûË®ÄËàáÊñáÂåñËÉåÊôØÁöÑÊáâÁî®ÈúÄÊ±Ç„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\nÊú¨Ë≥áÊñôÈõÜÂ∞àÁÇ∫ÁôºÂ±ïÂÖ∑ÂÇôÂº∑Â§ßÊé®ÁêÜËÉΩÂäõÁöÑÁπÅÈ´î‰∏≠ÊñáÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLarge Reasoning Models, LRMÔºâÊâÄË®≠Ë®àÔºåÂÖßÂÆπÊ∑±Â∫¶ÁµêÂêàÂè∞ÁÅ£ÁöÑË™ûË®ÄËàáÊñáÂåñËÑàÁµ°„ÄÇÊØèÁ≠ÜË≥áÊñôÈÄöÂ∏∏ÂåÖÂê´‰ΩøÁî®ËÄÖÁöÑÊèêÂïè„ÄÅÊ®°ÂûãÁöÑÂõûÊáâÔºå‰ª•ÂèäÊ∏ÖÊ•öÁöÑÊé®ÁêÜÈÅéÁ®ã„ÄÇË≥áÊñôÈõÜË®≠Ë®àÁõÆÊ®ôÁÇ∫ÂüπÈ§äÊ®°ÂûãÂÖ∑ÂÇôÈ°û‰∫∫ÈÇèËºØÁöÑÈÄêÊ≠•ÊÄùËÄÉËàáËß£ÈáãËÉΩÂäõ„ÄÇ\nÊ≠§Ë≥áÊñôÈõÜÈÅ©Áî®ÊñºË®ìÁ∑¥ËàáË©ï‰º∞‰ª•‰∏ã‰ªªÂãôÔºö\n\nÂè∞ÁÅ£Á§æÊúÉÁöÑÊó•Â∏∏Êé®ÁêÜ\nÊïôËÇ≤ÊÄßÂ∞çË©±\n‰ª•Ëß£ÈáãÁÇ∫Â∞éÂêëÁöÑÁîüÊàê‰ªªÂãô‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lima_dirty_tr","keyword":"sft","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emre/lima_dirty_tr","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","description":"\n\t\n\t\t\n\t\tLima Turkish Translated & Engineered for Alignment\n\t\n\n\n\t\n\t\t\n\t\tGiri≈ü\n\t\n\nBu veri seti, LIMA (Less Is More for Alignment) [^1] √ßalƒ±≈ümasƒ±ndan ilham alƒ±narak olu≈üturulmu≈ü, orijinal LIMA veri setinin T√ºrk√ße'ye √ßevrilmi≈ü ve hizalama (alignment) teknikleri i√ßin √∂zel olarak yapƒ±landƒ±rƒ±lmƒ±≈ü bir versiyonudur. LIMA'nƒ±n temel felsefesi, az sayƒ±da ancak y√ºksek kaliteli √∂rnekle dil modellerini etkili bir ≈üekilde hizalayabilmektir. Bu √ßalƒ±≈üma, bu felsefeyi T√ºrk√ße dil modelleri ekosistemine ta≈üƒ±mayƒ±‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/emre/lima_dirty_tr.","first_N":5,"first_N_keywords":["text-generation","Turkish","English","afl-3.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-pika2.2","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-pika2.2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Pika 2.2 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~756k human responses from ~29k human annotators were collected to evaluate Pika 2.2 video generation model on our benchmark. This dataset was collected in ~1 day total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-pika2.2.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"SFT_54k_reasoning","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/SFT_54k_reasoning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/SFT_54k_reasoning is a processed version of the XuHu6736/s1_54k_filter_with_isreasoning dataset, specifically reformatted for instruction fine-tuning (SFT) of language models.\nThe original question and solution pairs have been converted into an instruction-following format. Critically, the isreasoning_score and isreasoning labels from the parent dataset are preserved, allowing for targeted SFT on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning.","first_N":5,"first_N_keywords":["XuHu6736 (formatting and derivation)","derived from XuHu6736/s1_54k_filter_with_isreasoning","derived from source datasets","monolingual","XuHu6736/s1_54k_filter_with_isreasoning"],"keywords_longer_than_N":true},
	{"name":"SFT_54k_reasoning","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/SFT_54k_reasoning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/SFT_54k_reasoning is a processed version of the XuHu6736/s1_54k_filter_with_isreasoning dataset, specifically reformatted for instruction fine-tuning (SFT) of language models.\nThe original question and solution pairs have been converted into an instruction-following format. Critically, the isreasoning_score and isreasoning labels from the parent dataset are preserved, allowing for targeted SFT on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning.","first_N":5,"first_N_keywords":["XuHu6736 (formatting and derivation)","derived from XuHu6736/s1_54k_filter_with_isreasoning","derived from source datasets","monolingual","XuHu6736/s1_54k_filter_with_isreasoning"],"keywords_longer_than_N":true},
	{"name":"alpaca-style-QnA","keyword":"fine-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sweatSmile/alpaca-style-QnA","creator_name":"amitk17","creator_url":"https://huggingface.co/sweatSmile","description":"\n\t\n\t\t\n\t\tAlpaca-style Question and Answer Dataset\n\t\n\nThis dataset contains question-answer pairs formatted in the Alpaca instruction style, suitable for instruction fine-tuning of language models.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example contains:\n\ninstruction: The question\ninput: Empty string (can be used for context in other applications)\noutput: The answer\ntext: The formatted text using the Alpaca template\n\n\n\t\n\t\t\n\t\tTemplate\n\t\n\nBelow is an instruction that describes a task, paired with an input that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sweatSmile/alpaca-style-QnA.","first_N":5,"first_N_keywords":["English","cc-by-4.0","üá∫üá∏ Region: US","instruction","qa"],"keywords_longer_than_N":true},
	{"name":"Titanium2-DeepSeek-R1","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Click here to support our open-source dataset and model releases!\nTitanium2-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n32.4k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraform‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-sv","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv","creator_name":"Tim Isbister","creator_url":"https://huggingface.co/timpal0l","description":"\n\t\n\t\t\n\t\tOpenHermes-2.5-sv\n\t\n\nThis is a machine translated instruct dataset from OpenHermes-2.5. \nThe facebook/seamless-m4t-v2-large was used, and some post filtering is done to remove repetitive texts that occurred due to translation errors.\n\n\t\n\t\t\n\t\tExample data:\n\t\n\n[\n   {\n      \"from\":\"human\",\n      \"value\":\"Vilket naturfenomen, som orsakas av att ljus reflekteras och bryts genom vattendroppar, resulterar i en f√§rgglad b√•ge p√• himlen?\",\n      \"weight\":null\n   },\n   {\n      \"from\":\"gpt\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv.","first_N":5,"first_N_keywords":["text-generation","Swedish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Kannada-Dataset-v03","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/charanhu/Kannada-Dataset-v03","creator_name":"Charan H U","creator_url":"https://huggingface.co/charanhu","description":"charanhu/Kannada-Dataset-v03 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","Kannada","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","Hindi","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"hibo-function-calling-v1","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thibaud-perrin/hibo-function-calling-v1","creator_name":"Thibaud Perrin","creator_url":"https://huggingface.co/thibaud-perrin","description":"\n\t\n\t\t\n\t\thibo-function-calling-v1\n\t\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\tüìñ Dataset Description\n\t\n\nThis dataset, named \"hibo-function-calling-v1\", is designed to facilitate the fine-tuning of Large Language Models (LLMs) for function calling tasks. It comprises a single 'train' split containing 323,271 data points across three columns: 'dataset_origin', 'system', and 'chat'. \nThe dataset is a result of merging two distinct sources: gathnex/Gath_baize and glaiveai/glaive-function-calling-v2, with an aim to provide‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thibaud-perrin/hibo-function-calling-v1.","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"opin-pref","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\nhf.co/swaroop-nath/prompt-opin-summ dataset.\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\n{¬†¬†¬†¬†'unique-id': a unique id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref.","first_N":5,"first_N_keywords":["reinforcement-learning","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Mantis-Instruct","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tMantis-Instruct\n\t\n\nPaper | Website | Github | Models | Demo\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nMantis-Instruct is a fully text-image interleaved multimodal instruction tuning dataset, \ncontaining 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding. \nIt's been used to train Mantis Model families\n\nMantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\nAmong the 14‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Confession-Subreddit-Top500","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Oguzz07/Confession-Subreddit-Top500","creator_name":"Oƒüuzhan Yƒ±ldƒ±rƒ±m","creator_url":"https://huggingface.co/Oguzz07","description":"This dataset was prepared by taking into account the 500 most popular posts of all time in the confession subreddit and the comments with the most votes on these posts.\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","text","Text"],"keywords_longer_than_N":true},
	{"name":"pandora-rlhf","keyword":"fine-tuning","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-rlhf","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora RLHF\n\t\n\nA Reinforcement Learning from Human Feedback (RLHF) dataset for Direct Preference Optimization (DPO) fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the anthropic/hh-rlhf dataset.\n\n\t\n\t\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"pandora-instruct","keyword":"fine-tuning","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-instruct","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora Instruct\n\t\n\nAn instruction dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the existing datasets:\n\nteknium/openhermes\nise-uiuc/magicoder-evol-instruct-110k\nise-uiuc/magicoder-oss-instruct-75k\n\n\n\t\n\t\t\n\t\n\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"pandora-instruct","keyword":"instruct","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-instruct","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora Instruct\n\t\n\nAn instruction dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the existing datasets:\n\nteknium/openhermes\nise-uiuc/magicoder-evol-instruct-110k\nise-uiuc/magicoder-oss-instruct-75k\n\n\n\t\n\t\t\n\t\n\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"pandora-instruct","keyword":"sft","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-instruct","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora Instruct\n\t\n\nAn instruction dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the existing datasets:\n\nteknium/openhermes\nise-uiuc/magicoder-evol-instruct-110k\nise-uiuc/magicoder-oss-instruct-75k\n\n\n\t\n\t\t\n\t\n\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"pandora-tool-calling","keyword":"fine-tuning","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-tool-calling","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora Tool Calling\n\t\n\nA tool-calling dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the glaiveai/glaive-function-calling-v2 dataset.\n\n\t\n\t\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"pandora-tool-calling","keyword":"sft","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-tool-calling","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora Tool Calling\n\t\n\nA tool-calling dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the glaiveai/glaive-function-calling-v2 dataset.\n\n\t\n\t\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"CoIN","keyword":"instruction tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Zacks-Chen/CoIN","creator_name":"Cheng_Chen","creator_url":"https://huggingface.co/Zacks-Chen","description":"\n\t\n\t\t\n\t\n\t\n\t\tContinuaL Instruction Tuning Dataset Card\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset sources\n\t\n\nThis dataset is constructed using publicly available and commonly used instruction tuning datasets, including VQAv2, VizWiz, ScienceQA, TextVQA, GQA, and OCR-VQA. \nAdditionally, to enhance diversity, we introduce the classification task and referring expression comprehension task into CoIN with ImageNet, RefCOCO, RefCOCO+, and RefCOCOg.\nBefore proceeding with instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zacks-Chen/CoIN.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","üá∫üá∏ Region: US","instruction tuning"],"keywords_longer_than_N":true},
	{"name":"synthetic-confidential-information-injected-business-excerpts","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Rohit-D/synthetic-confidential-information-injected-business-excerpts","creator_name":"Rohit D","creator_url":"https://huggingface.co/Rohit-D","description":"\n\t\n\t\t\n\t\tSynthetic Confidential Information Injected Business Excerpts\n\t\n\nThis dataset aims to provide business report excerpts which contain relevant confidential/sensitive information.\nThis includes mentions of :\n  1. Internal Marketing Strategies.\n  2. Proprietary Product Composition.\n  3. License Internals.\n  4. Internal Sales Projections.\n  5. Confidential Patent Details.\n  6. others.\n\nThe dataset contains around 1k business excerpt - Reasons pairs. The Reason field contains the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rohit-D/synthetic-confidential-information-injected-business-excerpts.","first_N":5,"first_N_keywords":["question-answering","text-classification","feature-extraction","summarization","English"],"keywords_longer_than_N":true},
	{"name":"code-act","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xingyaoww/code-act","creator_name":"Xingyao Wang","creator_url":"https://huggingface.co/xingyaoww","description":" Executable Code Actions Elicit Better LLM Agents \n\n\nüíª Code\n‚Ä¢\nüìÉ Paper\n‚Ä¢\nü§ó Data (CodeActInstruct)\n‚Ä¢\nü§ó Model (CodeActAgent-Mistral-7b-v0.1)\n‚Ä¢\nü§ñ Chat with CodeActAgent!\n\n\nWe propose to use executable Python code to consolidate LLM agents‚Äô actions into a unified action space (CodeAct).\nIntegrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations (e.g., code execution results) through multi-turn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xingyaoww/code-act.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"CultriX-dpo","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/CultriX-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/CultriX-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs_dutch_cleaned","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\n\t\n\t\t\n\t\tDataset Card for Orca DPO Pairs Dutch Cleaned\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized-preferences-cleaned-kto","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned-kto","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned) KTO\n\t\n\n\nA KTO signal transformed version of the highly loved UltraFeedback Binarized Preferences Cleaned, the preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\nRead more about‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned-kto.","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"VIS","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/weiwei888/VIS","creator_name":"shiweiwei","creator_url":"https://huggingface.co/weiwei888","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weiwei888/VIS.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"wangwei","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guilty1987/wangwei","creator_name":"FAN","creator_url":"https://huggingface.co/guilty1987","description":"guilty1987/wangwei dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned-gpt4-turbo","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo","creator_name":"myles bruce","creator_url":"https://huggingface.co/mylesgoose","description":"I downloaded the dataset from Alpaca at https://huggingface.co/datasets/yahma/alpaca-cleaned and processed it using a script to convert the text to hashes. I removed any duplicate hashes along with their corresponding input and output columns. Subsequently, I utilized the GPT-4 Turbo API to feed each message, instruction, and input to the model for generating responses.\nHere are the outputs generated by the GPT-4 Turbo model. The information in the input column and instruction column should be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"AB1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MagedGaman/AB1","creator_name":"Maged Gaman","creator_url":"https://huggingface.co/MagedGaman","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MagedGaman/AB1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kushala/alpaca","creator_name":"Mummigatti","creator_url":"https://huggingface.co/Kushala","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kushala/alpaca.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned","creator_name":"Cristian Velasquez (Entreprenerdly.com)","creator_url":"https://huggingface.co/Entreprenerdly","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"AutoMathText","keyword":"finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/AutoMathText","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"üéâ This work, introducing the AutoMathText dataset and the AutoDS method, has been accepted to The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025 Findings)! üéâ\n\n\t\n\t\t\n\t\tAutoMathText\n\t\n\nAutoMathText is an extensive and carefully curated dataset encompassing around 200 GB of mathematical texts. It's a compilation sourced from a diverse range of platforms including various websites, arXiv, and GitHub (OpenWebMath, RedPajama, Algebraic Stack). This rich repository‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/AutoMathText.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"KnowCoder-Schema-Following-Data","keyword":"instruction tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/golaxy/KnowCoder-Schema-Following-Data","creator_name":"ICT-Golaxy","creator_url":"https://huggingface.co/golaxy","description":"\n   \n\n\n KnowCoder: Coding Structured Knowledge into LLMs for Universal\nInformation Extraction \n\n\n\nüìÉ Paper\n|\nü§ó Resource (Schema ‚Ä¢ Data ‚Ä¢ Model)\n|\nüöÄ Try KnowCoder (coming soon)!\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSchema Following Data\n\t\n\nThe schema following data is constructed on UniversalNER, InstructIE, and LSEE. The statistics of schema following data are presented as follows.\n\n   \n\n\n\nThe cases of schema following data are shown here.\nDue to data protection concerns, here we provide only 100 data samples for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/golaxy/KnowCoder-Schema-Following-Data.","first_N":5,"first_N_keywords":["English","apache-2.0","100K<n<1M","arxiv:2403.07969","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/StackMathQA","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\n\t\n\t\t\n\t\tStackMathQA\n\t\n\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\n\n\t\n\t\t\n\t\tConfigs\n\t\n\nconfigs:\n- config_name: stackmathqa1600k\n  data_files: data/stackmathqa1600k/all.jsonl\n  default: true\n- config_name: stackmathqa800k\n  data_files:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/StackMathQA.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"alpaca-ingen","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Walmart-the-bag/alpaca-ingen","creator_name":"wbag","creator_url":"https://huggingface.co/Walmart-the-bag","description":"\n\t\n\t\t\n\t\tAlpaca Ingen\n\t\n\nGoogle has added massive rate limits and other policies. I am unable to finish this.\nThis dataset was created using Gemini 1.0 Pro with minor adjustments for cleanliness. It may contain some issues, including 'I'm sorry' responses. The dataset will undergo further cleaning once it reaches completion, with a target of processing up to 23,000 rows.\n","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"code-action-sociale-familles","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-action-sociale-familles","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de l'action sociale et des familles, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-action-sociale-familles.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-aviation-civile","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-aviation-civile","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de l'aviation civile, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-aviation-civile.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-cinema-image-animee","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-cinema-image-animee","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode du cin√©ma et de l'image anim√©e, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-cinema-image-animee.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-communes","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-communes","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des communes, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-communes.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-communes-nouvelle-caledonie","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-communes-nouvelle-caledonie","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des communes de la Nouvelle-Cal√©donie, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-communes-nouvelle-caledonie.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-defense","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-defense","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la d√©fense, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-defense.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-deontologie-architectes","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-deontologie-architectes","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de d√©ontologie des architectes, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-deontologie-architectes.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-disciplinaire-penal-marine-marchande","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-disciplinaire-penal-marine-marchande","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode disciplinaire et p√©nal de la marine marchande, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-disciplinaire-penal-marine-marchande.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-domaine-etat","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode du domaine de l'Etat, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-domaine-etat-collectivites-mayotte","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat-collectivites-mayotte","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode du domaine de l'Etat et des collectivit√©s publiques applicable √† la collectivit√© territoriale de Mayotte, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat-collectivites-mayotte.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-domaine-public-fluvial-navigation-interieure","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-domaine-public-fluvial-navigation-interieure","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode du domaine public fluvial et de la navigation int√©rieure, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-domaine-public-fluvial-navigation-interieure.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-douanes-mayotte","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-douanes-mayotte","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des douanes de Mayotte, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-douanes-mayotte.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-electoral","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-electoral","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode √©lectoral, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-electoral.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-energie","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-energie","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de l'√©nergie, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-energie.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-entree-sejour-etrangers-droit-asile","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-entree-sejour-etrangers-droit-asile","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de l'entr√©e et du s√©jour des √©trangers et du droit d'asile, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-entree-sejour-etrangers-droit-asile.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-expropriation-utilite-publique","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-expropriation-utilite-publique","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de l'expropriation pour cause d'utilit√© publique, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-expropriation-utilite-publique.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-famille-aide-sociale","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-famille-aide-sociale","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la famille et de l'aide sociale, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-famille-aide-sociale.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-forestier-nouveau","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-forestier-nouveau","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode forestier (nouveau), non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-forestier-nouveau.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-fonction-publique","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-fonction-publique","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode g√©n√©ral de la fonction publique, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-fonction-publique.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-propriete-personnes-publiques","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-propriete-personnes-publiques","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode g√©n√©ral de la propri√©t√© des personnes publiques, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-propriete-personnes-publiques.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-collectivites-territoriales","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-collectivites-territoriales","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode g√©n√©ral des collectivit√©s territoriales, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-collectivites-territoriales.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-impots","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode g√©n√©ral des imp√¥ts, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-impots-annexe-i","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-i","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode g√©n√©ral des imp√¥ts, annexe I, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-i.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-impots-annexe-ii","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-ii","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode g√©n√©ral des imp√¥ts, annexe II, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-ii.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-impots-annexe-iii","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iii","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode g√©n√©ral des imp√¥ts, annexe III, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iii.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-impots-annexe-iv","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iv","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode g√©n√©ral des imp√¥ts, annexe IV, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iv.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-impositions-biens-services","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impositions-biens-services","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des impositions sur les biens et services, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impositions-biens-services.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-instruments-monetaires-medailles","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-instruments-monetaires-medailles","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des instruments mon√©taires et des m√©dailles, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-instruments-monetaires-medailles.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-juridictions-financieres","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-juridictions-financieres","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des juridictions financi√®res, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-juridictions-financieres.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-justice-militaire-nouveau","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-justice-militaire-nouveau","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de justice militaire (nouveau), non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-justice-militaire-nouveau.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-justice-penale-mineurs","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-justice-penale-mineurs","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la justice p√©nale des mineurs, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-justice-penale-mineurs.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-legion-honneur-medaille-militaire-ordre-national-merite","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-legion-honneur-medaille-militaire-ordre-national-merite","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la L√©gion d'honneur, de la M√©daille militaire et de l'ordre national du M√©rite, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-legion-honneur-medaille-militaire-ordre-national-merite.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"livre-procedures-fiscales","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/livre-procedures-fiscales","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tLivre des proc√©dures fiscales, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/livre-procedures-fiscales.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-minier","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-minier","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode minier, non-instruct (2025-06-04)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-minier.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-minier-nouveau","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-minier-nouveau","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode minier (nouveau), non-instruct (2025-06-03)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-minier-nouveau.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-organisation-judiciaire","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-organisation-judiciaire","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de l'organisation judiciaire, non-instruct (2025-05-31)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-organisation-judiciaire.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-patrimoine","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-patrimoine","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode du patrimoine, non-instruct (2025-05-20)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-patrimoine.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-penitentiaire","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-penitentiaire","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode p√©nitentiaire, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-penitentiaire.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-pensions-civiles-militaires-retraite","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-pensions-civiles-militaires-retraite","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des pensions civiles et militaires de retraite, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-pensions-civiles-militaires-retraite.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-pensions-retraite-marins-francais-commerce-peche-plaisance","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-pensions-retraite-marins-francais-commerce-peche-plaisance","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des pensions de retraite des marins fran√ßais du commerce, de p√™che ou de plaisance, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-pensions-retraite-marins-francais-commerce-peche-plaisance.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-pensions-militaires-invalidite-victimes-guerre","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-pensions-militaires-invalidite-victimes-guerre","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des pensions militaires d'invalidit√© et des victimes de guerre, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-pensions-militaires-invalidite-victimes-guerre.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-ports-maritimes","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-ports-maritimes","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode des ports maritimes, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-ports-maritimes.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-procedure-penale","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-procedure-penale","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de proc√©dure p√©nale, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-procedure-penale.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-recherche","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-recherche","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la recherche, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-recherche.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-rural-ancien","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-rural-ancien","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode rural (ancien), non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-rural-ancien.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-service-national","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-service-national","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode du service national, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-service-national.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-tourisme","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-tourisme","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode du tourisme, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-tourisme.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-travail-maritime","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-travail-maritime","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode du travail maritime, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-travail-maritime.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"code-voirie-routiere","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-voirie-routiere","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\n\t\n\t\t\n\t\tCode de la voirie routi√®re, non-instruct (2025-03-10)\n\t\n\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-voirie-routiere.","first_N":5,"first_N_keywords":["text-generation","table-question-answering","summarization","text-retrieval","question-answering"],"keywords_longer_than_N":true},
	{"name":"grad_school_math_instructions_fr_Mixtral","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/grad_school_math_instructions_fr_Mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for grad_school_math_instructions_fr_Mixtral\n\t\n\nThis dataset was made thanks to the instruction of the vigogne's dataset but the output were generated with Mixtral-8x7B-Instruct instead of GPT3.5 to make it open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Alpaca_french_mixtral","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/Alpaca_french_mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca_french_mixtral\n\t\n\nThis dataset was made by reusing the french alpaca instruction with Mixtral-8x7B-Instruct to make the output open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Alpaca_french_mixtral","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/Alpaca_french_mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca_french_mixtral\n\t\n\nThis dataset was made by reusing the french alpaca instruction with Mixtral-8x7B-Instruct to make the output open-source.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nrobinjo\n","first_N":5,"first_N_keywords":["question-answering","text-generation","French","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"classifai","keyword":"preference","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jhmejia/classifai","creator_name":"John Henry","creator_url":"https://huggingface.co/jhmejia","description":"jhmejia/classifai dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","gpl-3.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"data-centric-ml-sft","keyword":"sft","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/data-centric-ml-sft","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tData Centric Machine Learning Domain SFT dataset\n\t\n\nThe Data Centric Machine Learning Domain SFT dataset is an example of how to use distilabel to create a domain-specific fine-tuning dataset easily.\nIn particular using the Domain Specific Dataset Project Space. \nThe dataset focuses on the domain of data-centric machine learning and consists of chat conversations between a user and an AI assistant. \nIts purpose is to demonstrate the process of creating domain-specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/data-centric-ml-sft.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"aya_dataset_english_example","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dataset_english_example","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"data-is-better-together/aya_dataset_english_example dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"tw-legal-synthetic-qa","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lianghsun/tw-legal-synthetic-qa","creator_name":"Huang Liang Hsun","creator_url":"https://huggingface.co/lianghsun","description":"\n\t\n\t\t\n\t\tDataset Card for tw-legal-synthetic-qa\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nÊú¨ÂêàÊàêÂ∞çË©±Ë≥áÊñôÈõÜÔºà‰∏ãÁ®±Êú¨Ë≥áÊñôÈõÜÔºâÁî± THUDM/chatglm3-6b-32k Âíå lianghsun/tw-processed-judgmentsÔºåÁî±ÂØ¶È©óÂæåÁöÑ prompt ÂéªÁîüÊàêÁπÅÈ´î‰∏≠ÊñáÊ≥ïÂæãÂ∞çË©±ÂêàÊàêÈõÜ„ÄÇ\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nÊú¨Ë≥áÊñôÈõÜÂèØ‰ª•ÈÅãÁî®Âú® SFTÔºåËÆìÊ®°ÂûãÂ≠∏ÊúÉÂ¶Ç‰ΩïÂõûÁ≠îÊ≥ïÂæãÂïèÈ°å„ÄÇ\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nÁπÅÈ´î‰∏≠Êñá„ÄÇ\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n‰∏ÄÂÄãË≥áÊñôÊ®£Êú¨Â¶Ç‰∏ãÔºåÈ¶ñÂÖàÁî± user ÁôºÂïè‰∫Ü‰∏ÄÂÄãÂÖ∑ÊúâÔºàÊàñÂèØËÉΩÊúâÔºâÊ≥ïÂæãÊÉÖÂ¢ÉÁöÑÂïèÈ°åÔºåÁÑ∂Âæå assistant ÂõûÁ≠îÊ≥ïÂæãÁõ∏ÈóúÁü•Ë≠ò„ÄÇ\n{\n    \"messages\":[\n        {\n            \"role\":\"user\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lianghsun/tw-legal-synthetic-qa.","first_N":5,"first_N_keywords":["question-answering","Chinese","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"German_RisingWorld_Alpaca-Dataset","keyword":"instruction tuning","license":"GNU General Public License v2.0","license_url":"https://choosealicense.com/licenses/gpl-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Andzej-75/German_RisingWorld_Alpaca-Dataset","creator_name":"Andzej Ktowierzy","creator_url":"https://huggingface.co/Andzej-75","description":"\n\t\n\t\t\n\t\tGerman \"Rising World\"-Game Alpaca-Dataset\n\t\n\n\n\t\n\t\t\n\t\tData Description\n\t\n\nThis HF data repository contains the German Alpaca dataset for the open-world sandbox game \"Rising World\".\nDieses HF-Datenrepository enth√§lt den deutschen Alpaca-Datensatz f√ºr das Open-World-Sandbox-Spiel \"Rising World\".\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\nThis data is intended for fine-tuning\nThis data is useful for \"Rising World\" plug-in developers\nEach instance has an instruction, an output, and an optional input. An example is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Andzej-75/German_RisingWorld_Alpaca-Dataset.","first_N":5,"first_N_keywords":["text-generation","question-answering","German","gpl-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ascii_art_generation_140k","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\n\t\n\t\tData for LLM ASCII Art\n\t\n\nThis repo contains open-sourced SFT data for fine-tuning LLMs on ASCII Art Generation.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\t\n\t\t\nLink\nLanguage\nSize\n\n\n\t\t\nascii_art_generation_140k\nEnglish\n138,941\n\n\nascii_art_generation_140k_bilingual\nChinese & English\n138,941\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Preparation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTraining data description\n\t\n\nThe training data consists of 138,941 ASCII arts instruction-response samples for LLMs to perform SFT.\nThe source images of these‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"ascii_art_generation_140k_bilingual","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k_bilingual","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\n\t\n\t\tData for LLM ASCII Art\n\t\n\nThis repo contains open-sourced SFT data for fine-tuning LLMs on ASCII Art Generation.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\t\n\t\t\nLink\nLanguage\nSize\n\n\n\t\t\nascii_art_generation_140k\nEnglish\n138,941\n\n\nascii_art_generation_140k_bilingual\nChinese & English\n138,941\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Preparation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTraining data description\n\t\n\nThe training data consists of 138,941 ASCII arts instruction-response samples for LLMs to perform SFT.\nThe source images of these‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k_bilingual.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/OpenHermes-2.5-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\td0rj/OpenHermes-2.5-ru\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is translated version of teknium/OpenHermes-2.5 into Russian using Google Translate.\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"ru-instruct","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\t–ö–∞—Ä—Ç–æ—á–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n\t\n\n–°–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞ (—Å–ø–∞—Å–∏–±–æ –º–æ–¥–µ–ª–∏ Den4ikAI/nonsense_gibberish_detector). –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω SimHash'–æ–º.\n–û–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –Ω—ë–º –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞ –Ω–µ –∑–∞–≤—ë–∑, in progress.\n\n\t\n\t\t\n\t\t–°–æ—Å—Ç–∞–≤\n\t\n\n–°–æ–±—Ä–∞–ª –∏–∑ —ç—Ç–∏—Ö –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö:\n\nd0rj/OpenOrca-ru (–æ—Ç Open-Orca/OpenOrca)\nd0rj/OpenHermes-2.5-ru (–æ—Ç teknium/OpenHermes-2.5)\nd0rj/dolphin-ru (–æ—Ç ehartford/dolphin)\nd0rj/alpaca-cleaned-ru (–æ—Ç‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","machine-generated","found","translated"],"keywords_longer_than_N":true},
	{"name":"ru-instruct","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\t–ö–∞—Ä—Ç–æ—á–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n\t\n\n–°–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞ (—Å–ø–∞—Å–∏–±–æ –º–æ–¥–µ–ª–∏ Den4ikAI/nonsense_gibberish_detector). –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω SimHash'–æ–º.\n–û–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –Ω—ë–º –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞ –Ω–µ –∑–∞–≤—ë–∑, in progress.\n\n\t\n\t\t\n\t\t–°–æ—Å—Ç–∞–≤\n\t\n\n–°–æ–±—Ä–∞–ª –∏–∑ —ç—Ç–∏—Ö –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö:\n\nd0rj/OpenOrca-ru (–æ—Ç Open-Orca/OpenOrca)\nd0rj/OpenHermes-2.5-ru (–æ—Ç teknium/OpenHermes-2.5)\nd0rj/dolphin-ru (–æ—Ç ehartford/dolphin)\nd0rj/alpaca-cleaned-ru (–æ—Ç‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","machine-generated","found","translated"],"keywords_longer_than_N":true},
	{"name":"meme_dataset","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noxus09/meme_dataset","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","description":"Noxus09/meme_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"German_RisingWorld_prompt-text-rejected_Jsonl","keyword":"instruction tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Andzej-75/German_RisingWorld_prompt-text-rejected_Jsonl","creator_name":"Andzej Ktowierzy","creator_url":"https://huggingface.co/Andzej-75","description":"\n\t\n\t\t\n\t\tGerman \"Rising World\"-Game Dataset\n\t\n\n\n\t\n\t\t\n\t\tData Description\n\t\n\nThis HF data repository contains the German dataset for the open-world sandbox game \"Rising World\".\nDieses HF-Datenrepository enth√§lt den deutschen Datensatz f√ºr das Open-World-Sandbox-Spiel \"Rising World\".\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\nThis data is intended for fine-tuning\nThis data is useful for \"Rising World\" plug-in developers\n\n","first_N":5,"first_N_keywords":["text-generation","question-answering","German","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"guanjian_anli_test1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/papaya523/guanjian_anli_test1","creator_name":"zhaoyue","creator_url":"https://huggingface.co/papaya523","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/papaya523/guanjian_anli_test1.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"smart_home_control","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Charles95/smart_home_control","creator_name":"JingXiang","creator_url":"https://huggingface.co/Charles95","description":"Charles95/smart_home_control dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset aims to provide large-scale vision feedback data. \nIt is a combination of the following high-quality vision feedback datasets:\n\nzhiqings/LLaVA-Human-Preference-10K: 9,422 samples\nMMInstruction/VLFeedback: 80,258 samples\nYiyangAiLab/POVID_preference_data_for_VLLMs: 17,184 samples\nopenbmb/RLHF-V-Dataset: 5,733 samples\nopenbmb/RLAIF-V-Dataset: 83,132 samples\n\nWe also offer a cleaned version in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized.","first_N":5,"first_N_keywords":["mit","100K - 1M","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"PromisedChat_Instruction","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/netmouse/PromisedChat_Instruction","creator_name":"Matt Yeh","creator_url":"https://huggingface.co/netmouse","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/netmouse/PromisedChat_Instruction.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized-cleaned","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized-Cleaned\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset represents a cleaned version on wangclnlp/vision-feedback-mix-binarized.\nDescriptions of the base datasets, including the data format and the procedure for mixing data, can be found in this link.\n\n\t\n\t\t\n\t\tOur Methods for Cleaning Vision Feedback Data\n\t\n\nOur goal is to select vision feedback samples where the preferred outputs are significantly differentiated from the dispreferred ones, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned.","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"SFT-UZ-9k","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MLDataScientist/SFT-UZ-9k","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","description":"This is SFT version of MLDataScientist/DPO-uz-9k (Uzbek translated dataset with two answers for each prompt for DPO fine-tuning).\nI selected ['answer'][0] from each example and saved them in this dataset for easy fine-tuning of LLMs.\n","first_N":5,"first_N_keywords":["text-generation","Uzbek","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Feedback-Collection-ru","keyword":"instruct","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Feedback-Collection-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tFeedback-Collection-ru\n\t\n\nThis is russian version of prometheus-eval/Feedback-Collection translated using Google Translate.\n","first_N":5,"first_N_keywords":["text-generation","text-classification","translated","prometheus-eval/Feedback-Collection","Russian"],"keywords_longer_than_N":true},
	{"name":"shibing624_alpaca-zh","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/shibing624_alpaca-zh","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"\n\t\n\t\t\n\t\tDataset Card for \"alpaca-zh\"\n\t\n\nÊú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\n\n\t\n\t\t\n\t\n\t\n\t\tUsage and License Notices\n\t\n\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/botp/shibing624_alpaca-zh.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/evol-dpo-ita","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Card\n\t\n\nEvol-DPO-Ita is a high-quality dataset consisting of ~20,000 preference pairs derived from responses generated by two large language models: Claude Opus and GPT-3.5 Turbo.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n20,000 Preference Comparisons: Each entry contains two model responses ‚Äî Claude Opus (claude-3-opus-20240229) for the chosen response and GPT-3.5 Turbo for the rejected one, both generated from a translated prompt from the original Evol-Instruct dataset (prompt and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/efederici/evol-dpo-ita.","first_N":5,"first_N_keywords":["text2text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"bioinstruct","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bio-nlp-umass/bioinstruct","creator_name":"UMass BioNLP Lab","creator_url":"https://huggingface.co/bio-nlp-umass","description":"\n\t\n\t\t\n\t\tDataset Card for BioInstruct\n\t\n\nGitHub repo: https://github.com/bio-nlp/BioInstruct\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBioInstruct is a dataset of 25k instructions and demonstrations generated by OpenAI's GPT-4 engine in July 2023. \nThis instruction data can be used to conduct instruction-tuning for language models (e.g. Llama) and make the language model follow biomedical instruction better. \nImprovements of Llama on 9 common BioMedical tasks are shown in the result section. \nTaking‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bio-nlp-umass/bioinstruct.","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","zero-shot-classification","English"],"keywords_longer_than_N":true},
	{"name":"halfTurkish","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sasanikrali/halfTurkish","creator_name":"sasani krali","creator_url":"https://huggingface.co/sasanikrali","description":"\n\t\n\t\t\n\t\tDataset Card for HalfTurkish\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the given‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sasanikrali/halfTurkish.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"walt","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/canTooDdev/walt","creator_name":"Boris Marion-Dorier","creator_url":"https://huggingface.co/canTooDdev","description":"\n\t\n\t\t\n\t\tDataset Card for Alpaca-Cleaned\n\t\n\n\nRepository: https://github.com/gururise/AlpacaDataCleaned\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\n\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\n\n\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/canTooDdev/walt.","first_N":5,"first_N_keywords":["text-generation","French","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"deepresearch_trace","keyword":"sft","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Looogic/deepresearch_trace","creator_name":"Logicü§ó","creator_url":"https://huggingface.co/Looogic","description":"\n\t\n\t\t\n\t\tüî¨ DeepResearch Tool Use Conversations\n\t\n\nA high-quality dataset of multi-turn conversations between humans and AI agents, featuring sophisticated tool use for research and report generation tasks.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\nMulti-turn conversations with complex reasoning chains\nTool use integration including search, web scraping, and note-taking\nComprehensive metadata with execution metrics and performance tracking\nResearch-focused tasks requiring information synthesis and analysis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Looogic/deepresearch_trace.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","n<1K"],"keywords_longer_than_N":true},
	{"name":"deepsearch-llama-finetune","keyword":"fine-tune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/enosislabs/deepsearch-llama-finetune","creator_name":"Enosis Labs, Inc.","creator_url":"https://huggingface.co/enosislabs","description":"\n\t\n\t\t\n\t\tDeepSearch Llama Finetune Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe DeepSearch Llama Finetune Dataset is a specialized collection of high-quality, real-world prompts and responses, meticulously crafted for fine-tuning Llama-based conversational AI models. This dataset is optimized for:\n\nCreativity: Responses are original, engaging, and leverage creative formats (Markdown, tables, outlines, etc.).\nEffectiveness: Answers are highly relevant, actionable, and tailored for real-world applications.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/enosislabs/deepsearch-llama-finetune.","first_N":5,"first_N_keywords":["text-classification","depth-estimation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"deepsearch-llama-finetune","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/enosislabs/deepsearch-llama-finetune","creator_name":"Enosis Labs, Inc.","creator_url":"https://huggingface.co/enosislabs","description":"\n\t\n\t\t\n\t\tDeepSearch Llama Finetune Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe DeepSearch Llama Finetune Dataset is a specialized collection of high-quality, real-world prompts and responses, meticulously crafted for fine-tuning Llama-based conversational AI models. This dataset is optimized for:\n\nCreativity: Responses are original, engaging, and leverage creative formats (Markdown, tables, outlines, etc.).\nEffectiveness: Answers are highly relevant, actionable, and tailored for real-world applications.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/enosislabs/deepsearch-llama-finetune.","first_N":5,"first_N_keywords":["text-classification","depth-estimation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"alpaca-tat","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yasalma/alpaca-tat","creator_name":"Yasalma","creator_url":"https://huggingface.co/yasalma","description":"\n\t\n\t\t\n\t\tTatAlpaca\n\t\n\nDataset of Gemini-generated instructions in Tatar language.\n\nCode: tatlm/self_instruct\nCode is based on Stanford Alpaca and self-instruct.\n166,257 examples\n\nPrompt template:\n{{num_tasks}} “ó—ã–µ–ª–º–∞—Å—ã–Ω—ã“£ —Å–æ—Å—Ç–∞–≤—ã —Ç–µ–ª –º–æ–¥–µ–ª–µ–Ω ”©–π—Ä”ô–Ω“Ø ”©—á–µ–Ω —Ç”©—Ä–ª–µ:\n\n1. –ë–∏—Ä–µ–º–Ω”ô—Ä–Ω–µ –º–∞–∫—Å–∏–º–∞–ª—å —Ä”ô–≤–µ—à—Ç”ô —Ç–∏–ø–ª–∞—Ä—ã, —Å–æ—Ä–∞–ª–≥–∞–Ω –≥–∞–º”ô–ª–ª”ô—Ä–µ, —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–ª–∞—Ä—ã, –∫–µ—Ä“Ø –º”©–º–∫–∏–Ω–ª–µ–∫–ª”ô—Ä–µ –±—É–µ–Ω—á–∞ –±–µ—Ä-–±–µ—Ä—Å–µ–Ω”ô –æ—Ö—à–∞–º–∞–≥–∞–Ω –∏—Ç–µ–ø —ç—à–ª”ô.\n2. –ë–∏—Ä–µ–º–Ω”ô—Ä —Ä”ô—Å–µ–º–Ω”ô—Ä, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ –±–µ–ª”ô–Ω —ç—à–ª–∏ –±–µ–ª–º”ô–≥”ô–Ω “ª”ô–º —Ç—ã—à–∫—ã –¥”©–Ω—å—è–≥–∞ –∫–µ—Ä“Ø –º”©–º–∫–∏–Ω–ª–µ–≥–µ –±—É–ª–º–∞–≥–∞–Ω‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yasalma/alpaca-tat.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Tatar","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Mauxi-SFT-Persian","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüéØ Mauxi-SFT-Persian Dataset\n\t\n\n\n\t\n\t\t\n\t\tüåü Overview\n\t\n\nWelcome to the Mauxi-SFT-Persian dataset! A high-quality Persian language dataset specifically curated for Supervised Fine-Tuning (SFT) of Large Language Models.\n\n\t\n\t\t\n\t\tüìä Dataset Statistics\n\t\n\n\nüî¢ Total Conversations: 5,000\nüìù Total Tokens: 4,418,419\nüìà Average Tokens per Conversation: 883.7\nüéØ Format: JSONL with messages and token counts\n\n\n\t\n\t\t\n\t\tüîç Source & Creation\n\t\n\nThis dataset was created by translating the OpenHermes-100k‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian.","first_N":5,"first_N_keywords":["text-generation","Persian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Mauxi-SFT-Persian","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüéØ Mauxi-SFT-Persian Dataset\n\t\n\n\n\t\n\t\t\n\t\tüåü Overview\n\t\n\nWelcome to the Mauxi-SFT-Persian dataset! A high-quality Persian language dataset specifically curated for Supervised Fine-Tuning (SFT) of Large Language Models.\n\n\t\n\t\t\n\t\tüìä Dataset Statistics\n\t\n\n\nüî¢ Total Conversations: 5,000\nüìù Total Tokens: 4,418,419\nüìà Average Tokens per Conversation: 883.7\nüéØ Format: JSONL with messages and token counts\n\n\n\t\n\t\t\n\t\tüîç Source & Creation\n\t\n\nThis dataset was created by translating the OpenHermes-100k‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian.","first_N":5,"first_N_keywords":["text-generation","Persian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"BenchMAX_Model-based","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based","creator_name":"LLaMAX","creator_url":"https://huggingface.co/LLaMAX","description":"\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models\nLink: https://huggingface.co/papers/2502.07346\nRepository: https://github.com/CONE-MT/BenchMAX\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBenchMAX_Model-based is a dataset of BenchMAX, sourcing from m-ArenaHard, which evaluates the instruction following capability via model-based judgment.\nWe extend the original dataset to include languages that are not supported by m-ArenaHard through‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLaMAX/BenchMAX_Model-based.","first_N":5,"first_N_keywords":["text-generation","multilingual","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/oieieio/Finance-Instruct-500k","creator_name":"Jorge Alonso","creator_url":"https://huggingface.co/oieieio","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\nThe dataset includes content tailored for financial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/oieieio/Finance-Instruct-500k.","first_N":5,"first_N_keywords":["apache-2.0","üá∫üá∏ Region: US","finance","fine-tuning","conversational-ai"],"keywords_longer_than_N":true},
	{"name":"Persian-Math-SFT","keyword":"sft","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/Persian-Math-SFT","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüéØ Persian Math Questions Dataset for SFT\n\t\n\n\n\t\n\t\t\n\t\tüìù Description\n\t\n\nThis dataset contains Persian questions primarily focused on mathematical concepts, designed for Supervised Fine-Tuning (SFT) of Language Models.\n\n\t\n\t\t\n\t\tüîç Features\n\t\n\n\nHigh-quality Persian questions\nDetailed subtopic categorization\nFocused on mathematical concepts\nTokens count for each conversation\n\n\n\t\n\t\t\n\t\tüöÄ Coming Soon\n\t\n\n\nDetailed answers for each question\nAdditional topics beyond mathematics\nEnhanced‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Persian-Math-SFT.","first_N":5,"first_N_keywords":["text-generation","Persian","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Persian-Math-SFT","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/Persian-Math-SFT","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\n\t\n\t\t\n\t\tüéØ Persian Math Questions Dataset for SFT\n\t\n\n\n\t\n\t\t\n\t\tüìù Description\n\t\n\nThis dataset contains Persian questions primarily focused on mathematical concepts, designed for Supervised Fine-Tuning (SFT) of Language Models.\n\n\t\n\t\t\n\t\tüîç Features\n\t\n\n\nHigh-quality Persian questions\nDetailed subtopic categorization\nFocused on mathematical concepts\nTokens count for each conversation\n\n\n\t\n\t\t\n\t\tüöÄ Coming Soon\n\t\n\n\nDetailed answers for each question\nAdditional topics beyond mathematics\nEnhanced‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Persian-Math-SFT.","first_N":5,"first_N_keywords":["text-generation","Persian","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Arabic-Optimized-Reasoning-Dataset","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Jr23xd23/Arabic-Optimized-Reasoning-Dataset","creator_name":"Jaber","creator_url":"https://huggingface.co/Jr23xd23","description":"\n\t\n\t\t\n\t\tArabic Optimized Reasoning Dataset\n\t\n\nDataset Name: Arabic Optimized ReasoningLicense: Apache-2.0Formats: CSVSize: 1600 rowsBase Dataset: cognitivecomputations/dolphin-r1Libraries Used: Datasets, Dask, Croissant\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Arabic Optimized Reasoning Dataset helps AI models get better at reasoning in Arabic. While AI models are good at many tasks, they often struggle with reasoning in languages other than English. This dataset helps fix this problem by:\n\nUsing fewer tokens‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jr23xd23/Arabic-Optimized-Reasoning-Dataset.","first_N":5,"first_N_keywords":["question-answering","table-question-answering","Arabic","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ProcessedOpenAssistant","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant","creator_name":"Y. Yu","creator_url":"https://huggingface.co/PursuitOfDataScience","description":"\n\t\n\t\t\n\t\tRefined OASST1 Conversations\n\t\n\nDataset Name on Hugging Face: PursuitOfDataScience/ProcessedOpenAssistant\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is derived from the OpenAssistant/oasst1 conversations, with additional processing to:\n\nRemove single-turn or incomplete conversations (where a prompter/user message had no assistant reply),\nRename roles from \"prompter\" to \"User\" and \"assistant\" to \"Assistant\",\nOrganize each conversation as a list of turn objects.\n\nThe goal is to provide a clean‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"professor-academy-finetune","keyword":"fine-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Professoracademy/professor-academy-finetune","creator_name":"Professor Academy","creator_url":"https://huggingface.co/Professoracademy","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: Professor Academy (https://professoracademy.com)\nFunded by [optional]: Mr.Saravana Perumal\nShared by [optional]: Professor Academy Team\nLanguage(s) (NLP): English\nLicense: CC-BY-4.0\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Professoracademy/professor-academy-finetune.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"creative-rubrics-preferences","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences","creator_name":"Victor Gallego","creator_url":"https://huggingface.co/vicgalle","description":"\n\t\n\t\t\n\t\tcreative-rubrics-preferences üéè\n\t\n\nA dataset of creative responses using GPT-4.5, o3-mini and DeepSeek-R1. \nThis dataset contains several prompts seeking creative and diverse answers (like writing movie reviews, short stories, etc), and the style of the responses has been enhanced by prompting the model with custom rubrics that seek different creative styles.\nIt can be used for finetuning for custom styles in writing tasks.\nNote: This is a preference-formatted version of this other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Electrical-engineering-ru","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Electrical-engineering-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"Translated instructions from STEM-AI-mtl/Electrical-engineering into Russian using gemini-flash-1.5-8b.\n","first_N":5,"first_N_keywords":["text2text-generation","text-generation","translated","STEM-AI-mtl/Electrical-engineering","Russian"],"keywords_longer_than_N":true},
	{"name":"kazakh-ift","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nurkhan5l/kazakh-ift","creator_name":"Nurkhan Laiyk","creator_url":"https://huggingface.co/nurkhan5l","description":"Kazakh-IFT üá∞üáø\nAuthors: Nurkhan Laiyk, Daniil Orel, Rituraj Joshi, Maiya Goloburda, Yuxia Wang, Preslav Nakov, Fajri Koto\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nInstruction tuning in low-resource languages remains challenging due to limited coverage of region-specific institutional and cultural knowledge. To address this gap, we introduce a large-scale instruction-following dataset (~10,600 samples) focused on Kazakhstan, spanning domains such as governance, legal processes, cultural practices, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nurkhan5l/kazakh-ift.","first_N":5,"first_N_keywords":["Kazakh","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"sasha_smart_home_reasoning","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ThoughtfulThings/sasha_smart_home_reasoning","creator_name":"Thoughtful Things","creator_url":"https://huggingface.co/ThoughtfulThings","description":"This is a dataset of smart home user commands and JSON responses generated by zero-shot prompting of GPT-4. It can be used to fine-tune and/or evaluate language models for responding to user commands in smart homes. For more information, refer to our paper Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models.\nhttps://arxiv.org/abs/2305.09802\nIf you use the dataset in your work, please cite us:\n@article{king2024sasha,\n  title={Sasha: creative goal-oriented reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ThoughtfulThings/sasha_smart_home_reasoning.","first_N":5,"first_N_keywords":["mit","arxiv:2305.09802","üá∫üá∏ Region: US","llm","smarthome"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-wan2.1","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Alibaba Wan2.1 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Alibaba Wan 2.1 video generation model on our benchmark. The up to date benchmark‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"gemma-vs-gemma-preferences","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\n\t\n\t\t\n\t\tüíéüÜöüíé Gemma vs Gemma Preferences\n\t\n\nThis dataset contains on-policy collected preferences generated using anakin87/gemma-2-2b-ita-sft.\n‚ö†Ô∏è While this dataset may be valuable for didactic purposes, it is not recommended for training a model using Preference Tuning due to the following reasons:\n\nThe training would be off-policy for your model.\nThe dataset was generated with gemma-2-2b-ita-sft, a small model for Italian.\n\n\n\t\n\t\t\n\t\n\t\n\t\tMotivation\n\t\n\nWhile DPO (Direct Preference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences.","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-luma-ray2","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Luma Ray2 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Luma's Ray 2 video generation model on our benchmark. The up to date benchmark can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-Rich-Human-Feedback","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Rich Human Feedback Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~4 hours total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~22'000 human annotations were collected to evaluate AI-generated videos (using Sora) in 5 different categories. \n\nPrompt - Video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"express-legal-funding-reviews","keyword":"instruction-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/expresslegalfunding/express-legal-funding-reviews","creator_name":"Express Legal Funding","creator_url":"https://huggingface.co/expresslegalfunding","description":"A curated collection of real customer feedback and company replies for Express Legal Funding.  This dataset is designed for training and evaluating language models on tasks such as sentiment classification,  customer interaction modeling, and instruction tuning in the legal funding domain.\n","first_N":5,"first_N_keywords":["text-classification","text-generation","sentiment-classification","language-modeling","human"],"keywords_longer_than_N":true},
	{"name":"2k-ranked-images-open-image-preferences-v1","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/2k-ranked-images-open-image-preferences-v1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\t2k Ranked Images\n\t\n\nThis dataset contains roughly two thousand images ranked from most preferred to least preferred based on human feedback on pairwise comparisons (>25k responses). \nThe generated images, which are a sample from the open-image-preferences-v1 dataset \nfrom the team @data-is-better-together, are rated purely based on aesthetic preference, disregarding the prompt used for generation.\nWe provide the categories of the original dataset for easy filtering.\nThis is a new‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/2k-ranked-images-open-image-preferences-v1.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"mantra-14b-user-interaction-log","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/large-traversaal/mantra-14b-user-interaction-log","creator_name":"Traversaal.ai","creator_url":"https://huggingface.co/large-traversaal","description":"\n\t\n\t\t\n\t\tüß† Mantra-14B User Interaction Logs\n\t\n\nThis dataset captures real user interactions with a Gradio demo powered by large-traversaal/Mantra-14B. Each entry logs the user's prompt, the model's response, and additional metadata such as response time and generation parameters. This dataset is ideal for understanding how people engage with the model, evaluating responses, or fine-tuning on real-world usage data.\n\n\n\t\n\t\t\n\t\n\t\n\t\tüîç What‚Äôs Inside\n\t\n\nEach row in the dataset includes:\n\ntimestamp ‚Äì‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/large-traversaal/mantra-14b-user-interaction-log.","first_N":5,"first_N_keywords":["text-generation","mit","< 1K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"drill","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/drill","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tDrill\n\t\n\nThis dataset combines three instruction-following datasets:\n\nargilla/ifeval-like-data (filtered subset)  \nArliAI/Formax-v1.0  \nChristianAzinn/json-training\n\nIt contains prompts with detailed instructions and corresponding formatted outputs, suitable for training models on instruction adherence and structured text generation.\n\n  Definition of the word \"drill\" according to Merriam-Webster Dictionary\n  \n\ndrill (noun)\n\na physical or mental exercise aimed at perfecting facility and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/drill.","first_N":5,"first_N_keywords":["text2text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"swedish_healthcare_dpo_sft_dataset","keyword":"sft","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset","creator_name":"Rzgar","creator_url":"https://huggingface.co/rzgar","description":"\n\t\n\t\t\n\t\tSwedish Healthcare DPO/SFT Dataset\n\t\n\nThis dataset contains Swedish conversational prompts paired with chosen (preferred) and rejected (dispreferred) responses, along with English translations. It is designed for fine-tuning Large Language Models (LLMs) using preference-based methods. The (prompt, chosen, rejected) structure makes it particularly well-suited for Direct Preference Optimization (DPO) and similar algorithms.\nSweden is renowned for its high standards in healthcare and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset.","first_N":5,"first_N_keywords":["Swedish","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"s1_59k","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XuHu6736/s1_59k","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/s1_59k\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/s1_59k is a dataset specifically prepared for Supervised Fine-Tuning (SFT) of large language models. It is constructed by merging and processing two existing Hugging Face datasets: simplescaling/data_ablation_full59K and qfq/train_featurized.\nThe simplescaling/data_ablation_full59K dataset is a collection of approximately 59,000 questions and solutions spanning various domains including mathematics, science‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/s1_59k.","first_N":5,"first_N_keywords":["question-answering","text-generation","XuHu6736 (merging process)","simplescaling (source dataset: data_ablation_full59K)","qfq (source dataset: train_featurized, annotation based on 's1: Simple test-time scaling')"],"keywords_longer_than_N":true},
	{"name":"Finance","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mcube030/Finance","creator_name":"Mcube","creator_url":"https://huggingface.co/Mcube030","description":"\n\t\n\t\t\n\t\tFinance-Instruct-500k Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\nThe dataset includes content tailored for financial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mcube030/Finance.","first_N":5,"first_N_keywords":["apache-2.0","üá∫üá∏ Region: US","finance","fine-tuning","conversational-ai"],"keywords_longer_than_N":true},
	{"name":"secondKarlMarx-sft","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft","creator_name":"ChizhongWang","creator_url":"https://huggingface.co/ChizhongWang","description":"\n\t\n\t\t\n\t\tMarx Works SFT Instruction Prompts Dataset / È©¨ÂÖãÊÄùËëó‰ΩúSFTÊåá‰ª§ÊèêÁ§∫Êï∞ÊçÆÈõÜ\n\t\n\nEnglish | ‰∏≠Êñá\n\n\n\t\n\t\t\n\t\tEnglish\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains SFT (Supervised Fine-Tuning) instruction prompts generated from the works of Karl Marx. The dataset is specifically designed for training large language models, aiming to capture Marx's dialectical materialist analytical method and writing style.\n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nDiverse Prompt Types: Includes various styles of prompts such as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft.","first_N":5,"first_N_keywords":["text-generation","language-modeling","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"secondKarlMarx-sft","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft","creator_name":"ChizhongWang","creator_url":"https://huggingface.co/ChizhongWang","description":"\n\t\n\t\t\n\t\tMarx Works SFT Instruction Prompts Dataset / È©¨ÂÖãÊÄùËëó‰ΩúSFTÊåá‰ª§ÊèêÁ§∫Êï∞ÊçÆÈõÜ\n\t\n\nEnglish | ‰∏≠Êñá\n\n\n\t\n\t\t\n\t\tEnglish\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains SFT (Supervised Fine-Tuning) instruction prompts generated from the works of Karl Marx. The dataset is specifically designed for training large language models, aiming to capture Marx's dialectical materialist analytical method and writing style.\n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nDiverse Prompt Types: Includes various styles of prompts such as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChizhongWang/secondKarlMarx-sft.","first_N":5,"first_N_keywords":["text-generation","language-modeling","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"tame-the-weights-personas","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/leonvanbokhorst/tame-the-weights-personas","creator_name":"Leon Van Bokhorst","creator_url":"https://huggingface.co/leonvanbokhorst","description":"\n\t\n\t\t\n\t\tDataset Card for \"tame-the-weights-personas\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains instruction-following data designed for fine-tuning language models, specifically focused on generating Python code explanations and snippets while adopting distinct personas.\nThe data was synthetically generated using a large language model, prompted to adopt one of three personas:\n\nProfessor Snugglesworth: A friendly, encouraging, and slightly verbose persona, like a kind university‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/leonvanbokhorst/tame-the-weights-personas.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true}
]
;
