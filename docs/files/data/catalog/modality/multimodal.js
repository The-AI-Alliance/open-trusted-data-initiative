const data_for_modality_multimodal = 
[
	{"name":"MASH","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YRC10/MASH","creator_name":"Ruichen Yao","creator_url":"https://huggingface.co/YRC10","description":"We present a Multiplatform Annotated Dataset for Societal Impact of Hurricane (MASH) that includes 98,662 relevant social media data posts from Reddit, X, TikTok, and YouTube.  In addition, all relevant posts are annotated on three dimensions: Humanitarian Classes, Bias Classes, and Information Integrity Classes in a multi-modal approach that considers both textual and visual content, providing a rich labeled dataset for in-depth analysis. The dataset is also complemented by an Online Analytics Platform that not only allows users to view hurricane-related posts and articles, but also explores high-frequency keywords, user sentiment, and the locations where posts were made. To our best knowledge, MASH is the first large-scale, multi-platform, multimodal, and multi-dimensionally annotated hurricane dataset.  We envision that MASH can contribute to the study of hurricanes' impact on society, such as disaster severity classification, event detections, public sentiment analysis, and bias identification.\n","first_N":5,"first_N_keywords":["text-classification","image-classification","video-classification","expert-annotated","LLM"],"keywords_longer_than_N":true},
	{"name":"MolLangBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ChemFM/MolLangBench","creator_name":"ChemFM","creator_url":"https://huggingface.co/ChemFM","description":"\n\t\n\t\t\n\t\tMolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation\n\t\n\n\nThe MolLangBench paper is available on arXiv:2505.15054.\nThe code for using and evaluating the MolLangBench datasets is provided in this GitHub repository.\n\n\n\n\n\nMolLangBench is a comprehensive benchmark designed to evaluate the fundamental capabilities of AI models in language-prompted molecular structure recognition, editing, and generation.\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChemFM/MolLangBench.","first_N":5,"first_N_keywords":["question-answering","text-to-image","image-to-text","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"GeoGrid_Bench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bowen-upenn/GeoGrid_Bench","creator_name":"Lauren Jiang","creator_url":"https://huggingface.co/bowen-upenn","description":"\n\t\n\t\t\n\t\tGeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?\n\t\n\n\n\nWe present GeoGrid-Bench, a benchmark designed to evaluate the ability of foundation models to understand geo-spatial data in the grid structure. Geo-spatial datasets pose distinct challenges due to their dense numerical values, strong spatial and temporal dependencies, and unique multimodal representations including tabular data, heatmaps, and geographic visualizations. To assess how foundationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bowen-upenn/GeoGrid_Bench.","first_N":5,"first_N_keywords":["table-question-answering","visual-question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMMG","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMMGBench/MMMG","creator_name":"MMMG","creator_url":"https://huggingface.co/MMMGBench","description":"\n\t\n\t\t\n\t\tğŸ§  MMMG: Massive Multi-Discipline Multi-Tier Knowledge Image Benchmark\n\t\n\nMMMG introduces knowledge image generation as a new frontier in text-to-image research. This benchmark probes the reasoning capabilities of image generation models by challenging them to produce educational and scientific visuals grounded in structured knowledge.\nKnowledge imagesâ€”such as charts, diagrams, mind maps, and scientific illustrationsâ€”play a crucial role in human learning, as highlighted by dual-codingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MMMGBench/MMMG.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"CHOICE","keyword":"multi-modal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/An-Xiao/CHOICE","creator_name":"AnXiao","creator_url":"https://huggingface.co/An-Xiao","description":"\n\t\n\t\t\n\t\tCHOICE: Benchmarking The Remote Sensing Capabilities of Large Vision-Language Models\n\t\n\n Abstract: The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/An-Xiao/CHOICE.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMMU-LLM-R1-format","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xDAN-Vision/MMMU-LLM-R1-format","creator_name":"xDAN-RL-Group","creator_url":"https://huggingface.co/xDAN-Vision","description":"\n\t\n\t\t\n\t\tMMMU-LLM-R1 Reformatted Dataset\n\t\n\n","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"AmericanExpress_vision_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/AmericanExpress_vision_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tAMERICAN-EXPRESS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical and financial queries generated from American Express annual reports. It is designed to train and evaluate information retrieval models and improve AI understanding of financial documentation, with a specific focus on the credit card industry, payment processing, and banking services.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/AmericanExpress_vision_dataset.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"airbus-vision-dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/airbus-vision-dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tAIRBUS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from Airbus technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice at TW3 Partners, a company specialized in Generative AI. Passionate about artificial intelligence andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/airbus-vision-dataset.","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"unidisc_hq","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aswerdlow/unidisc_hq","creator_name":"Alexander Swerdlow","creator_url":"https://huggingface.co/aswerdlow","description":"This repository contains the dataset used in the paper Unified Multimodal Discrete Diffusion.\nCode: https://github.com/AlexSwerdlow/unidisc\nAdditionally, we release a synthetic dataset available here and the corresponding generation scripts as well as the raw data.\n","first_N":5,"first_N_keywords":["image-text-to-text","mit","10M - 100M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"videophy2_test","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/videophysics/videophy2_test","creator_name":"videophysics","creator_url":"https://huggingface.co/videophysics","description":"Project: https://github.com/Hritikbansal/videophy/tree/main/VIDEOPHY2\ncaption: original prompt in the dataset\nvideo_url: generated video (using original prompt or upsampled caption, depending on the video model)\nsa: semantic adherence score (1-5) from human evaluation\npc: physical commonsense score (1-5) from human evaluation\njoint: computed as sa >= 4, pc >= 4\nphysics_rules_followed: list of physics rules followed in the video as judged by human annotators (1)\nphysics_rules_unfollowed: listâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/videophysics/videophy2_test.","first_N":5,"first_N_keywords":["video-classification","mit","1K - 10K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"fire-exam-base64","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taean-yoo/fire-exam-base64","creator_name":"Taean Yoo","creator_url":"https://huggingface.co/taean-yoo","description":"\n\t\n\t\t\n\t\tğŸ”¥ Fire Exam Dataset with Images\n\t\n\nì´ ë°ì´í„°ì…‹ì€ ì†Œë°©ê³µë¬´ì› ì‹œí—˜ ë¬¸ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ëœ ë©€í‹°ëª¨ë‹¬ QA ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.ê° ìƒ˜í”Œì€ ë¬¸ì œ í…ìŠ¤íŠ¸, ì„ íƒì§€, ì •ë‹µ, ê·¸ë¦¬ê³  ì‹œê° ì •ë³´ë¥¼ ë‹´ì€ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Korean","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"hippovlog-dataset","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/linyueqian/hippovlog-dataset","creator_name":"Yueqian Lin","creator_url":"https://huggingface.co/linyueqian","description":"\n\t\n\t\t\n\t\tDataset Card for HippoVlog\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHippoVlog is a novel benchmark dataset designed for evaluating Multimodal Memory and Reasoning (MMR) systems. It consists of 25 long-form daily vlogs (682 minutes total) with naturalistic audiovisual content and 1,000 validated multiple-choice question-answer pairs.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset supports the following tasks:\n\nMultimodal Memory and Reasoning (MMR): The primary task involves answeringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/linyueqian/hippovlog-dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","visual-question-answering","multiple-choice-qa","English"],"keywords_longer_than_N":true},
	{"name":"MMR1-in-context-synthesizing","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing","creator_name":"Lai Wei","creator_url":"https://huggingface.co/WaltonFuture","description":"This dataset is designed for unsupervised post-training of Multi-Modal Large Language Models (MLLMs) focusing on enhancing reasoning capabilities. It contains image-problem-answer triplets, where the problem requires multimodal reasoning to derive the correct answer from the provided image. The dataset is intended for use with the MM-UPT framework described in the accompanying paper.\n\nğŸ™ GitHub Repo: waltonfuture/MM-UPT\nğŸ“œ Paper (arXiv): Unsupervised Post-Training for Multi-Modal LLM Reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"android_control_test","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Reallm-Labs/android_control_test","creator_name":"Reallm Labs","creator_url":"https://huggingface.co/Reallm-Labs","description":"\n\t\n\t\t\n\t\tProcessed Android Control Test Set for InfiGUI-R1 Evaluation\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the processed test set derived from the Android Control dataset by Google Research. It has been specifically prepared for evaluating the performance of our model, InfiGUI-R1.\nThe InfiGUI-R1 model is detailed in our paper:\n\nInfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners\n\nThis dataset facilitates standardized testing andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Reallm-Labs/android_control_test.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K<n<10K","arxiv:2504.14239"],"keywords_longer_than_N":true},
	{"name":"MixBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mixed-modality-search/MixBench","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench.","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"multiref-datasets","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wsnHowest/multiref-datasets","creator_name":"Sinan Wang","creator_url":"https://huggingface.co/wsnHowest","description":"\n\t\n\t\t\n\t\tMultiRef Datasets\n\t\n\nThis repository contains two datasets for multi-image reference tasks:\n\n\t\n\t\t\n\t\tMultiRef-Bench-Synthetic (900 samples)\n\t\n\n\nimages/: Processed images for the benchmark\noriginal_images/: Original unprocessed images\nbenchmark990v3.json: Benchmark data with 990 entries (first 900 used)\n\n\n\t\n\t\t\n\t\tMulti-Image-Benchmark (1000 samples)\n\t\n\n\ncompressed_images/: Compressed images for the benchmark\nfinal_1000_prompts_taxonomy.json: Taxonomy data with 1000 prompts\n\n\n\t\n\t\t\n\t\tUsageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wsnHowest/multiref-datasets.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SolidGeo","keyword":"multi-modal-qa","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HarryYancy/SolidGeo","creator_name":"HarryYancy","creator_url":"https://huggingface.co/HarryYancy","description":"\n\t\n\t\t\n\t\tSolidGeo: Measuring Multimodal Spatial Math Reasoning in Solid Geometry\n\t\n\n[ğŸŒ Homepage] [ğŸ’» Github]  [ğŸ¤— Huggingface Dataset] \n[ğŸ“Š Leaderboard ]  [ğŸ” Visualization]  [ğŸ“– Paper]\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nSolidGeo is the first large-scale benchmark specifically designed to evaluate the performance of MLLMs on mathematical reasoning tasks in solid geometry. SolidGeo consists of 3,113 real-world Kâ€“12 and competition-level problems, each paired with visual context and annotatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HarryYancy/SolidGeo.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-classification","English"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React-Single-Image","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Single-Image","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Single-Image.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K<n<1M","ğŸ‡ºğŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React-Multi-Images","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Multi-Images","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Multi-Images.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"HCTQA","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qcri-ai/HCTQA","creator_name":"Artificial Intelligence Research Group, Qatar Computing Research Institute","creator_url":"https://huggingface.co/qcri-ai","description":"\n\t\n\t\t\n\t\tHCT-QA: Human-Centric Tables Question Answering\n\t\n\nHCT-QA is a benchmark dataset designed to evaluate large language models (LLMs) on question answering over complex, human-centric tables (HCTs). These tables often appear in documents such as research papers, reports, and webpages and present significant challenges for traditional table QA due to their non-standard layouts and compositional structure.\nThe dataset includes:\n\n2,188 real-world tables with 9,835 human-annotated QA pairs\n4â€¦ See the full description on the dataset page: https://huggingface.co/datasets/qcri-ai/HCTQA.","first_N":5,"first_N_keywords":["question-answering","document-question-answering","visual-question-answering","expert-generated","English"],"keywords_longer_than_N":true},
	{"name":"ScanBot","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ed1son/ScanBot","creator_name":"zhiling chen","creator_url":"https://huggingface.co/ed1son","description":" \n\n\t\n\t\t\n\t\tScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tğŸ§  Dataset Summary\n\t\n\nScanBot is a dataset for instruction-conditioned, high-precision surface scanning with robots. Unlike existing datasets that focus on coarse tasks like grasping or navigation, ScanBot targets industrial laser scanning, where sub-millimeter accuracy and parameter stability are essential. It includes scanning trajectories across 12 objects and 6 task types, each driven byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ed1son/ScanBot.","first_N":5,"first_N_keywords":["robotics","English","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Resume-Analysis-CoTR","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","description":"\n\t\n\t\t\n\t\tResume Reasoning and Feedback Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains approximately 417 examples designed to facilitate research and development in automated resume analysis and feedback generation. Each data point consists of a user query regarding their resume, a simulated internal analysis (chain-of-thought) performed by an expert persona, and a final, user-facing feedback response derived solely from that analysis.\nThe dataset captures a two-step reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR.","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"MathVista","keyword":"multi-modal-qa","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AI4Math/MathVista","creator_name":"AI for Math Reasoning","creator_url":"https://huggingface.co/AI4Math","description":"\n\t\n\t\t\n\t\tDataset Card for MathVista\n\t\n\n\nDataset Description\nPaper Information\nDataset Examples\nLeaderboard\nDataset Usage\nData Downloading\nData Format\nData Visualization\nData Source\nAutomatic Evaluation\n\n\nLicense\nCitation\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nMathVista is a consolidated Mathematical reasoning benchmark within Visual contexts. It consists of three newly created datasets, IQTest, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate logicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI4Math/MathVista.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","text-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"TALI","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Antreas/TALI","creator_name":"Antreas Antoniou","creator_url":"https://huggingface.co/Antreas","description":"\n\t\n\t\t\n\t\tDataset Card for \"TALI\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nTALI is a large-scale, tetramodal dataset designed to facilitate a shift from unimodal and duomodal to tetramodal research in deep learning. It aligns text, video, images, and audio, providing a rich resource for innovative self-supervised learning tasks and multimodal research. TALI enables exploration of how different modalities and data/model scaling affect downstream performance, with the aim of inspiringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Antreas/TALI.","first_N":5,"first_N_keywords":["zero-shot-classification","cc-by-4.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Tuberculosis_Dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/moukaii/Tuberculosis_Dataset","creator_name":"Zhankai Ye","creator_url":"https://huggingface.co/moukaii","description":"\n\t\n\t\t\n\t\tMultimodal Dataset of Tuberculosis Patients including CT and Clinical Case Reports\n\t\n\nZhankai Ye    \nNetID: zy172\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is curated from the original â€œThe MultiCaRe Datasetâ€ to focus on the chest tuberculosis patients. This is a multimodal dataset consisting of lung computed tomography (CT) imaging data and the clinical case records of tuberculosis patients, along with their case keywords, the captions of their CT images, patient_id, gender, and ageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/moukaii/Tuberculosis_Dataset.","first_N":5,"first_N_keywords":["feature-extraction","text-classification","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ViStoryBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ViStoryBench/ViStoryBench","creator_name":"ViStoryBench","creator_url":"https://huggingface.co/ViStoryBench","description":"\n\t\n\t\t\n\t\tModel Card: ViStoryBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nViStoryBench is a comprehensive benchmark dataset for story visualization. It aims to thoroughly evaluate and advance the performance of story visualization models by providing diverse story types, artistic styles, and detailed annotations. The goal of story visualization is to generate a sequence of visually coherent and content-accurate images based on a given narrative text and character reference images.\nKey features ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ViStoryBench/ViStoryBench.","first_N":5,"first_N_keywords":["text-to-image","human-annotated","machine-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"TreeOfLife-200M","keyword":"multimodal","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imageomics/TreeOfLife-200M","creator_name":"HDR Imageomics Institute","creator_url":"https://huggingface.co/imageomics","description":"\n\t\n\t\t\n\t\tDataset Card for TreeOfLife-200M\n\t\n\nWith nearly 214 million images representing 952,257 taxa across the tree of life, TreeOfLife-200M is the largest and most diverse public ML-ready dataset for computer vision models in biology at release. This dataset combines images and metadata from four core biodiversity data providers: Global Biodiversity Information Facility (GBIF), Encyclopedia of Life (EOL), BIOSCAN-5M, and FathomNet to more than double the number of unique taxa covered byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/imageomics/TreeOfLife-200M.","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","English","Latin","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"MathVision","keyword":"multi-modal-qa","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MathVision","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\n\t\n\t\t\n\t\tMeasuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset\n\t\n\n[ğŸ’» Github] [ğŸŒ Homepage]  [ğŸ“Š Leaderboard ] [ğŸ“Š Open Source Leaderboard ] [ğŸ” Visualization] [ğŸ“– Paper]\n\n\t\n\t\t\n\t\n\t\n\t\tğŸš€ Data Usage\n\t\n\n\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MathLLMs/MathVision\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tğŸ’¥ News\n\t\n\n\n[2025.05.16] ğŸ’¥ We now support the official open-source leaderboard! ğŸ”¥ğŸ”¥ğŸ”¥ Skywork-R1V2-38B is the best open-source model, scoring 49.7% on MATH-Vision. ğŸ”¥ğŸ”¥ğŸ”¥â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MathVision.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","expert-generated"],"keywords_longer_than_N":true},
	{"name":"FinMME","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/luojunyu/FinMME","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","description":"Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, there is a notable lack of effective and specialized multimodal evaluation datasets in the financial domain. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20â€¦ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/FinMME.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"VS-Bench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zelaix/VS-Bench","creator_name":"Zelai Xu","creator_url":"https://huggingface.co/zelaix","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVS-Bench is a multimodal benchmark for evaluating VLMs in multi-agent environments. We evaluate fourteen state-of-the-art models in eight vision-grounded environments with two complementary dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return.\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@article{xu2025vs,\n  title={VS-Bench: Evaluating VLMs for Strategic Reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zelaix/VS-Bench.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ArxivCap","keyword":"multi-modal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMInstruction/ArxivCap","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for ArxivCap\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Instances\n\t\n\n\nExample-1 of single (image, caption) pairs\n\n\"......\" stands for omitted parts.\n\n{\n    'src': 'arXiv_src_2112_060/2112.08947', \n    'meta': \n    {\n        'meta_from_kaggle': \n        {\n            'journey': '', \n            'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', \n            'categories': 'cs.ET'\n        }, \n        'meta_from_s2': \n        {\n            'citationCount': 8â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/ArxivCap.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-HTML","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  ğŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nğŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ğŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ğŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"SpaceThinker","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/remyxai/SpaceThinker","creator_name":"Remyx AI","creator_url":"https://huggingface.co/remyxai","description":"\n\t\n\t\t\n\t\tSpaceThinker Dataset\n\t\n\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\nTry training a LLaVA-style VLM using the SpaceThinker Dataset\n\n\t\n\t\t\n\t\tEnhanced Quantitative Spatial Reasoning with Test-Time Compute\n\t\n\nThe SpaceThinker dataset is created using VQASynth to synthesize spatial reasoning traces from a subset of images \nin the localized narratives split of the cauldron.\n\n\t\n\t\t\n\t\tData Samples\n\t\n\n\n\t\n\t\t\n\n\n\n\n\n\t\t\nPrompt: How far is the man in the red hat from the pallet of boxes in feet?\nPrompt: How far is the Goalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/remyxai/SpaceThinker.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MM-MathInstruct","keyword":"multi-modal-qa","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MM-MathInstruct","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\n\t\n\t\t\n\t\tMathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning\n\t\n\nRepo: https://github.com/mathllm/MathCoder\nPaper: https://huggingface.co/papers/2505.10557\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MathCoder-VL, a series of open-source large multimodal models (LMMs) specifically tailored for general math problem-solving. We also introduce FigCodifier-8B, an image-to-code model.\n\n\t\n\t\t\nBase Model\nOurs\n\n\n\t\t\nMini-InternVL-Chat-2B-V1-5\nMathCoder-VL-2B\n\n\nInternVL2-8Bâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MM-MathInstruct.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nUnite-Instruct-Retrieval-Train contains 2 subsets covering 21 datasets, organized as follows:\nUnite-Instruct-Retrieval-Train\nâ”œâ”€â”€ MMEB-train\nâ”‚   â”œâ”€â”€ A-OKVQA.json\nâ”‚   â”œâ”€â”€ ChartQA.json\nâ”‚   â”œâ”€â”€ ...\nâ”‚   â”œâ”€â”€ images\nâ”‚   â”‚   â”œâ”€â”€ A-OKVQA\nâ”‚   â”‚   â”œâ”€â”€ CIRR\nâ”‚   â”‚   â”œâ”€â”€ ChartQA\nâ”‚   â”‚   â”œâ”€â”€ DocVQA\nâ”‚   â”‚   â”œâ”€â”€ HatefulMemes\nâ”‚   â”‚   â”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"clevr-math","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dali-does/clevr-math","creator_name":"Adam Dahlgren LindstrÃ¶m","creator_url":"https://huggingface.co/dali-does","description":"CLEVR-Math is a dataset for compositional language, visual and mathematical reasoning. CLEVR-Math poses questions about mathematical operations on visual scenes using subtraction and addition, such as \"Remove all large red cylinders. How many objects are left?\". There are also adversarial (e.g. \"Remove all blue cubes. How many cylinders are left?\") and multihop questions (e.g. \"Remove all blue cubes. Remove all small purple spheres. How many objects are left?\").","first_N":5,"first_N_keywords":["visual-question-answering","visual-question-answering","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"ScienceQA","keyword":"multi-modal-qa","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/derek-thomas/ScienceQA","creator_name":"Derek Thomas","creator_url":"https://huggingface.co/derek-thomas","description":"\n\t\n\t\t\n\t\tDataset Card Creation Guide\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nLearn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nMulti-modal Multiple Choice\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nEnglish\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nExplore more samples here.\n{'image': Image,\n 'question': 'Which of these states is farthest north?',\n 'choices': ['West Virginia', 'Louisiana', 'Arizona', 'Oklahoma'],\n 'answer': 0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/derek-thomas/ScienceQA.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","other","visual-question-answering","text-classification"],"keywords_longer_than_N":true},
	{"name":"vsr_random","keyword":"multimodality","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cambridgeltl/vsr_random","creator_name":"Language Technology Lab @University of Cambridge","creator_url":"https://huggingface.co/cambridgeltl","description":"\n\t\n\t\t\n\t\tVSR: Visual Spatial Reasoning\n\t\n\nThis is the random set of VSR: Visual Spatial Reasoning (TACL 2023) [paper].\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndata_files = {\"train\": \"train.jsonl\", \"dev\": \"dev.jsonl\", \"test\": \"test.jsonl\"}\ndataset = load_dataset(\"cambridgeltl/vsr_random\", data_files=data_files)\n\nNote that the image files still need to be downloaded separately. See data/ for details.\nGo to our github repo for more introductions.\n\n\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find VSRâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cambridgeltl/vsr_random.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"vsr_zeroshot","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cambridgeltl/vsr_zeroshot","creator_name":"Language Technology Lab @University of Cambridge","creator_url":"https://huggingface.co/cambridgeltl","description":"\n\t\n\t\t\n\t\tVSR: Visual Spatial Reasoning\n\t\n\nThis is the zero-shot set of VSR: Visual Spatial Reasoning (TACL 2023) [paper].\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndata_files = {\"train\": \"train.jsonl\", \"dev\": \"dev.jsonl\", \"test\": \"test.jsonl\"}\ndataset = load_dataset(\"cambridgeltl/vsr_zeroshot\", data_files=data_files)\n\nNote that the image files still need to be downloaded separately. See data/ for details.\nGo to our github repo for more introductions.\n\n\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you findâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cambridgeltl/vsr_zeroshot.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VisIT-Bench","keyword":"multi-modal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/VisIT-Bench","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n\t\n\t\t\n\t\tDataset Card for VisIT-Bench\n\t\n\n\nDataset Description\nLinks\nDataset Structure\nData Fields\nData Splits\nData Loading\n\n\nLicensing Information\nAnnotations\nConsiderations for Using the Data\nCitation Information\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nVisIT-Bench is a dataset and benchmark for vision-and-language instruction following. The dataset is comprised of image-instruction pairs and corresponding example outputs, spanning a wide range of tasks, from simple object recognition to complexâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/VisIT-Bench.","first_N":5,"first_N_keywords":["crowdsourced","found","original","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Mid-Data","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Mid-Data","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","description":"\n\t\n\t\t\n\t\tDataset Card for LLaVA-OneVision\n\t\n\n\nDue to unknow reasons, we are unable to process dataset with large amount into required HF format. So we directly upload the json files and image folders (compressed into tar.gz files).\n\n\nYou can use the following link to directly download and decompress them.\nhttps://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Mid-Data/tree/main/evol_instruct\n\nWe provide the whole details of LLaVA-OneVision Dataset. In this dataset, we include the data splitsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Mid-Data.","first_N":5,"first_N_keywords":["text-generation","Chinese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-HQ-311K","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tHumanCaption-HQ-311K\n\t\n\nHumanCaption-HQ-311K: Approximately 311,000 human-related images and their corresponding natural language descriptions.\nCompared to HumanCaption-10M, this dataset not only includes associated facial language descriptions but also filters out images with higher resolution and employs the powerful visual understanding capabilities of GPT-4V to generate more detailed and accurate text descriptions.\nThis dataset is used for the second phase of training HumanVLMâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"geotechnie","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MatteoKhan/geotechnie","creator_name":"Khan Matteo","creator_url":"https://huggingface.co/MatteoKhan","description":"\n\t\n\t\t\n\t\tAbout Me\n\t\n\nAbout Me\nI'm Matteo Khan, a computer science apprentice at TW3 Partners, specializing in Generative AI and NLP. My focus is on creating datasets that improve AI's ability to process complex technical documents.\nYou can connect with me on LinkedIn: Matteo Khan\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tPurpose\n\t\n\nThis dataset is designed to fine-tune models for expertise in geotechnical engineering by generating structured queries from soil mechanics and construction-relatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MatteoKhan/geotechnie.","first_N":5,"first_N_keywords":["English","French","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Eval_Real","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"A newer version of this dataset is available.\nhttps://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real_v1.1\n\n\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nğŸŒ Project Website | ğŸ“„ Paper | ğŸ¤— Dataset | ğŸ”¥ VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"LiveXiv","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LiveXiv/LiveXiv","creator_name":"LiveXiv","creator_url":"https://huggingface.co/LiveXiv","description":"LiveXiv - an evolving multi-modal dataset based on ArXiv (ICLR 2025)\nhttps://arxiv.org/abs/2410.10783\n\n\t\n\t\t\n\t\tLiveXiv Leaderboard\n\t\n\n\n\t\n\t\t\nModel\nV0 VQA\nV0 TQA\nV1 VQA\nV1 TQA\nV2 VQA\nV2 TQA\nV3 VQA\nV3 TQA\nV4 VQA\nV4 TQA\n\n\n\t\t\nClaude-Sonnet\n0.75\n0.81\n0.75\n0.84\n0.78\n0.82\n0.78\n0.84\n0.80\n0.78\n\n\nQwen2-VL-7B\n0.67\n0.58\n0.67\n0.60\n0.68\n0.58\n0.69\n0.67\n0.71\n0.53\n\n\nPixtral\n-\n-\n-\n-\n0.73\n0.59\n0.71\n0.39\n0.73\n0.55\n\n\nInternVL2-8B\n0.62\n0.62\n0.62\n0.62\n0.64\n0.60\n0.65\n0.69\n0.67\n0.57\nGPT4o\n0.51\n0.48\n0.60\n0.55\n0.59\n0.49â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiveXiv/LiveXiv.","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","table-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"polymath","keyword":"multi-modal-qa","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/him1411/polymath","creator_name":"Himanshu Gupta","creator_url":"https://huggingface.co/him1411","description":"\n\t\n\t\t\n\t\tPaper Information\n\t\n\nWe present PolyMATH, a challenging benchmark aimed at evaluating the general cognitive reasoning abilities of MLLMs. \nPolyMATH comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning. \nWe conducted a comprehensive, and quantitative evaluation of 15 MLLMs using four diverse prompting strategies, including Chain-of-Thoughtâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/him1411/polymath.","first_N":5,"first_N_keywords":["multiple-choice","expert-generated","found","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"rdt-ft-data","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data","creator_name":"Robotics Diffusion Transformer Collaboration","creator_url":"https://huggingface.co/robotics-diffusion-transformer","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card\n\t\n\nThis is the fine-tuning dataset used in the paper RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation.\n\n\t\n\t\t\n\t\n\t\n\t\tSource\n\t\n\n\nProject Page: https://rdt-robotics.github.io/rdt-robotics/\nPaper: https://arxiv.org/pdf/2410.07864\nCode: https://github.com/thu-ml/RoboticsDiffusionTransformer\nModel: https://huggingface.co/robotics-diffusion-transformer/rdt-1b\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nDownload all archive files and use the following command to extract:\ncatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data.","first_N":5,"first_N_keywords":["mit","arxiv:2410.07864","ğŸ‡ºğŸ‡¸ Region: US","robotics","multimodal"],"keywords_longer_than_N":true},
	{"name":"BioTrove-Train","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BGLab/BioTrove-Train","creator_name":"Baskar Group","creator_url":"https://huggingface.co/BGLab","description":"\n\t\n\t\t\n\t\tBioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity\n\t\n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nSee the BioTrove dataset card on HuggingFace to access the main BioTrove dataset (161.9M)\nBioTrove comprises well-processed metadata with full taxa information and URLs pointing to image files. The metadata can be used to filter specific categories, visualize data distribution, and manage imbalance effectively. We provide a collection of software toolsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BGLab/BioTrove-Train.","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","English","mit","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"BioTrove","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BGLab/BioTrove","creator_name":"Baskar Group","creator_url":"https://huggingface.co/BGLab","description":"\n\t\n\t\t\n\t\n\t\n\t\tBioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity\n\t\n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nSee the BioTrove-Train dataset card on HuggingFace to access the samller BioTrove-Train dataset (40M)\nBioTrove comprises well-processed metadata with full taxa information and URLs pointing to image files. The metadata can be used to filter specific categories, visualize data distribution, and manage imbalance effectively. We provide a collectionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BGLab/BioTrove.","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","English","mit","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"BHM-Bengali-Hateful-Memes","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Eftekhar/BHM-Bengali-Hateful-Memes","creator_name":"Eftekhar Hossain","creator_url":"https://huggingface.co/Eftekhar","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBHM is a novel multimodal dataset for Bengali Hateful Memes detection. The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, \ntailored for two tasks: (i) detecting hateful memes and (ii) detecting the social entities they target (i.e., Individual, Organization, Community, and Society).\n\n\t\n\t\t\n\t\tPaper Information\n\t\n\n\nPaper: https://aclanthology.org/2024.acl-long.454/\nCode:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Eftekhar/BHM-Bengali-Hateful-Memes.","first_N":5,"first_N_keywords":["other","image-classification","image-to-text","Bengali","mit"],"keywords_longer_than_N":true},
	{"name":"robonar","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/robonar/robonar","creator_name":"RoboNar","creator_url":"https://huggingface.co/robonar","description":"\n\t\n\t\t\n\t\tğŸ“‡ RONAR (RoboNar) Dataset\n\t\n\nğŸ“„ Paper on arXiv  | ğŸŒ Project Website\nRONAR introduces a real-world multimodal dataset paired with natural language narrations for robotic experience grounding. Built on the Stretch SE3 mobile manipulator in real home environments, the dataset supports behavior transparency, risk estimation, and failure recovery for intelligent robotics systems. It underlies the RONAR framework described in the CoRL 2024 paper: \"I Can Tell What I Am Doing: Towardâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/robonar/robonar.","first_N":5,"first_N_keywords":["text-generation","summarization","object-detection","robotics","English"],"keywords_longer_than_N":true},
	{"name":"teamcraft_data","keyword":"multi-modal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/teamcraft/teamcraft_data","creator_name":"TeamCraft","creator_url":"https://huggingface.co/teamcraft","description":"\n\t\n\t\t\n\t\tDataset Card for TeamCraft\n\t\n\nThe TeamCraft dataset is designed to develop multi-modal, multi-agent collaboration in Minecraft. It features 55,000 task variants defined by multi-modal prompts and procedurally generated expert demonstrations.\nThis repository contains the data for the validation set and its visualizations. \nTo use the validation set, download TeamCraft-Data-Valid.zip and extract using unzip TeamCraft-Data-Valid.zip.\nIn addition, the training set is available in twoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/teamcraft/teamcraft_data.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"vlsbench","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Foreshhh/vlsbench","creator_name":"XuHao Hu","creator_url":"https://huggingface.co/Foreshhh","description":"ğŸ‰ VLSBench has been accpeted to ACL2025 Main Conference, see you in Vienna.\nâœ… Update data.json with safety reason and image description for more efficient and reliable evaluaiton.\n\n\t\n\t\t\n\t\tDataset Card for VLSBench\n\t\n\nThis dataset is for paper VLSBench: Unveiling Information Leakage In Multimodal Safety\nYou can check our Paper, Github, Project Page for more information.\ndataset = load_dataset(\"Foreshhh/vlsbench\", split='train') \n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nOur dataset statistics is listedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Foreshhh/vlsbench.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"medmax_data","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mint-medmax/medmax_data","creator_name":"mint-medmax","creator_url":"https://huggingface.co/mint-medmax","description":"\n\t\n\t\t\n\t\tMedMax Dataset\n\t\n\n\n\t\n\t\t\n\t\tMixed-Modal Instruction Tuning for Training Biomedical Assistants\n\t\n\nAuthors: Hritik Bansal, Daniel Israelâ€ , Siyan Zhaoâ€ , Shufan Li, Tung Nguyen, Aditya GroverInstitution: University of California, Los Angelesâ€  Equal Contribution\n\n\t\n\t\t\n\t\tOverview\n\t\n\nLarge Language Models (LLMs) and Large Multimodal Models (LMMs) have demonstrated remarkable capabilities in multimodal information integration, opening transformative possibilities for biomedical AI in imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mint-medmax/medmax_data.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"AniGamePersonaCaps","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/AniGamePersonaCaps","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tAniGamePersonaCap\n\t\n\nThis multimodal dataset curates a collection of 633,565 beloved anime, manga and game characters from 3,860 Fandom wiki sites, organized across the following components:\n\nImage Modality\n\nVisuals of character figures.\n\n\nText Modality\n\nFandom Wiki Metadata: Meta information about characters from HTML contents.  \nCaptions:  \nVLM-Generated: Descriptions of visual appearance and inferred personality generated by Vision-Language Models (e.g., Qwen-VL-72B-Instruct).â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/AniGamePersonaCaps.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","100K - 1M","Image","Text"],"keywords_longer_than_N":true},
	{"name":"SmallMinesDS","keyword":"multi-modal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ellaampy/SmallMinesDS","creator_name":"SOA","creator_url":"https://huggingface.co/ellaampy","description":"\n\t\n\t\t\n\t\tSmallMinesDS\n\t\n\nThe gradual expansion of unregularized artisanal small-scale gold mining (ASGM) fuels environmental degradation and poses risk to miners and mining communities. To enforce sustainable mining, support reclamation initiatives and pave the way for understudying the impacts of mining, we present SmallMinesDS, a benchmark dataset for mapping artisanal small-scale gold mining from multi-sensor satellite images. The initial version of the dataset covers five districts inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ellaampy/SmallMinesDS.","first_N":5,"first_N_keywords":["cc-by-sa-4.0","1K - 10K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"blip3-grounding-50m","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/blip3-grounding-50m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n\t\n\t\t\n\t\tBLIP3-GROUNDING-50M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-GROUNDING-50M dataset is designed to enhance the ability of Vision-Language Models (VLMs) to ground semantic concepts in visual features, which is crucial for tasks like object detection, semantic segmentation, and understanding referring expressions (e.g., \"the object to the left of the dog\"). Traditional datasets often lack the necessary granularity for such tasks, making it challenging for models to accurately localize andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-grounding-50m.","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Leopard-Instruct","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wyu1/Leopard-Instruct","creator_name":"Wenhao Yu","creator_url":"https://huggingface.co/wyu1","description":"\n\t\n\t\t\n\t\tLeopard-Instruct\n\t\n\nPaper | Github | Models-LLaVA | Models-Idefics2\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nLeopard-Instruct is a large instruction-tuning dataset, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. It's been used to train Leopard-LLaVA [checkpoint]  and Leopard-Idefics2 [checkpoint].\n\n\t\n\t\t\n\t\tLoading dataset\n\t\n\n\nto load the dataset without automatically downloading and process the images (Please run the following codes with datasets==2.18.0)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wyu1/Leopard-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Geoperception","keyword":"multi-modal-qa","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/euclid-multimodal/Geoperception","creator_name":"Euclid Multimodal LLM","creator_url":"https://huggingface.co/euclid-multimodal","description":"Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions\n\n\t\n\t\t\n\t\tDataset Card for Geoperception\n\t\n\nA Benchmark for Low-level Geometric Perception\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nGeoperception is a benchmark focused specifically on accessing model's low-level visual perception ability in 2D geometry.\nIt is sourced from the Geometry-3K corpus, which offers precise logical forms for geometric diagrams, compiled from popular high-schoolâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/euclid-multimodal/Geoperception.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Ar-MUSA","keyword":"multimodal","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Skhaled/Ar-MUSA","creator_name":"Salma Khaled Ali","creator_url":"https://huggingface.co/Skhaled","description":"\n\t\n\t\t\n\t\tData Directory Structure\n\t\n\nThe Ar-MUSA directory contains annotated datasets organized by batches and annotation teams. Each batch is labeled with a number, and the annotation team is indicated by a letter. The structure is as follows:\nAr-MUSA\nâ”œâ”€â”€ Annotation 1a\nâ”‚   â”œâ”€â”€ frames        # Contains the extracted frames for each record\nâ”‚   â”œâ”€â”€ audios        # Contains the corresponding audio files\nâ”‚   â”œâ”€â”€ transcripts   # Contains the transcripts of the audio files\nâ”‚   â””â”€â”€ annotations.csv  #â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Skhaled/Ar-MUSA.","first_N":5,"first_N_keywords":["text-classification","audio-classification","image-classification","Arabic","afl-3.0"],"keywords_longer_than_N":true},
	{"name":"MIS_Test","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Tuwhy/MIS_Test","creator_name":"Yi Ding","creator_url":"https://huggingface.co/Tuwhy","description":"\n\t\n\t\t\n\t\tRethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models\n\t\n\n\nOur paper, code, data, models can be found at MIS.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nOur MIS test set contains three split \"MIS-easy\", \"MIS-hard\", \"MIS-real\".\n{\n  \"question\": \"str\",\n  \"category\": \"str\",\n  \"sub_category\": \"str\",\n  \"image_path1\": \"str\",\n  \"image_path2\": \"str\",\n  \"id\": int\n}\n\n\n\t\n\t\t\n\t\tStatistics\n\t\n\nOur 'MIS-easy' and 'MIS-hard' datasets together contain 2,185 samples across 6 categories and 12â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Tuwhy/MIS_Test.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K<n<10K","ğŸ‡ºğŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2024-18","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-18","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  ğŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nğŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ğŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ğŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-18.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"tmp","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YANG-Cheng/tmp","creator_name":"Cheng Yang","creator_url":"https://huggingface.co/YANG-Cheng","description":"\n\n ChartMimic: Evaluating LMMâ€™s Cross-Modal Reasoning Capability via Chart-to-Code Generation \n\n\nThis is the official dataset repository of ChartMimic. \n\n\t\n\t\t\n\t\t1. Data Overview\n\t\n\nChartMimic aims at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.\nChartMimic includes 1000 human-curatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YANG-Cheng/tmp.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"test-public","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taiseimatsuoka/test-public","creator_name":"taisei matsuoka","creator_url":"https://huggingface.co/taiseimatsuoka","description":"\n\t\n\t\t\n\t\tnanoLLaVA - Sub 1B Vision-Language Model\n\t\n\n\n  \n\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nnanoLLaVA is a \"small but mighty\" 1B vision-language model designed to run efficiently on edge devices.\n\nBase LLM: Quyen-SE-v0.1 (Qwen1.5-0.5B)\nVision Encoder: google/siglip-so400m-patch14-384\n\n\n\t\n\t\t\nModel\nVQA v2\nTextVQA\nScienceQA\nPOPE\nMMMU (Test)\nMMMU (Eval)\nGQA\nMM-VET\n\n\n\t\t\nScore\n70.84\n46.71\n58.97\n84.1\n28.6\n30.4\n54.79\n23.9\n\n\n\t\n\t\t\n\t\tTraining Data\n\t\n\nTraining Data will be released later as I am still writing aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/taiseimatsuoka/test-public.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","text","Image"],"keywords_longer_than_N":true},
	{"name":"MMT-Bench","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenGVLab/MMT-Bench","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for MMT-Bench\n\t\n\n\n\nRepository: https://github.com/OpenGVLab/MMT-Bench\nPaper: https://openreview.net/forum?id=R4Ng8zYaiz\nPoint of Contact: Wenqi Shao\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nLarge Vision-Language Models (LVLMs) show significant strides in general-propose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling shortâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMT-Bench.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"data-juicer-t2v-optimal-data-pool","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/data-juicer-t2v-optimal-data-pool","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\n\t\n\t\t\n\t\tData-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development\n\t\n\n\n\t\n\t\t\n\t\tProject description\n\t\n\nThe emergence of large-scale multi-modal generative models has drastically advanced artificial intelligence, introducing unprecedented levels of performance and functionality. \nHowever, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/data-juicer-t2v-optimal-data-pool.","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"pentomino-easy-vsft","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Koshti10/pentomino-easy-vsft","creator_name":"Koshti","creator_url":"https://huggingface.co/Koshti10","description":"\n\t\n\t\t\n\t\tIndividual Module\n\t\n\nThis dataset is created for instruction tuning llava models based on the dataset created here - llava-instruct-mix-vsft\nThis dataset is based on a Pentomino game - More details -> github\n","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"multi-modal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","description":"\n ğŸ¦„ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n\n\n\n      \n    [ArXiv] | [ğŸ¤—HuggingFace] | [Website]\n    \n    \n\n\nğŸŒŸ Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\n\t\n\t\tğŸ”¥News\n\t\n\n\nğŸ–ï¸ Our work is accepted by ACL2024.\n\nğŸ”¥ We have release benchmark on [ğŸ¤—HuggingFace].\n\nğŸ”¥ The paper is also available on [ArXiv].\n\nğŸ”® Interactive benchmark website & more exploration are available onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-sa-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"image-textualization","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sterzhang/image-textualization","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","description":"\n\t\n\t\t\n\t\tImage-Textualization Dataset\n\t\n\nExciting to announce the open-sourcing of our Image-Text Matching Dataset, which consists of 220K image-text pairs. We also release fine-grained annotations, which may be helpful for many downstream tasks.\nThis dataset is designed to facilitate research and development in the field of large mutimodal language model, particularly for tasks such as image captioning, visual question answering, and multimodal understanding.\nNote that our framework can beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/image-textualization.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"PIN-100M","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-a-p/PIN-100M","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","description":"\n\t\n\t\t\n\t\tPIN-100M\n\t\n\nThe full version of the dataset, related to the paper \"PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents\"\nPaper: https://arxiv.org/abs/2406.13923\nThis dataset contains 100M samples with PIN format.\nPlease note that the required storage space exceeds 150TB!!\nğŸš€ News\n[ 2024.12.20 ] !NEW! ğŸ”¥The currently available version is not the complete version; this project is still ongoing! (It has been released early because we reached the privateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/PIN-100M.","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"PIN-14M","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-a-p/PIN-14M","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","description":"\n\t\n\t\t\n\t\tPIN-14M\n\t\n\nA mini version of \"PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents\"\nPaper: https://arxiv.org/abs/2406.13923\nThis dataset contains 14M samples in PIN format, with at least 7.33B tokens.\nğŸš€ News\n[ 2024.12.12 ] !NEW! ğŸ”¥ We have updated the quality signals for all subsets, with the dataset now containing 7.33B tokens after Llama3 tokenization.\n[ 2024.12.06 ] !NEW! ğŸ”¥ We have updated the quality signals, enabling a swift assessment of whether aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/PIN-14M.","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"vwp","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tonyhong/vwp","creator_name":"Xudong Hong","creator_url":"https://huggingface.co/tonyhong","description":"\n\t\n\t\t\n\t\tDataset Card for Visual Writing Prompts Dataset (VWP)\n\t\n\nWebsite | Github Repository | arXiv e-Print\n\n\nThe Visual Writing Prompts (VWP) dataset contains almost 2K selected sequences of\nmovie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which are collected via crowdsourcing given the image sequences and up to 5  grounded characters from the corresponding image sequence.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\n\nTACLâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tonyhong/vwp.","first_N":5,"first_N_keywords":["image-to-text","text-generation","monolingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"vwp","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tonyhong/vwp","creator_name":"Xudong Hong","creator_url":"https://huggingface.co/tonyhong","description":"\n\t\n\t\t\n\t\tDataset Card for Visual Writing Prompts Dataset (VWP)\n\t\n\nWebsite | Github Repository | arXiv e-Print\n\n\nThe Visual Writing Prompts (VWP) dataset contains almost 2K selected sequences of\nmovie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which are collected via crowdsourcing given the image sequences and up to 5  grounded characters from the corresponding image sequence.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\n\nTACLâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tonyhong/vwp.","first_N":5,"first_N_keywords":["image-to-text","text-generation","monolingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"table-vqa","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cmarkea/table-vqa","creator_name":"Credit Mutuel Arkea","creator_url":"https://huggingface.co/cmarkea","description":"\n\t\n\t\t\n\t\tDataset description\n\t\n\nThe table-vqa Dataset integrates images of tables from the dataset AFTdb (Arxiv Figure Table Database) curated by cmarkea. \nThis dataset consists of pairs of table images and corresponding LaTeX source code, with each image linked to an average of ten questions and answers. Half of the Q&A pairs are in English and the other half in French. These questions and answers were generated using Gemini 1.5 Pro and Claude 3.5 sonnet, making the dataset well-suited forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cmarkea/table-vqa.","first_N":5,"first_N_keywords":["text-generation","text-to-image","image-to-text","table-question-answering","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisualWebInstruct-Recall","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Recall","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is the dataset recalled from Google Search from the seed images.\n\n\t\n\t\t\n\t\tLinks\n\t\n\nGithub|\nPaper|\nWebsite\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{visualwebinstruct,\n    title={VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},\n    author = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},\n    journal={arXiv preprint arXiv:2503.10582},\n    year={2025}\n}\n\n","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Iranian_olympiad_of_informatics_multimodal_questions","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ckodser/Iranian_olympiad_of_informatics_multimodal_questions","creator_name":"Arshia Soltani Moakhar","creator_url":"https://huggingface.co/ckodser","description":"ckodser/Iranian_olympiad_of_informatics_multimodal_questions dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"pokemon-gpt4o-captions","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llamafactory/pokemon-gpt4o-captions","creator_name":"LLaMA Factory","creator_url":"https://huggingface.co/llamafactory","description":"Borrowed from: https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions\nYou can use it in LLaMA Factory by specifying dataset: pokemon_cap.\n","first_N":5,"first_N_keywords":["text-generation","question-answering","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"blip3-ocr-200m","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/blip3-ocr-200m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n\t\n\t\t\n\t\tBLIP3-OCR-200M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-OCR-200M dataset is designed to address the limitations of current Vision-Language Models (VLMs) in processing and interpreting text-rich images, such as documents and charts. Traditional image-text datasets often struggle to capture nuanced textual information, which is crucial for tasks requiring complex text comprehension and reasoning. \n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nOCR Integration: The dataset incorporates Optical Characterâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-ocr-200m.","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-10M","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\n\t\n\t\tHumanCaption-10M\n\t\n\nHumanCaption-10M: a large, diverse, high-quality dataset of human-related images with natural language descriptions (image to text). The dataset is designed to facilitate research on human-centered tasks. HumanCaption-10M contains approximately 10 million human-related images and their corresponding facial features in natural language descriptions and is the second generation version of FaceCaption-15M \n\n\t\n\t\t\n\t\n\t\n\t\tIllustrations\n\t\n\n\nPiplines of constructingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"MultiBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/juliusbroomfield/MultiBench","creator_name":"Julius Broomfield","creator_url":"https://huggingface.co/juliusbroomfield","description":"\n\t\n\t\t\n\t\tMultiBench: Safety Evaluation Benchmark for Vision-Language Models\n\t\n\nLarge language models have been extensively studied for their vulnerabilities, particularly in the context of adversarial attacks. \nHowever, the emergence of Vision Language Models introduces new modalities of risk that have not yet been thoroughly explored, \nespecially when processing multiple images simultaneously. To address this, we present a new safety evaluation dataset for multimodal LLMs called MultiBenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/juliusbroomfield/MultiBench.","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"marqo-GS-10M","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Marqo/marqo-GS-10M","creator_name":"Marqo","creator_url":"https://huggingface.co/Marqo","description":"\n  \n    \n  \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tMarqo-GS-10M\n\t\n\nThis dataset is our multimodal, fine-grained, ranking Google Shopping dataset, Marqo-GS-10M, followed by our novel training framework: Generalized Contrastive Learning (GCL). GCL aims to improve and measure the ranking performance of information retrieval models, \nespecially for retrieving relevant products given a search query.\nBlog post:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Marqo/marqo-GS-10M.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"YGD-mix","keyword":"multimodal","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/alibabasglab/YGD-mix","creator_name":"Alibaba_Speech_Lab_SG","creator_url":"https://huggingface.co/alibabasglab","description":"A modified version of the Youtube Gesture Dataset. Original data can be downloaded and processed here and here. \nThis dataset is used for Audio-visual speaker extraction conditioned on body gestures in the SEG paper, which the code can be found here. \n","first_N":5,"first_N_keywords":["bsd-3-clause","< 1K","text","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"PVIT-3M","keyword":"multi-modal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sterzhang/PVIT-3M","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","description":"\n\t\n\t\t\n\t\tPVIT-3M\n\t\n\nThe paper titled \"Personalized Visual Instruction Tuning\" introduces a novel dataset called PVIT-3M. This dataset is specifically designed for tuning MLLMs in the context of personalized visual instruction tasks. The dataset consists of 3 million image-text pairs that aim to improve MLLMs' abilities to generate responses based on personalized visual inputs, making them more tailored and adaptable to individual user needs and preferences.\nHereâ€™s the PVIT-3M statistics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/PVIT-3M.","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Eval_Synthetic","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Synthetic","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nğŸŒ Project Website | ğŸ“„ Paper | ğŸ¤— Dataset | ğŸ”¥ VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Synthetic.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Train","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Train","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nğŸŒ Project Website | ğŸ“„ Paper | ğŸ¤— Dataset | ğŸ”¥ VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Train.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"MMToM-QA","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Chuanyang-Jin/MMToM-QA","creator_name":"Chuanyang Jin","creator_url":"https://huggingface.co/Chuanyang-Jin","description":"\n\t\n\t\t\n\t\tMMToM-QA: Multimodal Theory of Mind Question Answering  ğŸ† Outstanding Paper Award at ACL 2024\n\t\n\n[ğŸ Homepage] [ğŸ’»Code] [ğŸ“Paper]\nMMToM-QA is the first multimodal benchmark to evaluate machine Theory of Mind (ToM), the ability to understand people's minds.\nIt systematically evaluates Theory of Mind both on multimodal data and different unimodal data. \nMMToM-QA consists of 600 questions. \nThe questions are categorized into seven types, evaluating belief inference and goal inference inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Chuanyang-Jin/MMToM-QA.","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Electrohydrodynamics","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Taylor658/Electrohydrodynamics","creator_name":"atayloraerospace","creator_url":"https://huggingface.co/Taylor658","description":"\n\t\n\t\t\n\t\tElectrohydrodynamics in Hall Effect Thrusters Dataset for Mistral-Large-Instruct-2411 Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of 6,000 high fidelity training instances tailored for fine-tuning the Mistral-Large-Instruct-2411 foundation model. It captures theoretical, computational, and experimental aspects of electrohydrodynamics in Hall Effect Thrusters. \n\n\t\n\t\t\n\t\tKey Features:\n\t\n\n\nMultimodal elements: Includes LaTeX equations, code snippets, textualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Taylor658/Electrohydrodynamics.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"genshin-impact-outfits","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/genshin-impact-outfits","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tGenshin Impact Outfit\n\t\n\nThis is a collection of Genshin Impact character outfits (both wish and in-game version), with outfit description and detailed appearance, parsed from Fandom Wiki.\n","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"muchomusic","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mulab-mir/muchomusic","creator_name":"mulab-mir","creator_url":"https://huggingface.co/mulab-mir","description":"\n\t\n\t\t\n\t\tMuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models\n\t\n\nMuChoMusic is a benchmark designed to evaluate music understanding in multimodal language models focused on audio. It includes 1,187 multiple-choice questions validated by human annotators, based on 644 music tracks from two publicly available music datasets. These questions cover a wide variety of genres and assess knowledge and reasoning across several musical concepts and their cultural and functionalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mulab-mir/muchomusic.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","1K - 10K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"ColonINST-v1","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai4colonoscopy/ColonINST-v1","creator_name":"ai4colonoscopy","creator_url":"https://huggingface.co/ai4colonoscopy","description":"\n\t\n\t\t\n\t\tColonINST-v1 Data Card\n\t\n\nA large-scale mutlimodal instruction tuning dataset for colonoscopy research. More details refer to our project page: https://github.com/ai4colonoscopy/ColonGPT.\n\n\n\t\n\t\t\n\t\n\t\n\t\tData description\n\t\n\nWe introduce a pioneering instruction tuning dataset for multimodal colonoscopy research, aimed at instructing models to execute user-driven tasks interactively. This dataset comprises of 62 categories, 300K+ colonoscopic images, 128K+ medical captions (GPT-4V)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai4colonoscopy/ColonINST-v1.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"NL-Eye","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MorVentura/NL-Eye","creator_name":"Mor Ventura","creator_url":"https://huggingface.co/MorVentura","description":"\n\t\n\t\t\n\t\tNL-Eye Benchmark\n\t\n\nWill a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? \nRecent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. \nNL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MorVentura/NL-Eye.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"RoboMatrix","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WayneMao/RoboMatrix","creator_name":"WayneMao","creator_url":"https://huggingface.co/WayneMao","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis is the fine-tuning dataset used in the paper RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World.\n\n\t\n\t\t\n\t\tSource\n\t\n\nProject Page: https://robo-matrix.github.io/Paper: https://arxiv.org/abs/2412.00171Code: https://github.com/WayneMao/RoboMatrixModel: \n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you find our work helpful, please cite us:\n@article{mao2024robomatrix,\n  title={RoboMatrix: A Skill-centric Hierarchical Framework forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WayneMao/RoboMatrix.","first_N":5,"first_N_keywords":["mit","1K - 10K","Datasets","Croissant","arxiv:2412.00171"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"multi-modal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","description":"\n\n\nğŸš€ Project Page | ğŸ“œ arXiv | ğŸ‘¨â€ğŸ’» Github | ğŸ† Leaderboard | ğŸ“ blog | ğŸ¤— HF Paper | ğ• Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizationsâ€™ flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"ChineseBQB","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/ChineseBQB","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tChinese BQB\n\t\n\nThis is a data reupload of the repository zhaoolee/ChineseBQB, containing 5k+ Chinese stickers\nä¸­æ–‡è¡¨æƒ…åŒ…æ•°æ®ï¼Œæ¥è‡ªäºzhaoolee/ChineseBQB\n\n","first_N":5,"first_N_keywords":["Chinese","apache-2.0","1K - 10K","arrow","Image"],"keywords_longer_than_N":true},
	{"name":"llava-critic-113k","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lmms-lab/llava-critic-113k","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","description":"\n\t\n\t\t\n\t\tDataset Card for LLaVA-Critic-113k\n\t\n\n\nğŸª Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic/\nğŸ“° Paper: https://arxiv.org/abs/2410.02712\nğŸ¤— Huggingface Collection: https://huggingface.co/collections/lmms-lab/llava-critic-66fe3ef8c6e586d8435b4af8\nğŸ‘‹ Point of Contact: Tianyi Xiong\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nLLaVA-Critic-113k is a high quality critic instruction-following dataset tailored to follow instructions in complex evaluation setting, providing bothâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/llava-critic-113k.","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"LayoutSAM-eval","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","description":"\n\t\n\t\t\n\t\tLayoutSAM-eval Benchmark\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nLayoutSAM-Eval is a comprehensive benchmark for evaluating the quality of Layout-to-Image (L2I) generation models. This benchmark assesses L2I generation quality from two perspectives: region-wise quality (spatial and attribute accuracy) and global-wise quality (visual quality and prompt following). It employs the VLMâ€™s visual question answering to evaluate spatial and attribute adherence, and utilizes various metrics including IR scoreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"VMCBench","keyword":"multi-modal-qa","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/suyc21/VMCBench","creator_name":"Yuchang Su","creator_url":"https://huggingface.co/suyc21","description":"\n\t\n\t\t\n\t\tVMCBench (Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation)\n\t\n\nğŸŒ Homepage | ğŸ¤— Dataset | ğŸ“– arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Introduction\n\t\n\nWe introduce VMCBench: a benchmark that unifies 20 existing visual question answering (VQA) datasets into a consistent multiple-choice format. VMCBench spans a diverse array of visual and linguistic contexts, rigorously testing various model capabilities. Byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/suyc21/VMCBench.","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"banque_vision","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MatteoKhan/banque_vision","creator_name":"Khan Matteo","creator_url":"https://huggingface.co/MatteoKhan","description":"\n\t\n\t\t\n\t\tğŸ“Š Banque_Vision: A Multimodal Dataset for Document Understanding\n\t\n\n\n\t\n\t\t\n\t\tğŸ“Œ Overview\n\t\n\nBanque_Vision is a multimodal dataset designed for document-based question answering (QA) and information retrieval. It combines textual data and visual document representations, enabling research on how vision models and language models interact for document comprehension.\nğŸ”— Created by: Matteo KhanğŸ“ Affiliation: TW3Partners \nğŸ“ License: MIT  \nğŸ”— Connect with me on LinkedInğŸ”— Dataset onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MatteoKhan/banque_vision.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"mmE5-Synthetic","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mmE5/mmE5-Synthetic","creator_name":"mmE5","creator_url":"https://huggingface.co/mmE5","description":"\n\t\n\t\t\n\t\tmmE5 Synthetic Data\n\t\n\n","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Eval_Real_v1.1","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real_v1.1","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nğŸŒ Project Website | ğŸ“„ Paper | ğŸ¤— Dataset | ğŸ”¥ VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real_v1.1.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisQuant","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Anas-Mohiuddin-Syed/VisQuant","creator_name":"Syed Anas Mohiuddin","creator_url":"https://huggingface.co/Anas-Mohiuddin-Syed","description":"\nlicense: cc-by-4.0\ndatasets:\n\nvisquant\nlanguage:\nen\ntags:\nvisual-question-answering\nobject-counting\nspatial-reasoning\nsynthetic\nmultimodal\nbenchmark\n\n\n\t\n\t\t\n\t\tVisQuant: A Synthetic Benchmark for Object Counting and Spatial Reasoning\n\t\n\nVisQuant is a synthetic dataset of 100 annotated image scenarios, purpose-built to evaluate AI systems on object counting, spatial layout understanding, and visual question answering (VQA).\nThis dataset is ideal for benchmarking vision-language models (e.g.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Anas-Mohiuddin-Syed/VisQuant.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Emirates_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/Emirates_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tEMIRATES-AIRWAYS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical, financial, and sustainability queries generated from Emirates Airways annual and sustainability reports. It is designed to train and evaluate information retrieval models and improve AI understanding of aviation industry documentation, with a specific focus on airline operations, sustainability initiatives, and international business strategies.\n\n\t\n\t\t\n\t\n\t\n\t\tAbout Me\n\t\n\nI'm Davidâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/Emirates_dataset.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"DRGBT603","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zhaodong2061/DRGBT603","creator_name":"zhaodongding","creator_url":"https://huggingface.co/zhaodong2061","description":"zhaodong2061/DRGBT603 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","apache-2.0","100K<n<1M","Image","doi:10.57967/hf/5438"],"keywords_longer_than_N":true},
	{"name":"Mantis-Instruct","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tMantis-Instruct\n\t\n\nPaper | Website | Github | Models | Demo\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nMantis-Instruct is a fully text-image interleaved multimodal instruction tuning dataset, \ncontaining 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding. \nIt's been used to train Mantis Model families\n\nMantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\nAmong the 14â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MAGB","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sherirto/MAGB","creator_name":"Sherirto","creator_url":"https://huggingface.co/Sherirto","description":"\n\t\n\t\t\n\t\tMAGBï¼š A Comprehensive Benchmark for Multimodal Attributed Graphs\n\t\n\nIn many real-world scenarios, graph nodes are associated with multimodal attributes, such as texts and images, resulting in Multimodal Attributed Graphs (MAGs).\nMAGB first provide 5 dataset from E-Commerce and Social Networks. And we evaluate two major paradigms: GNN-as Predictor and VLM-as-Predictor . The datasets are publicly available:\n\n     ğŸ¤— Hugging FaceÂ Â   | Â Â ğŸ“‘ PaperÂ Â \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tğŸ“– Table of Contents\n\t\n\n\nğŸ“–â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sherirto/MAGB.","first_N":5,"first_N_keywords":["graph-ml","cc-by-4.0","Image","arxiv:2410.09132","ğŸ‡ºğŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"llava-pretrain-refined-by-data-juicer","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\n\t\n\t\t\n\t\tLLaVA pretrain -- LCS-558k (refined by Data-Juicer)\n\t\n\nA refined version of LLaVA pretrain dataset (LCS-558k) by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Multimodal Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 115MB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 500,380 (Keep ~89.65% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"DEEPFRUlT_DATASET","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sc890/DEEPFRUlT_DATASET","creator_name":"shangrong chi","creator_url":"https://huggingface.co/sc890","description":"\n\t\n\t\t\n\t\tDeepFruit Dataset\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis dataset contains total of 21,122 fully labeled images, featuring 20 different kinds of fruits. It is structured into an 80% training set (16,899 images) and a 20% testing set (4,223 images), facilitating a ready-to-use framework for model training and evaluation.\nAdditionally, there are two CSV files that label the types of fruits depicted in each image.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe \"DeepFruit\" dataset is a comprehensiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sc890/DEEPFRUlT_DATASET.","first_N":5,"first_N_keywords":["feature-extraction","text-classification","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MultiCaRe_Dataset","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset","creator_name":"Mauro Nievas Offidani","creator_url":"https://huggingface.co/mauro-nievoff","description":"The dataset contains multi-modal data from over 75,000 open access and de-identified case reports, including metadata, clinical cases, image captions and more than 130,000 images. Images and clinical cases belong to different medical specialties, such as oncology, cardiology, surgery and pathology. The structure of the dataset allows to easily map images with their corresponding article metadata, clinical case, captions and image labels. Details of the data structure can be found in the fileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MARVEL","keyword":"multi-modal-qa","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kianasun/MARVEL","creator_name":"Kiana Sun","creator_url":"https://huggingface.co/kianasun","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMARVEL is a new comprehensive benchmark dataset that evaluates multi-modal large language models' abstract reasoning abilities in six patterns across five different task configurations, revealing significant performance gaps between humans and SoTA MLLMs.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]\n\t\n\n\nRepository: https://github.com/1171-jpg/MARVEL_AVR\nPaper [optional]: https://arxiv.org/abs/2404.13591\nDemo [optional]:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/kianasun/MARVEL.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"airbnb_embeddings","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MongoDB/airbnb_embeddings","creator_name":"MongoDB","creator_url":"https://huggingface.co/MongoDB","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset consists of AirBnB listings with property descriptions, reviews, and other metadata. \nIt also contains text embeddings of the property descriptions as well as image embeddings of the listing image. The text embeddings were created using OpenAI's text-embedding-3-small model and the image embeddings using OpenAI's clip-vit-base-patch32 model available on Hugging Face. \nThe text embeddings have 1536 dimensions, while the image embeddings have 512 dimensions.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MongoDB/airbnb_embeddings.","first_N":5,"first_N_keywords":["question-answering","text-retrieval","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"VL-ICL","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ys-zong/VL-ICL","creator_name":"Yongshuo Zong","creator_url":"https://huggingface.co/ys-zong","description":"\n\t\n\t\t\n\t\tVL-ICL Bench\n\t\n\nVL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning\n[Webpage] [Paper] [Code]\n\n\t\n\t\t\n\t\tImage-to-Text Tasks\n\t\n\nIn all image-to-text tasks image is a list of image paths (typically one item - for interleaved cases there are two items).\n\n\t\n\t\t\n\t\tFast Open-Ended MiniImageNet\n\t\n\nFrozen introduces the task of fast concept binding for MiniImageNet. The benchmark has a fixed structure so only the given support examples can be used for a givenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ys-zong/VL-ICL.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","mit","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-ArXiv","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-ArXiv","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  ğŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nğŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ğŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ğŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-ArXiv.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"unusual-objects-unusual-places_text-image","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zer0int/unusual-objects-unusual-places_text-image","creator_name":"zer0int","creator_url":"https://huggingface.co/zer0int","description":"\n\t\n\t\t\n\t\t(Un-)usual objects in (un-)usual places\n\t\n\n\n\t\n\t\t\n\t\tA small Text-Image dataset to confuse, probe (and improve) SOTA (2024) machine vision models.\n\t\n\nTo be continued (with further examples added)...\nExample results from LMSYS ARENA (June 2024):\n\n\n","first_N":5,"first_N_keywords":["English","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"CGTSF","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChengsenWang/CGTSF","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","description":"\n\t\n\t\t\n\t\tCGTSF: Context-Guided Time Series Forecasting\n\t\n\n\n\t\n\t\t\n\t\tâœ¨ Introduction\n\t\n\nThe context-guided time series forecasting task entails the transformation of text into time series data. Relevant multimodal datasets are limited. To address these data gaps, we have collected three multimodal datasets that offer valuable resources for future research. The following table summarizes the statistics of these datasets. MSPG comprises 13 months of solar power generation data on 27 photovoltaicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/CGTSF.","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"TSQA","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChengsenWang/TSQA","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","description":"\n\t\n\t\t\n\t\tTSQA: Time Series Question Answering\n\t\n\n\n\t\n\t\t\n\t\tâœ¨ Introduction\n\t\n\nIn the time series question answering task, we employ the KernelSynth to generate a variable-length multimodal question and answer pairs based on identifying four generic typical time series features, which aid ChatTime in comprehending the fundamental principles of time series. The following table summarizes the statistics of this dataset. Trend encompasses three categories: upward trend, downward trend, and constantâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/TSQA.","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"ChatTime-1-Pretrain-1M","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Pretrain-1M","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","description":"\n\t\n\t\t\n\t\tChatTime: A Multimodal Time Series Foundation Model\n\t\n\n\n\t\n\t\t\n\t\tâœ¨ Introduction\n\t\n\nIn this paper, we innovatively model time series as a foreign language and construct ChatTime, a unified framework for time series and text processing. As an out-of-the-box multimodal time series foundation model, ChatTime provides zero-shot forecasting capability and supports bimodal input/output for both time series and text. We design a series of experiments to verify the superior performance ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Pretrain-1M.","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"ChatTime-1-Finetune-100K","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Finetune-100K","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","description":"\n\t\n\t\t\n\t\tChatTime: A Multimodal Time Series Foundation Model\n\t\n\n\n\t\n\t\t\n\t\tâœ¨ Introduction\n\t\n\nIn this paper, we innovatively model time series as a foreign language and construct ChatTime, a unified framework for time series and text processing. As an out-of-the-box multimodal time series foundation model, ChatTime provides zero-shot forecasting capability and supports bimodal input/output for both time series and text. We design a series of experiments to verify the superior performance ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Finetune-100K.","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"MMC","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xywang1/MMC","creator_name":"Xiaoyang Wang","creator_url":"https://huggingface.co/xywang1","description":"\n\t\n\t\t\n\t\tMMC: Advancing Multimodal Chart Understanding with LLM Instruction Tuning\n\t\n\nThis repo releases data introduced in our paper MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning.\n\nThe paper was published in NAACL 2024.\nSee our GithHub repo for demo code and more.\n\n\n\t\n\t\t\n\t\n\t\n\t\tHighlights\n\t\n\n\nWe introduce a large-scale MultiModal Chart Instruction (MMC-Instruction) dataset supporting diverse tasks and chart types. Leveraging this data.\nWe also propose aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xywang1/MMC.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-sa-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Set_Eval","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AarushSah/Set_Eval","creator_name":"Aarush Sah","creator_url":"https://huggingface.co/AarushSah","description":"AarushSah/Set_Eval dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Chinese_interactive_novels_3k","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/Chinese_interactive_novels_3k","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tä¸­æ–‡äº’åŠ¨å°è¯´ç»“æ„åŒ–è¯­æ–™\n\t\n\nThis dataset contains uncleaned (!) 3534 structured Chinese interactive novels (ä¸­æ–‡äº’åŠ¨å°è¯´), accounting for around 0.25B (gpt-3.5) tokens in total.\nAll contents are parsed from certain online sources.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis dataset can be potentially used for LLM training. But be aware that you'd better clean the data yourself to remove undesired low-quality contents.\nEach novel is a dict structured as follows:\nclass Novel:\n    book_title: str\n    book_author: strâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/Chinese_interactive_novels_3k.","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1K - 10K","arrow"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2024-10","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-10","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  ğŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nğŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ğŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ğŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-10.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-50","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-50","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  ğŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nğŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ğŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ğŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-50.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-40","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  ğŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nğŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ğŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ğŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-23","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-23","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  ğŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nğŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ğŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ğŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-23.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-14","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-14","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  ğŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nğŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ğŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ğŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-14.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-06","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-06","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  ğŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nğŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ğŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ğŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-06.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"tabfquad_test_subsampled","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/tabfquad_test_subsampled","creator_name":"Illuin Technology - Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings. Using a vision language model (GPT4V), we create additional queries to augment the existing human-annotated ones.\n\n\t\n\t\t\n\t\tData Curation\n\t\n\nTo ensure homogeneity across our benchmarked datasets, we subsampled the original test set to 280 pairs, leaving the rest for training and renaming the different columns.\n\n\t\n\t\t\n\t\tLoad the dataset\n\t\n\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/tabfquad_test_subsampled.","first_N":5,"first_N_keywords":["table-question-answering","visual-question-answering","French","English","mit"],"keywords_longer_than_N":true},
	{"name":"docvqa_test_subsampled","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/docvqa_test_subsampled","creator_name":"Illuin Technology - Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the test set taken from the DocVQA dataset. It includes collected images from the UCSF Industry Documents Library. Questions and answers were manually annotated.\nExample of data (see viewer)\n\n\t\n\t\t\n\t\tData Curation\n\t\n\nTo ensure homogeneity across our benchmarked datasets, we subsampled the original test set to 500 pairs and renamed the different columns.\n\n\t\n\t\t\n\t\tLoad the dataset\n\t\n\nfrom datasets import load_dataset\nds =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/docvqa_test_subsampled.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"II-Bench","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-a-p/II-Bench","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","description":"\n\t\n\t\t\n\t\tII-Bench\n\t\n\nğŸŒ Homepage | ğŸ¤— Paper | ğŸ“– arXiv | ğŸ¤— Dataset | GitHub\n\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nII-Bench comprises 1,222 images, each accompanied by 1 to 3 multiple-choice questions, totaling 1,434 questions. II-Bench encompasses images from six distinct domains: Life, Art, Society, Psychology, Environment and Others. It also features a diverse array of image types, including Illustrations, Memes, Posters, Multi-panel Comics, Single-panel Comics, Logos and Paintings. The detailedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/II-Bench.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_energy_test","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/syntheticDocQA_energy_test","creator_name":"Illuin Technology - Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about Energy that allow ViDoRe to benchmark technical documentation about energy. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('energy'). From these documents, we randomly sampled 1000 pages.\nWe associated theseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_energy_test.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","visual-document-retrieval","English","mit"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_healthcare_industry_test","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/syntheticDocQA_healthcare_industry_test","creator_name":"Illuin Technology - Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about the Healthcare Industry that allow ViDoRe to benchmark medical documents. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('healthcare industry'). From these documents, we randomly sampled 1000 pages.\nWeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_healthcare_industry_test.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_government_reports_test","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/syntheticDocQA_government_reports_test","creator_name":"Illuin Technology - Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about the Government Reports that allow ViDoRe to benchmark administrative/legal documents. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('government reports'). From these documents, we randomly sampled 1000 pages.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_government_reports_test.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_artificial_intelligence_test","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/syntheticDocQA_artificial_intelligence_test","creator_name":"Illuin Technology - Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about the Artificial Intelligence. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('artificial intelligence'). From these documents, we randomly sampled 1000 pages.\nWe associated these with 100 questions and answersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_artificial_intelligence_test.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"M3GIA","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Songweii/M3GIA","creator_name":"Wei Song","creator_url":"https://huggingface.co/Songweii","description":"\n\t\n\t\t\n\t\tM3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability\n\t\n\n[ğŸŒ Homepage] | ğŸ¤— Dataset | ğŸ¤— Paper | ğŸ“– arXiv | ğŸ’» GitHub\nThe evaluation code can be found in ğŸ’» GitHub.\n[Abstract]\nAs recent multi-modality large language models (MLLMs) have shown formidable proficiency on various complex tasks, there has been increasing attention on debating whether these models could eventually mirror human intelligence. However, existing benchmarks mainly focus on evaluatingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Songweii/M3GIA.","first_N":5,"first_N_keywords":["English","Chinese","Spanish","French","Portuguese"],"keywords_longer_than_N":true},
	{"name":"ChartMimic","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChartMimic/ChartMimic","creator_name":"ChartMimic","creator_url":"https://huggingface.co/ChartMimic","description":"\n\n ChartMimic: Evaluating LMMâ€™s Cross-Modal Reasoning Capability via Chart-to-Code Generation \n\n\nThis is the official dataset repository of ChartMimic. \n\n\t\n\t\t\n\t\t1. Data Overview\n\t\n\nChartMimic aims at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.\nChartMimic includes 1000 human-curatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChartMimic/ChartMimic.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"CoMDataset","keyword":"multi-modal-qa","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/qijimrc/CoMDataset","creator_name":"Ji Qi","creator_url":"https://huggingface.co/qijimrc","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWe open-source both the Automatically Synthesized CoM Data and the Manually Annotated CoM-Math Data to facilitate potential research. The automatically synthesized CoM data (i.e., com.jsonl) consists of 84K positive reasoning chains, which was produced by an automated data generation pipeline with an LLM-based (GPT-4) linguistic solving steps generation and a VFMs-based (GroundingDINO, PaddleOCR) visual evidence compensation upon massive public VQA samples. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/qijimrc/CoMDataset.","first_N":5,"first_N_keywords":["visual-question-answering","visual-question-answering","expert-generated","found","expert-generated"],"keywords_longer_than_N":true},
	{"name":"nesteo-prototype","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nesteo-datasets/nesteo-prototype","creator_name":"NestEO Datasets","creator_url":"https://huggingface.co/nesteo-datasets","description":"\n\t\n\t\t\n\t\tNestEO: Modular and Hierarchical EO Dataset Framework\n\t\n\nNestEO is a hierarchical, resolution-aligned, UTM-based nested grid dataset framework supporting general-purpose, multi-scale multimodal Earth Observation workflows. Built from diverse EO sources and enriched with metadata for landcover, climate zones, and population, it enables scalable, representative and progressive sampling for AI4EO.\nGrid Levels: 120000m, 12000m, 2400m, 1200m, 600m, 300m, 150mGrid Metadata: ESA WorldCoverâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nesteo-datasets/nesteo-prototype.","first_N":5,"first_N_keywords":["image-segmentation","image-classification","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"VCRBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pritamqu/VCRBench","creator_name":"Pritam Sarkar","creator_url":"https://huggingface.co/pritamqu","description":"\n\t\n\t\t\n\t\tVCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models\n\t\n\n \n \n \n \n \nAuthors: Pritam Sarkar and Ali Etemad\nThis repository provides the official implementation of VCRBench.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nPlease check our GitHub repo for the details of usage: VCRBench\nfrom dataset import VCRBench\ndataset=VCRBench(question_file=\"data.json\", \n                video_root=\"./\",\n                mode='default', \n                )\n    \nfor sample in dataset:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pritamqu/VCRBench.","first_N":5,"first_N_keywords":["video-text-to-text","visual-question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"CHASM-Covert_Advertisement_on_RedNote","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Jingyi77/CHASM-Covert_Advertisement_on_RedNote","creator_name":"Jingyi","creator_url":"https://huggingface.co/Jingyi77","description":"\n\t\n\t\t\n\t\tRedNote Covert Advertisement Detection Dataset\n\t\n\nThis dataset contains posts from the RedNote platform for covert advertisement detection tasks.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\nSplit\nPosts\nAd Posts\nNon-Ad Posts\nTotal Images\n\n\n\t\t\nTrain\n3493\n426\n3067\n18543\n\n\nValidation\n499\n57\n442\n2678\n\n\nTest\n1000\n130\n870\n5103\n\n\nTotal\n4992\n613\n4379\n26324\n\n\n\t\n\n\nNote: The viewer shows a small example subset of the data (60 samples) for demonstration purposes. The complete dataset is available viaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jingyi77/CHASM-Covert_Advertisement_on_RedNote.","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MARBLE","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrble/MARBLE","creator_name":"marble","creator_url":"https://huggingface.co/mrble","description":"mrble/MARBLE dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"SeePhys","keyword":"multi-modal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SeePhys/SeePhys","creator_name":"AI4Science","creator_url":"https://huggingface.co/SeePhys","description":"\n\t\n\t\t\n\t\tSeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning\n\t\n\nCan AI truly see the Physics? Test your model with the newly released SeePhys Benchmark!\nCovering 2,000 vision-text multimodal physics problems spanning from middle school to doctoral qualification exams, the SeePhys benchmark systematically evaluates LLMs/MLLMs on tasks integrating complex scientific diagrams with theoretical derivations. Experiments reveal that even SOTA models like Gemini-2.5-Proâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SeePhys/SeePhys.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"NOAH-mini","keyword":"multi-modal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mutakabbirCarleton/NOAH-mini","creator_name":"Mutakabbir","creator_url":"https://huggingface.co/mutakabbirCarleton","description":"\n\t\n\t\t\n\t\tMOAH mini\n\t\n\nThe dataset prest here is a very samll sample of NOAH dataset.\nIn the original dataset each satellite image is ~650MB with 234,089 images present in 11 bands.\nIt is not feasible to upload the complete dataset. \nA sample of the dataset across diffrent modalities can be seen in the figure below:\n\nThe diffrence between NOAH and NOAH mini is hilighted in the figure below.\nEach subplot is a band of Landsat 8 in NOAH.\nThe region hilighted in red is the region available in NOAHâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mutakabbirCarleton/NOAH-mini.","first_N":5,"first_N_keywords":["image-to-image","mit","< 1K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"vpi-bench","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VPI-Bench/vpi-bench","creator_name":"VPI Bench","creator_url":"https://huggingface.co/VPI-Bench","description":"\n\t\n\t\t\n\t\tDataset Card for VPI-Bench\n\t\n\n\n\n\nVPI-Bench is a benchmark dataset of testcases and web platforms used to evaluate the robustness of computer-use and browser-use agents under visual prompt injection attacks.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\nLanguage(s) (NLP): English\nLicense: Creative Commons Attribution 4.0\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\n\n\nRepository: VPI-Bench\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\n\n\t\n\t\t\n\t\tDirect Use\n\t\n\n\n\n\nBenchmarking the Attempted Rate (AR) and Success Rateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VPI-Bench/vpi-bench.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nUnite-Base-Retrieval-Train contains 12 subsets, each contains a metadata.json and images/videos folder (if available), organized as follows:\nUnite-Base-Retrieval-Train\nâ”œâ”€â”€ FEVER\nâ”‚   â”œâ”€â”€ metadata.json\n...\nâ”œâ”€â”€ TriviaQA\nâ”‚   â”œâ”€â”€ metadata.json\nâ”œâ”€â”€ CapsFusion\nâ”‚   â”œâ”€â”€ images\nâ”‚   â”‚   â”œâ”€â”€ ...\nâ”‚   â”œâ”€â”€ metadata.json\nâ”œâ”€â”€ LAION-Art\nâ”‚   â”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"MixBench25","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mixed-modality-search/MixBench25","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench25.","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"MM-IQ","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huanqia/MM-IQ","creator_name":"huanqiacai","creator_url":"https://huggingface.co/huanqia","description":"\n\t\n\t\t\n\t\tDataset Card for \"MM-IQ\"\n\t\n\n\nIntroduction\nPaper Information\nDataset Examples\nLeaderboard\nDataset Usage\nData Downloading\nData Format\nAutomatic Evaluation\n\n\nCitation\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nIQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/huanqia/MM-IQ.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"UnLOK-VQA","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vaidehi99/UnLOK-VQA","creator_name":"Vaidehi Patil","creator_url":"https://huggingface.co/vaidehi99","description":"\n\t\n\t\t\n\t\tğŸ“Š Dataset: UnLOK-VQA (Unlearning Outside Knowledge VQA)\n\t\n\nPaper: Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation\nCode: https://github.com/Vaidehi99/mmmedit\nLink: Dataset Link\nThis dataset contains approximately 500 entries with the following key attributes:\n\n\"id\": Unique Identifier for each entry\n\"src\": The question whose answer is to be deleted â“\n\"pred\": The answer to the question meant for deletion âŒ\n\"loc\": Related neighborhood questionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vaidehi99/UnLOK-VQA.","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"XTD-10","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Haon-Chen/XTD-10","creator_name":"Haonan Chen","creator_url":"https://huggingface.co/Haon-Chen","description":"\n\t\n\t\t\n\t\tXTD Multimodal Multilingual Data With Instruction\n\t\n\nThis dataset contains datasets (with English instruction) used for evaluating the multilingual capability of a multimodal embedding model, including seven languages:\n\nit, es, ru, zh, pl, tr, ko\n\n\n\t\n\t\t\n\t\tDataset Usage\n\t\n\n\nThe instruction on the query side is: \"Retrieve an image of this caption.\"\nThe instruction on the document side is: \"Represent the given image.\"\nEach example contains a query and a set of targets. The first one inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Haon-Chen/XTD-10.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"mmE5-MMEB-hardneg","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","description":"\n\t\n\t\t\n\t\tmmE5 Labeled Data\n\t\n\nThis dataset contains datasets used for the supervised finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\n\nMMEB (with hard negative)\nInfoSeek (from M-BEIR)\nTAT-DQA\nArxivQA\n\nGithub\n\n\t\n\t\t\n\t\n\t\n\t\tImage Preparation\n\t\n\nFirst, you should prepare the images used for training:\n\n\t\n\t\t\n\t\n\t\n\t\tImage Downloads\n\t\n\n\nDownload All Images Used in mmE5:\n\nYou can use the script provided in our source code to download all images usedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg.","first_N":5,"first_N_keywords":["English","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"mmE5-synthetic","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/intfloat/mmE5-synthetic","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","description":"\n\t\n\t\t\n\t\tmmE5 Synthetic Data\n\t\n\nThis dataset contains synthetic datasets used for the finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\n\nClassification\nRetrieval\nVQA\n\nGithub\n\n\t\n\t\t\n\t\n\t\n\t\tImage Preparation\n\t\n\nFirst, you should prepare the images used for training:\n\n\t\n\t\t\n\t\n\t\n\t\tImage Downloads\n\t\n\n\nDownload mmE5-synthetic Images:\n\nRun the following command to download and extract the images only in this dataset.\nmkdir -p images && cd images\nwgetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-synthetic.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"FrenchBee_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/FrenchBee_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tFRENCHBEE-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from FrenchBee technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice at TW3 Partners, a company specialized in Generative AI. Passionate about artificial intelligenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/FrenchBee_dataset.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"SPATIAL-v1.0","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Youlln/SPATIAL-v1.0","creator_name":"Lalain Youri","creator_url":"https://huggingface.co/Youlln","description":"\n\t\n\t\t\n\t\tğŸš€ Aerospace Knowledge Dataset (VLM)\n\t\n\n\n\t\n\t\t\n\t\tğŸ“Œ Overview\n\t\n\nThe Aerospace Knowledge Dataset is a large-scale, multi-modal dataset designed for training Vision-Language Models (VLMs) in the aerospace domain. It is built from over 26,000 pages of technical documents, research papers, engineering reports, and mission data from leading space organizations such as NASA, ArianeGroup, SpaceX, ESA, and others.  \nThis dataset is structured in a query + image format, allowing AI models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Youlln/SPATIAL-v1.0.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"StoryFrames","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/StoryFrames","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tThe StoryFrames Dataset\n\t\n\nStoryFrames is a human-annotated dataset created to enhance a model's capability of understanding and reasoning over sequences of images.\nIt is specifically designed for tasks like generating a description for the next scene in a story based on previous visual and textual information.\nThe dataset repurposes the StoryBench dataset, a video dataset originally designed to predict future frames of a video.\nStoryFrames subsamples frames from those videos and pairsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/StoryFrames.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"AstroM3Processed","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AstroMLCore/AstroM3Processed","creator_name":"AstroMLCore","creator_url":"https://huggingface.co/AstroMLCore","description":"\n\t\n\t\t\n\t\tAstroM3Processed\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nAstroM3Processed is a time-series astronomy dataset containing photometry, spectra, and metadata features for variable stars. \nThe dataset was constructed by cross-matching publicly available astronomical datasets, \nprimarily from the ASAS-SN (Shappee et al. 2014) variable star catalog (Jayasinghe et al. 2019) \nand LAMOST spectroscopic survey (Cui et al. 2012), along with data from \nWISE (Wright et al. 2010), GALEX (Morrissey et al. 2007)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AstroMLCore/AstroM3Processed.","first_N":5,"first_N_keywords":["mit","100K - 1M","parquet","Time-series","Datasets"],"keywords_longer_than_N":true},
	{"name":"MixtureVitae-Small","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ontocord/MixtureVitae-Small","creator_name":"Ontocord.AI","creator_url":"https://huggingface.co/ontocord","description":"\n  \n\n\n\n\n\t\n\t\t\n\t\tMixtureVitae: A Permissive, High-Performance, Open-Access Pretraining Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nMixtureVitae is an open-source, permissive, high-quality dataset designed for pretraining large language models (LLMs) across a wide variety of modalities, domains, and languages. The goal of MixtureVitae is to accelerate the development of transparent, open-access AI while lowering legal uncertainty around copyright and data provenance. See our blog.\n\nPlease note this dataset isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ontocord/MixtureVitae-Small.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","100M - 1B","json","Text"],"keywords_longer_than_N":true},
	{"name":"CathayPacific_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/CathayPacific_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tCATHAY-PACIFIC-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from Cathay Pacific technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation, with a specific focus on international airline operations in the Asia-Pacific region.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/CathayPacific_dataset.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"QatarAirways_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/QatarAirways_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tAMERICAN-EXPRESS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical and financial queries generated from American Express annual reports. It is designed to train and evaluate information retrieval models and improve AI understanding of financial documentation, with a specific focus on the credit card industry, payment processing, and banking services.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/QatarAirways_dataset.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"open-pmc","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vector-institute/open-pmc","creator_name":"Vector Institute","creator_url":"https://huggingface.co/vector-institute","description":"\n\t\n\t\t\n\t\tOPEN-PMC\n\t\n\n\n    \n\n\n\n  Arxiv: Arxiv \n  Â Â Â Â |Â Â Â Â \n Code: Open-PMC Github\n  Â Â Â Â |Â Â Â Â \n Model Checkpoint: Hugging Face\n \n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of image-text pairs extracted from medical papers available on PubMed Central. It has been curated to support research in medical image understanding, particularly in natural language processing (NLP) and computer vision tasks related to medical imagery. The dataset includes:\n\nExtracted images from research articles.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/vector-institute/open-pmc.","first_N":5,"first_N_keywords":["English","mit","1M - 10M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"LayoutSAM","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","description":"\n\t\n\t\t\n\t\tLayoutSAM Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe LayoutSAM dataset is a large-scale layout dataset derived from the SAM dataset, containing 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a spatial position (i.e., bounding box) and a textual description.\nTraditional layout datasets often exhibit a closed-set and coarse-grained nature, which may limit the model's ability to generate complex attributes such as color, shape, and texture.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tKeyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/LayoutSAM.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"multimodal-genshin-impact","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/multimodal-genshin-impact","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tGenshin Impact Fandom Wiki Multimodal Dataset\n\t\n\nGithub repo here\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset is a comprehensive collection of 22,162 fandom wiki pages for the popular game Genshin Impact.\nThe dataset includes markdown-formatted English content from the wiki, featuring interleaved text, as well as image, video, and audio file links. Additionally, the associated multimodal files (images, videos, and audio) have been downloaded and organized to facilitate the multimodal datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/multimodal-genshin-impact.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","10K<n<100K","Audio","Image"],"keywords_longer_than_N":true},
	{"name":"NIH-CXR14-BiomedCLIP-Features","keyword":"multi-modal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Yasintuncer/NIH-CXR14-BiomedCLIP-Features","creator_name":"TunÃ§er","creator_url":"https://huggingface.co/Yasintuncer","description":"\n\t\n\t\t\n\t\tNIH-CXR14-BiomedCLIP-Features Dataset\n\t\n\nThis dataset is derived from the NIH Chest X-ray Dataset (NIH-CXR14) and processed using the BiomedCLIP-PubMedBERT_256-vit_base_patch16_224 model from Microsoft. It contains image and text features extracted from chest X-ray images and their corresponding textual findings.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe original NIH-CXR14 dataset comprises 112,120 chest X-ray images with disease labels from 30,805 unique patients. This processed datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yasintuncer/NIH-CXR14-BiomedCLIP-Features.","first_N":5,"first_N_keywords":["image-classification","text-retrieval","text-classification","image-feature-extraction","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"multimodal_low-resource_language_translation","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qianstats/multimodal_low-resource_language_translation","creator_name":"Qian Shen","creator_url":"https://huggingface.co/qianstats","description":"\n\t\n\t\t\n\t\tDataset Card for Multimodal Low-Resource Language Translation Dataset\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the dataset for our paper \"From Text to Multi-Modal: Advancing Low-Resource-Language Translation through Synthetic Data Generation and Cross-Modal Alignments\" accepted by the workshop LoResMT 2025 of NAACL 2025\n\nShared by [optional]: Bushiâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/qianstats/multimodal_low-resource_language_translation.","first_N":5,"first_N_keywords":["translation","English","Yoruba","Tigrinya","Hausa"],"keywords_longer_than_N":true},
	{"name":"csszengarden","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Technologic101/csszengarden","creator_name":"Anthony Chapman","creator_url":"https://huggingface.co/Technologic101","description":"Technologic101/csszengarden dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["feature-extraction","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"M3DRS","keyword":"multi-modal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/heig-vd-geo/M3DRS","creator_name":"HEIG-Vd Geomatic","creator_url":"https://huggingface.co/heig-vd-geo","description":"\n\t\n\t\t\n\t\tM3DRS: Multi-Modal Multi-Resolution Remote Sensing Dataset\n\t\n\nThis repository hosts the M3DRS dataset, a comprehensive collection of 5-channel remote sensing images (RGB, NIR, nDSM) from Switzerland, France, and Italy. The dataset is unlabelled and specifically designed to support self-supervised learning tasks. It is part of our submission to the NeurIPS 2025 Datasets and Benchmarks Track. The dataset is organized into three folders, each containing ZIP archives of images grouped byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/heig-vd-geo/M3DRS.","first_N":5,"first_N_keywords":["feature-extraction","English","cc-by-4.0","100B<n<1T","ğŸ‡ºğŸ‡¸ Region: US"],"keywords_longer_than_N":true}
]
;
