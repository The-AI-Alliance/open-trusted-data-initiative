const data_for_modality_multimodal = 
[
	{"name":"SMMILE","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMs haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE.","url":"https://huggingface.co/datasets/smmile/SMMILE","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MuSLR","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ§© MuSLR: Multimodal Symbolic Logical Reasoning Benchmark\n\t\n\nProject page: \"Multimodal Symbolic Logical Reasoning\".\nPaper Link: https://arxiv.org/abs/2509.25851\nMultimodal symbolic logical reasoning, which aims to deduce new facts from multimodal input via formal logic, is critical in high-stakes applications such as autonomous driving and medical diagnosis, where rigorous, deterministic reasoning helps prevent serious consequences.  \nTo evaluate such capabilities of currentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Aiden0526/MuSLR.","url":"https://huggingface.co/datasets/Aiden0526/MuSLR","creator_name":"Jundong Xu","creator_url":"https://huggingface.co/Aiden0526","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"visual-head","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ” Visual Head Analysis Dataset\n\t\n\n\"Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach\" (CVPR 2025)\n\n\n\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tðŸ“– Overview\n\t\n\nThis dataset contains comprehensive attention analysis results from various Large Multimodal Models (LMMs) across multiple vision-language benchmarks. The data enables research into visual attention patterns, attention head behavior, and multimodal interpretability.\n\t\n\t\t\n\t\tðŸ› ï¸ Associated Tools\n\t\n\nThe accompanying codebaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jing-bi/visual-head.","url":"https://huggingface.co/datasets/jing-bi/visual-head","creator_name":"jing bi","creator_url":"https://huggingface.co/jing-bi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["feature-extraction","text-to-image","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"MATRIX","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ“¦ MATRIX Dataset\n\t\n\nMATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning\n\n\n\t\n\t\t\n\t\tðŸ§  Dataset Description\n\t\n\nMATRIX is a large-scale multimodal dataset designed for training vision-language agents capable of grounded, step-wise reasoning and robust tool-use.It includes high-quality JSON-formatted trajectories, preference pairs, and associated multimodal contexts (images, text, tables, code).\nThis dataset supports **Direct Preference Optimization (DPO)**â€“based training andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Tajamul21/MATRIX.","url":"https://huggingface.co/datasets/Tajamul21/MATRIX","creator_name":"Tajamul Ashraf","creator_url":"https://huggingface.co/Tajamul21","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","Image","arxiv:2510.08567","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"AstroM3Dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAstroM3Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nAstroM3Dataset is a time-series astronomy dataset containing photometry, spectra, and metadata features for variable stars. \nThe dataset was constructed by cross-matching publicly available astronomical datasets, \nprimarily from the ASAS-SN (Shappee et al. 2014) variable star catalog (Jayasinghe et al. 2019) \nand LAMOST spectroscopic survey (Cui et al. 2012), along with data from \nWISE (Wright et al. 2010), GALEX (Morrissey et al. 2007), 2MASSâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AstroMLCore/AstroM3Dataset.","url":"https://huggingface.co/datasets/AstroMLCore/AstroM3Dataset","creator_name":"AstroMLCore","creator_url":"https://huggingface.co/AstroMLCore","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["mit","10K<n<100K","arxiv:2411.08842","ðŸ‡ºðŸ‡¸ Region: US","astronomy"],"keywords_longer_than_N":true},
	{"name":"inference-PhD","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tæ•°æ®æ ¼å¼\n\t\n\næ¯ä¸ªæ ·æœ¬åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n\nquestion_id: é—®é¢˜ID\nquestion: é—®é¢˜æ–‡æœ¬\nmodel_output: æ¨¡åž‹è¾“å‡º\nground_truth: çœŸå®žç­”æ¡ˆ\ntask: ä»»åŠ¡ç±»åž‹\nimage_name: å›¾ç‰‡åç§°\nmodel_name: æ¨¡åž‹åç§°\ndetailed_prompt: è¯¦ç»†æç¤º\nimage: å›¾ç‰‡æ•°æ®\n\n\n## åˆ†å‰²é…ç½®\n\n```yaml\n    # R1-Onevision-7B æ¨¡åž‹\n    - split: r1_onevision_7b_phd_ccs\n      path: \"r1_onevision_7b/r1_onevision_7b_phd_ccs.parquet\"\n    - split: r1_onevision_7b_phd_sec\n      path: \"r1_onevision_7b/r1_onevision_7b_phd_sec.parquet\"\n    - split: r1_onevision_7b_phd_icc\n      path:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/1Jin1/inference-PhD.","url":"https://huggingface.co/datasets/1Jin1/inference-PhD","creator_name":"jinxiwei","creator_url":"https://huggingface.co/1Jin1","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"MWS-Vision-Bench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMWS-Vision-Bench\n\t\n\n\nðŸ‡·ðŸ‡º Ð ÑƒÑÑÐºÐ¾ÑÐ·Ñ‹Ñ‡Ð½Ð¾Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð½Ð¸Ð¶Ðµ / Russian summary below.\n\nMWS Vision Bench â€” the first Russian-language business-OCR benchmark designed for multimodal large language models (MLLMs).This is the validation split - publicly available for open evaluation and comparison.ðŸ§© Paper is coming soon.\nðŸ”— Official repository: github.com/mts-ai/MWS-Vision-BenchðŸ¢ Organization: MTSAIR on Hugging FaceðŸ“° Article on Habr (in Russian): â€œMWS Vision Bench â€” the first Russianâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MTSAIR/MWS-Vision-Bench.","url":"https://huggingface.co/datasets/MTSAIR/MWS-Vision-Bench","creator_name":"MTSAIR","creator_url":"https://huggingface.co/MTSAIR","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","document-question-answering","expert-generated","Russian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"auxlinesolidmath","keyword":"multimodal","description":"shasha/auxlinesolidmath dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/shasha/auxlinesolidmath","creator_name":"Shasha Guo","creator_url":"https://huggingface.co/shasha","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"UGC-VideoCap","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tUGC-VideoCaptioner Dataset\n\t\n\nReal-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio-visual content. However, existing video captioning benchmarks and models remain predominantly visual-centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of full-modality datasets and lightweight, capable models hampers progress in fine-grained, multimodal videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Memories-ai/UGC-VideoCap.","url":"https://huggingface.co/datasets/Memories-ai/UGC-VideoCap","creator_name":"Memories.ai","creator_url":"https://huggingface.co/Memories-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2507.11336","ðŸ‡ºðŸ‡¸ Region: US","video-captioning"],"keywords_longer_than_N":true},
	{"name":"vlm-image-captioning-dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tkasvnmtp/vlm-image-captioning-dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a custom Vision Language Model (VLM) dataset for image captioning tasks. The dataset contains image-text pairs suitable for finetuning vision-language models.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Samples: 149,997\nTrain Samples: 74,998\nTest Samples: 74,999\nFeatures: image, text, sample_id\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nimage: PIL Image object\ntext: Caption/description text for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kasvnmtp/vlm-image-captioning-dataset.","url":"https://huggingface.co/datasets/kasvnmtp/vlm-image-captioning-dataset","creator_name":"KAUSHAL KUMAR SINGH","creator_url":"https://huggingface.co/kasvnmtp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MIG-RS-Bench","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tBeyond Single and Earthbound: Advancing Multi-image Grounding in Remote Sensing with Large Vision-Language Models\n\t\n\n","url":"https://huggingface.co/datasets/An-Xiao/MIG-RS-Bench","creator_name":"AnXiao","creator_url":"https://huggingface.co/An-Xiao","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"StoryFrames","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tThe StoryFrames Dataset\n\t\n\nStoryFrames is a human-annotated dataset created to enhance a model's capability of understanding and reasoning over sequences of images.\nIt is specifically designed for tasks like generating a description for the next scene in a story based on previous visual and textual information.\nThe dataset repurposes the StoryBench dataset, a video dataset originally designed to predict future frames of a video.\nStoryFrames subsamples frames from those videos and pairsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/StoryFrames.","url":"https://huggingface.co/datasets/ingoziegler/StoryFrames","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"BioTrove-Train","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tBioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity\n\t\n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nSee the BioTrove dataset card on HuggingFace to access the main BioTrove dataset (161.9M)\nBioTrove comprises well-processed metadata with full taxa information and URLs pointing to image files. The metadata can be used to filter specific categories, visualize data distribution, and manage imbalance effectively. We provide a collection of software toolsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BGLab/BioTrove-Train.","url":"https://huggingface.co/datasets/BGLab/BioTrove-Train","creator_name":"Baskar Group","creator_url":"https://huggingface.co/BGLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","English","mit","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"multi-modal","description":"\n\n\nðŸš€ Project Page | ðŸ“œ arXiv | ðŸ‘¨â€ðŸ’» Github | ðŸ† Leaderboard | ðŸ“ blog | ðŸ¤— HF Paper | ð• Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizationsâ€™ flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"MASH","keyword":"multimodal","description":"We present a Multiplatform Annotated Dataset for Societal Impact of Hurricane (MASH) that includes 98,662 relevant social media data posts from Reddit, X, TikTok, and YouTube.  In addition, all relevant posts are annotated on three dimensions: Humanitarian Classes, Bias Classes, and Information Integrity Classes in a multi-modal approach that considers both textual and visual content, providing a rich labeled dataset for in-depth analysis. The dataset is also complemented by an Online Analytics Platform that not only allows users to view hurricane-related posts and articles, but also explores high-frequency keywords, user sentiment, and the locations where posts were made. To our best knowledge, MASH is the first large-scale, multi-platform, multimodal, and multi-dimensionally annotated hurricane dataset.  We envision that MASH can contribute to the study of hurricanesâ€™ impact on society, such as disaster severity classification, public sentiment analysis, disaster policy making, and bias identification.  \n","url":"https://huggingface.co/datasets/YRC10/MASH","creator_name":"Ruichen Yao","creator_url":"https://huggingface.co/YRC10","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","image-classification","video-classification","expert-annotated","LLM"],"keywords_longer_than_N":true},
	{"name":"SpaCE-10","keyword":"multimodal","description":"This repository contains the dataset for the paper SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence.\n\n SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence\n\n\nGitHub Repository: https://github.com/Cuzyoung/SpaCE-10\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ§  What is SpaCE-10?\n\t\n\nSpaCE-10 is a compositional spatial intelligence benchmark for evaluating Multimodal Large Language Models (MLLMs) in indoorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cusyoung/SpaCE-10.","url":"https://huggingface.co/datasets/Cusyoung/SpaCE-10","creator_name":"ZiYang Gong","creator_url":"https://huggingface.co/Cusyoung","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Ar-MUSA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tData Directory Structure\n\t\n\nThe Ar-MUSA directory contains annotated datasets organized by batches and annotation teams. Each batch is labeled with a number, and the annotation team is indicated by a letter. The structure is as follows:\nAr-MUSA\nâ”œâ”€â”€ Annotation 1a\nâ”‚   â”œâ”€â”€ frames        # Contains the extracted frames for each record\nâ”‚   â”œâ”€â”€ audios        # Contains the corresponding audio files\nâ”‚   â”œâ”€â”€ transcripts   # Contains the transcripts of the audio files\nâ”‚   â””â”€â”€ annotations.csv  #â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Skhaled/Ar-MUSA.","url":"https://huggingface.co/datasets/Skhaled/Ar-MUSA","creator_name":"Salma Khaled Ali","creator_url":"https://huggingface.co/Skhaled","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","audio-classification","image-classification","Arabic","afl-3.0"],"keywords_longer_than_N":true},
	{"name":"MulSeT","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tMulSeT: A Benchmark for Multi-view Spatial Understanding Tasks\n\t\n\nPaper: Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture\nCode: https://github.com/WanyueZhang-ai/spatial-understanding\n\nA high-level overview of the MulSeT benchmark. The dataset challenges models to integrate information from two distinct viewpoints of a 3D scene to answer spatial reasoning questions.\n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nMulSeT is a comprehensive benchmark designedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WanyueZhang/MulSeT.","url":"https://huggingface.co/datasets/WanyueZhang/MulSeT","creator_name":"zhang","creator_url":"https://huggingface.co/WanyueZhang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","Image","arxiv:2509.02359","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"VMCBench","keyword":"multi-modal-qa","description":"\n\t\n\t\t\n\t\tVMCBench (Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation)\n\t\n\nðŸŒ Homepage | ðŸ¤— Dataset | ðŸ“– arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Introduction\n\t\n\nWe introduce VMCBench: a benchmark that unifies 20 existing visual question answering (VQA) datasets into a consistent multiple-choice format. VMCBench spans a diverse array of visual and linguistic contexts, rigorously testing various model capabilities. Byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/suyc21/VMCBench.","url":"https://huggingface.co/datasets/suyc21/VMCBench","creator_name":"Yuchang Su","creator_url":"https://huggingface.co/suyc21","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"hippovlog-dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for HippoVlog\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHippoVlog is a novel benchmark dataset designed for evaluating Multimodal Memory and Reasoning (MMR) systems. It consists of 25 long-form daily vlogs (682 minutes total) with naturalistic audiovisual content and 1,000 validated multiple-choice question-answer pairs.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset supports the following tasks:\n\nMultimodal Memory and Reasoning (MMR): The primary task involves answeringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/linyueqian/hippovlog-dataset.","url":"https://huggingface.co/datasets/linyueqian/hippovlog-dataset","creator_name":"Yueqian Lin","creator_url":"https://huggingface.co/linyueqian","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","visual-question-answering","multiple-choice-qa","English"],"keywords_longer_than_N":true},
	{"name":"topviewrs","keyword":"multimodal","description":"TopViewRS dataset, comprising 11,384 multiple-choice questions with either photo-realistic \nor semantic top-view maps of real-world scenarios through a pipeline of automatic collection followed by human alignment.","url":"https://huggingface.co/datasets/chengzu/topviewrs","creator_name":"Chengzu Li","creator_url":"https://huggingface.co/chengzu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["visual-question-answering","English","mit","1K<n<10K","arxiv:2406.02537"],"keywords_longer_than_N":true},
	{"name":"coco-fastvlm-2k-val2017","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for COCO FastVLM 2K Val2017 Structured Captions\n\t\n\nThis dataset contains 2,000 high-quality image-text pairs generated from the COCO 2017 validation set using a FastVLM-based vision-language model with structured prompt engineering and automated distillation. Each caption follows a structured 7-point format to describe the visual content in detail, enabling high-fidelity fine-tuning of multimodal models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/riddhimanrana/coco-fastvlm-2k-val2017.","url":"https://huggingface.co/datasets/riddhimanrana/coco-fastvlm-2k-val2017","creator_name":"Riddhiman Rana","creator_url":"https://huggingface.co/riddhimanrana","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Grammer_dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸŒŸ ReVisual-R1 (7B) â€” Open-Source Multimodal Reasoner\n\t\n\n\nOne cold-start, two RL stages, endless reasoning power.\n\n\n\n\t\n\t\t\n\t\tðŸ”‘ Highlights\n\t\n\n\nSOTA on 9 tough benchmarks covering visualâ€“math + text reasoning.\n\nThree-Stage SRO Training\n\nText Cold-Start â€” seed deep reflection\nMultimodal RL â€” align vision & logic\nText RL â€” polish fluency & brevity\n\n\nPAD (Prioritized Advantage Distillation) keeps gradients alive.\n\nEfficient-Length Reward = concise, self-reflective CoT.\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“šâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/csfufu/Grammer_dataset.","url":"https://huggingface.co/datasets/csfufu/Grammer_dataset","creator_name":"Shawn","creator_url":"https://huggingface.co/csfufu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Spatial457","keyword":"multimodal","description":"\n  \n\n\n\n  \n    Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models\n  \n\n\n\n  Xingrui Wang1,\n  Wufei Ma1,\n  Tiezheng Zhang1,\n  Celso M. de Melo2,\n  Jieneng Chen1,\n  Alan Yuille1\n\n\n\n  1 Johns Hopkins University Â Â Â Â \n  2 DEVCOM Army Research Laboratory\n\n\n\n  ðŸŒ Project Pageâ€¢\n  ðŸ“„ Paper â€¢\n  ðŸ¤— Dataset â€¢\n  ðŸ’» Code\n\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\tðŸ§  Introduction\n\t\n\nSpatial457 is a diagnostic benchmark designed to evaluate 6D spatial reasoning in large multimodal models (LMMs). Itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RyanWW/Spatial457.","url":"https://huggingface.co/datasets/RyanWW/Spatial457","creator_name":"Ryan Wang","creator_url":"https://huggingface.co/RyanWW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K<n<100K","arxiv:2502.08636"],"keywords_longer_than_N":true},
	{"name":"PARADE_audio","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAHELM: A Holistic Evaluation of Audio-Language Models\n\t\n\nThis repository contains datasets used in AHELM: A Holistic Evaluation of Audio-Language Models.\nPaper: AHELM: A Holistic Evaluation of Audio-Language Models\nProject Page: https://crfm.stanford.edu/helm/audio/v1.0.0/\nCode (HELM framework): https://github.com/stanford-crfm/helm\nAHELM is a benchmark designed to holistically measure the performance of Audio-Language Models (ALMs) across 10 key aspects: audio perception, knowledgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/PARADE_audio.","url":"https://huggingface.co/datasets/UCSC-VLAA/PARADE_audio","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["audio-text-to-text","mit","< 1K","soundfolder","Audio"],"keywords_longer_than_N":true},
	{"name":"FinRAGBench-V","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tFinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain ðŸ¤— Code ðŸ“„ Paper\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinRAGBench-V is a comprehensive benchmark for visual retrieval-augmented generation (RAG) in finance, addressing the challenge that most existing financial RAG research focuses predominantly on text while overlooking rich visual content in financial documents. By integrating multimodal data and providing visual citation, FinRAGBench-V ensures traceabilityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zhaosuifeng/FinRAGBench-V.","url":"https://huggingface.co/datasets/zhaosuifeng/FinRAGBench-V","creator_name":"Suifeng Zhao","creator_url":"https://huggingface.co/zhaosuifeng","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MSLoRA_CR","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ§¬ Contrastive Regularization with LoRA for Multimodal Biomedical Image Incremental Learning\n\t\n\nLast updated: Jun 26th, 2025Maintainer: @VentusAislantPaper: MSLoRA-CR\n\n\n\t\n\t\t\n\t\tðŸ“¦ Dataset Overview\n\t\n\nThis dataset supports MSLoRA-CR: Contrastive Regularization with LoRA for Multimodal Biomedical Image Incremental Learning.It includes curated annotations (train.json, test.jsonl) for multiple biomedical imaging modalities. The dataset is intended to facilitate incremental learning andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VentusAislant/MSLoRA_CR.","url":"https://huggingface.co/datasets/VentusAislant/MSLoRA_CR","creator_name":"VentusAislant","creator_url":"https://huggingface.co/VentusAislant","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"VoiceAssistant-Eval","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ”¥ VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing\n\t\n\n \n \n \n \n\n\n\n\n\n\n\nðŸŒŸ  This is the official repository for the paper \"VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing\", which contains the evaluation code for the VoiceAssistant-Eval benchmark.\n[ðŸŒ Homepage] [ðŸ’» Github] [ðŸ“Š Leaderboard ] [ðŸ“Š Detailed Leaderboard ] [ðŸ“Š Roleplay Leaderboard ] [ðŸ“– Paper]\n\n\n\n\n\n\t\n\t\t\n\t\tðŸš€ Data Usage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/VoiceAssistant-Eval.","url":"https://huggingface.co/datasets/MathLLMs/VoiceAssistant-Eval","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","audio-to-audio","any-to-any","multiple-choice"],"keywords_longer_than_N":true},
	{"name":"LayoutSAM-eval","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tLayoutSAM-eval Benchmark\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nLayoutSAM-Eval is a comprehensive benchmark for evaluating the quality of Layout-to-Image (L2I) generation models. This benchmark assesses L2I generation quality from two perspectives: region-wise quality (spatial and attribute accuracy) and global-wise quality (visual quality and prompt following). It employs the VLMâ€™s visual question answering to evaluate spatial and attribute adherence, and utilizes various metrics including IR scoreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval.","url":"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Emobench-M","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tEmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nEmoBench-M is a comprehensive benchmark designed to evaluate the Emotional Intelligence (EI) of Multimodal Large Language Models (MLLMs). It provides a challenging testbed for assessing a model's ability to understand and interpret human emotions from video, a critical step towards developing more empathetic and human-like AI systems.\nThe dataset consists of videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GMLHUHE/Emobench-M.","url":"https://huggingface.co/datasets/GMLHUHE/Emobench-M","creator_name":"HU HE","creator_url":"https://huggingface.co/GMLHUHE","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"TSQA","keyword":"multimodality","description":"\n\t\n\t\t\n\t\tTSQA: Time Series Question Answering\n\t\n\n\n\t\n\t\t\n\t\tâœ¨ Introduction\n\t\n\nIn the time series question answering task, we employ the KernelSynth to generate a variable-length multimodal question and answer pairs based on identifying four generic typical time series features, which aid ChatTime in comprehending the fundamental principles of time series. The following table summarizes the statistics of this dataset. Trend encompasses three categories: upward trend, downward trend, and constantâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/TSQA.","url":"https://huggingface.co/datasets/ChengsenWang/TSQA","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"open-pmc-18m","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOPEN-PMC\n\t\n\n\n    \n\n\n  Arxiv: Arxiv \n  Â Â Â Â |Â Â Â Â \n Code: Open-PMC Github\n  Â Â Â Â |Â Â Â Â \n Model Checkpoint: Hugging Face\n \n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of image-text pairs extracted from medical papers available on PubMed Central. It has been curated to support research in medical image understanding, particularly in natural language processing (NLP) and computer vision tasks related to medical imagery. The dataset includes:\n\nExtracted images from research articles.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/vector-institute/open-pmc-18m.","url":"https://huggingface.co/datasets/vector-institute/open-pmc-18m","creator_name":"Vector Institute","creator_url":"https://huggingface.co/vector-institute","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10M - 100M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"nicu-vitalsigns-ts-description","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tNICU Vitalsigns Time Series with Text Descriptions\n\t\n\nThis dataset provides multimodal samples consisting of NICU patient vital sign time series paired with natural language descriptions. It is designed to support research on language-time series multimodal modeling in clinical settings.\nThe dataset contains two physiological signals â€” heart rate (hr/) and oxygen saturation (sp/) â€” and is split into train, test, and left sets for each signal.\nEach sample contains a time series segmentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JJoy333/nicu-vitalsigns-ts-description.","url":"https://huggingface.co/datasets/JJoy333/nicu-vitalsigns-ts-description","creator_name":"Q","creator_url":"https://huggingface.co/JJoy333","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","mit","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"japanese-humor-evaluation-v2","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tJapanese Multimodal Humor Evaluation Dataset (v2)\n\t\n\nç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆã®ãŠé¡Œã«å¯¾ã™ã‚‹é¢ç™½ã„å›žç­”ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚boketeï¼ˆç”»åƒâ†’ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¨keitaiï¼ˆãƒ†ã‚­ã‚¹ãƒˆâ†’ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’çµ±åˆã€‚\n\n\t\n\t\t\n\t\tä½¿ã„æ–¹\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"iammytoo/japanese-humor-evaluation-v2\")\n\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿æ§‹é€ \n\t\n\n\nodai_type: 'image' or 'text'\nimage: ç”»åƒãŠé¡Œï¼ˆtextã‚¿ã‚¤ãƒ—ã§ã¯Noneï¼‰\nodai: ãƒ†ã‚­ã‚¹ãƒˆãŠé¡Œï¼ˆimageã‚¿ã‚¤ãƒ—ã§ã¯Noneï¼‰\nresponse: å›žç­”ãƒ†ã‚­ã‚¹ãƒˆ\nscore: 0-4ã®æ­£è¦åŒ–ã‚¹ã‚³ã‚¢\n\n\n\t\n\t\t\n\t\tã‚½ãƒ¼ã‚¹\n\t\n\n\nYANS-official/ogiri-bokete\nYANS-official/ogiri-keitai\n\n","url":"https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation-v2","creator_name":"Miyamoto Ryoto","creator_url":"https://huggingface.co/iammytoo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","Japanese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Geoperception","keyword":"multi-modal-qa","description":"Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions\n\n\t\n\t\t\n\t\tDataset Card for Geoperception\n\t\n\nA Benchmark for Low-level Geometric Perception\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nGeoperception is a benchmark focused specifically on accessing model's low-level visual perception ability in 2D geometry.\nIt is sourced from the Geometry-3K corpus, which offers precise logical forms for geometric diagrams, compiled from popular high-schoolâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/euclid-multimodal/Geoperception.","url":"https://huggingface.co/datasets/euclid-multimodal/Geoperception","creator_name":"Euclid Multimodal LLM","creator_url":"https://huggingface.co/euclid-multimodal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"HumanSense_Benchmark","keyword":"multimodal","description":"\n\nHumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs\n\n\n    Zheng Qin1,\n    Ruobing Zheng*2,\n    Yabing Wang1,\n    Tianqi Li2,\n    Yi Yuan2,\n    Jingdong Chen2,\n    Le Wangâ€ 1 \n    \n    \n    *Co-first authors. Project Lead.\n    â€ Corresponding Author.\n    \n    1Xiâ€™an Jiaotong University. 2Ant Group.\n    \n    \n\n\n Hugging Face Paper\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n arXiv:2508.10576\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n Homepage\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n GitHub\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n\n  \n    \n    Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/antgroup/HumanSense_Benchmark.","url":"https://huggingface.co/datasets/antgroup/HumanSense_Benchmark","creator_name":"Ant Group","creator_url":"https://huggingface.co/antgroup","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","arxiv:2508.10576","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React-Single-Image","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Single-Image.","url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Single-Image","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"MaCBench-Prompt-Ablations","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMaCBench-Prompt-Ablations\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\tâš ï¸ IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tðŸš« THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY ðŸš«\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench-Prompt-Ablations.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench-Prompt-Ablations","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"vlsbench","keyword":"multimodal","description":"ðŸŽ‰ VLSBench has been accpeted to ACL2025 Main Conference, see you in Vienna.\nâœ… Update data.json with safety reason and image description for more efficient and reliable evaluaiton.\n\n\t\n\t\t\n\t\tDataset Card for VLSBench\n\t\n\nThis dataset is for paper VLSBench: Unveiling Information Leakage In Multimodal Safety\nYou can check our Paper, Github, Project Page for more information.\nYou can directly use this image-text dataset with naive huggingface support:\ndataset = load_dataset(\"Foreshhh/vlsbench\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Foreshhh/vlsbench.","url":"https://huggingface.co/datasets/Foreshhh/vlsbench","creator_name":"XuHao Hu","creator_url":"https://huggingface.co/Foreshhh","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"TreeOfLife-200M","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for TreeOfLife-200M\n\t\n\nWith nearly 214 million images representing 952,257 taxa across the tree of life, TreeOfLife-200M is the largest and most diverse public ML-ready dataset for computer vision models in biology at release. This dataset combines images and metadata from four core biodiversity data providers: Global Biodiversity Information Facility (GBIF), Encyclopedia of Life (EOL), BIOSCAN-5M, and FathomNet to more than double the number of unique taxa covered byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/imageomics/TreeOfLife-200M.","url":"https://huggingface.co/datasets/imageomics/TreeOfLife-200M","creator_name":"HDR Imageomics Institute","creator_url":"https://huggingface.co/imageomics","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","English","Latin","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"MMMU-LLM-R1-format","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMMMU-LLM-R1 Reformatted Dataset\n\t\n\n","url":"https://huggingface.co/datasets/xDAN-Vision/MMMU-LLM-R1-format","creator_name":"xDAN-RL-Group","creator_url":"https://huggingface.co/xDAN-Vision","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"unusual-objects-unusual-places_text-image","keyword":"multimodal","description":"\n\t\n\t\t\n\t\t(Un-)usual objects in (un-)usual places\n\t\n\n\n\t\n\t\t\n\t\tA small Text-Image dataset to confuse, probe (and improve) SOTA (2024) machine vision models.\n\t\n\nTo be continued (with further examples added)...\nExample results from LMSYS ARENA (June 2024):\n\n\n","url":"https://huggingface.co/datasets/zer0int/unusual-objects-unusual-places_text-image","creator_name":"zer0int","creator_url":"https://huggingface.co/zer0int","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"DocMMIR","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDocMMIR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDocMMIR (Document-level Multimodal Information Retrieval) is a dataset for document-level multimodal information retrieval. This dataset contains image-text pairs from arXiv papers, Wikipedia, and presentations, specifically designed for multimodal retrieval tasks.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\n\t\n\t\t\nStatistic\nWiki\nArXiv\nSlide\nTotal\n\n\n\t\t\n#Train\n360,285\n62,764\n27,057\n450,079\n\n\n#Valid\n14,775\n3,000\n1,409\n19,184\n\n\n#Test\n14,805\n3,000\n1,399â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Lord-Jim/DocMMIR.","url":"https://huggingface.co/datasets/Lord-Jim/DocMMIR","creator_name":"Zirui Li","creator_url":"https://huggingface.co/Lord-Jim","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-40","keyword":"multimodal","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"MECAT-Caption","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks\n\t\n\nðŸ“– Paper | ðŸ› ï¸ GitHub |  ðŸ”Š MECAT-Caption Dataset |  ðŸ”Š MECAT-QA Dataset\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nMECAT (Multi-Expert Chain for Audio Tasks) is a comprehensive benchmark constructed on large-scale data to evaluate machine understanding of audio content through two core tasks:\n\nAudio Captioning: Generating textual descriptions for given audio\nAudio Question Answering: Answering questionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mispeech/MECAT-Caption.","url":"https://huggingface.co/datasets/mispeech/MECAT-Caption","creator_name":"Horizon Team, Xiaomi MiLM Plus","creator_url":"https://huggingface.co/mispeech","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["audio-classification","audio-text-to-text","summarization","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"Euclid30K","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tEuclid30K Dataset\n\t\n\nPaper | Project Page | Code\nSpatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity.\nHowever, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs). \nTo fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiamLian0727/Euclid30K.","url":"https://huggingface.co/datasets/LiamLian0727/Euclid30K","creator_name":"Shijie Lian","creator_url":"https://huggingface.co/LiamLian0727","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"AVATAR","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAVATAR: Whatâ€™s Making That Sound Right Now? Video-centric Audio-Visual Localization\n\t\n\nAVATAR stands for Audio-Visual localizAtion benchmark for a spatio-TemporAl peRspective in video.\nAVATAR is a benchmark dataset designed to evaluate video-centric audio-visual localization (AVL) in complex and dynamic real-world scenarios.Unlike previous benchmarks that rely on static image-level annotations and assume simplified conditions, AVATAR offers high-resolution temporal annotations overâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mipal/AVATAR.","url":"https://huggingface.co/datasets/mipal/AVATAR","creator_name":"Machine Intelligence and Pattern Analysis Lab","creator_url":"https://huggingface.co/mipal","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","Video","Audio"],"keywords_longer_than_N":true},
	{"name":"h0_post_train_db_2508","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tBeing-H0: Vision-Language-Action Pretraining from Large-Scale  Human Videos\n\t\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n    \n\n\nWe introduce Being-H0, the first dexterous Vision-Language-Action model pretrained from large-scale human videos via explicit hand motion modeling.\n\n\t\n\t\t\n\t\tNews\n\t\n\n\n[2025-08-02]: We release the Being-H0 codebase and pretrained models! Check our Hugging Face Model Hub for more details. ðŸ”¥ðŸ”¥ðŸ”¥\n[2025-07-21]: We publish Being-H0! Check our paper here. ðŸŒŸðŸŒŸðŸŒŸ\n\n\n\t\n\t\t\n\t\tModel Checkpointsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BeingBeyond/h0_post_train_db_2508.","url":"https://huggingface.co/datasets/BeingBeyond/h0_post_train_db_2508","creator_name":"BeingBeyond","creator_url":"https://huggingface.co/BeingBeyond","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","mit","10K - 100K","json","Image"],"keywords_longer_than_N":true},
	{"name":"MM-MathInstruct","keyword":"multi-modal-qa","description":"\n\t\n\t\t\n\t\tMathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning\n\t\n\nRepo: https://github.com/mathllm/MathCoder\nPaper: https://huggingface.co/papers/2505.10557\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MathCoder-VL, a series of open-source large multimodal models (LMMs) specifically tailored for general math problem-solving. We also introduce FigCodifier-8B, an image-to-code model.\n\n\t\n\t\t\nBase Model\nOurs\n\n\n\t\t\nMini-InternVL-Chat-2B-V1-5\nMathCoder-VL-2B\n\n\nInternVL2-8Bâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MM-MathInstruct.","url":"https://huggingface.co/datasets/MathLLMs/MM-MathInstruct","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"Art-Vision-Question-Answering-Dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tArt Vision Question Answering Dataset\n\t\n\nðŸŽ¨ A curated dataset for training AI models on digital artwork analysis and visual question answering.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset contains 577 question-answer pairs extracted from artwork conversations, designed for training multimodal AI models on art analysis tasks.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nðŸ–¼ï¸ Visual Thumbnails: Artwork images displayed directly in the dataset viewer\nðŸ’¬ Rich Q&A: Expert-level questions and answers aboutâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OneEyeDJ/Art-Vision-Question-Answering-Dataset.","url":"https://huggingface.co/datasets/OneEyeDJ/Art-Vision-Question-Answering-Dataset","creator_name":"Zeyang Zhang","creator_url":"https://huggingface.co/OneEyeDJ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"labeled-gaits-instruct-200k","keyword":"multimodal","description":"Housto4/labeled-gaits-instruct-200k dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Housto4/labeled-gaits-instruct-200k","creator_name":"Ivan H","creator_url":"https://huggingface.co/Housto4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","tabular-regression","tabular-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"GPT-Image-Edit-1.5M","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tGPT-Image-Edit-1.5M A Million-Scale, GPT-Generated Image Dataset\n\t\n\nðŸ“ƒArxiv | ðŸŒ Project Page | ðŸ’»Github\nGPT-Image-Edit-1.5M is a comprehensive image editing dataset that is built upon HQ-Edit, UltraEdit, OmniEdit and Complex-Edit, with all output images regenerated with GPT-Image-1.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“£ News\n\t\n\n\n[2025.08.20] ðŸš€ We provide a script for multi-process downloading. See Multi-process Download.\n[2025.07.27] ðŸ¤— We release GPT-Image-Edit, a state-of-the-art image editing model withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/GPT-Image-Edit-1.5M.","url":"https://huggingface.co/datasets/UCSC-VLAA/GPT-Image-Edit-1.5M","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-image","English","cc-by-4.0","1M - 10M","webdataset"],"keywords_longer_than_N":true},
	{"name":"KoLLaVA-v1.5-Instruct-581k-tmp","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tKoLLaVA-v1.5-Instruct-581k-tmp\n\t\n\ní•œêµ­ì–´ Vision-Language ëª¨ë¸ì„ ìœ„í•œ instruction tuning ë°ì´í„°ì…‹ìž…ë‹ˆë‹¤.\n\n\t\n\t\t\n\t\të°ì´í„°ì…‹ ì •ë³´\n\t\n\n\nì›ë³¸: KoLLaVA-v1.5-Instruct-581k\ní˜•ì‹: ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜\nì´ë¯¸ì§€: COCO ë°ì´í„°ì…‹\nì–¸ì–´: í•œêµ­ì–´\nì´ íŒŒì¼: 362ê°œ\nì´ ìƒ˜í”Œ: 362,000ê°œ\n\n\n\t\n\t\t\n\t\të°ì´í„°ì…‹ êµ¬ì¡°\n\t\n\nê° ìƒ˜í”Œì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:\n\nimage: PIL Image ê°ì²´\nconversations: ChatML í˜•ì‹ì˜ ëŒ€í™” \nrole: \"user\" ë˜ëŠ” \"assistant\"\ncontent: ë©”ì‹œì§€ ë‚´ìš©\n\n\nid: ìƒ˜í”Œ ê³ ìœ  ID\n\n\n\t\n\t\t\n\t\tì‚¬ìš©ë²•\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"ko-vlm/KoLLaVA-v1.5-Instruct-581k-tmp\")\n\n# ì˜ˆì‹œ: ì²« ë²ˆì§¸ ìƒ˜í”Œâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ko-vlm/KoLLaVA-v1.5-Instruct-581k-tmp.","url":"https://huggingface.co/datasets/ko-vlm/KoLLaVA-v1.5-Instruct-581k-tmp","creator_name":"korean-vision-language","creator_url":"https://huggingface.co/ko-vlm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Korean","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"MixBench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench.","url":"https://huggingface.co/datasets/mixed-modality-search/MixBench","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"SingaporeAirline_vision_dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tSINGAPORE-AIRLINES-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from Singapore Airlines official documentation. It is designed to train and evaluate information retrieval models and improve AI understanding of commercial aviation operational documentation.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, an engineering student specializing in Computer Science, Big Data, and AI, currently working as an apprentice at TW3 Partners, aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/SingaporeAirline_vision_dataset.","url":"https://huggingface.co/datasets/Davidsv/SingaporeAirline_vision_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"visual-qa-llama-format","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOpen Paws Visual Qa Llama Format\n\t\n\nThis dataset is part of the Open Paws initiative to develop AI training data aligned with animal liberation and advocacy principles. Created to train AI systems that understand and promote animal welfare, rights, and liberation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nDataset Type: Multimodal Data\nFormat: JSONL (JSON Lines)\nLanguages: Multilingual (primarily English)\nFocus: Animal advocacy and ethical reasoning\nOrganization: Open Paws\nLicense: Apache 2.0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/open-paws/visual-qa-llama-format.","url":"https://huggingface.co/datasets/open-paws/visual-qa-llama-format","creator_name":"Open Paws","creator_url":"https://huggingface.co/open-paws","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"amazon_reviews_for_rec","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAmazon Reviews for Multimodal Recommendation\n\t\n\n\n\n\t\n\t\t\n\t\tæ•°æ®é›†æ¦‚è§ˆ\n\t\n\nè¿™æ˜¯ä¸€ä¸ªä¸ºç«¯åˆ°ç«¯å¤šæ¨¡æ€æŽ¨èç³»ç»Ÿè®¾è®¡çš„WebDataset æ ¼å¼çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æºè‡ª Amazon è¯„è®ºï¼ŒåŒ…å«äº†å¤„ç†åŽçš„æ–‡æœ¬ï¼ˆè¯„è®ºï¼‰å’Œå›¾åƒæ•°æ®ï¼Œæ—¨åœ¨æ”¯æŒé«˜æ•ˆçš„åˆ†å¸ƒå¼è®­ç»ƒã€‚\nè¯¥æ•°æ®é›†æ˜¯ GitHub é¡¹ç›® çš„ä¸€éƒ¨åˆ†ï¼Œè¯¥é¡¹ç›®åŸºäºŽ Apache Beam å’Œ PyTorch DDP æž„å»ºï¼Œæ¶µç›–äº†ä»Žåˆ†å¸ƒå¼ç‰¹å¾å·¥ç¨‹ã€æ•°æ®åŠ è½½åˆ°å¤æ‚æ¨¡åž‹ï¼ˆMMoEï¼‰åˆ†å¸ƒå¼è®­ç»ƒçš„æ•´ä¸ªå·¥ä½œæµã€‚æˆ‘ä»¬å·²å°†æ‰€ç”¨çš„æ•°æ®é›†ã€éªŒè¯é›†å’Œæ¨¡åž‹å…¨éƒ¨å¼€æºã€‚\n\n\t\n\t\t\n\t\tæ•°æ®é›†ç»Ÿè®¡\n\t\n\n\n\t\n\t\t\nå­é›†åç§°\næ–‡ä»¶æ ¼å¼\næ ·æœ¬æ•°é‡\næ–‡ä»¶å¤§å°\n\n\n\t\t\ntrain\n.tar.gz\n1848930\n128 GB\n\n\nvalid\n.tar.gz\n22281\n2 GB\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tå¦‚ä½•ä¸‹è½½å’ŒåŠ è½½æ•°æ®é›†ï¼Ÿ\n\t\n\nç”±äºŽæ•°æ®é›†æ˜¯ WebDataset æ ¼å¼ï¼Œæˆ‘ä»¬æŽ¨èä½¿ç”¨ WebDataset åº“ è¿›è¡Œæµå¼åŠ è½½ï¼Œè¿™å¯¹äºŽåˆ†å¸ƒå¼è®­ç»ƒéžå¸¸é«˜æ•ˆã€‚\n\n\t\n\t\t\n\t\n\t\n\t\tä½¿ç”¨â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jingxiang11111/amazon_reviews_for_rec.","url":"https://huggingface.co/datasets/jingxiang11111/amazon_reviews_for_rec","creator_name":"jingxiangqu","creator_url":"https://huggingface.co/jingxiang11111","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1M<n<10M","WebDataset","ðŸ‡ºðŸ‡¸ Region: US","recommendation-system"],"keywords_longer_than_N":true},
	{"name":"image-textualization","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tImage-Textualization Dataset\n\t\n\nExciting to announce the open-sourcing of our Image-Text Matching Dataset, which consists of 220K image-text pairs. We also release fine-grained annotations, which may be helpful for many downstream tasks.\nThis dataset is designed to facilitate research and development in the field of large mutimodal language model, particularly for tasks such as image captioning, visual question answering, and multimodal understanding.\nNote that our framework can beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/image-textualization.","url":"https://huggingface.co/datasets/Sterzhang/image-textualization","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"ToolVQA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tToolVQA: A Dataset for Real-World VQA with External Tools (ICCV 2025)\n\t\n\n\n    \n    \n\n\nAuthors: Shaofeng Yin, Ting Lei, Yang Liu\n\n\t\n\t\t\n\t\n\t\n\t\t1. Introduction ðŸ“£\n\t\n\n\nIntegrating external tools into Large Foundation Models (LFMs) has emerged as a promising approach to enhance their problem-solving capabilities. While existing studies have demonstrated strong performance in tool-augmented Visual Question Answering (VQA), recent benchmarks reveal significant gaps in real-world tool-useâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DietCoke4671/ToolVQA.","url":"https://huggingface.co/datasets/DietCoke4671/ToolVQA","creator_name":"Shaofeng Yin","creator_url":"https://huggingface.co/DietCoke4671","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","Image","arxiv:2508.03284","ðŸ‡ºðŸ‡¸ Region: US","VQA"],"keywords_longer_than_N":true},
	{"name":"blip3-grounding-50m","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tBLIP3-GROUNDING-50M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-GROUNDING-50M dataset is designed to enhance the ability of Vision-Language Models (VLMs) to ground semantic concepts in visual features, which is crucial for tasks like object detection, semantic segmentation, and understanding referring expressions (e.g., \"the object to the left of the dog\"). Traditional datasets often lack the necessary granularity for such tasks, making it challenging for models to accurately localize andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-grounding-50m.","url":"https://huggingface.co/datasets/Salesforce/blip3-grounding-50m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"android_control_train","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tProcessed Android Control Training Set\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the processed training set derived from the Android Control dataset by Google Research.\nThe data processing methodology is identical to that used for our corresponding test set, which can be found at Reallm-Labs/android_control_test.\n\n\t\n\t\t\n\t\n\t\n\t\tData Content and Image Extraction\n\t\n\nImportant Note: Due to the large size of the dataset, this repository contains only the processed text files.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/InfiX-ai/android_control_train.","url":"https://huggingface.co/datasets/InfiX-ai/android_control_train","creator_name":"InfiX.ai","creator_url":"https://huggingface.co/InfiX-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"XTD-10","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tXTD Multimodal Multilingual Data With Instruction\n\t\n\nThis dataset contains datasets (with English instruction) used for evaluating the multilingual capability of a multimodal embedding model, including seven languages:\n\nit, es, ru, zh, pl, tr, ko\n\n\n\t\n\t\t\n\t\tDataset Usage\n\t\n\n\nThe instruction on the query side is: \"Retrieve an image of this caption.\"\nThe instruction on the document side is: \"Represent the given image.\"\nEach example contains a query and a set of targets. The first one inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Haon-Chen/XTD-10.","url":"https://huggingface.co/datasets/Haon-Chen/XTD-10","creator_name":"Haonan Chen","creator_url":"https://huggingface.co/Haon-Chen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"SACap-1M","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tSACap-1M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSACap-1M is a large-scale, open-vocabulary dataset for segmentation-mask-to-image generation, sourced from the high-resolution SA-1B. It contains 1 M images and 5.9 M instance-level segmentation masks. Each mask is annotated with a regional caption (average 14.1 words) generated by Qwen2-VL-72B, and every image is paired with a global caption (average 58.6 words).\n\n\t\n\t\t\n\t\n\t\n\t\tRelated links:\n\t\n\n\nSACap-eval:  a 4K sample benchmark derived fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/0xLDF/SACap-1M.","url":"https://huggingface.co/datasets/0xLDF/SACap-1M","creator_name":"0xLDF","creator_url":"https://huggingface.co/0xLDF","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"IDKVQA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tI Don't Know Visual Question Answering - IDKVQA dataset - ICCV 25\n\t\n\n\n\nWe introduce IDKVQA, an embodied dataset specifically designed and annotated for visual question answering using the agentâ€™s observations during navigation,\nwhere the answer includes not only Yes and No, but also I donâ€™t know.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nPlease see our ICCV 25 accepted paper: Collaborative Instance Object Navigation: Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues\nFor moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ftaioli/IDKVQA.","url":"https://huggingface.co/datasets/ftaioli/IDKVQA","creator_name":"Francesco Taioli","creator_url":"https://huggingface.co/ftaioli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","zero-shot-classification","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"II-Bench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tII-Bench\n\t\n\nðŸŒ Homepage | ðŸ¤— Paper | ðŸ“– arXiv | ðŸ¤— Dataset | GitHub\n\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nII-Bench comprises 1,222 images, each accompanied by 1 to 3 multiple-choice questions, totaling 1,434 questions. II-Bench encompasses images from six distinct domains: Life, Art, Society, Psychology, Environment and Others. It also features a diverse array of image types, including Illustrations, Memes, Posters, Multi-panel Comics, Single-panel Comics, Logos and Paintings. The detailedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/II-Bench.","url":"https://huggingface.co/datasets/m-a-p/II-Bench","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"MM-IQ","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for \"MM-IQ\"\n\t\n\n\nIntroduction\nPaper Information\nDataset Examples\nLeaderboard\nDataset Usage\nData Downloading\nData Format\nAutomatic Evaluation\n\n\nCitation\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nIQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/huanqia/MM-IQ.","url":"https://huggingface.co/datasets/huanqia/MM-IQ","creator_name":"huanqiacai","creator_url":"https://huggingface.co/huanqia","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"PIN-200M","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tPIN-200M\n\t\n\nA mini version of \"PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents\"\nPaper: https://arxiv.org/abs/2406.13923\nThis dataset contains around 200M samples in PIN format, with around 280 TB storage.\nðŸš€ News\n[ 2025.09.22 ] !NEW! ðŸ”¥ We have completed the final version of the PIN-200M dataset and conducted some simple statistics on it.\n[ 2024.12.06 ] !NEW! ðŸ”¥ We have updated the quality signals, enabling a swift assessment of whether a sample meetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/PIN-200M.","url":"https://huggingface.co/datasets/m-a-p/PIN-200M","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"MaCBench-Ablations","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMaCBench-Ablations\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\tâš ï¸ IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tðŸš« THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY ðŸš«\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench-Ablations.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench-Ablations","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"Mantis-Instruct","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMantis-Instruct\n\t\n\nPaper | Website | Github | Models | Demo\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nMantis-Instruct is a fully text-image interleaved multimodal instruction tuning dataset, \ncontaining 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding. \nIt's been used to train Mantis Model families\n\nMantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\nAmong the 14â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct.","url":"https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"textrl","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸŒŸ ReVisual-R1 (7B) â€” Open-Source Multimodal Reasoner\n\t\n\n\nOne cold-start, two RL stages, endless reasoning power.\n\n\n\n\t\n\t\t\n\t\tðŸ”‘ Highlights\n\t\n\n\nSOTA on 9 tough benchmarks covering visualâ€“math + text reasoning.\n\nThree-Stage SRO Training\n\nText Cold-Start â€” seed deep reflection\nMultimodal RL â€” align vision & logic\nText RL â€” polish fluency & brevity\n\n\nPAD (Prioritized Advantage Distillation) keeps gradients alive.\n\nEfficient-Length Reward = concise, self-reflective CoT.\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“šâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/csfufu/textrl.","url":"https://huggingface.co/datasets/csfufu/textrl","creator_name":"Shawn","creator_url":"https://huggingface.co/csfufu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"UGC-VideoCap","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tUGC-VideoCaptioner Dataset\n\t\n\nReal-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio-visual content. However, existing video captioning benchmarks and models remain predominantly visual-centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of full-modality datasets and lightweight, capable models hampers progress in fine-grained, multimodal videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openinterx/UGC-VideoCap.","url":"https://huggingface.co/datasets/openinterx/UGC-VideoCap","creator_name":"Memories.ai Research","creator_url":"https://huggingface.co/openinterx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2507.11336","ðŸ‡ºðŸ‡¸ Region: US","video-captioning"],"keywords_longer_than_N":true},
	{"name":"livevqa-benchmark","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tLiveVQA Benchmark Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nLiveVQA is a comprehensive Visual Question Answering benchmark that evaluates multimodal models across three dynamic domains: News, Academic Papers, and Videos. The dataset features both level1 (basic comprehension) and level2 (advanced reasoning) questions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nid: Unique identifier for each question\nimage: Path to the associated image\nquestion: The question text\noptions: Listâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fmy666/livevqa-benchmark.","url":"https://huggingface.co/datasets/fmy666/livevqa-benchmark","creator_name":"fmy666","creator_url":"https://huggingface.co/fmy666","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"DataSeeds.AI-Sample-Dataset-DSD","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataSeeds.AI Sample Dataset (DSD)\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe DataSeeds.AI Sample Dataset (DSD) is a high-fidelity, human-curated computer vision-ready dataset comprised of 7,772 peer-ranked, fully annotated photographic images, 350,000+ words of descriptive text, and comprehensive metadata. While the DSD is being released under an open source license, a sister dataset of over 10,000 fully annotated and segmented images is available for immediate commercial licensing, and theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dataseeds/DataSeeds.AI-Sample-Dataset-DSD.","url":"https://huggingface.co/datasets/Dataseeds/DataSeeds.AI-Sample-Dataset-DSD","creator_name":"Dataseeds AI","creator_url":"https://huggingface.co/Dataseeds","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","object-detection","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-1.5-Instruct-Data","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-1.5 Instruction Data\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset, LLaVA-OneVision-1.5-Instruct, was collected and integrated during the development of LLaVA-OneVision-1.5. LLaVA-OneVision-1.5 is a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. This meticulously curated 22M instruction dataset (LLaVA-OneVision-1.5-Instruct) is part of a comprehensive andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mvp-lab/LLaVA-OneVision-1.5-Instruct-Data.","url":"https://huggingface.co/datasets/mvp-lab/LLaVA-OneVision-1.5-Instruct-Data","creator_name":"Mobile Vision Perception Lab","creator_url":"https://huggingface.co/mvp-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2509.23661","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"DenseLayout","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDenseLayout Benchmark\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nDenseLayout is a benchmark for Layout-to-Image (L2I) generation in dense scenes. Each image contains 15+ instances on average with bounding boxes, categories, and captions. The dataset supports evaluation from:\n\nRegion level â€“ spatial alignment and attribute accuracy\n\nGlobal level â€“ overall image quality and prompt faithfulness\n\n\nWith its crowded layouts and fine-grained annotations, DenseLayout provides a challenging and reliable benchmarkâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FireRedTeam/DenseLayout.","url":"https://huggingface.co/datasets/FireRedTeam/DenseLayout","creator_name":"FireRedTeam","creator_url":"https://huggingface.co/FireRedTeam","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"brain-tumor-mri-colorized-ehr","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tBrain Tumor (MRI) Detection Colourized with EHR\n\t\n\nâš ï¸ SYNTHETIC DATA - For research and education only\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive multimodal medical AI dataset combining 11,505 colorized brain MRI images (embedded as bytes) with synthetic Nigerian Electronic Health Records (EHR). All clinical data is FHIR R4 compliant and includes authentic Nigerian healthcare context.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nðŸ–¼ï¸ Images Embedded: All 11,505 MRI images stored as bytes in parquetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/electricsheepafrica/brain-tumor-mri-colorized-ehr.","url":"https://huggingface.co/datasets/electricsheepafrica/brain-tumor-mri-colorized-ehr","creator_name":"Electric Sheep","creator_url":"https://huggingface.co/electricsheepafrica","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","object-detection","image-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"FinRAGBench-V","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tFinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain ðŸ¤— Code ðŸ“„ Paper\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFinRAGBench-V is a comprehensive benchmark for visual retrieval-augmented generation (RAG) in finance, addressing the challenge that most existing financial RAG research focuses predominantly on text while overlooking rich visual content in financial documents. By integrating multimodal data and providing visual citation, FinRAGBench-V ensures traceabilityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zhaosuifeng/FinRAGBench-V.","url":"https://huggingface.co/datasets/zhaosuifeng/FinRAGBench-V","creator_name":"Suifeng Zhao","creator_url":"https://huggingface.co/zhaosuifeng","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LayoutSAM","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tLayoutSAM Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe LayoutSAM dataset is a large-scale layout dataset derived from the SAM dataset, containing 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a spatial position (i.e., bounding box) and a textual description.\nTraditional layout datasets often exhibit a closed-set and coarse-grained nature, which may limit the model's ability to generate complex attributes such as color, shape, and texture.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tKeyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/LayoutSAM.","url":"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"vsr_random","keyword":"multimodality","description":"\n\t\n\t\t\n\t\tVSR: Visual Spatial Reasoning\n\t\n\nThis is the random set of VSR: Visual Spatial Reasoning (TACL 2023) [paper].\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndata_files = {\"train\": \"train.jsonl\", \"dev\": \"dev.jsonl\", \"test\": \"test.jsonl\"}\ndataset = load_dataset(\"cambridgeltl/vsr_random\", data_files=data_files)\n\nNote that the image files still need to be downloaded separately. See data/ for details.\nGo to our github repo for more introductions.\n\n\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find VSRâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cambridgeltl/vsr_random.","url":"https://huggingface.co/datasets/cambridgeltl/vsr_random","creator_name":"Language Technology Lab @University of Cambridge","creator_url":"https://huggingface.co/cambridgeltl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"VL-ICL","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tVL-ICL Bench\n\t\n\nVL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning\n[Webpage] [Paper] [Code]\n\n\t\n\t\t\n\t\tImage-to-Text Tasks\n\t\n\nIn all image-to-text tasks image is a list of image paths (typically one item - for interleaved cases there are two items).\n\n\t\n\t\t\n\t\tFast Open-Ended MiniImageNet\n\t\n\nFrozen introduces the task of fast concept binding for MiniImageNet. The benchmark has a fixed structure so only the given support examples can be used for a givenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ys-zong/VL-ICL.","url":"https://huggingface.co/datasets/ys-zong/VL-ICL","creator_name":"Yongshuo Zong","creator_url":"https://huggingface.co/ys-zong","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","mit","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"TALI","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for \"TALI\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nTALI is a large-scale, tetramodal dataset designed to facilitate a shift from unimodal and duomodal to tetramodal research in deep learning. It aligns text, video, images, and audio, providing a rich resource for innovative self-supervised learning tasks and multimodal research. TALI enables exploration of how different modalities and data/model scaling affect downstream performance, with the aim of inspiringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Antreas/TALI.","url":"https://huggingface.co/datasets/Antreas/TALI","creator_name":"Antreas Antoniou","creator_url":"https://huggingface.co/Antreas","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","cc-by-4.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"infovqa_colqwen2_embeddings","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tInfoVQA ColQwen2.5 Embeddings\n\t\n\nThis dataset contains pre-computed embeddings for the InfoVQA dataset using the ColQwen2.5 model.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of three configurations:\n\n\t\n\t\t\n\t\tCorpus Configuration\n\t\n\nContains document images with their embeddings.\nfrom datasets import load_dataset\ncorpus = load_dataset(\"WenxingZhu/infovqa_colqwen2_embeddings\", \"corpus\", split=\"test\")\n\nFields:\n\ncorpus-id (int): Document identifier\nimage (Image): Original documentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WenxingZhu/infovqa_colqwen2_embeddings.","url":"https://huggingface.co/datasets/WenxingZhu/infovqa_colqwen2_embeddings","creator_name":"WenxingZhu","creator_url":"https://huggingface.co/WenxingZhu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","feature-extraction","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MJ-Bench-Image","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMJ-Bench-Image Dataset\n\t\n\nThis dataset contains image pairs from the MJ-Bench benchmark for evaluating multimodal judges in text-to-image generation.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized into multiple categories:\n\nAlignment: Evaluates how well images follow prompt instructions\nBias: Tests for demographic and contextual biases\nComposition: Tests physics laws, perspective, and depth ordering\nQuality: Evaluates image fidelity, color, lighting, and texture\nSafety: Testsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zhaorun/MJ-Bench-Image.","url":"https://huggingface.co/datasets/Zhaorun/MJ-Bench-Image","creator_name":"Zhaorun Chen","creator_url":"https://huggingface.co/Zhaorun","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","text-to-image","mit","1K<n<10K","arxiv:2407.04842"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_healthcare_industry_test","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about the Healthcare Industry that allow ViDoRe to benchmark medical documents. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('healthcare industry'). From these documents, we randomly sampled 1000 pages.\nWeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_healthcare_industry_test.","url":"https://huggingface.co/datasets/vidore/syntheticDocQA_healthcare_industry_test","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ChatTime-1-Pretrain-1M","keyword":"multimodality","description":"\n\t\n\t\t\n\t\tChatTime: A Multimodal Time Series Foundation Model\n\t\n\n\n\t\n\t\t\n\t\tâœ¨ Introduction\n\t\n\nIn this paper, we innovatively model time series as a foreign language and construct ChatTime, a unified framework for time series and text processing. As an out-of-the-box multimodal time series foundation model, ChatTime provides zero-shot forecasting capability and supports bimodal input/output for both time series and text. We design a series of experiments to verify the superior performance ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Pretrain-1M.","url":"https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Pretrain-1M","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"RoboMatrix","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis is the fine-tuning dataset used in the paper RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World.\n\n\t\n\t\t\n\t\tSource\n\t\n\nProject Page: https://robo-matrix.github.io/Paper: https://arxiv.org/abs/2412.00171Code: https://github.com/WayneMao/RoboMatrixModel: \n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you find our work helpful, please cite us:\n@article{mao2024robomatrix,\n  title={RoboMatrix: A Skill-centric Hierarchical Framework forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WayneMao/RoboMatrix.","url":"https://huggingface.co/datasets/WayneMao/RoboMatrix","creator_name":"WayneMao","creator_url":"https://huggingface.co/WayneMao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","Datasets","Croissant","arxiv:2412.00171"],"keywords_longer_than_N":true},
	{"name":"ReachQA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tReachQA: Reasoning-Intensive Chart Q&A\n\t\n\n\nDisclaimer: This dataset is composed of synthetic data and may contain inaccuracies. Users are advised to exercise caution when working with this dataset and consider additional filtering.\nWe plan to release a manually curated version in the future.\n\nThis repository contains the ðŸ“ˆReachQA dataset proposed in Distill Visual Chart Reasoning Ability from LLMs to MLLMs.\nReachQA is a multimodal instruction dataset synthesized primarily using LLMs.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hewei2001/ReachQA.","url":"https://huggingface.co/datasets/hewei2001/ReachQA","creator_name":"Wei He","creator_url":"https://huggingface.co/hewei2001","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"CathayPacific_dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tCATHAY-PACIFIC-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from Cathay Pacific technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation, with a specific focus on international airline operations in the Asia-Pacific region.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/CathayPacific_dataset.","url":"https://huggingface.co/datasets/Davidsv/CathayPacific_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"multimodal-ai-taxonomy","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMultimodal AI Taxonomy\n\t\n\nA comprehensive, structured taxonomy for mapping multimodal AI model capabilities across input and output modalities.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset provides a systematic categorization of multimodal AI capabilities, enabling users to:\n\nNavigate the complex landscape of multimodal AI models\nFilter models by specific input/output modality combinations\nUnderstand the nuanced differences between similar models (e.g., image-to-video with/without audioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/danielrosehill/multimodal-ai-taxonomy.","url":"https://huggingface.co/datasets/danielrosehill/multimodal-ai-taxonomy","creator_name":"Daniel Rosehill","creator_url":"https://huggingface.co/danielrosehill","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["other","English","cc0-1.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MixBench2025","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench2025.","url":"https://huggingface.co/datasets/mixed-modality-search/MixBench2025","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"data-juicer-t2v-optimal-data-pool","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tData-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development\n\t\n\n\n\t\n\t\t\n\t\tProject description\n\t\n\nThe emergence of large-scale multi-modal generative models has drastically advanced artificial intelligence, introducing unprecedented levels of performance and functionality. \nHowever, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/data-juicer-t2v-optimal-data-pool.","url":"https://huggingface.co/datasets/datajuicer/data-juicer-t2v-optimal-data-pool","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_energy_test","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about Energy that allow ViDoRe to benchmark technical documentation about energy. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('energy'). From these documents, we randomly sampled 1000 pages.\nWe associated theseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_energy_test.","url":"https://huggingface.co/datasets/vidore/syntheticDocQA_energy_test","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VisualWebInstruct-Recall","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is the dataset recalled from Google Search from the seed images.\n\n\t\n\t\t\n\t\tLinks\n\t\n\nGithub|\nPaper|\nWebsite\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{visualwebinstruct,\n    title={VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},\n    author = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},\n    journal={arXiv preprint arXiv:2503.10582},\n    year={2025}\n}\n\n","url":"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Recall","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MAGB","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMAGBï¼š A Comprehensive Benchmark for Multimodal Attributed Graphs\n\t\n\nIn many real-world scenarios, graph nodes are associated with multimodal attributes, such as texts and images, resulting in Multimodal Attributed Graphs (MAGs).\nMAGB first provide 5 dataset from E-Commerce and Social Networks. And we evaluate two major paradigms: GNN-as Predictor and VLM-as-Predictor . The datasets are publicly available:\n\n     ðŸ¤— Hugging FaceÂ Â   | Â Â ðŸ“‘ PaperÂ Â \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“– Table of Contents\n\t\n\n\nðŸ“–â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sherirto/MAGB.","url":"https://huggingface.co/datasets/Sherirto/MAGB","creator_name":"Sherirto","creator_url":"https://huggingface.co/Sherirto","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["graph-ml","cc-by-4.0","Image","arxiv:2410.09132","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"matQnA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMatQnA: A Benchmark Dataset for Multi-modal Large Language Models in Materials Characterization and Analysis\n\t\n\nThis repository hosts the MatQnA dataset, a multi-modal benchmark dataset presented in the paper MatQnA: A Benchmark Dataset for Multi-modal Large Language Models in Materials Characterization and Analysis.\nMatQnA is specifically designed to evaluate the capabilities of AI models in the specialized field of materials characterization and analysis. It includes data from tenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/richardhzgg/matQnA.","url":"https://huggingface.co/datasets/richardhzgg/matQnA","creator_name":"richardhzgg","creator_url":"https://huggingface.co/richardhzgg","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"M3DRS","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tM3DRS: Multi-Modal Multi-Resolution Remote Sensing Dataset\n\t\n\nThis repository hosts the M3DRS dataset, a comprehensive collection of 5-channel remote sensing images (RGB, NIR, nDSM) from Switzerland, France, and Italy. The dataset is unlabelled and specifically designed to support self-supervised learning tasks. It is part of our submission to the NeurIPS 2025 Datasets and Benchmarks Track. The dataset is organized into three folders, each containing ZIP archives of images grouped byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/heig-vd-geo/M3DRS.","url":"https://huggingface.co/datasets/heig-vd-geo/M3DRS","creator_name":"HEIG-Vd Geomatic","creator_url":"https://huggingface.co/heig-vd-geo","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","cc-by-4.0","100B<n<1T","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"mmCultural","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tCultural Competence Dataset for Vision-Language Models\n\t\n\nThis dataset contains culturally diverse images and prompts for evaluating cultural competence in Vision-Language Models (VLMs), as presented in the paper Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation.\nCode: https://github.com/ArkaMukherjee0/mmCultural\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\n\nConcepts: 35 unique concepts (e.g., honesty, empathy, cooperation)\nCulturalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ArkaMukherjee/mmCultural.","url":"https://huggingface.co/datasets/ArkaMukherjee/mmCultural","creator_name":"Arka Mukherjee","creator_url":"https://huggingface.co/ArkaMukherjee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2024-18","keyword":"multimodal","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-18.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-18","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"csszengarden","keyword":"multimodal","description":"Technologic101/csszengarden dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Technologic101/csszengarden","creator_name":"Anthony Chapman","creator_url":"https://huggingface.co/Technologic101","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"android_control_test","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tProcessed Android Control Test Set for InfiGUI-R1 Evaluation\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the processed test set derived from the Android Control dataset by Google Research. It has been specifically prepared for evaluating the performance of our model, InfiGUI-R1.\nThe InfiGUI-R1 model is detailed in our paper:\n\nInfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners\n\nThis dataset facilitates standardized testing andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/InfiX-ai/android_control_test.","url":"https://huggingface.co/datasets/InfiX-ai/android_control_test","creator_name":"InfiX.ai","creator_url":"https://huggingface.co/InfiX-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K<n<10K","arxiv:2504.14239"],"keywords_longer_than_N":true},
	{"name":"engineering-drawings-as1100","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tEngineering Drawings AS1100 Compliance Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains engineering drawings with various AS1100 (Australian Standard for Technical Drawing) compliance issues for training AI models to identify missing elements and non-compliance issues in technical drawings.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Engineering Drawings AS1100 Compliance Dataset is designed to train and evaluate vision-language models on identifying compliance issues in technicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jcrzd/engineering-drawings-as1100.","url":"https://huggingface.co/datasets/jcrzd/engineering-drawings-as1100","creator_name":"JC","creator_url":"https://huggingface.co/jcrzd","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","visual-question-answering","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"SPATIAL-v1.0","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸš€ Aerospace Knowledge Dataset (VLM)\n\t\n\n\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\nThe Aerospace Knowledge Dataset is a large-scale, multi-modal dataset designed for training Vision-Language Models (VLMs) in the aerospace domain. It is built from over 26,000 pages of technical documents, research papers, engineering reports, and mission data from leading space organizations such as NASA, ArianeGroup, SpaceX, ESA, and others.  \nThis dataset is structured in a query + image format, allowing AI models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Youlln/SPATIAL-v1.0.","url":"https://huggingface.co/datasets/Youlln/SPATIAL-v1.0","creator_name":"Lalain Youri","creator_url":"https://huggingface.co/Youlln","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"HueManity","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tHueManity: A Benchmark for Testing Human-Like Visual Perception in MLLMs\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nHueManity is a benchmark dataset featuring 83,850 images designed to test the fine-grained visual perception of Multimodal Large Language Models (MLLMs). Each image presents a two-character alphanumeric string embedded within Ishihara-style dot patterns, challenging models to perform precise pattern recognition in visually cluttered environments.\nThe dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jayant-Sravan/HueManity.","url":"https://huggingface.co/datasets/Jayant-Sravan/HueManity","creator_name":"Jayant Sravan Tamarapalli","creator_url":"https://huggingface.co/Jayant-Sravan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","image-feature-extraction","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"android_control_test","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tProcessed Android Control Test Set for InfiGUI-R1 Evaluation\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the processed test set derived from the Android Control dataset by Google Research. It has been specifically prepared for evaluating the performance of our model, InfiGUI-R1.\nThe InfiGUI-R1 model is detailed in our paper:\n\nInfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners\n\nThis dataset facilitates standardized testing andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Reallm-Labs/android_control_test.","url":"https://huggingface.co/datasets/Reallm-Labs/android_control_test","creator_name":"Reallm Labs","creator_url":"https://huggingface.co/Reallm-Labs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K<n<10K","arxiv:2504.14239"],"keywords_longer_than_N":true},
	{"name":"DORI-Benchmark","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDORI (Discriminative Orientation Reasoning Intelligence) is a comprehensive benchmark designed to evaluate object orientation understanding in multimodal large language models (MLLMs). The benchmark isolates and evaluates orientation perception as a primary capability, offering a systematic assessment framework that spans four essential dimensions of orientation comprehension: frontal alignment, rotational transformations, relativeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/appledora/DORI-Benchmark.","url":"https://huggingface.co/datasets/appledora/DORI-Benchmark","creator_name":"Nazia Tasnim","creator_url":"https://huggingface.co/appledora","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Iranian_olympiad_of_informatics_multimodal_questions","keyword":"multimodal","description":"ckodser/Iranian_olympiad_of_informatics_multimodal_questions dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/ckodser/Iranian_olympiad_of_informatics_multimodal_questions","creator_name":"Arshia Soltani Moakhar","creator_url":"https://huggingface.co/ckodser","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"KoLLaVA-v1.5-Instruct-581k","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tKoLLaVA-v1.5-Instruct-581k\n\t\n\ní•œêµ­ì–´ Vision-Language ëª¨ë¸ì„ ìœ„í•œ instruction tuning ë°ì´í„°ì…‹ìž…ë‹ˆë‹¤.\n\n\t\n\t\t\n\t\të°ì´í„°ì…‹ ì •ë³´\n\t\n\n\nì´ ìƒ˜í”Œ ìˆ˜: 435,093ê°œ\ní˜•ì‹: ChatML í˜•ì‹ (role: user/assistant, content: í…ìŠ¤íŠ¸)\nì´ë¯¸ì§€: COCO + GQA + Visual Genome ë°ì´í„°ì…‹\nì–¸ì–´: í•œêµ­ì–´\n\n\n\t\n\t\t\n\t\tí¬í•¨ëœ ë°ì´í„°ì…‹\n\t\n\n\nCOCO ë°ì´í„°: 362,953ê°œ ìƒ˜í”Œ\n\nMS COCO 2017 ì´ë¯¸ì§€ ê¸°ë°˜\ní•œêµ­ì–´ ëŒ€í™” ë°ì´í„°\n\n\nGQA ë°ì´í„°: 72,140ê°œ ìƒ˜í”Œ\n\nGQA (Visual Question Answering) ì´ë¯¸ì§€ ê¸°ë°˜\ní•œêµ­ì–´ ëŒ€í™” ë°ì´í„°\n\n\nVisual Genome ë°ì´í„°: í¬í•¨\n\nVisual Genome ì´ë¯¸ì§€ ê¸°ë°˜\ní•œêµ­ì–´ ëŒ€í™” ë°ì´í„°\n\n\n\n\n\t\n\t\t\n\t\tì œì™¸ëœ ë°ì´í„°ì…‹\n\t\n\n\nEKVQA ë°ì´í„°: AI Hub ë¼ì´ì„ ìŠ¤ë¡œ ì¸í•´ ê³µê°œ ë¶ˆê°€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ko-vlm/KoLLaVA-v1.5-Instruct-581k.","url":"https://huggingface.co/datasets/ko-vlm/KoLLaVA-v1.5-Instruct-581k","creator_name":"korean-vision-language","creator_url":"https://huggingface.co/ko-vlm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Korean","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"CoreCognition","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tCoreCognition: A Core Knowledge Benchmark for Multi-modal Large Language Models\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCoreCognition is a large-scale benchmark encompassing 12 core knowledge grounded in developmental cognitive science, designed to evaluate the fundamental core abilities of Multi-modal Large Language Models (MLLMs).\nWhile MLLMs demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild remains limited, often falling short on tasksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/williamium/CoreCognition.","url":"https://huggingface.co/datasets/williamium/CoreCognition","creator_name":"William Li","creator_url":"https://huggingface.co/williamium","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"VS-Bench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVS-Bench is a multimodal benchmark for evaluating VLMs in multi-agent environments. We evaluate fourteen state-of-the-art models in eight vision-grounded environments with two complementary dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return.\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@article{xu2025vs,\n  title={VS-Bench: Evaluating VLMs for Strategic Reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zelaix/VS-Bench.","url":"https://huggingface.co/datasets/zelaix/VS-Bench","creator_name":"Zelai Xu","creator_url":"https://huggingface.co/zelaix","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"GameQA-140K","keyword":"multimodal","description":"\n\t\n\t\t\n\t\t1. Overview\n\t\n\nGameQA is a large-scale, diverse, and challenging multimodal reasoning dataset designed to enhance the general reasoning capabilities of Vision Language Models (VLMs). Generated using the innovative Code2Logic framework, it leverages game code to synthesize high-quality visual-language Chain-of-Thought (CoT) data. The dataset addresses the scarcity of multimodal reasoning data, critical for advancing complex multi-step reasoning in VLMs. Each sample includes visual gameâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Code2Logic/GameQA-140K.","url":"https://huggingface.co/datasets/Code2Logic/GameQA-140K","creator_name":"Game-RL","creator_url":"https://huggingface.co/Code2Logic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"AutoCaption","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ·ï¸ AutoCaption\n\t\n\nðŸ“„ Paper: Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search\nðŸ§  GitHub: AutoCaption  \nThis repository provides the SFT training data and MCTS-VCB evaluation benchmark generated by the AutoCaption framework.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“¦ Dataset Summary\n\t\n\nThis dataset contains 11,184 total samples across 2 subsets:\n\nsft_data â€“ for supervised fine-tuning of caption models  \nmcts_vcb â€“ for evaluation using MCTS-generated captions and keypointsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HasuerYu/AutoCaption.","url":"https://huggingface.co/datasets/HasuerYu/AutoCaption","creator_name":"Linhao Yu","creator_url":"https://huggingface.co/HasuerYu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","video-classification","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"unified-math-vision-dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tUnified Math Vision Dataset Bundle\n\t\n\nGenerated at: 2025-09-19 17:12:44\nThis is a unified dataset bundle containing multiple math and vision reasoning datasets.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\nTotal samples: 15858\n\nmathvision: 3344 samples\nwemath: 500 samples\nmmmu: 415 samples\nmathvista: 6141 samples\nlogicvista: 448 samples\ndynamath: 5010 samples\n\n\n\t\n\t\t\n\t\tContents\n\t\n\n\nmanifest.jsonl: Complete dataset in JSONL format (1 JSON per line)\nmanifest.csv: Summary in CSV format\nimages/: Directoryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Haonian/unified-math-vision-dataset.","url":"https://huggingface.co/datasets/Haonian/unified-math-vision-dataset","creator_name":"Haonian Ji","creator_url":"https://huggingface.co/Haonian","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2024-10","keyword":"multimodal","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-10.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-10","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"Resume-Analysis-CoTR","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tResume Reasoning and Feedback Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains approximately 417 examples designed to facilitate research and development in automated resume analysis and feedback generation. Each data point consists of a user query regarding their resume, a simulated internal analysis (chain-of-thought) performed by an expert persona, and a final, user-facing feedback response derived solely from that analysis.\nThe dataset captures a two-step reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR.","url":"https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"MMPR-Tiny","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMMPR-Tiny\n\t\n\nThis is the training data used during the online RL stage of InternVL3.5, which greatly improves the overall performance of InternVL3.5 across all scales. Our training code is also open-sourced.\nBased on MMPR-v1.2, we compute the accuracy of each query using the provided rollouts and select those whose model accuracy falls between 0.2 and 0.8 for online RL.\nWe further extend the dataset with recent multimodal datasets to enhance diversity.\nPlease refer to our paper forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMPR-Tiny.","url":"https://huggingface.co/datasets/OpenGVLab/MMPR-Tiny","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1M<n<10M","arxiv:2508.18265"],"keywords_longer_than_N":true},
	{"name":"AL-GR-Tiny","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAL-GR-Tiny: A Complete & Sampled Generative Recommendation Dataset\n\t\n\nPaper | Code | Project Page (AL-GR Org)\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAL-GR-Tiny is a compact, self-contained, and sampled version of the large-scale AL-GR ecosystem. It is designed for users who want to quickly experiment, develop, or understand the full pipeline of generative recommendation without needing to process terabytes of data.\nThis \"all-in-one\" repository bundles everything you need:\n\nPre-processedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AL-GR/AL-GR-Tiny.","url":"https://huggingface.co/datasets/AL-GR/AL-GR-Tiny","creator_name":"ALGR","creator_url":"https://huggingface.co/AL-GR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-retrieval","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-50","keyword":"multimodal","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-50.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-50","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"african-medical-multimodal-fracture","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAfrican Medical Multimodal Bone Fracture Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a comprehensive, multimodal bone break classification dataset specifically designed for African healthcare contexts. It addresses critical gaps in medical AI for resource-constrained environments while ensuring cultural sensitivity and local relevance.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Records: 1,129 multimodal medical cases\nOriginal Images: 1,128 X-ray images (89% of dataset)\nAugmentedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/electricsheepafrica/african-medical-multimodal-fracture.","url":"https://huggingface.co/datasets/electricsheepafrica/african-medical-multimodal-fracture","creator_name":"Electric Sheep","creator_url":"https://huggingface.co/electricsheepafrica","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","text-classification","other","English","French"],"keywords_longer_than_N":true},
	{"name":"ASPEDvb","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tASPED: Audio-Based Pedestrian Detection Dataset Card\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Audio Sensing for PEdestrian Detection (ASPED) v.b dataset is a comprehensive, 1,321-hour roadside collection of audio and video recordings designed for the task of pedestrian detection in the presence of vehicular noise. As urban sound emerges as a cost-effective and privacy-preserving alternative to vision-based or GPS-based monitoring, this dataset addresses the key challenge of detectingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/urbanaudiosensing/ASPEDvb.","url":"https://huggingface.co/datasets/urbanaudiosensing/ASPEDvb","creator_name":"Urban Audio Sensing","creator_url":"https://huggingface.co/urbanaudiosensing","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","Audio","Video","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"mmE5-synthetic","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tmmE5 Synthetic Data\n\t\n\nThis dataset contains synthetic datasets used for the finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\n\nClassification\nRetrieval\nVQA\n\nGithub\n\n\t\n\t\t\n\t\n\t\n\t\tImage Preparation\n\t\n\nFirst, you should prepare the images used for training:\n\n\t\n\t\t\n\t\n\t\n\t\tImage Downloads\n\t\n\n\nDownload mmE5-synthetic Images:\n\nRun the following command to download and extract the images only in this dataset.\nmkdir -p images && cd images\nwgetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-synthetic.","url":"https://huggingface.co/datasets/intfloat/mmE5-synthetic","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MoCa-CL-Pairs","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMoCa Contrastive Learning Data\n\t\n\nðŸ  Homepage | ðŸ’» Code | ðŸ¤– MoCa-Qwen25VL-7B | ðŸ¤– MoCa-Qwen25VL-3B | ðŸ“š Datasets | ðŸ“„ Paper\nThis dataset contains datasets used for the supervised finetuning of MoCa (MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings):\n\nMMEB (with hard negative)\nInfoSeek (from M-BEIR)\nTAT-DQA\nArxivQAVisRAG\nViDoRe\nColPali\nE5 text pairs (can not release due to restrictions of Microsoft)\n\n\n\t\n\t\t\n\t\tImage Preparation\n\t\n\nFirst, youâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/moca-embed/MoCa-CL-Pairs.","url":"https://huggingface.co/datasets/moca-embed/MoCa-CL-Pairs","creator_name":"MoCa","creator_url":"https://huggingface.co/moca-embed","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"robonar","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ“‡ RONAR (RoboNar) Dataset\n\t\n\nðŸ“„ Paper on arXiv  | ðŸŒ Project Website\nRONAR introduces a real-world multimodal dataset paired with natural language narrations for robotic experience grounding. Built on the Stretch SE3 mobile manipulator in real home environments, the dataset supports behavior transparency, risk estimation, and failure recovery for intelligent robotics systems. It underlies the RONAR framework described in the CoRL 2024 paper: \"I Can Tell What I Am Doing: Towardâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/robonar/robonar.","url":"https://huggingface.co/datasets/robonar/robonar","creator_name":"RoboNar","creator_url":"https://huggingface.co/robonar","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","summarization","object-detection","robotics","English"],"keywords_longer_than_N":true},
	{"name":"EEE-Bench","keyword":"multi-modal-qa","description":"\n\t\n\t\t\n\t\tEEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark\n\t\n\n\n\t\n\t\t\n\t\tIntroduction:\n\t\n\nEEE-Bench is a multimodal benchmark designed to evaluate the practical engineering capabilities of large multimodal models (LMMs), using electrical and electronics engineering (EEE) as the domain focus. It comprises 2,860 carefully curated problems across 10 core subdomains, including analog circuits and control systems, featuring complex visual inputs such as abstractâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/afdsafas/EEE-Bench.","url":"https://huggingface.co/datasets/afdsafas/EEE-Bench","creator_name":"Ming Li","creator_url":"https://huggingface.co/afdsafas","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"LexSemBridge_eval","keyword":"multimodal","description":"This dataset contains the training set and test set required for LexSemBridge.\n\nPaper: LexSemBridge: Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation\n\nCode: https://github.com/Jasaxion/LexSemBridge/\n\n\n\n\t\n\t\t\n\t\tPreparation\n\t\n\n1. You need to clone or download the entire repository.\n2. conda create -n lexsem python=3.10\n3. conda activate lexsem\n4. cd LexSemBridge\n5. pip install -r requirements.txt\n\n\n\t\n\t\t\n\t\tDataset and Model\n\t\n\n\nDataset Downloadâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jasaxion/LexSemBridge_eval.","url":"https://huggingface.co/datasets/Jasaxion/LexSemBridge_eval","creator_name":"Shaoxiong Zhan","creator_url":"https://huggingface.co/Jasaxion","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","arxiv:2508.17858","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"ChartMimic","keyword":"multimodal","description":"\n\n ChartMimic: Evaluating LMMâ€™s Cross-Modal Reasoning Capability via Chart-to-Code Generation \n\n\nThis is the official dataset repository of ChartMimic.\n\nKind Note: ChartMimic has been integrated into VLMEvalKit. Welcome to use ChartMimic through VLMEvalKit! Special thanks to the VLMEvalKit team.\n\n\n\t\n\t\t\n\t\n\t\n\t\t1. Data Overview\n\t\n\nChartMimic aims at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChartMimic/ChartMimic.","url":"https://huggingface.co/datasets/ChartMimic/ChartMimic","creator_name":"ChartMimic","creator_url":"https://huggingface.co/ChartMimic","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"airbus-vision-dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAIRBUS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from Airbus technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice at TW3 Partners, a company specialized in Generative AI. Passionate about artificial intelligence andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/airbus-vision-dataset.","url":"https://huggingface.co/datasets/Davidsv/airbus-vision-dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"finevision-sample","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tFineVision Sample Dataset\n\t\n\nA comprehensive multimodal dataset containing samples across multiple categories, designed for visual question answering and multimodal understanding tasks. This dataset follows the same format as the official HuggingFaceM4/FineVision dataset.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset is organized into separate folders for each source category, making it easy to load specific subsets of the data.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach sample contains:\n\nid: Uniqueâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dinesh-vlmrun/finevision-sample.","url":"https://huggingface.co/datasets/dinesh-vlmrun/finevision-sample","creator_name":"vlmrun","creator_url":"https://huggingface.co/dinesh-vlmrun","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Item-EMB","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAL-GR/Item-EMB: Multi-modal Item Embeddings\n\t\n\nPaper: FORGE: Forming Semantic Identifiers for Generative Retrieval in Industrial Datasets\nCode: https://github.com/selous123/al_sid\nProject Page: https://huggingface.co/AL-GR\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository, AL-GR/Item-EMB, is a companion dataset to the main AL-GR generative recommendation dataset. It contains the 512-dimensional multi-modal embeddings for over 500 million items that appear in the AL-GR sequences.\nEach item isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AL-GR/Item-EMB.","url":"https://huggingface.co/datasets/AL-GR/Item-EMB","creator_name":"ALGR","creator_url":"https://huggingface.co/AL-GR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","image-feature-extraction","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React-Multi-Images","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Multi-Images.","url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Multi-Images","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"MedXpertQA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for MedXpertQA\n\t\n\n\n\nMedXpertQA is a highly challenging and comprehensive benchmark designed to evaluate expert-level medical knowledge and advanced reasoning capabilities. It features both text-based and multimodal question-answering tasks, with the multimodal subset leveraging structured clinical information alongside images.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMedXpertQA comprises 4,460 questions spanning diverse medical specialties, tasks, body systems, and image types. Itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA.","url":"https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA","creator_name":"TsinghuaC3I","creator_url":"https://huggingface.co/TsinghuaC3I","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","question-answering","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"FrenchBee_dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tFRENCHBEE-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from FrenchBee technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice at TW3 Partners, a company specialized in Generative AI. Passionate about artificial intelligenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/FrenchBee_dataset.","url":"https://huggingface.co/datasets/Davidsv/FrenchBee_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"StepEval-Audio-Toolcall","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tStepEval-Audio-Toolcall\n\t\n\nPaper: Step-Audio 2 Technical ReportCode: https://github.com/stepfun-ai/Step-Audio2Project Page: https://www.stepfun.com/docs/en/step-audio2  \n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nStepEval Audio Toolcall evaluates the invocation performance of four tool types. For each tool, the benchmark contains approximately 200 multi-turn dialogue sets for both positive and negative scenarios:\n\nPositive samples: The assistant is required to invoke the specified tool in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stepfun-ai/StepEval-Audio-Toolcall.","url":"https://huggingface.co/datasets/stepfun-ai/StepEval-Audio-Toolcall","creator_name":"StepFun","creator_url":"https://huggingface.co/stepfun-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["audio-text-to-text","apache-2.0","arxiv:2507.16632","ðŸ‡ºðŸ‡¸ Region: US","benchmark"],"keywords_longer_than_N":true},
	{"name":"mmE5-MMEB-hardneg","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tmmE5 Labeled Data\n\t\n\nThis dataset contains datasets used for the supervised finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\n\nMMEB (with hard negative)\nInfoSeek (from M-BEIR)\nTAT-DQA\nArxivQA\n\nGithub\n\n\t\n\t\t\n\t\n\t\n\t\tImage Preparation\n\t\n\nFirst, you should prepare the images used for training:\n\n\t\n\t\t\n\t\n\t\n\t\tImage Downloads\n\t\n\n\nDownload All Images Used in mmE5:\n\nYou can use the script provided in our source code to download all images usedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg.","url":"https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ScanBot","keyword":"multimodal","description":" \n\n\t\n\t\t\n\t\tScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems\n\t\n\n\n\nThis dataset is presented in the paper ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems.\n\n\t\n\t\t\n\t\tðŸ§  Dataset Summary\n\t\n\nScanBot is a dataset for instruction-conditioned, high-precision surface scanning with robots. Unlike existing datasets that focus on coarse tasks like grasping or navigation, ScanBot targets industrial laser scanning, where sub-millimeter accuracy and parameterâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ed1son/ScanBot.","url":"https://huggingface.co/datasets/ed1son/ScanBot","creator_name":"zhiling chen","creator_url":"https://huggingface.co/ed1son","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","English","apache-2.0","arxiv:2505.17295","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"NOAH-mini","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tMOAH mini\n\t\n\nThe dataset prest here is a very samll sample of NOAH dataset.\nIn the original dataset each satellite image is ~650MB with 234,089 images present in 11 bands.\nIt is not feasible to upload the complete dataset. \nA sample of the dataset across diffrent modalities can be seen in the figure below:\n\nThe diffrence between NOAH and NOAH mini is hilighted in the figure below.\nEach subplot is a band of Landsat 8 in NOAH.\nThe region hilighted in red is the region available in NOAHâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mutakabbirCarleton/NOAH-mini.","url":"https://huggingface.co/datasets/mutakabbirCarleton/NOAH-mini","creator_name":"Mutakabbir","creator_url":"https://huggingface.co/mutakabbirCarleton","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-image","mit","< 1K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"banque_vision","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ“Š Banque_Vision: A Multimodal Dataset for Document Understanding\n\t\n\n\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\nBanque_Vision is a multimodal dataset designed for document-based question answering (QA) and information retrieval. It combines textual data and visual document representations, enabling research on how vision models and language models interact for document comprehension.\nðŸ”— Created by: Matteo KhanðŸŽ“ Affiliation: TW3Partners \nðŸ“ License: MIT  \nðŸ”— Connect with me on LinkedInðŸ”— Dataset onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MatteoKhan/banque_vision.","url":"https://huggingface.co/datasets/MatteoKhan/banque_vision","creator_name":"Khan Matteo","creator_url":"https://huggingface.co/MatteoKhan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"ShowUI-web-8k","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tShowUI-web-8K\n\t\n\nThis dataset is a curated 8K-sample subset from the original ShowUI-web dataset, as mentioned in our paper. It contributes to the training of GUI grounding models, with a focus on realistic web user interfaces collected from diverse websites.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: Sampled from ShowUI-web  \nDomain: Web GUI screenshots  \nDiversity: Covers a wide variety of website layouts and components  \nUse case: GUI grounding pretraining for web environmentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zonghanHZH/ShowUI-web-8k.","url":"https://huggingface.co/datasets/zonghanHZH/ShowUI-web-8k","creator_name":"Hsieh ZongHan","creator_url":"https://huggingface.co/zonghanHZH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"videophy2_test","keyword":"multimodal","description":"Project: https://github.com/Hritikbansal/videophy/tree/main/VIDEOPHY2\ncaption: original prompt in the dataset\nvideo_url: generated video (using original prompt or upsampled caption, depending on the video model)\nsa: semantic adherence score (1-5) from human evaluation\npc: physical commonsense score (1-5) from human evaluation\njoint: computed as sa >= 4, pc >= 4\nphysics_rules_followed: list of physics rules followed in the video as judged by human annotators (1)\nphysics_rules_unfollowed: listâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/videophysics/videophy2_test.","url":"https://huggingface.co/datasets/videophysics/videophy2_test","creator_name":"videophysics","creator_url":"https://huggingface.co/videophysics","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-classification","mit","1K - 10K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"repository-learning","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tRepository Learning Training Dataset\n\t\n\nThis dataset contains training data extracted from GitHub repositories for training context-aware code review models. The dataset supports three primary machine learning tasks: contrastive learning, fine-tuning, and semantic indexing.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nPurpose: Enable training of AI models that understand repository-specific code review patterns and provide contextual feedback.\nSource: GitHub repositories with rich pull request historyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kotlarmilos/repository-learning.","url":"https://huggingface.co/datasets/kotlarmilos/repository-learning","creator_name":"Milos Kotlar","creator_url":"https://huggingface.co/kotlarmilos","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","text-retrieval","feature-extraction","machine-generated"],"keywords_longer_than_N":true},
	{"name":"GenS-Video-150K","keyword":"multimodal","description":"\nðŸ”— Project Page Â· ðŸ“– Paper Â· â­ GitHub Â· ðŸ“Š Dataset Â· ðŸ¤— Checkpoints\n\n\n\n\n\t\n\t\t\n\t\tGenS-Video-150K Dataset\n\t\n\nTo enable effective frame sampling, we introduce GenS-Video-150K, a large-scale synthetic dataset specifically designed for training frame sampling models. Annotated by GPT-4o, this dataset features:\n\nDense coverage: Annotates ~20% of all frames with relevance scores.\nFine-grained assessment: Assigns confidence scores (level 1 to 5) to relevant frames.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statisticsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yaolily/GenS-Video-150K.","url":"https://huggingface.co/datasets/yaolily/GenS-Video-150K","creator_name":"Linli Yao","creator_url":"https://huggingface.co/yaolily","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K<n<100K","arxiv:2503.09146"],"keywords_longer_than_N":true},
	{"name":"MultiCaRe_Dataset","keyword":"multimodal","description":"The dataset contains multi-modal data from over 75,000 open access and de-identified case reports, including metadata, clinical cases, image captions and more than 130,000 images. Images and clinical cases belong to different medical specialties, such as oncology, cardiology, surgery and pathology. The structure of the dataset allows to easily map images with their corresponding article metadata, clinical case, captions and image labels. Details of the data structure can be found in the fileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset.","url":"https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset","creator_name":"Mauro Nievas Offidani","creator_url":"https://huggingface.co/mauro-nievoff","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"central-florida-native-plants","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDeepEarth Central Florida Native Plants Dataset v0.2.0\n\t\n\n\n\t\n\t\t\n\t\tðŸŒ¿ Dataset Summary\n\t\n\nA comprehensive multimodal dataset featuring 33,665 observations of 232 native plant species from Central Florida. This dataset combines citizen science observations with state-of-the-art vision and language embeddings for advancing multimodal self-supervised ecological intelligence research.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nðŸŒ Spatiotemporal Coverage: Complete GPS coordinates and timestamps for allâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deepearth/central-florida-native-plants.","url":"https://huggingface.co/datasets/deepearth/central-florida-native-plants","creator_name":"DeepEarth","creator_url":"https://huggingface.co/deepearth","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","feature-extraction","zero-shot-classification","English","mit"],"keywords_longer_than_N":true},
	{"name":"PixelsPointsPolygons","keyword":"multimodal","description":"The P3 dataset is a large-scale multimodal benchmark for building vectorization, constructed from aerial LiDAR point clouds, high-resolution aerial imagery, and vectorized 2D building outlines, collected across three continents.","url":"https://huggingface.co/datasets/rsi/PixelsPointsPolygons","creator_name":"Raphael Sulzer","creator_url":"https://huggingface.co/rsi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-segmentation","object-detection","English","cc-by-4.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"vwp","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for Visual Writing Prompts Dataset (VWP)\n\t\n\nWebsite | Github Repository | arXiv e-Print\n\n\nThe Visual Writing Prompts (VWP) dataset contains almost 2K selected sequences of\nmovie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which are collected via crowdsourcing given the image sequences and up to 5  grounded characters from the corresponding image sequence.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\n\nTACLâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tonyhong/vwp.","url":"https://huggingface.co/datasets/tonyhong/vwp","creator_name":"Xudong Hong","creator_url":"https://huggingface.co/tonyhong","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","monolingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"llava-pretrain-refined-by-data-juicer","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tLLaVA pretrain -- LCS-558k (refined by Data-Juicer)\n\t\n\nA refined version of LLaVA pretrain dataset (LCS-558k) by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Multimodal Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 115MB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 500,380 (Keep ~89.65% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Oden-Image250KP","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOden-Image250KP\n\t\n\nOden-Image250KP is a comprehensive dataset comprising 250,000 meticulously crafted image prompts designed for training and evaluating text-to-image generation models. The dataset encompasses a diverse range of categories, from natural scenes and human activities to abstract concepts, providing a rich resource for researchers and developers in the field of generative AI.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is provided in CSV format with the following columns:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BBSRguy/Oden-Image250KP.","url":"https://huggingface.co/datasets/BBSRguy/Oden-Image250KP","creator_name":"Rashmi Ranjan Dash","creator_url":"https://huggingface.co/BBSRguy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-to-image","English","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"vwp","keyword":"multimodality","description":"\n\t\n\t\t\n\t\tDataset Card for Visual Writing Prompts Dataset (VWP)\n\t\n\nWebsite | Github Repository | arXiv e-Print\n\n\nThe Visual Writing Prompts (VWP) dataset contains almost 2K selected sequences of\nmovie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which are collected via crowdsourcing given the image sequences and up to 5  grounded characters from the corresponding image sequence.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\n\nTACLâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tonyhong/vwp.","url":"https://huggingface.co/datasets/tonyhong/vwp","creator_name":"Xudong Hong","creator_url":"https://huggingface.co/tonyhong","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","monolingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MaCBench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMaCBench\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\tâš ï¸ IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tðŸš« THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY ðŸš«\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluation results. Pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"Set_Eval","keyword":"multimodal","description":"AarushSah/Set_Eval dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/AarushSah/Set_Eval","creator_name":"Aarush Sah","creator_url":"https://huggingface.co/AarushSah","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"BioTrove","keyword":"multimodal","description":"\n\t\n\t\t\n\t\n\t\n\t\tBioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity\n\t\n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nSee the BioTrove-Train dataset card on HuggingFace to access the samller BioTrove-Train dataset (40M)\nBioTrove comprises well-processed metadata with full taxa information and URLs pointing to image files. The metadata can be used to filter specific categories, visualize data distribution, and manage imbalance effectively. We provide a collectionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BGLab/BioTrove.","url":"https://huggingface.co/datasets/BGLab/BioTrove","creator_name":"Baskar Group","creator_url":"https://huggingface.co/BGLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","English","mit","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"T2I-Keypoints-Eval","keyword":"multimodal","description":"\n\n\n\t\n\t\t\n\t\tT2I-Keypoints-Eval Dataset\n\t\n\nA Bilingual Text-to-Image Keypoints Evaluation Benchmark\nLinqing Wang Â·\nXiming Xing Â·\nYiji Cheng Â·\nZhiyuan Zhao Â·\nJiale Tao Â·\nQiXun Wang Â·\nRuihuang Li Â·\nComi Chen Â·\nXin Li Â·\nMingrui Wu Â·\nXinchi Deng Â·\nChunyu Wangâ€  Â·\nQinglin Lu*\nTencent Hunyuan\nâ€ Project Lead Â· *Corresponding Author\n\n\n\n  \n  \n  \n  \n  \n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nT2I-Keypoints-Eval is a comprehensive bilingual evaluation dataset designed to assess text-to-image models' ability to generate imagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PromptEnhancer/T2I-Keypoints-Eval.","url":"https://huggingface.co/datasets/PromptEnhancer/T2I-Keypoints-Eval","creator_name":"PromptEnhancer","creator_url":"https://huggingface.co/PromptEnhancer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"test-public","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tnanoLLaVA - Sub 1B Vision-Language Model\n\t\n\n\n  \n\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nnanoLLaVA is a \"small but mighty\" 1B vision-language model designed to run efficiently on edge devices.\n\nBase LLM: Quyen-SE-v0.1 (Qwen1.5-0.5B)\nVision Encoder: google/siglip-so400m-patch14-384\n\n\n\t\n\t\t\nModel\nVQA v2\nTextVQA\nScienceQA\nPOPE\nMMMU (Test)\nMMMU (Eval)\nGQA\nMM-VET\n\n\n\t\t\nScore\n70.84\n46.71\n58.97\n84.1\n28.6\n30.4\n54.79\n23.9\n\n\n\t\n\t\t\n\t\tTraining Data\n\t\n\nTraining Data will be released later as I am still writing aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/taiseimatsuoka/test-public.","url":"https://huggingface.co/datasets/taiseimatsuoka/test-public","creator_name":"taisei matsuoka","creator_url":"https://huggingface.co/taiseimatsuoka","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","text","Image"],"keywords_longer_than_N":true},
	{"name":"DRGBT603","keyword":"multimodal","description":"zhaodong2061/DRGBT603 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/zhaodong2061/DRGBT603","creator_name":"zhaodongding","creator_url":"https://huggingface.co/zhaodong2061","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K<n<1M","Image","doi:10.57967/hf/5438"],"keywords_longer_than_N":true},
	{"name":"SACap-eval","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tSACap-1M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSACap-Eval, a benchmark curated from a subset of SACap-1M for evaluating segmentation-mask-to-image quality. It comprises 4,000 prompts with detailed entity descriptions and corresponding segmentation masks, with an average of 5.7 entities per image. Evaluation is conducted from two perspectives: Spatial and Attribute. Both aspects are assessed using the vision-language model Qwen2-VL-72B via a visual question answering manner.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/0xLDF/SACap-eval.","url":"https://huggingface.co/datasets/0xLDF/SACap-eval","creator_name":"0xLDF","creator_url":"https://huggingface.co/0xLDF","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"SMMILE-plusplus","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE-plusplus.","url":"https://huggingface.co/datasets/smmile/SMMILE-plusplus","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"UGround-V1-8k","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tUGround-WebHybrid-8K\n\t\n\nThis dataset is a curated 8K-sample subset from the original UGround-V1-Data (Web-Hybrid), as mentioned in our paper. It serves as part of the training corpus for GUI grounding tasks, focusing on diverse web interface screenshots across resolutions and aspect ratios.\n\n\t\n\t\t\n\t\tPaper and Code\n\t\n\n\nPaper: ZonUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding\nCode: https://github.com/zonghanHZH/ZonUI-3B\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zonghanHZH/UGround-V1-8k.","url":"https://huggingface.co/datasets/zonghanHZH/UGround-V1-8k","creator_name":"Hsieh ZongHan","creator_url":"https://huggingface.co/zonghanHZH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","json","Image"],"keywords_longer_than_N":true},
	{"name":"ColonINST-v1","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tColonINST-v1 Data Card\n\t\n\nA large-scale mutlimodal instruction tuning dataset for colonoscopy research. More details refer to our project page: https://github.com/ai4colonoscopy/ColonGPT.\n\n\n\t\n\t\t\n\t\n\t\n\t\tData description\n\t\n\nWe introduce a pioneering instruction tuning dataset for multimodal colonoscopy research, aimed at instructing models to execute user-driven tasks interactively. This dataset comprises of 62 categories, 300K+ colonoscopic images, 128K+ medical captions (GPT-4V)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai4colonoscopy/ColonINST-v1.","url":"https://huggingface.co/datasets/ai4colonoscopy/ColonINST-v1","creator_name":"ai4colonoscopy","creator_url":"https://huggingface.co/ai4colonoscopy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"MMPR-v1.2","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMMPR-v1.2\n\t\n\n[ðŸ“‚ GitHub] | [ðŸŒ Project Page] | [ðŸ“œ Paper (InternVL3.5)] | [ðŸ“œ Paper (MMPR/MPO)] | [ðŸ†• Blog (MPO)] | [ðŸ“– Documents]\nThis is a newer version of MMPR and MMPR-v1.1, which includes additional data sources to enhance the data diversity and greatly improves the overall performance of InternVL3.5 across all scales. The prompts used to build this dataset is released in MMPR-v1.2-prompts.\nTo unzip the archive of images, please first run cat images.zip_* > images.zip and then runâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2.","url":"https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1M<n<10M","arxiv:2508.18265"],"keywords_longer_than_N":true},
	{"name":"OceanGym","keyword":"multi-modal","description":" ðŸŒŠ OceanGym ðŸ¦¾ \n A Benchmark Environment for Underwater Embodied Agents \n\n\n  ðŸŒ Home Page\n  ðŸ“„ Paper\n  ðŸ’» Code\n  ðŸ¤— Hugging Face\n  â˜ï¸ Google Drive\n  â˜ï¸ Baidu Drive\n\n\n\n\nOceanGym is a high-fidelity embodied underwater environment that simulates a realistic ocean setting with diverse scenes. As illustrated in figure, OceanGym establishes a robust benchmark for evaluating autonomous agents through a series of challenging tasks, encompassing various perception analyses and decision-makingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zjunlp/OceanGym.","url":"https://huggingface.co/datasets/zjunlp/OceanGym","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","English","mit","arxiv:2509.26536","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"polymath","keyword":"multi-modal-qa","description":"\n\t\n\t\t\n\t\tPaper Information\n\t\n\nWe present PolyMATH, a challenging benchmark aimed at evaluating the general cognitive reasoning abilities of MLLMs. \nPolyMATH comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning. \nWe conducted a comprehensive, and quantitative evaluation of 15 MLLMs using four diverse prompting strategies, including Chain-of-Thoughtâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/him1411/polymath.","url":"https://huggingface.co/datasets/him1411/polymath","creator_name":"Himanshu Gupta","creator_url":"https://huggingface.co/him1411","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","expert-generated","found","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"MMAT-1M","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMMAT-1M Dataset Card\n\t\n\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\tDataset details\n\t\n\n\n\t\n\t\t\n\t\tDataset type\n\t\n\nMMAT-1M is a million-scale multimodal agent tuning dataset, built by consolidating subsets of five publicly available multimodal question-answer datasets: Visual CoT, LLaVA-CoT, The Cauldron, TabMWP, and Infoseek. It integrates dynamically generated API calls and Retrieval Augmented Generation (RAG) information through a GPT-4o-powered multi-turn paradigm, with rationales refined viaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VIS-MPU-Agent/MMAT-1M.","url":"https://huggingface.co/datasets/VIS-MPU-Agent/MMAT-1M","creator_name":"VIS-MPU-Agent","creator_url":"https://huggingface.co/VIS-MPU-Agent","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","cc-by-4.0","cc-by-nc-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"SAMM","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tResource ðŸ“–\n\t\n\n\nÂ  [ACM MM Paper] | [SAMM HF] | [CAP HF] | [Github Code]\n\n\n\n\n\t\n\t\t\n\t\tNotes âš ï¸\n\t\n\n\nIf you want to import the CAP data into your own dataset, please refer to this.\nIf you want to run RamDG on datasets other than SAMM and use CNCL to incorporate external knowledge, please ensure to configure idx_cap_texts and idx_cap_images in the dataset jsons.\nWe have upgraded the SAMM JSON files. The latest versions (SAMM with CAP or without CAP) are available on July 24, 2025. Pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SJJ0854/SAMM.","url":"https://huggingface.co/datasets/SJJ0854/SAMM","creator_name":"Jinjie Shen","creator_url":"https://huggingface.co/SJJ0854","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","text-classification","image-text-to-text","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"multimodal_low-resource_language_translation","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for Multimodal Low-Resource Language Translation Dataset\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the dataset for our paper \"From Text to Multi-Modal: Advancing Low-Resource-Language Translation through Synthetic Data Generation and Cross-Modal Alignments\" accepted by the workshop LoResMT 2025 of NAACL 2025â€¦ See the full description on the dataset page: https://huggingface.co/datasets/qianstats/multimodal_low-resource_language_translation.","url":"https://huggingface.co/datasets/qianstats/multimodal_low-resource_language_translation","creator_name":"Qian Shen","creator_url":"https://huggingface.co/qianstats","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","English","Yoruba","Tigrinya","Hausa"],"keywords_longer_than_N":true},
	{"name":"pokemon-gpt4o-captions","keyword":"multimodal","description":"Borrowed from: https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions\nYou can use it in LLaMA Factory by specifying dataset: pokemon_cap.\n","url":"https://huggingface.co/datasets/llamafactory/pokemon-gpt4o-captions","creator_name":"LLaMA Factory","creator_url":"https://huggingface.co/llamafactory","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"Clothing","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ“– Model Card: [REARM]\n\t\n\n\n\t\n\t\t\n\t\t\"[Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation]\",Shouxing Ma, Yawen Zeng, Shiqing Wu, and Guandong XuPublished in [ACM MM], 2025.[Paper Link] [Code Repository]\n\t\n\n","url":"https://huggingface.co/datasets/MrShouxingMa/Clothing","creator_name":"ShouxingMa","creator_url":"https://huggingface.co/MrShouxingMa","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["English","cc-by-4.0","arxiv:2508.13745","ðŸ‡ºðŸ‡¸ Region: US","recommender-system"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-ArXiv","keyword":"multimodal","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-ArXiv.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-ArXiv","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"FreeAskWorld","keyword":"multimodal","description":"doraemonILoveYou/FreeAskWorld dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/doraemonILoveYou/FreeAskWorld","creator_name":"peng","creator_url":"https://huggingface.co/doraemonILoveYou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["apache-2.0","100M<n<1B","ðŸ‡ºðŸ‡¸ Region: US","E2E","Multimodal"],"keywords_longer_than_N":true},
	{"name":"CountQA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nCountQA is the new benchmark designed to stress-test the Achilles' heel of even the most advanced Multimodal Large Language Models (MLLMs): object counting. While modern AI demonstrates stunning visual fluency, it often fails at this fundamental cognitive skill, a critical blind spot limiting its real-world reliability.\nThis dataset directly confronts that weakness with over 1,500 challenging question-answer pairs built on real-world images, hand-captured to featureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jayant-Sravan/CountQA.","url":"https://huggingface.co/datasets/Jayant-Sravan/CountQA","creator_name":"Jayant Sravan Tamarapalli","creator_url":"https://huggingface.co/Jayant-Sravan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"universal-preference-hijacking-datasets","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tPhi: Preference Hijacking in Multi-modal Large Language Models at Inference Time\n\t\n\n\n  \n  \n  Figure 1: Examples of Phi, which can hijack MLLM's preference toward the image.\n\n\n\n  \n  \n  Figure 2: Example of a universal hijacking perturbation, which can be transferred across different images.\n\n\nThis dataset is used to train and evaluate the universal hijacking perturbations in the paper \"Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time\", accepted at EMNLPâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yflantmy/universal-preference-hijacking-datasets.","url":"https://huggingface.co/datasets/yflantmy/universal-preference-hijacking-datasets","creator_name":"Yifan Lan","creator_url":"https://huggingface.co/yflantmy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","image-text-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-1.5-Instruct-Data","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-1.5 Instruction Data\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset, LLaVA-OneVision-1.5-Instruct, was collected and integrated during the development of LLaVA-OneVision-1.5. LLaVA-OneVision-1.5 is a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. This meticulously curated 22M instruction dataset (LLaVA-OneVision-1.5-Instruct) is part of a comprehensive andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Instruct-Data.","url":"https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Instruct-Data","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2509.23661","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"MAGIC-CT","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMAGIC-CT: Multiorgan Annotation and Grounded Image Captioning in CT for Cancer\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nMAGIC-CT is a comprehensive, multimodal dataset designed to advance artificial intelligence in abdominal oncology. It addresses the critical need for resources that bridge the gap between radiological imaging and clinical language by pairing high-resolution 3D Computed Tomography (CT) scans with expert-authored radiology reports.\nThe dataset includes 562 patients with variousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zangir/MAGIC-CT.","url":"https://huggingface.co/datasets/zangir/MAGIC-CT","creator_name":"Zangir Iklassov","creator_url":"https://huggingface.co/zangir","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["English","Russian","kz","cc0-1.0","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Sanctuaria-Gaze","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tSanctuaria-Gaze\n\t\n\nSanctuaria-Gaze is a multimodal egocentric dataset collected from visits to four architecturally and culturally significant sanctuaries in Northern Italy.The dataset captures human gaze behavior, head motion, and visual exploration in real-world sacred environments, providing a unique resource for research on visual attention, embodied perception, and humanâ€“environment interaction.\nðŸ“˜ Paper: Sanctuaria-Gaze: A Multimodal Egocentric Dataset for Human Attentionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vcuculo/Sanctuaria-Gaze.","url":"https://huggingface.co/datasets/vcuculo/Sanctuaria-Gaze","creator_name":"vittorio cuculo","creator_url":"https://huggingface.co/vcuculo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-classification","object-detection","image-segmentation","other","English"],"keywords_longer_than_N":true},
	{"name":"inoi","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tINOI Math Olympiad Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 1,135 math problems from the Iranian National Olympiad in Informatics (INOI), spanning multiple competition rounds from 2006-2024. Each problem includes the original problem statement, detailed solution, and associated images.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nðŸŽ¯ 1,135 curated problems with full solutions\nðŸ“Š Train/Test split: 908 / 227 examples\nðŸ–¼ï¸ 1,228 embedded images (100% coverage)\nðŸ“ Multiple problem types:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/combviz/inoi.","url":"https://huggingface.co/datasets/combviz/inoi","creator_name":"CombiGraph-Vis","creator_url":"https://huggingface.co/combviz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ArxivCap","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tDataset Card for ArxivCap\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n\nExample-1 of single (image, caption) pairs\n\n\"......\" stands for omitted parts.\n\n{\n    'src': 'arXiv_src_2112_060/2112.08947', \n    'meta': \n    {\n        'meta_from_kaggle': \n        {\n            'journey': '', \n            'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', \n            'categories': 'cs.ET'\n        }, \n        'meta_from_s2': \n        {\n            'citationCount': 8â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/ArxivCap.","url":"https://huggingface.co/datasets/MMInstruction/ArxivCap","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"clevr-math","keyword":"multimodal","description":"CLEVR-Math is a dataset for compositional language, visual and mathematical reasoning. CLEVR-Math poses questions about mathematical operations on visual scenes using subtraction and addition, such as \"Remove all large red cylinders. How many objects are left?\". There are also adversarial (e.g. \"Remove all blue cubes. How many cylinders are left?\") and multihop questions (e.g. \"Remove all blue cubes. Remove all small purple spheres. How many objects are left?\").","url":"https://huggingface.co/datasets/dali-does/clevr-math","creator_name":"Adam Dahlgren LindstrÃ¶m","creator_url":"https://huggingface.co/dali-does","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","visual-question-answering","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"albi-captioned-photos","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAlbi, France Image Dataset\n\t\n\nA collection of high-resolution scenic photographs from Albi, France with AI-generated descriptive captions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains scenic photographs from Albi, France, including the city center, the Toulouse Lautrec museum, and the Sainte-CÃ©cile Cathedral. All images were captured using a Sony A6600 camera and are paired with detailed English captions generated by Mistral AI's Pixtral-Large model.\nKey Features:\n\nHigh-resolutionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/albi-captioned-photos.","url":"https://huggingface.co/datasets/NoeFlandre/albi-captioned-photos","creator_name":"NoÃ© Flandre","creator_url":"https://huggingface.co/NoeFlandre","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"geo_70k_multiplets_natural_language_annotation","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset contains a representation of RNA sequencing data and text descriptions.\nDataset type: multiplets (suitable for relevant contrastive-learning or inference tasks).\nCell Sentence Length: The cell sentences in this dataset have a length of 4096 genes.\nThe RNA sequencing data used for training was originally gathered and annotated in the CellWhisperer project. It is derived from\nCellxGene and GEO. Detailed information on the gathering and annotation of the dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jo-mengr/geo_70k_multiplets_natural_language_annotation.","url":"https://huggingface.co/datasets/jo-mengr/geo_70k_multiplets_natural_language_annotation","creator_name":"Jonatan Menger","creator_url":"https://huggingface.co/jo-mengr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","code","mit","ðŸ‡ºðŸ‡¸ Region: US","multimodal"],"keywords_longer_than_N":true},
	{"name":"MIG-RS-Instruct","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tBeyond Single and Earthbound: Advancing Multi-image Grounding in Remote Sensing with Large Vision-Language Models\n\t\n\n","url":"https://huggingface.co/datasets/An-Xiao/MIG-RS-Instruct","creator_name":"AnXiao","creator_url":"https://huggingface.co/An-Xiao","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K<n<100K","Image","Geospatial"],"keywords_longer_than_N":true},
	{"name":"table-vqa","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset description\n\t\n\nThe table-vqa Dataset integrates images of tables from the dataset AFTdb (Arxiv Figure Table Database) curated by cmarkea. \nThis dataset consists of pairs of table images and corresponding LaTeX source code, with each image linked to an average of ten questions and answers. Half of the Q&A pairs are in English and the other half in French. These questions and answers were generated using Gemini 1.5 Pro and Claude 3.5 sonnet, making the dataset well-suited forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cmarkea/table-vqa.","url":"https://huggingface.co/datasets/cmarkea/table-vqa","creator_name":"Credit Mutuel Arkea","creator_url":"https://huggingface.co/cmarkea","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-to-image","image-to-text","table-question-answering","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-HTML","keyword":"multimodal","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"FLAIR-1-2","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for FLAIR land-cover semantic segmentation\n\t\n\n\n\t\n\t\t\n\t\tContext & Data\n\t\n\n\nThe hereby FLAIR (#1 and #2) dataset is sampled countrywide and is composed of over 20 billion annotated pixels of very high resolution aerial imagery at 0.2 m spatial resolution, acquired over three years and different months (spatio-temporal domains). \nAerial imagery patches consist of 5 channels (RVB-Near Infrared-Elevation) and have corresponding annotation (with 19 semantic classes or 13 for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/IGNF/FLAIR-1-2.","url":"https://huggingface.co/datasets/IGNF/FLAIR-1-2","creator_name":"Institut national de l'information gÃ©ographique et forestiÃ¨re","creator_url":"https://huggingface.co/IGNF","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["image-segmentation","etalab-2.0","10B<n<100B","arxiv:2211.12979","arxiv:2310.13336"],"keywords_longer_than_N":true},
	{"name":"Android-Control-84k","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAndroid Control Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis directory contains two dataset files (and_ctrl_train.json and and_ctrl_test.json) derived from the Android Control project by Google Research. These datasets have been formatted specifically for GUI grounding training in LLaMA-Factory.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Android Control dataset consists of episodes where each episode contains multiple steps. Each step includes:\n\nStep instructions: Natural language instructions for UIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OfficerChul/Android-Control-84k.","url":"https://huggingface.co/datasets/OfficerChul/Android-Control-84k","creator_name":"Kyochul Jang","creator_url":"https://huggingface.co/OfficerChul","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SRUM_6k_CompBench_Train","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tSRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models - CompBench Training Dataset\n\t\n\nThis repository contains the training dataset for SRUM (Self-Rewarding for Unified Multimodal Models), a post-training framework that leverages a model's own understanding module to provide corrective signals and improve its generation module. This dataset is specifically designed for enhancing performance on compositionality benchmarks like T2I-CompBench.\nPaper: SRUM: Fine-Grainedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Wayne-King/SRUM_6k_CompBench_Train.","url":"https://huggingface.co/datasets/Wayne-King/SRUM_6k_CompBench_Train","creator_name":"Weiyang Jin","creator_url":"https://huggingface.co/Wayne-King","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Tuberculosis_Dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMultimodal Dataset of Tuberculosis Patients including CT and Clinical Case Reports\n\t\n\nZhankai Ye    \nNetID: zy172\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is curated from the original â€œThe MultiCaRe Datasetâ€ to focus on the chest tuberculosis patients. This is a multimodal dataset consisting of lung computed tomography (CT) imaging data and the clinical case records of tuberculosis patients, along with their case keywords, the captions of their CT images, patient_id, gender, and ageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/moukaii/Tuberculosis_Dataset.","url":"https://huggingface.co/datasets/moukaii/Tuberculosis_Dataset","creator_name":"Zhankai Ye","creator_url":"https://huggingface.co/moukaii","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","text-classification","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Egyptian_People_Speaking_Video_Dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tEgyptian People Speaking Video Dataset\n\t\n\nThis dataset contains high-quality video recordings of Egyptian people speaking on a range of topics. It is curated for AI research in speech recognition, multimodal analysis, topic understanding, and spoken-language modeling.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask Categories:  \n\nVideo Classification  \nSpeech-to-Textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Egyptian_People_Speaking_Video_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Egyptian_People_Speaking_Video_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","Arabic","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"nesteo-prototype","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tNestEO: Modular and Hierarchical EO Dataset Framework\n\t\n\nNestEO is a hierarchical, resolution-aligned, UTM-based nested grid dataset framework supporting general-purpose, multi-scale multimodal Earth Observation workflows. Built from diverse EO sources and enriched with metadata for landcover, climate zones, and population, it enables scalable, representative and progressive sampling for AI4EO.\nGrid Levels: 120000m, 12000m, 2400m, 1200m, 600m, 300m, 150mGrid Metadata: ESA WorldCoverâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nesteo-datasets/nesteo-prototype.","url":"https://huggingface.co/datasets/nesteo-datasets/nesteo-prototype","creator_name":"NestEO Datasets","creator_url":"https://huggingface.co/nesteo-datasets","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-segmentation","image-classification","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"PhysUniBench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tPhysUniBench\n\t\n\nAn Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models\nPaper: https://arxiv.org/abs/2506.17667\nRepository: https://github.com/PrismaX-Team/PhysUniBenchmark\nProject page: https://prismax-team.github.io/PhysUniBenchmark/\nPhysUniBench is the first large-scale multimodal physics benchmark specifically designed for undergraduate-level understanding, reasoning, and problem-solving. It provides a valuable testbed for advancing multimodal large language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PrismaX/PhysUniBench.","url":"https://huggingface.co/datasets/PrismaX/PhysUniBench","creator_name":"PrismaX","creator_url":"https://huggingface.co/PrismaX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"french-lot-department-captioned-photos","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tLot Department, France Image Dataset\n\t\n\nA collection of high-resolution scenic photographs from the Lot region of France with AI-generated descriptive captions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains scenic photographs from three notable locations in France's Lot department: Rocamadour, Autoire, and Padirac. All images were captured using a Sony A6600 camera and are paired with detailed English captions generated by Mistral AI's Pixtral-Large model.\nKey Features:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/french-lot-department-captioned-photos.","url":"https://huggingface.co/datasets/NoeFlandre/french-lot-department-captioned-photos","creator_name":"NoÃ© Flandre","creator_url":"https://huggingface.co/NoeFlandre","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"MotionSight","keyword":"multimodal","description":"This is the dataset proposed in our paper MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs.\nWe split the dataset into multiple small files, you can recover by cat:\ncat MotionSightDataset_part* > MotionSightDataset.zip\nunzip MotionSightDataset\n\nProject Page | Github\n","url":"https://huggingface.co/datasets/nkp37/MotionSight","creator_name":"nkp","creator_url":"https://huggingface.co/nkp37","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K<n<100K","arxiv:2506.01674"],"keywords_longer_than_N":true},
	{"name":"FlowGen","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸŒŸ FlowGen\n\t\n\nFlowGen is a controllable flowchart synthesizer that generates diagrams with tunable structural features and supports multiple rendering styles.\n\n\t\n\t\t\n\t\tðŸ“‘ Dataset description\n\t\n\nThis dataset contains different types of renderer flowchart images with different difficulty levels.\n\n\t\n\t\t\nTypes\nTrain (Easy)\nTrain (Medium)\nTrain (Hard)\nTest (Graph Easy)\nTest (Graph Medium)\nTest (Graph Hard)\nTest (Scanned Easy)\nTest (Scanned Medium)\nTest (Scanned Hard)\n\n\n\t\t\nMermaid\n960\n960\n960â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Anonymous112233/FlowGen.","url":"https://huggingface.co/datasets/Anonymous112233/FlowGen","creator_name":"Anonymous","creator_url":"https://huggingface.co/Anonymous112233","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","visual-question-answering","image-captioning","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"VCRBench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tVCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models\n\t\n\n \n \n \n \n \nAuthors: Pritam Sarkar and Ali Etemad\nThis repository provides the official implementation of VCRBench.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nPlease check our GitHub repo for the details of usage: VCRBench\nfrom dataset import VCRBench\ndataset=VCRBench(question_file=\"data.json\", \n                video_root=\"./\",\n                mode='default', \n                )\n    \nfor sample in dataset:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pritamqu/VCRBench.","url":"https://huggingface.co/datasets/pritamqu/VCRBench","creator_name":"Pritam Sarkar","creator_url":"https://huggingface.co/pritamqu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","visual-question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"CV_Arena_ipfSubset","keyword":"multi-modal","description":"IPF/CV_Arena_ipfSubset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/IPF/CV_Arena_ipfSubset","creator_name":"Isaac_GHX","creator_url":"https://huggingface.co/IPF","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-to-image","image-to-image","Chinese","English","mit"],"keywords_longer_than_N":true},
	{"name":"uniocc","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tUniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving\n\t\n\n\n\n\n\nPaper | Project Page | Code\n\n\n\nAutonomous Driving researchers, have you ever been bothered by the fact that popular datasets all have their different\nformats, and standardizing them is a pain? Have you ever been frustrated by the difficulty of just understanding\nthe file semantics? This challenge is even worse in the occupancy domain. But, UniOcc is here to help.\n\nUniOcc is a unifiedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tasl-lab/uniocc.","url":"https://huggingface.co/datasets/tasl-lab/uniocc","creator_name":"Trustworthy Autonomous Systems Laboratory","creator_url":"https://huggingface.co/tasl-lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-3d","mit","3D","arxiv:2503.24381","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"R1-Reward-RL","keyword":"multimodal","description":"\n  \n\n\n[ðŸ“– arXiv Paper] \n[ðŸ“Š R1-Reward Code] \n[ðŸ“ R1-Reward Model] \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tTraining Multimodal Reward Model Through Stable Reinforcement Learning\n\t\n\nðŸ”¥ We are proud to open-source R1-Reward, a comprehensive project for improve reward modeling through reinforcement learning. This release includes:\n\nR1-Reward Model: A state-of-the-art (SOTA) multimodal reward model demonstrating substantial gains (Voting@15):\n13.5% improvement on VL Reward-Bench.3.5% improvement on MM-RLHF Reward-Bench.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL.","url":"https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"VisuLogic-Train","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tVisuLogic-Train (OmniTool)\n\t\n\nShort description of the datasetâ€¦\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\nds = load_dataset(\"OmniTool/VisuLogic-Train\", \"solution\", split=\"train\")\"internvl\"\nprint(ds[0])\n\n","url":"https://huggingface.co/datasets/OmniTool/VisuLogic-Train","creator_name":"OmniTool","creator_url":"https://huggingface.co/OmniTool","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"AmericanExpress_vision_dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAMERICAN-EXPRESS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical and financial queries generated from American Express annual reports. It is designed to train and evaluate information retrieval models and improve AI understanding of financial documentation, with a specific focus on the credit card industry, payment processing, and banking services.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/AmericanExpress_vision_dataset.","url":"https://huggingface.co/datasets/Davidsv/AmericanExpress_vision_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"MMR1-in-context-synthesizing","keyword":"multimodal","description":"This dataset is designed for unsupervised post-training of Multi-Modal Large Language Models (MLLMs) focusing on enhancing reasoning capabilities. It contains image-problem-answer triplets, where the problem requires multimodal reasoning to derive the correct answer from the provided image. The dataset is intended for use with the MM-UPT framework described in the accompanying paper.\n\nðŸ™ GitHub Repo: waltonfuture/MM-UPT\nðŸ“œ Paper (arXiv): Unsupervised Post-Training for Multi-Modal LLM Reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing.","url":"https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing","creator_name":"Lai Wei","creator_url":"https://huggingface.co/WaltonFuture","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"msdd","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ“š Misraj Structured Data Dump (MSDD)\n\t\n\nMisraj Structured Data Dump (MSDD) is a large-scale Arabic multimodal dataset created using our WASM pipeline. It is extracted and filtered from the Common Crawl dumps and uniquely preserves the structural integrity of web content by providing markdown output. This dataset aims to address the lack of high-quality, structured multimodal data for Arabic and accelerate research in large language and multimodal models.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“Œ Dataset Summaryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Misraj/msdd.","url":"https://huggingface.co/datasets/Misraj/msdd","creator_name":"Misraj Ai","creator_url":"https://huggingface.co/Misraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10M - 100M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"amazon-qwen2vl-listing","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAmazon Qwen2-VL Listing Dataset\n\t\n\nThis tiny dataset accompanies the LoRA adapter:\n\nModel: https://huggingface.co/soupstick/qwen2vl-amazon-ft-lora\n\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\ndata/train.json â€” LLaMA-Factory style JSON with fields:\nimages (list of filenames or a single filename)\ninstruction (prompt)\noutput (JSON-formatted string with title/bullets/description)\n\n\neval/eval_predictions.jsonl â€” model generations used for quick evaluation.\n\n\nNote: This repo is intentionally lightweight forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/soupstick/amazon-qwen2vl-listing.","url":"https://huggingface.co/datasets/soupstick/amazon-qwen2vl-listing","creator_name":"Souptik Chakraborty","creator_url":"https://huggingface.co/soupstick","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"tabfquad_test_subsampled","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings. Using a vision language model (GPT4V), we create additional queries to augment the existing human-annotated ones.\n\n\t\n\t\t\n\t\tData Curation\n\t\n\nTo ensure homogeneity across our benchmarked datasets, we subsampled the original test set to 280 pairs, leaving the rest for training and renaming the different columns.\n\n\t\n\t\t\n\t\tLoad the dataset\n\t\n\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/tabfquad_test_subsampled.","url":"https://huggingface.co/datasets/vidore/tabfquad_test_subsampled","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","French","English","mit"],"keywords_longer_than_N":true},
	{"name":"Orsta-Data-47k","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOrsta-Data-47k Dataset\n\t\n\n\nðŸ™ GitHub Repo: MiniMax-AI/One-RL-to-See-Them-All\nðŸ“œ Paper (arXiv): V-Triune: One RL to See Them All (arXiv:2505.18129)\n\n\n\t\n\t\t\n\t\tDataset Description ðŸ“–\n\t\n\nOrsta-Data-47k is a specialized dataset curated for the post-training of Vision-Language Models (VLMs) using our V-Triune unified reinforcement learning system. Its primary purpose is to enable robust joint training across a diverse spectrum of both visual reasoning and visual perception tasks, poweringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/One-RL-to-See-Them-All/Orsta-Data-47k.","url":"https://huggingface.co/datasets/One-RL-to-See-Them-All/Orsta-Data-47k","creator_name":"One-RL-to-See-Them-All","creator_url":"https://huggingface.co/One-RL-to-See-Them-All","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K<n<100K","arxiv:2505.18129"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-06","keyword":"multimodal","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-06.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-06","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-23","keyword":"multimodal","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-23.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-23","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"mmrl","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸŒŸ ReVisual-R1 (7B) â€” Open-Source Multimodal Reasoner\n\t\n\n\nOne cold-start, two RL stages, endless reasoning power.\n\n\n\n\t\n\t\t\n\t\tðŸ”‘ Highlights\n\t\n\n\nSOTA on 9 tough benchmarks covering visualâ€“math + text reasoning.\n\nThree-Stage SRO Training\n\nText Cold-Start â€” seed deep reflection\nMultimodal RL â€” align vision & logic\nText RL â€” polish fluency & brevity\n\n\nPAD (Prioritized Advantage Distillation) keeps gradients alive.\n\nEfficient-Length Reward = concise, self-reflective CoT.\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“šâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/csfufu/mmrl.","url":"https://huggingface.co/datasets/csfufu/mmrl","creator_name":"Shawn","creator_url":"https://huggingface.co/csfufu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"CHASM-Covert_Advertisement_on_RedNote","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tRedNote Covert Advertisement Detection Dataset\n\t\n\nThis dataset contains posts from the RedNote platform for covert advertisement detection tasks.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\nSplit\nPosts\nAd Posts\nNon-Ad Posts\nTotal Images\n\n\n\t\t\nTrain\n3493\n426\n3067\n18543\n\n\nValidation\n499\n57\n442\n2678\n\n\nTest\n1000\n130\n870\n5103\n\n\nTotal\n4992\n613\n4379\n26324\n\n\n\t\n\n\nNote: The viewer shows a small example subset of the data (60 samples) for demonstration purposes. The complete dataset is available viaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jingyi77/CHASM-Covert_Advertisement_on_RedNote.","url":"https://huggingface.co/datasets/Jingyi77/CHASM-Covert_Advertisement_on_RedNote","creator_name":"Jingyi","creator_url":"https://huggingface.co/Jingyi77","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"wine-images-126k","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tWine Images Dataset 126K\n\t\n\nA comprehensive dataset of 107,821 wine bottle images linked to the Wine Text Dataset 126K. This companion dataset provides high-quality wine bottle images for computer vision, multimodal machine learning, and wine recognition tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains wine bottle images scraped from wine retailer websites. Each image is linked to detailed wine information (descriptions, pricing, categories, regions) via stable IDs thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cipher982/wine-images-126k.","url":"https://huggingface.co/datasets/cipher982/wine-images-126k","creator_name":"David Rose","creator_url":"https://huggingface.co/cipher982","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","feature-extraction","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"rdt-ft-data","keyword":"multimodal","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card\n\t\n\nThis is the fine-tuning dataset used in the paper RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation.\n\n\t\n\t\t\n\t\n\t\n\t\tSource\n\t\n\n\nProject Page: https://rdt-robotics.github.io/rdt-robotics/\nPaper: https://arxiv.org/pdf/2410.07864\nCode: https://github.com/thu-ml/RoboticsDiffusionTransformer\nModel: https://huggingface.co/robotics-diffusion-transformer/rdt-1b\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nDownload all archive files and use the following command to extract:\ncatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data.","url":"https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data","creator_name":"Robotics Diffusion Transformer Collaboration","creator_url":"https://huggingface.co/robotics-diffusion-transformer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","arxiv:2410.07864","ðŸ‡ºðŸ‡¸ Region: US","robotics","multimodal"],"keywords_longer_than_N":true},
	{"name":"SRPO_RL_datasets","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tSRPO Dataset: Reflection-Aware RL Training Data\n\t\n\nThis repository provides the multimodal reasoning dataset used in the paper:\nSRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning\nWe release two versions of the dataset:\n\n39K version (modified_39Krelease.jsonl + images.zip)  \nEnhanced 47K+ version (47K_release_plus.jsonl + 47K_release_plus.zip)\n\nBoth follow the same unified format, containing multimodal (imageâ€“text) reasoning data with self-reflectionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bruce360568/SRPO_RL_datasets.","url":"https://huggingface.co/datasets/bruce360568/SRPO_RL_datasets","creator_name":"zhongwei666","creator_url":"https://huggingface.co/bruce360568","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","10K<n<100K","Image"],"keywords_longer_than_N":true},
	{"name":"s2lcd","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tSentinel-2 Land-cover Captioning Dataset\n\t\n\nThe Sentinel-2 Land-cover Captioning Dataset (S2LCD) is a newly proposed dataset specifically designed for deep learning research on remote sensing image captioning. It comprises 1533 image patches, each of size 224 Ã— 224 pixels, derived from Sentinel-2 L2A images. The dataset ensures a diverse representation of land cover and land use types in temperate regions, including forests, mountains, agricultural lands, and urban areas, each one withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neuronelab/s2lcd.","url":"https://huggingface.co/datasets/neuronelab/s2lcd","creator_name":"NeuRoNeLab","creator_url":"https://huggingface.co/neuronelab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","image-classification","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"vpi-bench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for VPI-Bench\n\t\n\n\n\n\nVPI-Bench is a benchmark dataset of testcases and web platforms used to evaluate the robustness of computer-use and browser-use agents under visual prompt injection attacks.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\nLanguage(s) (NLP): English\nLicense: Creative Commons Attribution 4.0\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\n\n\nRepository: VPI-Bench\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\n\n\t\n\t\t\n\t\tDirect Use\n\t\n\n\n\n\nBenchmarking the Attempted Rate (AR) and Success Rateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VPI-Bench/vpi-bench.","url":"https://huggingface.co/datasets/VPI-Bench/vpi-bench","creator_name":"VPI Bench","creator_url":"https://huggingface.co/VPI-Bench","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MARVEL","keyword":"multi-modal-qa","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMARVEL is a new comprehensive benchmark dataset that evaluates multi-modal large language models' abstract reasoning abilities in six patterns across five different task configurations, revealing significant performance gaps between humans and SoTA MLLMs.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]\n\t\n\n\nRepository: https://github.com/1171-jpg/MARVEL_AVR\nPaper [optional]: https://arxiv.org/abs/2404.13591\nDemo [optional]:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/kianasun/MARVEL.","url":"https://huggingface.co/datasets/kianasun/MARVEL","creator_name":"Kiana Sun","creator_url":"https://huggingface.co/kianasun","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"MixBench25","keyword":"multimodal","description":"MixBench is a benchmark for evaluating mixed-modality retrieval. It contains queries and corpora from four datasets: MSCOCO, Google_WIT, VisualNews, and OVEN. Each subset provides: query, corpus, mixed_corpus, and qrel splits.","url":"https://huggingface.co/datasets/mixed-modality-search/MixBench25","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"mmE5-Synthetic","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tmmE5 Synthetic Data\n\t\n\n","url":"https://huggingface.co/datasets/backup-mmE5/mmE5-Synthetic","creator_name":"backup","creator_url":"https://huggingface.co/backup-mmE5","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MM-MathInstruct-longest-20k-solutions-with-images","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMM-MathInstruct Longest 20K Solutions with Images\n\t\n\nThis dataset contains the top 20,000 samples from MathLLMs/MM-MathInstruct selected by solution length, filtered to include only samples with valid images.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tSource\n\t\n\nThis dataset is derived from MathLLMs/MM-MathInstruct by selecting the 20,000 samples with the longest solution text.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nApache 2.0 (inherited from source dataset)\n","url":"https://huggingface.co/datasets/penfever/MM-MathInstruct-longest-20k-solutions-with-images","creator_name":"Benjamin Feuer","creator_url":"https://huggingface.co/penfever","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"FRABench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nFRABench is a large-scale fine-grained pairwise evaluation datasets across four tasks- Natural Language Generation(NLG), Image Understanding(IU), Image Generation(IG), and Interleaved Text-and-Image Generation(ITIG)- comprising 28 sub-tasks and 60.4 pairwise samples with 325k evaluation labels, which is based on our constructed evaluation aspect tree.\nFRABench incorporates the assessment outcomes of multiple evaluation aspects for each pairwise sample, establishingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SPUH/FRABench.","url":"https://huggingface.co/datasets/SPUH/FRABench","creator_name":"HONG SHIBO","creator_url":"https://huggingface.co/SPUH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","arxiv:2505.12795","ðŸ‡ºðŸ‡¸ Region: US","multimodal"],"keywords_longer_than_N":false},
	{"name":"HCTQA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tHCT-QA: Human-Centric Tables Question Answering\n\t\n\nHCT-QA is a benchmark dataset designed to evaluate large language models (LLMs) on question answering over complex, human-centric tables (HCTs). These tables often appear in documents such as research papers, reports, and webpages and present significant challenges for traditional table QA due to their non-standard layouts and compositional structure.\nThe dataset includes:\n\n2,188 real-world tables with 9,835 human-annotated QA pairs\n4â€¦ See the full description on the dataset page: https://huggingface.co/datasets/qcri-ai/HCTQA.","url":"https://huggingface.co/datasets/qcri-ai/HCTQA","creator_name":"Artificial Intelligence Research Group, Qatar Computing Research Institute","creator_url":"https://huggingface.co/qcri-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","document-question-answering","visual-question-answering","expert-generated","English"],"keywords_longer_than_N":true},
	{"name":"test_lerobot","keyword":"multimodal","description":"\n\t\n\t\t\n\t\ttest_lerobot\n\t\n\nThis dataset contains robotic manipulation data extracted from LeRobot datasets.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\ndata/: Episode data in parquet format\nmeta/: Metadata including episodes, tasks, and statistics\ntask_images/: Task-related multimodal images (reference, goal, instruction images)\n\n\n\t\n\t\t\n\t\tTask Images\n\t\n\nThis dataset includes multimodal task images:\n\nReference Images: Visual examples of each task\nGoal Images: Target states extracted from successful episodesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vgyuan/test_lerobot.","url":"https://huggingface.co/datasets/vgyuan/test_lerobot","creator_name":"zhangyuan","creator_url":"https://huggingface.co/vgyuan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"ViStoryBench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tModel Card: ViStoryBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nViStoryBench is a comprehensive benchmark dataset for story visualization. It aims to thoroughly evaluate and advance the performance of story visualization models by providing diverse story types, artistic styles, and detailed annotations. The goal of story visualization is to generate a sequence of visually coherent and content-accurate images based on a given narrative text and character reference images.\nKey features ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ViStoryBench/ViStoryBench.","url":"https://huggingface.co/datasets/ViStoryBench/ViStoryBench","creator_name":"ViStoryBench","creator_url":"https://huggingface.co/ViStoryBench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","human-annotated","machine-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"Emirates_dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tEMIRATES-AIRWAYS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical, financial, and sustainability queries generated from Emirates Airways annual and sustainability reports. It is designed to train and evaluate information retrieval models and improve AI understanding of aviation industry documentation, with a specific focus on airline operations, sustainability initiatives, and international business strategies.\n\n\t\n\t\t\n\t\n\t\n\t\tAbout Me\n\t\n\nI'm Davidâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/Emirates_dataset.","url":"https://huggingface.co/datasets/Davidsv/Emirates_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"JudgeAnything","keyword":"multimodal","description":"This dataset is described in the paper Judge Anything: MLLM as a Judge Across Any Modality.\n","url":"https://huggingface.co/datasets/pudashi/JudgeAnything","creator_name":"Shu Pu","creator_url":"https://huggingface.co/pudashi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["any-to-any","English","apache-2.0","1K<n<10K","arxiv:2503.17489"],"keywords_longer_than_N":true},
	{"name":"InfiMM-WebMath-40B","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tInfiMM-WebMath-40B Dataset\n\t\n\nArXiv| PDF\nThis dataset is also discussed in the survey paper A Survey of Deep Learning for Geometry Problem Solving.\nThe accompanying reading list/code for the survey can be found at: https://github.com/majianz/gps-survey\nInfiMM-WebMath-40B is a large-scale, open-source multimodal dataset specifically designed for mathematical reasoning tasks. It incorporates both text and images, extracted from web documents, to advance the pre-training of Multimodalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Infi-MM/InfiMM-WebMath-40B.","url":"https://huggingface.co/datasets/Infi-MM/InfiMM-WebMath-40B","creator_name":"InfiMM","creator_url":"https://huggingface.co/Infi-MM","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","odc-by","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"Mobile-R1","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for Mobile-R1\n\t\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nimages/: All the screenshots\ndata.jsonl: The trajectory data\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nAll screenshots are stored in the images/ directory.\nWe describe the structure of a single trajectory entry from the file data.jsonl, which contains the full interaction trajectories and action history.\n\napp_name: String. The name of the mobile application (e.g., \"é—²é±¼\" / Xianyu) where the task is performed.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/PG23/Mobile-R1.","url":"https://huggingface.co/datasets/PG23/Mobile-R1","creator_name":"Qihang Ai","creator_url":"https://huggingface.co/PG23","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Chinese","apache-2.0","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"ROVI","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nROVI is a high-quality synthetic dataset featuring 1M curated web images with comprehensive image descriptions and bounding box annotations. Using a novel VLM-LLM re-captioning strategy, ROVI exceeds existing detection-centric datasets in image description, quality, and resolution, while containing two orders of magnitude more categories with an open-vocabularyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CHang/ROVI.","url":"https://huggingface.co/datasets/CHang/ROVI","creator_name":"Cihang Peng","creator_url":"https://huggingface.co/CHang","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","1M<n<10M","arxiv:2508.01008"],"keywords_longer_than_N":true},
	{"name":"Mono-InternVL-2B-Synthetic-Data","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMono-InternVL-2B Synthetic Data\n\t\n\nThis dataset is used for training the S1.2 stage of Mono-InternVL-2B, as described in the paper Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models.\n\nProject Page: https://internvl.github.io/blog/2024-10-10-Mono-InternVL/\nCode: https://github.com/OpenGVLab/Mono-InternVL\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nThis dataset is used for training the S1.2 stage of Mono-InternVL-2B.\n\n\t\n\t\t\n\t\n\t\n\t\tDataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/Mono-InternVL-2B-Synthetic-Data.","url":"https://huggingface.co/datasets/OpenGVLab/Mono-InternVL-2B-Synthetic-Data","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","webdataset"],"keywords_longer_than_N":true},
	{"name":"LiveXiv","keyword":"multimodal","description":"LiveXiv - an evolving multi-modal dataset based on ArXiv (ICLR 2025)\nhttps://arxiv.org/abs/2410.10783\n\n\t\n\t\t\n\t\tLiveXiv Leaderboard\n\t\n\n\n\t\n\t\t\nModel\nV0 VQA\nV0 TQA\nV1 VQA\nV1 TQA\nV2 VQA\nV2 TQA\nV3 VQA\nV3 TQA\nV4 VQA\nV4 TQA\n\n\n\t\t\nClaude-Sonnet\n0.75\n0.81\n0.75\n0.84\n0.78\n0.82\n0.78\n0.84\n0.80\n0.78\n\n\nQwen2-VL-7B\n0.67\n0.58\n0.67\n0.60\n0.68\n0.58\n0.69\n0.67\n0.71\n0.53\n\n\nPixtral\n-\n-\n-\n-\n0.73\n0.59\n0.71\n0.39\n0.73\n0.55\n\n\nInternVL2-8B\n0.62\n0.62\n0.62\n0.62\n0.64\n0.60\n0.65\n0.69\n0.67\n0.57\nGPT4o\n0.51\n0.48\n0.60\n0.55\n0.59\n0.49â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiveXiv/LiveXiv.","url":"https://huggingface.co/datasets/LiveXiv/LiveXiv","creator_name":"LiveXiv","creator_url":"https://huggingface.co/LiveXiv","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","table-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"airline-vision-dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAirline Industry VQA Dataset\n\t\n\nâš ï¸ Note: This dataset currently contains only the text data (questions). The images are being processed and will be added in a future update.\nThis dataset contains a comprehensive collection of visual question-answering (VQA) pairs generated from official documentation of 18 major airline companies.\n\n\t\n\t\t\n\t\tAbout the Creator\n\t\n\nI'm David Soeiro-Vuong, an engineering student specializing in Computer Science, Big Data, and AI, currently working as anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/airline-vision-dataset.","url":"https://huggingface.co/datasets/Davidsv/airline-vision-dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"PureForest","keyword":"multimodal","description":"\n\t\n\t\t\n\t\n\t\n\t\tPureForest: A Large-Scale Aerial Lidar and Aerial Imagery Dataset for Tree Species Classification in Monospecific Forests\n\t\n\n\n\nPureForest dataset is derived from 449 different forests located in 40 French departments, mainly in the southern regions. \nThis dataset includes 135,569 patches, each measuring 50 m x 50 m, covering a cumulative exploitable area of 339 kmÂ². \nEach patch represents a monospecific forest, annotated with a single tree species label.\nThe proposed classificationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/IGNF/PureForest.","url":"https://huggingface.co/datasets/IGNF/PureForest","creator_name":"Institut national de l'information gÃ©ographique et forestiÃ¨re","creator_url":"https://huggingface.co/IGNF","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["image-classification","other","etalab-2.0","100K<n<1M","Image"],"keywords_longer_than_N":true},
	{"name":"OGC_Renewable_Regulation","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOGC_Renewable_Regulation - Overview\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nOGC_Renewable_Regulation is a curated multimodal dataset focused on renewable energy technical documents, regulations, and legal frameworks. It combines text and image data extracted from real scientific and regulatory PDFs to support tasks such as RAG DSE, question answering, document search, and vision-language model training.\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\nThis dataset was created using our open-source toolâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/racineai/OGC_Renewable_Regulation.","url":"https://huggingface.co/datasets/racineai/OGC_Renewable_Regulation","creator_name":"racine.ai","creator_url":"https://huggingface.co/racineai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","text-retrieval","English","French"],"keywords_longer_than_N":true},
	{"name":"FakeParts","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tFakeParts: A New Family of AI-Generated DeepFakes\n\t\n\n\n\n\n\n\n\n  \n\n\n\n  \n\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nWe introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hi-paris/FakeParts.","url":"https://huggingface.co/datasets/hi-paris/FakeParts","creator_name":"Hi! PARIS","creator_url":"https://huggingface.co/hi-paris","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","English","cc0-1.0","10K - 100K","Video"],"keywords_longer_than_N":true},
	{"name":"VGGSound-50k","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tVGGSound-50k Preprocessed Dataset\n\t\n\nThis dataset contains preprocessed data from the VGGSound dataset, specifically processed using the VGGSound-AVEL50k subset for cross-modal knowledge distillation research. The preprocessing is optimized for MST-Distill (Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation) method.\nThis preprocessing work is based on the VGGSound-AVEL50k subset from: jasongief/CPSP: [2023 TPAMI] Contrastive Positive Sample Propagation along theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Gray1y/VGGSound-50k.","url":"https://huggingface.co/datasets/Gray1y/VGGSound-50k","creator_name":"Hui Li","creator_url":"https://huggingface.co/Gray1y","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["other","cc-by-4.0","10K<n<100K","arxiv:2507.07015","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"NIH-CXR14-BiomedCLIP-Features","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tNIH-CXR14-BiomedCLIP-Features Dataset\n\t\n\nThis dataset is derived from the NIH Chest X-ray Dataset (NIH-CXR14) and processed using the BiomedCLIP-PubMedBERT_256-vit_base_patch16_224 model from Microsoft. It contains image and text features extracted from chest X-ray images and their corresponding textual findings.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe original NIH-CXR14 dataset comprises 112,120 chest X-ray images with disease labels from 30,805 unique patients. This processed datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yasintuncer/NIH-CXR14-BiomedCLIP-Features.","url":"https://huggingface.co/datasets/Yasintuncer/NIH-CXR14-BiomedCLIP-Features","creator_name":"TunÃ§er","creator_url":"https://huggingface.co/Yasintuncer","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","text-retrieval","text-classification","image-feature-extraction","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"TreeVGR-SFT-35K","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tTreeBench: Traceable Evidence Enhanced Visual Grounded Reasoning Benchmark\n\t\n\nThis repository contains TreeBench, a diagnostic benchmark dataset proposed in the paper Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTreeBench is designed to holistically evaluate \"thinking with images\" capabilities by dynamically referencing visual regions. It is built on three core principles:\n\nFocused visual perception of subtle targets in complex scenes.\nTraceableâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K.","url":"https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Chinese_interactive_novels_3k","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tä¸­æ–‡äº’åŠ¨å°è¯´ç»“æž„åŒ–è¯­æ–™\n\t\n\nThis dataset contains uncleaned (!) 3534 structured Chinese interactive novels (ä¸­æ–‡äº’åŠ¨å°è¯´), accounting for around 0.25B (gpt-3.5) tokens in total.\nAll contents are parsed from certain online sources.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis dataset can be potentially used for LLM training. But be aware that you'd better clean the data yourself to remove undesired low-quality contents.\nEach novel is a dict structured as follows:\nclass Novel:\n    book_title: str\n    book_author: strâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/Chinese_interactive_novels_3k.","url":"https://huggingface.co/datasets/mrzjy/Chinese_interactive_novels_3k","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1K - 10K","arrow"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-14","keyword":"multimodal","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-14.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-14","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"BHM-Bengali-Hateful-Memes","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBHM is a novel multimodal dataset for Bengali Hateful Memes detection. The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, \ntailored for two tasks: (i) detecting hateful memes and (ii) detecting the social entities they target (i.e., Individual, Organization, Community, and Society).\n\n\t\n\t\t\n\t\tPaper Information\n\t\n\n\nPaper: https://aclanthology.org/2024.acl-long.454/\nCode:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Eftekhar/BHM-Bengali-Hateful-Memes.","url":"https://huggingface.co/datasets/Eftekhar/BHM-Bengali-Hateful-Memes","creator_name":"Eftekhar Hossain","creator_url":"https://huggingface.co/Eftekhar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["other","image-classification","image-to-text","Bengali","mit"],"keywords_longer_than_N":true},
	{"name":"full-modality-data","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tFull Modality Dataset Statistics\n\t\n\n\n\t\n\t\t\n\t\tVideo Statistics\n\t\n\n\nTotal Videos: 28,472\nTotal Duration: 1422.33 hours\nAverage Duration: 179.84 seconds\nMedian Duration: 160.08 seconds\nDuration Range: 10.04s - 1780.03s\n\n\n\t\n\t\t\n\t\tQA Statistics\n\t\n\n\nTotal Questions: 1,444,526\nAverage Questions per Video: 50.7\nQuestions per Video Range: 14 - 450\n\n\n\t\n\t\t\n\t\tQuestion Type Distribution\n\t\n\n\nOE: 1,444,526 (100.0%)\n\n\n\t\n\t\t\n\t\tQuestion Category Distribution\n\t\n\n\ntemporal: 96,873 (6.7%)\ncausal: 96,873â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/full-modality-data.","url":"https://huggingface.co/datasets/lmms-lab/full-modality-data","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"open-pmc","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOPEN-PMC\n\t\n\n\n    \n\n\n\n  Arxiv: Arxiv \n  Â Â Â Â |Â Â Â Â \n Code: Open-PMC Github\n  Â Â Â Â |Â Â Â Â \n Model Checkpoint: Hugging Face\n \n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of image-text pairs extracted from medical papers available on PubMed Central. It has been curated to support research in medical image understanding, particularly in natural language processing (NLP) and computer vision tasks related to medical imagery. The dataset includes:\n\nExtracted images from research articles.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/vector-institute/open-pmc.","url":"https://huggingface.co/datasets/vector-institute/open-pmc","creator_name":"Vector Institute","creator_url":"https://huggingface.co/vector-institute","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1M - 10M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"Sports","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ“– Model Card: [REARM]\n\t\n\n\n\t\n\t\t\n\t\t\"[Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation]\",Shouxing Ma, Yawen Zeng, Shiqing Wu, and Guandong XuPublished in [ACM MM], 2025.[Paper Link] [Code Repository]\n\t\n\n","url":"https://huggingface.co/datasets/MrShouxingMa/Sports","creator_name":"ShouxingMa","creator_url":"https://huggingface.co/MrShouxingMa","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["English","cc-by-4.0","arxiv:2508.13745","ðŸ‡ºðŸ‡¸ Region: US","recommender-system"],"keywords_longer_than_N":true},
	{"name":"ToxiMol-benchmark","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tToxiMol: A Benchmark for Structure-Level Molecular Detoxification\n\t\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nToxiMol is the first comprehensive benchmark for molecular toxicity repair tailored to general-purpose Multimodal Large Language Models (MLLMs). This is the dataset repository for the paper \"Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?\".\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n\t\n\t\t\n\t\tðŸ§¬ Comprehensive Dataset\n\t\n\n\n560 representative toxic molecules spanning diverseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DeepYoke/ToxiMol-benchmark.","url":"https://huggingface.co/datasets/DeepYoke/ToxiMol-benchmark","creator_name":"DeepYoke","creator_url":"https://huggingface.co/DeepYoke","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["tabular-classification","tabular-regression","multi-class-classification","tabular-single-column-regression","monolingual"],"keywords_longer_than_N":true},
	{"name":"medmax_data","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMedMax Dataset\n\t\n\n\n\t\n\t\t\n\t\tMixed-Modal Instruction Tuning for Training Biomedical Assistants\n\t\n\nAuthors: Hritik Bansal, Daniel Israelâ€ , Siyan Zhaoâ€ , Shufan Li, Tung Nguyen, Aditya GroverInstitution: University of California, Los Angelesâ€  Equal Contribution\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nLarge Language Models (LLMs) and Large Multimodal Models (LMMs) have demonstrated remarkable capabilities in multimodal information integration, opening transformative possibilitiesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mint-medmax/medmax_data.","url":"https://huggingface.co/datasets/mint-medmax/medmax_data","creator_name":"mint-medmax","creator_url":"https://huggingface.co/mint-medmax","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"pdf-rag-embed-bench","keyword":"multimodal","description":"This is a benchmark dataset for PDF RAG embedding systems.\nSee gpahal/pdf-rag-embed-bench for more details.\n","url":"https://huggingface.co/datasets/gpahal/pdf-rag-embed-bench","creator_name":"Garvit Pahal","creator_url":"https://huggingface.co/gpahal","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","mit","< 1K","Document","Image"],"keywords_longer_than_N":true},
	{"name":"ABC-Pretraining-Data","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tABC Pretraining Data\n\t\n\nThis dataset contains the pretraining data for ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions, advancing the state of visual embeddings with natural language control.\nThis dataset is derived from Google's Conceptual Captions dataset.\nEach item in the dataset contains a URL where the corresponding image can be downloaded and mined negatives for eachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-Pretraining-Data.","url":"https://huggingface.co/datasets/TIGER-Lab/ABC-Pretraining-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MMMG","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ§  MMMG: Massive Multi-Discipline Multi-Tier Knowledge Image Benchmark\n\t\n\n\n  ðŸ§¬ Project Page â€¢\n  ðŸ“‚ Code\n\n\nMMMG introduces knowledge image generation as a new frontier in text-to-image research. This benchmark probes the reasoning capabilities of image generation models by challenging them to produce educational and scientific visuals grounded in structured knowledge.\nKnowledge imagesâ€”such as charts, diagrams, mind maps, and scientific illustrationsâ€”play a crucial role in humanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MMMGBench/MMMG.","url":"https://huggingface.co/datasets/MMMGBench/MMMG","creator_name":"MMMG","creator_url":"https://huggingface.co/MMMGBench","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"multimodal","description":"\n ðŸ¦„ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n      \n      \n    \n    \n       \n      \n       \n       \n      \n      \n       \n      \n    \n      \n\n\n\n      \n    [ArXiv] | [ðŸ¤—HuggingFace] | [Website]\n    \n    \n\n\nðŸŒŸ Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\tðŸ”¥News\n\t\n\n\nðŸŽ–ï¸ Our work is accepted by ACL2024.\n\nðŸ”¥ We have release benchmark on [ðŸ¤—HuggingFace].\n\nðŸ”¥ The paper is also available on [ArXiv].\n\nðŸ”®â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"multi-modal","description":"\n ðŸ¦„ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n      \n      \n    \n    \n       \n      \n       \n       \n      \n      \n       \n      \n    \n      \n\n\n\n      \n    [ArXiv] | [ðŸ¤—HuggingFace] | [Website]\n    \n    \n\n\nðŸŒŸ Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\tðŸ”¥News\n\t\n\n\nðŸŽ–ï¸ Our work is accepted by ACL2024.\n\nðŸ”¥ We have release benchmark on [ðŸ¤—HuggingFace].\n\nðŸ”¥ The paper is also available on [ArXiv].\n\nðŸ”®â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"Prompt2MusicBench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for Prompt2MusicBench\n\t\n\nPrompt2MusicBench is a large-scale dataset of 24800 structured text prompts designed for studying controllability in text-to-music models such as MusicGen.\nPrompts vary systematically across genre, tempo (BPM), instrument, and mood, and include 8 structural variants Ã— 2 paraphrase forms for each combination.\nThis dataset contains only prompts (CSV) â€” no audio files. \nA companion dataset (Prompt2MusicLibrary:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bodhisattamaiti/Prompt2MusicBench.","url":"https://huggingface.co/datasets/bodhisattamaiti/Prompt2MusicBench","creator_name":"Bodhisatta Maiti","creator_url":"https://huggingface.co/bodhisattamaiti","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["audio-classification","zero-shot-classification","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SpaceThinker","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tSpaceThinker Dataset\n\t\n\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\nTry training a LLaVA-style VLM using the SpaceThinker Dataset\n\n\t\n\t\t\n\t\tEnhanced Quantitative Spatial Reasoning with Test-Time Compute\n\t\n\nThe SpaceThinker dataset is created using VQASynth to synthesize spatial reasoning traces from a subset of images \nin the localized narratives split of the cauldron.\n\n\t\n\t\t\n\t\tData Samples\n\t\n\n\n\t\n\t\t\n\n\n\n\n\n\t\t\nPrompt: How far is the man in the red hat from the pallet of boxes in feet?\nPrompt: How far is the Goalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/remyxai/SpaceThinker.","url":"https://huggingface.co/datasets/remyxai/SpaceThinker","creator_name":"Remyx AI","creator_url":"https://huggingface.co/remyxai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ChatTime-1-Finetune-100K","keyword":"multimodality","description":"\n\t\n\t\t\n\t\tChatTime: A Multimodal Time Series Foundation Model\n\t\n\n\n\t\n\t\t\n\t\tâœ¨ Introduction\n\t\n\nIn this paper, we innovatively model time series as a foreign language and construct ChatTime, a unified framework for time series and text processing. As an out-of-the-box multimodal time series foundation model, ChatTime provides zero-shot forecasting capability and supports bimodal input/output for both time series and text. We design a series of experiments to verify the superior performance ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Finetune-100K.","url":"https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Finetune-100K","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"MusiXQA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMusiXQA ðŸŽµ\n\t\n\nMusiXQA is a multimodal dataset for evaluating and training music sheet understanding systems. Each data sample is composed of:\n\nA scanned music sheet image (.png)\nIts corresponding MIDI file (.mid)\nA structured annotation (from metadata.json)\nQuestionâ€“Answer (QA) pairs targeting musical structure, semantics, and optical music recognition (OMR)\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“‚ Dataset Structure\n\t\n\nMusiXQA/\nâ”œâ”€â”€ images.tar             # PNG files of music sheets (e.g., 0000000.png)\nâ”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/puar-playground/MusiXQA.","url":"https://huggingface.co/datasets/puar-playground/MusiXQA","creator_name":"Jian Chen","creator_url":"https://huggingface.co/puar-playground","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","arxiv:2506.23009","ðŸ‡ºðŸ‡¸ Region: US","music"],"keywords_longer_than_N":true},
	{"name":"TAMMs","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tTAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting\n\t\n\nTAMMs is a large-scale dataset derived from the Functional Map of the World (fMoW) dataset, curated to support multimodal and temporal reasoning tasks such as change detection and future prediction.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 37,003 high-quality temporal sequences, each consisting of at least four distinct satellite images of the same location captured at differentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/IceInPot/TAMMs.","url":"https://huggingface.co/datasets/IceInPot/TAMMs","creator_name":"é”…ä¸­å†°","creator_url":"https://huggingface.co/IceInPot","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ImageNet-Think","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tImageNet-Think 250K\n\t\n\nA 250k-image dataset with JSONL/Parquet metadata providing prompts and answers for multimodal reasoning tasks.\n\n\t\n\t\t\n\t\tLoad\n\t\n\nfrom datasets import load_dataset\nds = load_dataset(\"krishnateja95/ImageNet-Think\", split=\"train\")\nds[0]\n\n","url":"https://huggingface.co/datasets/krishnateja95/ImageNet-Think","creator_name":"Krishna Teja Chitty-Venkata","creator_url":"https://huggingface.co/krishnateja95","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["summarization","synthetic","machine-generated","monolingual","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MECAT-QA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks\n\t\n\nðŸ“– Paper | ðŸ› ï¸ GitHub | ðŸ”Š MECAT-Caption Dataset | ðŸ”Š MECAT-QA Dataset\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nMECAT (Multi-Expert Chain for Audio Tasks) is a comprehensive benchmark constructed on large-scale data to evaluate machine understanding of audio content through two core tasks:\n\nAudio Captioning: Generating textual descriptions for given audio\nAudio Question Answering: Answering questionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mispeech/MECAT-QA.","url":"https://huggingface.co/datasets/mispeech/MECAT-QA","creator_name":"Horizon Team, Xiaomi MiLM Plus","creator_url":"https://huggingface.co/mispeech","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["audio-classification","audio-text-to-text","summarization","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"M3GIA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tM3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability\n\t\n\n[ðŸŒ Homepage] | ðŸ¤— Dataset | ðŸ¤— Paper | ðŸ“– arXiv | ðŸ’» GitHub\nThe evaluation code can be found in ðŸ’» GitHub.\n[Abstract]\nAs recent multi-modality large language models (MLLMs) have shown formidable proficiency on various complex tasks, there has been increasing attention on debating whether these models could eventually mirror human intelligence. However, existing benchmarks mainly focus on evaluatingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Songweii/M3GIA.","url":"https://huggingface.co/datasets/Songweii/M3GIA","creator_name":"Wei Song","creator_url":"https://huggingface.co/Songweii","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","Spanish","French","Portuguese"],"keywords_longer_than_N":true},
	{"name":"ChineseBQB","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tChinese BQB\n\t\n\nThis is a data reupload of the repository zhaoolee/ChineseBQB, containing 5k+ Chinese stickers\nä¸­æ–‡è¡¨æƒ…åŒ…æ•°æ®ï¼Œæ¥è‡ªäºŽzhaoolee/ChineseBQB\n\n","url":"https://huggingface.co/datasets/mrzjy/ChineseBQB","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Chinese","apache-2.0","1K - 10K","arrow","Image"],"keywords_longer_than_N":true},
	{"name":"marqo-GS-10M","keyword":"multimodal","description":"\n  \n    \n  \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tMarqo-GS-10M\n\t\n\nThis dataset is our multimodal, fine-grained, ranking Google Shopping dataset, Marqo-GS-10M, followed by our novel training framework: Generalized Contrastive Learning (GCL). GCL aims to improve and measure the ranking performance of information retrieval models, \nespecially for retrieving relevant products given a search query.\nBlog post:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Marqo/marqo-GS-10M.","url":"https://huggingface.co/datasets/Marqo/marqo-GS-10M","creator_name":"Marqo","creator_url":"https://huggingface.co/Marqo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"MultiBench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMultiBench: Safety Evaluation Benchmark for Vision-Language Models\n\t\n\nLarge language models have been extensively studied for their vulnerabilities, particularly in the context of adversarial attacks. \nHowever, the emergence of Vision Language Models introduces new modalities of risk that have not yet been thoroughly explored, \nespecially when processing multiple images simultaneously. To address this, we present a new safety evaluation dataset for multimodal LLMs called MultiBenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/juliusbroomfield/MultiBench.","url":"https://huggingface.co/datasets/juliusbroomfield/MultiBench","creator_name":"Julius Broomfield","creator_url":"https://huggingface.co/juliusbroomfield","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MixBench2025","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/iclr2026-anonymous/MixBench2025.","url":"https://huggingface.co/datasets/iclr2026-anonymous/MixBench2025","creator_name":"iclr2026-anonymous","creator_url":"https://huggingface.co/iclr2026-anonymous","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"realworld-chartqa","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for RealWorld-ChartQA\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nRealWorld-ChartQA is a benchmark dataset for chart question answering (CQA), derived from real-world analytical narratives. It contains 205 manually validated multiple-choice questionâ€“answer pairs grounded in student-authored literate visualization notebooks. Unlike previous CQA datasets, RealWorld-ChartQA includes multi-view and interactive charts, along with questions rooted in ecologically valid analytical workflows.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/maevehutch/realworld-chartqa.","url":"https://huggingface.co/datasets/maevehutch/realworld-chartqa","creator_name":"Maeve Hutchinson","creator_url":"https://huggingface.co/maevehutch","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","n<1K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"android_control_train","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tProcessed Android Control Training Set\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the processed training set derived from the Android Control dataset by Google Research.\nThe data processing methodology is identical to that used for our corresponding test set, which can be found at Reallm-Labs/android_control_test.\n\n\t\n\t\t\n\t\n\t\n\t\tData Content and Image Extraction\n\t\n\nImportant Note: Due to the large size of the dataset, this repository contains only the processed text files.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Reallm-Labs/android_control_train.","url":"https://huggingface.co/datasets/Reallm-Labs/android_control_train","creator_name":"Reallm Labs","creator_url":"https://huggingface.co/Reallm-Labs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"VisQuant","keyword":"multimodal","description":"\nlicense: cc-by-4.0\ndatasets:\n\nvisquant\nlanguage:\nen\ntags:\nvisual-question-answering\nobject-counting\nspatial-reasoning\nsynthetic\nmultimodal\nbenchmark\n\n\n\t\n\t\t\n\t\tVisQuant: A Synthetic Benchmark for Object Counting and Spatial Reasoning\n\t\n\nVisQuant is a synthetic dataset of 100 annotated image scenarios, purpose-built to evaluate AI systems on object counting, spatial layout understanding, and visual question answering (VQA).\nThis dataset is ideal for benchmarking vision-language models (e.g.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Anas-Mohiuddin-Syed/VisQuant.","url":"https://huggingface.co/datasets/Anas-Mohiuddin-Syed/VisQuant","creator_name":"Syed Anas Mohiuddin","creator_url":"https://huggingface.co/Anas-Mohiuddin-Syed","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them â€” the content remains exactly the same. The only difference lies in the compression method, which now allows for quickerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Mid-Data","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for LLaVA-OneVision\n\t\n\n\nDue to unknow reasons, we are unable to process dataset with large amount into required HF format. So we directly upload the json files and image folders (compressed into tar.gz files).\n\n\nYou can use the following link to directly download and decompress them.\nhttps://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Mid-Data/tree/main/evol_instruct\n\nWe provide the whole details of LLaVA-OneVision Dataset. In this dataset, we include the data splitsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Mid-Data.","url":"https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Mid-Data","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"TreeSatAI-Time-Series","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tTreeSatAI-Time-Series\n\t\n\n\nThis dataset was introduced in the ECCV24 paper OmniSat.\nAhlswede et al. (https://essd.copernicus.org/articles/15/681/2023/) introduced the TreeSatAI Benchmark Archive, a new dataset for tree species classification in Central Europe based on multi-sensor data from aerial, \nSentinel-1 and Sentinel-2. The dataset contains labels of 20 European tree species (i.e., 15 tree genera) derived from forest administration data of the federal state of Lower Saxonyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/IGNF/TreeSatAI-Time-Series.","url":"https://huggingface.co/datasets/IGNF/TreeSatAI-Time-Series","creator_name":"Institut national de l'information gÃ©ographique et forestiÃ¨re","creator_url":"https://huggingface.co/IGNF","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","cc-by-4.0","Image","Geospatial","arxiv:2404.08351"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_government_reports_test","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about the Government Reports that allow ViDoRe to benchmark administrative/legal documents. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('government reports'). From these documents, we randomly sampled 1000 pages.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_government_reports_test.","url":"https://huggingface.co/datasets/vidore/syntheticDocQA_government_reports_test","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them â€” the content remains exactly the same. The only difference lies in the compression method, which now allows for quickerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"condition-checking-dataset","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tCondition Checking Dataset\n\t\n\nThis dataset contains condition checking conversations for robotics applications, with embedded base64 images from multiple camera viewpoints.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nid: Unique identifier for each sample\nimages: Dictionary containing base64-encoded images from multiple camera viewpoints\nconversations: List of conversation turns (human question + assistant answer)\n\n\n\t\n\t\t\n\t\tCamera Viewpoints\n\t\n\nThe dataset includes images from 5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jeffshen4011/condition-checking-dataset.","url":"https://huggingface.co/datasets/jeffshen4011/condition-checking-dataset","creator_name":"Zhexin Shen","creator_url":"https://huggingface.co/jeffshen4011","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-classification","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"pcmb-ots-dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tPCMB OTS Dataset\n\t\n\nHigh-quality Dataset focused on Indian Engineering Competitive Examination Topics\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 400,000 question-answer pairs focused on competitive indian engineering examination content, covering Physics, Chemistry, Mathematics, and Biology. It includes textual and image-based content.\n\n\t\n\t\t\n\t\tHow to Utilise the Dataset\n\t\n\n\nRefer to the attached Excel files.\nTo fetch the correct image ID for a specific Q&A pair, check theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/pcmb-ots-dataset.","url":"https://huggingface.co/datasets/Kratos-AI/pcmb-ots-dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"VLA-OS-Dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis is the training dataset used in the paper VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models.\n\n\t\n\t\t\n\t\tSource\n\t\n\n\nProject Page: https://nus-lins-lab.github.io/vlaos/\nPaper: https://arxiv.org/abs/2506.17561\nCode: https://github.com/HeegerGao/VLA-OS\nModel: https://huggingface.co/Linslab/VLA-OS\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nEnsure you have installed git lfs:\ncurl -sâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Linslab/VLA-OS-Dataset.","url":"https://huggingface.co/datasets/Linslab/VLA-OS-Dataset","creator_name":"LinS Lab","creator_url":"https://huggingface.co/Linslab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["mit","arxiv:2506.17561","ðŸ‡ºðŸ‡¸ Region: US","robotics","multimodal"],"keywords_longer_than_N":true},
	{"name":"SeePhys","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tSeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning\n\t\n\nCan AI truly see the Physics? Test your model with the newly released SeePhys Benchmark!\nCovering 2,000 vision-text multimodal physics problems spanning from middle school to doctoral qualification exams, the SeePhys benchmark systematically evaluates LLMs/MLLMs on tasks integrating complex scientific diagrams with theoretical derivations. Experiments reveal that even SOTA models like Gemini-2.5-Proâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SeePhys/SeePhys.","url":"https://huggingface.co/datasets/SeePhys/SeePhys","creator_name":"AI4Science","creator_url":"https://huggingface.co/SeePhys","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"DEEPFRUlT_DATASET","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDeepFruit Dataset\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis dataset contains total of 21,122 fully labeled images, featuring 20 different kinds of fruits. It is structured into an 80% training set (16,899 images) and a 20% testing set (4,223 images), facilitating a ready-to-use framework for model training and evaluation.\nAdditionally, there are two CSV files that label the types of fruits depicted in each image.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe \"DeepFruit\" dataset is a comprehensiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sc890/DEEPFRUlT_DATASET.","url":"https://huggingface.co/datasets/sc890/DEEPFRUlT_DATASET","creator_name":"shangrong chi","creator_url":"https://huggingface.co/sc890","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","text-classification","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Electrohydrodynamics","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tElectrohydrodynamics in Hall Effect Thrusters Dataset for Mistral-Large-Instruct-2411 Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of 6,000 high fidelity training instances tailored for fine-tuning the Mistral-Large-Instruct-2411 foundation model. It captures theoretical, computational, and experimental aspects of electrohydrodynamics in Hall Effect Thrusters. \n\n\t\n\t\t\n\t\tKey Features:\n\t\n\n\nMultimodal elements: Includes LaTeX equations, code snippets, textualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Taylor658/Electrohydrodynamics.","url":"https://huggingface.co/datasets/Taylor658/Electrohydrodynamics","creator_name":"atayloraerospace","creator_url":"https://huggingface.co/Taylor658","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"ABC-VG-Instruct","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tVG Instruct\n\t\n\nThis is the instruction finetuning dataset for ABC: Achieving better control of multimodal embeddings using VLMs.\nEach element in this dataset contains 4 instruction-captions pairs for images in the visual genome dataset, corresponding to different bounding boxes in the image.\nWe use this dataset to train an embedding model that can use instruction to embeds specific aspects of a scene.\n\nCombined with our pretraining step, this results in a model that can create highâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct.","url":"https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"GeoGrid_Bench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tGeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?\n\t\n\n\n\nWe present GeoGrid-Bench, a benchmark designed to evaluate the ability of foundation models to understand geo-spatial data in the grid structure. Geo-spatial datasets pose distinct challenges due to their dense numerical values, strong spatial and temporal dependencies, and unique multimodal representations including tabular data, heatmaps, and geographic visualizations. To assess how foundationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bowen-upenn/GeoGrid_Bench.","url":"https://huggingface.co/datasets/bowen-upenn/GeoGrid_Bench","creator_name":"Lauren Jiang","creator_url":"https://huggingface.co/bowen-upenn","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","visual-question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"we-math-captions","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tkalkiAI3000/we-math-captions\n\t\n\nThis dataset augments We-Math/We-Math2.0-Standard with a single-sentence caption for every image to enhance image-based mathematical reasoning. The captions serve as concise visual summaries that can be used for pretraining, instruction tuning, or as auxiliary supervision alongside the original visual QA pairs.\n\n\t\n\t\t\n\t\tMotivation\n\t\n\n\nGoal: strengthen image comprehension for math problems in multimodal models.\nApproach: attach a one-line, plain-Englishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kalkiai3000/we-math-captions.","url":"https://huggingface.co/datasets/kalkiai3000/we-math-captions","creator_name":"Kalki AI","creator_url":"https://huggingface.co/kalkiai3000","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["visual-question-answering","mit","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US","math"],"keywords_longer_than_N":true},
	{"name":"IN-Scientific","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ“¥ IN-Scientific\n\t\n\nIN-Scientific: An Open Multimodal Interleaved Dataset for Scientific Knowledge Representation\nThis project is a subproject of the ðŸ“ŒPIN project, focusing on the development of the largest scientific document multimodal dataset, which integrates both text and images.\nðŸ“‘: https://arxiv.org/abs/2406.13923\nðŸ¤—: https://huggingface.co/datasets/m-a-p/PIN-14M\n\n\t\n\t\t\n\t\n\t\n\t\tDataset statistics\n\t\n\n\n\t\n\t\t\nSource\nContent Images (#)\nContent Images (Size GB)\nDocuments (#)\nDocumentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/IN-Scientific.","url":"https://huggingface.co/datasets/m-a-p/IN-Scientific","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"agentic-rag-redteam-bench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAgentic RAG Red Teaming Dataset v1.0.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTest-only corpus of successful adversarial prompts and scenarios targeting agentic RAG systems (multimodal where applicable).\nOne file per attack type; no consolidated master file is provided.\nExisting, per-type schemas are vendorized locally and left unmodified relative to the record structures used in this work.\nThis dataset was not used to train any model. It is intended strictly for evaluation, diagnostics, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Fujitsu/agentic-rag-redteam-bench.","url":"https://huggingface.co/datasets/Fujitsu/agentic-rag-redteam-bench","creator_name":"Fujitsu Laboratories","creator_url":"https://huggingface.co/Fujitsu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","text-generation","other","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"pentomino-easy-vsft","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tIndividual Module\n\t\n\nThis dataset is created for instruction tuning llava models based on the dataset created here - llava-instruct-mix-vsft\nThis dataset is based on a Pentomino game - More details -> github\n","url":"https://huggingface.co/datasets/Koshti10/pentomino-easy-vsft","creator_name":"Koshti","creator_url":"https://huggingface.co/Koshti10","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Baby","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ“– Model Card: [REARM]\n\t\n\n\n\t\n\t\t\n\t\t\"[Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation]\",Shouxing Ma, Yawen Zeng, Shiqing Wu, and Guandong XuPublished in [ACM MM], 2025.[Paper Link] [Code Repository]\n\t\n\n","url":"https://huggingface.co/datasets/MrShouxingMa/Baby","creator_name":"ShouxingMa","creator_url":"https://huggingface.co/MrShouxingMa","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["English","cc-by-4.0","arxiv:2508.13745","ðŸ‡ºðŸ‡¸ Region: US","recommender-system"],"keywords_longer_than_N":true},
	{"name":"MolLangBench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation\n\t\n\n\nThe MolLangBench paper is available on arXiv:2505.15054.\nThe code for using and evaluating the MolLangBench datasets is provided in this GitHub repository.\n\n\n\n\n\nMolLangBench is a comprehensive benchmark designed to evaluate the fundamental capabilities of AI models in language-prompted molecular structure recognition, editing, and generation.\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChemFM/MolLangBench.","url":"https://huggingface.co/datasets/ChemFM/MolLangBench","creator_name":"ChemFM","creator_url":"https://huggingface.co/ChemFM","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-to-image","image-to-text","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"cellxgene_pseudo_bulk_full_multiplets_natural_language_annotation","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset contains a representation of RNA sequencing data and text descriptions.\nDataset type: multiplets (suitable for relevant contrastive-learning or inference tasks).\nCell Sentence Length: The cell sentences in this dataset have a length of 4096 genes.\nThe RNA sequencing data used for training was originally gathered and annotated in the CellWhisperer project. It is derived from\nCellxGene and GEO. Detailed information on the gathering and annotation of the dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jo-mengr/cellxgene_pseudo_bulk_full_multiplets_natural_language_annotation.","url":"https://huggingface.co/datasets/jo-mengr/cellxgene_pseudo_bulk_full_multiplets_natural_language_annotation","creator_name":"Jonatan Menger","creator_url":"https://huggingface.co/jo-mengr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","code","mit","ðŸ‡ºðŸ‡¸ Region: US","multimodal"],"keywords_longer_than_N":true},
	{"name":"blip3-ocr-200m","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tBLIP3-OCR-200M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-OCR-200M dataset is designed to address the limitations of current Vision-Language Models (VLMs) in processing and interpreting text-rich images, such as documents and charts. Traditional image-text datasets often struggle to capture nuanced textual information, which is crucial for tasks requiring complex text comprehension and reasoning. \n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nOCR Integration: The dataset incorporates Optical Characterâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-ocr-200m.","url":"https://huggingface.co/datasets/Salesforce/blip3-ocr-200m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"aftdb","keyword":"multimodal","description":"The Arxiv Figure Table Database (AFTdb) facilitates the linking of documentary\nobjects, such as figures and tables, with their captions. This enables a\ncomprehensive description of document-oriented images (excluding images from\ncameras). For the table component, the character structure is preserved in\naddition to the image of the table and its caption. This database is ideal\nfor multimodal processing of documentary images.","url":"https://huggingface.co/datasets/cmarkea/aftdb","creator_name":"Credit Mutuel Arkea","creator_url":"https://huggingface.co/cmarkea","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","text-to-image","image-to-text","French","English"],"keywords_longer_than_N":true},
	{"name":"core-sdo","keyword":"multimodal","description":"\n\n\t\n\t\t\n\t\tML-Ready Multi-Modal Image Dataset from SDO\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset provides machine learning (ML)-ready solar data curated from NASAâ€™s Solar Dynamics Observatory (SDO), covering observations from May 13, 2010, to Dec 31, 2024. It includes Level-1.5 processed data from: Atmospheric Imaging Assembly (AIA)\nand Helioseismic and Magnetic Imager (HMI). \nThe dataset is designed to facilitate large-scale learning applications in heliophysics, such as space weather forecastingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nasa-ibm-ai4science/core-sdo.","url":"https://huggingface.co/datasets/nasa-ibm-ai4science/core-sdo","creator_name":"NASA-IBM AI4Science","creator_url":"https://huggingface.co/nasa-ibm-ai4science","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US","Helio","Pretraining"],"keywords_longer_than_N":true},
	{"name":"allava4v-train-regenerated","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tALLaVA-4V Train Dataset (Regenerated)\n\t\n\nThis dataset is a regenerated version of the ALLaVA-4V training dataset, processed using Qwen/Qwen3-VL-8B-Instruct model.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: ALLaVA-4V training data\nModel Used: Qwen/Qwen3-VL-8B-Instruct\nOriginal Format: JSONL (252,924 samples)\nOutput Format: Parquet\nTemperature: 0.0 (deterministic generation)\nProcessing Status: In progress (~21% complete as of upload)\n\n\n\t\n\t\t\n\t\n\t\n\t\tGeneration Details\n\t\n\nThe dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vincent-4/allava4v-train-regenerated.","url":"https://huggingface.co/datasets/vincent-4/allava4v-train-regenerated","creator_name":"Vincent","creator_url":"https://huggingface.co/vincent-4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"test-phd-annotations","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tPhD Hallucination Annotations\n\t\n\nThis dataset contains hallucination annotations for the PhD dataset.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"alita01/test-phd-annotations\")\nprint(dataset)\n\n# View a sample\nsample = dataset['phd_ccs'][0]\nprint(sample['question'])\nsample['image'].show()\n\n\n\t\n\t\t\n\t\n\t\n\t\tFields\n\t\n\n\nimage: Original image (PIL Image)\nquestion: Input question\nmodel_output: Model's generated response\nground_truth: Ground truth answerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alita01/test-phd-annotations.","url":"https://huggingface.co/datasets/alita01/test-phd-annotations","creator_name":"jack","creator_url":"https://huggingface.co/alita01","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","< 1K","Image"],"keywords_longer_than_N":true},
	{"name":"airbnb_embeddings","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset consists of AirBnB listings with property descriptions, reviews, and other metadata. \nIt also contains text embeddings of the property descriptions as well as image embeddings of the listing image. The text embeddings were created using OpenAI's text-embedding-3-small model and the image embeddings using OpenAI's clip-vit-base-patch32 model available on Hugging Face. \nThe text embeddings have 1536 dimensions, while the image embeddings have 512 dimensions.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MongoDB/airbnb_embeddings.","url":"https://huggingface.co/datasets/MongoDB/airbnb_embeddings","creator_name":"MongoDB","creator_url":"https://huggingface.co/MongoDB","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-retrieval","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"DRIFT-TL-Distill-4K","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDRIFT-TL-Distill-4K Dataset\n\t\n\nThis dataset contains multimodal reasoning examples with images and step-by-step thinking processes.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\nmessages: Conversation between user and assistant with image references\nimages: Paths to associated images\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"ChaoHuangCS/DRIFT-TL-Distill-4K\")\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite our paper.\n","url":"https://huggingface.co/datasets/ChaoHuangCS/DRIFT-TL-Distill-4K","creator_name":"Chao Huang","creator_url":"https://huggingface.co/ChaoHuangCS","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"vsr_zeroshot","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tVSR: Visual Spatial Reasoning\n\t\n\nThis is the zero-shot set of VSR: Visual Spatial Reasoning (TACL 2023) [paper].\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndata_files = {\"train\": \"train.jsonl\", \"dev\": \"dev.jsonl\", \"test\": \"test.jsonl\"}\ndataset = load_dataset(\"cambridgeltl/vsr_zeroshot\", data_files=data_files)\n\nNote that the image files still need to be downloaded separately. See data/ for details.\nGo to our github repo for more introductions.\n\n\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you findâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cambridgeltl/vsr_zeroshot.","url":"https://huggingface.co/datasets/cambridgeltl/vsr_zeroshot","creator_name":"Language Technology Lab @University of Cambridge","creator_url":"https://huggingface.co/cambridgeltl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VisIT-Bench","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tDataset Card for VisIT-Bench\n\t\n\n\nDataset Description\nLinks\nDataset Structure\nData Fields\nData Splits\nData Loading\n\n\nLicensing Information\nAnnotations\nConsiderations for Using the Data\nCitation Information\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nVisIT-Bench is a dataset and benchmark for vision-and-language instruction following. The dataset is comprised of image-instruction pairs and corresponding example outputs, spanning a wide range of tasks, from simple object recognition to complexâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/VisIT-Bench.","url":"https://huggingface.co/datasets/mlfoundations/VisIT-Bench","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["crowdsourced","found","original","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"vlaa-thinking-grpo","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tVLAA-Thinking-SFT-126K\n\t\n\nLarge-scale vision-language dataset with 126K instruction-following samples featuring chain-of-thought reasoning\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains vision-language samples with instruction-following conversations. Each sample includes:\n\nimage: PIL Image object\nquestion: Question or instruction text\nanswer or gt: Response with thinking process (SFT dataset) or ground truth answer (GRPO dataset)\ncaption: Image caption (may be empty for someâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/penfever/vlaa-thinking-grpo.","url":"https://huggingface.co/datasets/penfever/vlaa-thinking-grpo","creator_name":"Benjamin Feuer","creator_url":"https://huggingface.co/penfever","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"MathVision_with_difficulty_level","keyword":"multi-modal-qa","description":"\n\t\n\t\t\n\t\tMathVision with difficulty level tags\n\t\n\nThis dataset extends the ðŸ¤— MathVision  benchmark by introducing two additional tags: passrate_for_qwen2.5_vl_7b and difficulty_level_for_qwen2.5_vl_7b. Further details are available in our paper The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸš€ Data Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"JierunChen/MathVision_with_difficulty_level\")\nprint(dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/JierunChen/MathVision_with_difficulty_level.","url":"https://huggingface.co/datasets/JierunChen/MathVision_with_difficulty_level","creator_name":"Jierun Chen","creator_url":"https://huggingface.co/JierunChen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","expert-generated"],"keywords_longer_than_N":true},
	{"name":"EO-via-NLP","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nToward Open Earth Science as Fast and Accessible as Natural Language\nThis dataset was curated to accompany the EO-via-NLP code and the following paper:\n\nEllis, M., Gurung, I., Ramasubramanian, M., & Ramachandran, R. (2025).Toward Open Earth Science as Fast and Accessible as Natural Language.arXiv:2505.15690\n\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nThis dataset was primarily designed for:\n\nNamed Entity Recognition (NER) in earth science contexts.\n\n\n\t\n\t\t\n\t\n\t\n\t\tLanguagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nasa-impact/EO-via-NLP.","url":"https://huggingface.co/datasets/nasa-impact/EO-via-NLP","creator_name":"NASA-IMPACT","creator_url":"https://huggingface.co/nasa-impact","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"AstroM3Processed","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAstroM3Processed\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nAstroM3Processed is a time-series astronomy dataset containing photometry, spectra, and metadata features for variable stars. \nThe dataset was constructed by cross-matching publicly available astronomical datasets, \nprimarily from the ASAS-SN (Shappee et al. 2014) variable star catalog (Jayasinghe et al. 2019) \nand LAMOST spectroscopic survey (Cui et al. 2012), along with data from \nWISE (Wright et al. 2010), GALEX (Morrissey et al. 2007)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AstroMLCore/AstroM3Processed.","url":"https://huggingface.co/datasets/AstroMLCore/AstroM3Processed","creator_name":"AstroMLCore","creator_url":"https://huggingface.co/AstroMLCore","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","parquet","Time-series","Datasets"],"keywords_longer_than_N":true},
	{"name":"smapper-light","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ“‚ SMapper-light Dataset\n\t\n\nSMapper-light is a publicly available multimodal dataset collected using the SMapper platform, an open-hardware, multi-sensor device designed for SLAM (Simultaneous Localization and Mapping) research.\nThe dataset provides synchronized LiDAR, multi-camera, and IMU measurements, enabling benchmarking of visual, LiDAR, and visualâ€“inertial SLAM methods.\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸš€ Applications\n\t\n\nSMapper-light can be used for:  \n\nBenchmarking LiDAR SLAM frameworks (e.g.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/snt-arg/smapper-light.","url":"https://huggingface.co/datasets/snt-arg/smapper-light","creator_name":"Automation and Robotics (ARG) - SnT - University of Luxembourg","creator_url":"https://huggingface.co/snt-arg","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["any-to-any","English","apache-2.0","10B<n<100B","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"geotechnie","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAbout Me\n\t\n\nAbout Me\nI'm Matteo Khan, a computer science apprentice at TW3 Partners, specializing in Generative AI and NLP. My focus is on creating datasets that improve AI's ability to process complex technical documents.\nYou can connect with me on LinkedIn: Matteo Khan\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tPurpose\n\t\n\nThis dataset is designed to fine-tune models for expertise in geotechnical engineering by generating structured queries from soil mechanics and construction-relatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MatteoKhan/geotechnie.","url":"https://huggingface.co/datasets/MatteoKhan/geotechnie","creator_name":"Khan Matteo","creator_url":"https://huggingface.co/MatteoKhan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","French","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"WeatherQA-CoT","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tModality-Bridging WeatherQA-CoT Datasets\n\t\n\nThis repository provides automatically constructed Chain-of-Thought (CoT) datasets for the WeatherQA benchmark, released as part of our study \"Modality-Bridging for Automated Chain-of-Thought Construction in Meteorological Reasoning: A Study on WeatherQA\".\nChain-of-Thought reasoning has become a crucial mechanism for enhancing the interpretability and robustness of Multimodal Large Language Models (MLLMs). However, in specialized domains suchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SparkingStar/WeatherQA-CoT.","url":"https://huggingface.co/datasets/SparkingStar/WeatherQA-CoT","creator_name":"Xue Wen","creator_url":"https://huggingface.co/SparkingStar","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","English","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"Visco-Attack","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tVisCo Attack: Visual Contextual Jailbreak Dataset\n\t\n\nðŸ“„ arXiv:2507.02844 Â· ðŸ’» Code â€“ Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\nThis dataset contains the adversarial contexts, prompts, and images from the paper: \"Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\".\n\n\t\n\t\t\n\t\n\t\n\t\tâš ï¸ Content Warning\n\t\n\nThis dataset contains content that is offensive and/or harmful. It was created for research purposes to study the safetyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/miaozq/Visco-Attack.","url":"https://huggingface.co/datasets/miaozq/Visco-Attack","creator_name":"miaozq","creator_url":"https://huggingface.co/miaozq","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","Image","arxiv:2507.02844"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-10M","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tHumanCaption-10M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ðŸ¤—The Original Images, are Released Completing the Agreement\nHumanCaption-10M: a large, diverse, high-quality dataset of human-related images with natural language descriptions (image to text).â€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M.","url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"Prompt2SceneBench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for Prompt2SceneBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPrompt2SceneBench is a structured prompt dataset with 12,606 text descriptions designed for evaluating text-to-image models in realistic indoor environments. \nEach prompt describes the spatial arrangement of 1â€“4 common household objects on compatible surfaces and in contextually appropriate scenes, sampled using strict objectâ€“surfaceâ€“scene compatibility mappings.\nA usecase of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench.","url":"https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench","creator_name":"Bodhisatta Maiti","creator_url":"https://huggingface.co/bodhisattamaiti","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","question-answering","zero-shot-classification","image-to-text","English"],"keywords_longer_than_N":true},
	{"name":"tmp","keyword":"multimodal","description":"\n\n ChartMimic: Evaluating LMMâ€™s Cross-Modal Reasoning Capability via Chart-to-Code Generation \n\n\nThis is the official dataset repository of ChartMimic. \n\n\t\n\t\t\n\t\t1. Data Overview\n\t\n\nChartMimic aims at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.\nChartMimic includes 1000 human-curatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YANG-Cheng/tmp.","url":"https://huggingface.co/datasets/YANG-Cheng/tmp","creator_name":"Cheng Yang","creator_url":"https://huggingface.co/YANG-Cheng","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"OGC_Nuclear","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOGC_Nuclear - Overview\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nOGC_Nuclear is a curated multimodal dataset focused on nuclear technical documents, regulations, and legal frameworks. It combines text and image data extracted from real scientific and regulatory PDFs to support tasks such as RAG DSE, question answering, document search, and vision-language model training.\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\nThis dataset was created using our open-source tool OGC_pdf-to-parquet.\nNuclear-related PDFs wereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/racineai/OGC_Nuclear.","url":"https://huggingface.co/datasets/racineai/OGC_Nuclear","creator_name":"racine.ai","creator_url":"https://huggingface.co/racineai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","text-retrieval","English","French"],"keywords_longer_than_N":true},
	{"name":"OGC_Qualitative","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOGC_Qualitative\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nOGC_Qualitative is a high-quality multimodal dataset created through the merge of multiple domain-specific datasets with enhanced data processing techniques. This dataset represents our most refined approach to multimodal data generation, incorporating filtering algorithms and improved AI-assisted content generation to deliver superior quality for RAG, DSE, question answering, document search, and vision-language model training tasks.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/racineai/OGC_Qualitative.","url":"https://huggingface.co/datasets/racineai/OGC_Qualitative","creator_name":"racine.ai","creator_url":"https://huggingface.co/racineai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","text-retrieval","English","French"],"keywords_longer_than_N":true},
	{"name":"NL-Eye","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tNL-Eye Benchmark\n\t\n\nWill a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? \nRecent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. \nNL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MorVentura/NL-Eye.","url":"https://huggingface.co/datasets/MorVentura/NL-Eye","creator_name":"Mor Ventura","creator_url":"https://huggingface.co/MorVentura","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"ScienceOlympiad","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tScienceOlympiad: Challenging AI with Olympiad-Level Multimodal Science Problems\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe ScienceOlympiad dataset is a meticulously curated benchmark designed to test the limits of current AI models in scientific reasoning. It comprises elite, competition-level problems in physics and chemistry. Addressing the need for more diverse and realistic challenges, ScienceOlympiad introduces multimodal integration as a key dimension. Unlike purely text-basedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ByteDance-Seed/ScienceOlympiad.","url":"https://huggingface.co/datasets/ByteDance-Seed/ScienceOlympiad","creator_name":"ByteDance Seed","creator_url":"https://huggingface.co/ByteDance-Seed","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["cc0-1.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"FLAIR-HUB","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tFLAIR-HUB : Large-scale Multimodal Dataset for Land Cover and Crop Mapping\n\t\n\nFLAIR-HUB builds upon and includes the FLAIR#1 and FLAIR#2 datasets, expanding them into a unified, large-scale, multi-sensor land-cover resource with very-high-resolution \nannotations. Spanning over 2,500 kmÂ² of diverse French ecoclimates and landscapes, it features 63 billion hand-annotated pixels across 19 land-cover and \n23 crop type classes.\nThe dataset integrates complementary data sources includingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/IGNF/FLAIR-HUB.","url":"https://huggingface.co/datasets/IGNF/FLAIR-HUB","creator_name":"Institut national de l'information gÃ©ographique et forestiÃ¨re","creator_url":"https://huggingface.co/IGNF","license_name":"Etalab Open License 2.0 English","license_url":"https://scancode-licensedb.aboutcode.org/etalab-2.0-en.html","language":"en","first_N":5,"first_N_keywords":["image-segmentation","etalab-2.0","100K<n<1M","arxiv:2506.07080","arxiv:2508.10894"],"keywords_longer_than_N":true},
	{"name":"Item-SID","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for AL-GR-Item-SID\n\t\n\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\tðŸ“– Dataset Description\n\t\n\nAL-GR-Item-SID is a dataset containing Semantic IDs (SIDs) for products from an anonymized e-commerce platform. These IDs are generated using a multi-modal model and are specifically designed to serve as dense, meaningful features for Generative Recommendation systems, such as the LLM model.\nUnlike traditional sparse item IDs (e.g., item_12345), Semantic IDs are sequences of discrete tokensâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AL-GR/Item-SID.","url":"https://huggingface.co/datasets/AL-GR/Item-SID","creator_name":"ALGR","creator_url":"https://huggingface.co/AL-GR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-retrieval","feature-extraction","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"labeled-gaits-500k","keyword":"multimodal","description":"Housto4/labeled-gaits-500k dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Housto4/labeled-gaits-500k","creator_name":"Ivan H","creator_url":"https://huggingface.co/Housto4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","tabular-regression","tabular-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-HQ-311K","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tHumanCaption-HQ-311K\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ðŸ¤—The Original Images, are Released Completing Agreement\nHumanCaption-HQ-311K: Approximately 311,000 human-related images and their corresponding natural language descriptions.\nCompared toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K.","url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HiPhO","keyword":"multimodal","description":"\n\nðŸ¥‡ HiPhO: High School Physics Olympiad Benchmark\n\n[ðŸ† Leaderboard]\n[ðŸ“Š Dataset]\n[âœ¨ GitHub]\n[ðŸ“„ Paper]\n\n\n\n\n\nðŸ† New (Sep. 16): We launched \"PhyArena\", a physics reasoning leaderboard incorporating the HiPhO benchmark.\n\n\t\n\t\n\t\n\t\tðŸŒ Introduction\n\t\n\nHiPhO (High School Physics Olympiad Benchmark) is the first benchmark specifically designed to evaluate the physical reasoning abilities of (M)LLMs on real-world Physics Olympiads from 2024â€“2025.\n\n  \n\n\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nUp-to-date Coverage:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SciYu/HiPhO.","url":"https://huggingface.co/datasets/SciYu/HiPhO","creator_name":"Fangchen Yu","creator_url":"https://huggingface.co/SciYu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"UnLOK-VQA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ“Š Dataset: UnLOK-VQA (Unlearning Outside Knowledge VQA)\n\t\n\nPaper: Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation\nCode: https://github.com/Vaidehi99/mmmedit\nLink: Dataset Link\nThis dataset contains approximately 500 entries with the following key attributes:\n\n\"id\": Unique Identifier for each entry\n\"src\": The question whose answer is to be deleted â“\n\"pred\": The answer to the question meant for deletion âŒ\n\"loc\": Related neighborhood questionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vaidehi99/UnLOK-VQA.","url":"https://huggingface.co/datasets/vaidehi99/UnLOK-VQA","creator_name":"Vaidehi Patil","creator_url":"https://huggingface.co/vaidehi99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MathVision","keyword":"multi-modal-qa","description":"\n\t\n\t\t\n\t\tMeasuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset\n\t\n\n[ðŸ’» Github] [ðŸŒ Homepage]  [ðŸ“Š Leaderboard ] [ðŸ“Š Open Source Leaderboard ] [ðŸ” Visualization] [ðŸ“– Paper]\n\n\t\n\t\t\n\t\n\t\n\t\tðŸš€ Data Usage\n\t\n\n\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MathLLMs/MathVision\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tðŸ’¥ News\n\t\n\n\n[2025.05.16] ðŸ’¥ We now support the official open-source leaderboard! ðŸ”¥ðŸ”¥ðŸ”¥ Skywork-R1V2-38B is the best open-source model, scoring 49.7% on MATH-Vision. ðŸ”¥ðŸ”¥ðŸ”¥â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MathVision.","url":"https://huggingface.co/datasets/MathLLMs/MathVision","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","expert-generated"],"keywords_longer_than_N":true},
	{"name":"viet-cultural-vqa","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tVietnamese Cultural VQA Dataset\n\t\n\nðŸ‡»ðŸ‡³ Bá»™ dá»¯ liá»‡u VQA VÄƒn hÃ³a Viá»‡t Nam - Táº­p dá»¯ liá»‡u Ä‘áº§u tiÃªn vá» VQA Ä‘a phÆ°Æ¡ng thá»©c vá»›i kháº£ nÄƒng giáº£i thÃ­ch vÄƒn hÃ³a Viá»‡t Nam.\n\n\t\n\t\t\n\t\tðŸ“Š Thá»‘ng kÃª Dataset\n\t\n\n\nðŸ“ Tá»•ng sá»‘ máº«u: 28,484\nâ“ Tá»•ng sá»‘ cÃ¢u há»i: 135,645\nðŸ·ï¸ Sá»‘ danh má»¥c: 12 danh má»¥c vÄƒn hÃ³a\nðŸŽ¯ Loáº¡i VQA: Few-shot learning vá»›i giáº£i thÃ­ch vÄƒn hÃ³a\nðŸŒ NgÃ´n ngá»¯: Tiáº¿ng Viá»‡t\n\n\n\t\n\t\t\n\t\tðŸŽ­ Danh má»¥c VÄƒn hÃ³a\n\t\n\n\n\t\n\t\t\nDanh má»¥c\nTiáº¿ng Viá»‡t\nMÃ´ táº£\n\n\n\t\t\nam_thuc\náº¨m thá»±c\nMÃ³n Äƒn, Ä‘á»“ uá»‘ng truyá»n thá»‘ng\n\n\ndoi_song_hang_ngayâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dangindev/viet-cultural-vqa.","url":"https://huggingface.co/datasets/Dangindev/viet-cultural-vqa","creator_name":"Nguyen Hai Dang","creator_url":"https://huggingface.co/Dangindev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","Vietnamese","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"MMR1-RL","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThis repository introduces the MMR1 project, focusing on enhancing large multimodal reasoning models. While rapid progress has been made, advancements are constrained by two major limitations:\n\nThe absence of open, large-scale, high-quality long chain-of-thought (CoT) data.\nThe instability of reinforcement learning (RL) algorithms in post-training, where standard Groupâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MMR1/MMR1-RL.","url":"https://huggingface.co/datasets/MMR1/MMR1-RL","creator_name":"MMR1","creator_url":"https://huggingface.co/MMR1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"unidisc_hq","keyword":"multimodal","description":"This repository contains the dataset used in the paper Unified Multimodal Discrete Diffusion.\nCode: https://github.com/AlexSwerdlow/unidisc\nAdditionally, we release a synthetic dataset available here and the corresponding generation scripts as well as the raw data.\n","url":"https://huggingface.co/datasets/aswerdlow/unidisc_hq","creator_name":"Alexander Swerdlow","creator_url":"https://huggingface.co/aswerdlow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","10M - 100M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"KokushiMD-10","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tKokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations\n\t\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nKokushiMD-10 is the first comprehensive multimodal benchmark constructed from ten Japanese national healthcare licensing examinations. This dataset addresses critical gaps in existing medical AI evaluation by providing a linguistically grounded, multimodal, and multi-profession assessment framework for large language models (LLMs) inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/humanalysis-square/KokushiMD-10.","url":"https://huggingface.co/datasets/humanalysis-square/KokushiMD-10","creator_name":"Tako AI","creator_url":"https://huggingface.co/humanalysis-square","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Japanese","English","mit"],"keywords_longer_than_N":true},
	{"name":"FinMME","keyword":"multimodal","description":"Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, there is a notable lack of effective and specialized multimodal evaluation datasets in the financial domain. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20â€¦ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/FinMME.","url":"https://huggingface.co/datasets/luojunyu/FinMME","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"japanese-humor-evaluation","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tJapanese Multimodal Humor Evaluation Dataset\n\t\n\nThis dataset combines two Japanese humor datasets for evaluating the funniness of responses to prompts (odai).\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset merges:\n\nbokete dataset: Image prompts with text responses\nkeitai dataset: Text prompts with text responses\n\nAll scores are normalized to a 0-4 scale for consistency.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nodai_id: Unique identifier for the prompt\nodai_type: Type of promptâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation.","url":"https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation","creator_name":"Miyamoto Ryoto","creator_url":"https://huggingface.co/iammytoo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Japanese","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"QatarAirways_dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAMERICAN-EXPRESS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical and financial queries generated from American Express annual reports. It is designed to train and evaluate information retrieval models and improve AI understanding of financial documentation, with a specific focus on the credit card industry, payment processing, and banking services.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/QatarAirways_dataset.","url":"https://huggingface.co/datasets/Davidsv/QatarAirways_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"CGTSF","keyword":"multimodality","description":"\n\t\n\t\t\n\t\tCGTSF: Context-Guided Time Series Forecasting\n\t\n\n\n\t\n\t\t\n\t\tâœ¨ Introduction\n\t\n\nThe context-guided time series forecasting task entails the transformation of text into time series data. Relevant multimodal datasets are limited. To address these data gaps, we have collected three multimodal datasets that offer valuable resources for future research. The following table summarizes the statistics of these datasets. MSPG comprises 13 months of solar power generation data on 27 photovoltaicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/CGTSF.","url":"https://huggingface.co/datasets/ChengsenWang/CGTSF","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"llava-critic-113k","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for LLaVA-Critic-113k\n\t\n\n\nðŸª Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic/\nðŸ“° Paper: https://arxiv.org/abs/2410.02712\nðŸ¤— Huggingface Collection: https://huggingface.co/collections/lmms-lab/llava-critic-66fe3ef8c6e586d8435b4af8\nðŸ‘‹ Point of Contact: Tianyi Xiong\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nLLaVA-Critic-113k is a high quality critic instruction-following dataset tailored to follow instructions in complex evaluation setting, providing bothâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/llava-critic-113k.","url":"https://huggingface.co/datasets/lmms-lab/llava-critic-113k","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"VisNumBench","keyword":"multimodal","description":"This dataset is designed for research in Deep Learning for Geometry Problem Solving (DL4GPS) and accompanies the survey paper A Survey of Deep Learning for Geometry Problem Solving. It aims to provide a structured resource for evaluating and training AI models, particularly multimodal large language models (MLLMs), on mathematical reasoning tasks involving geometric contexts.\nThe dataset provides a collection of geometry problems, each consisting of a textual question and a correspondingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GML-FMGroup/VisNumBench.","url":"https://huggingface.co/datasets/GML-FMGroup/VisNumBench","creator_name":"Foundation Model Group at Guangming Laboratory","creator_url":"https://huggingface.co/GML-FMGroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_artificial_intelligence_test","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about the Artificial Intelligence. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('artificial intelligence'). From these documents, we randomly sampled 1000 pages.\nWe associated these with 100 questions and answersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_artificial_intelligence_test.","url":"https://huggingface.co/datasets/vidore/syntheticDocQA_artificial_intelligence_test","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"docvqa_test_subsampled","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the test set taken from the DocVQA dataset. It includes collected images from the UCSF Industry Documents Library. Questions and answers were manually annotated.\nExample of data (see viewer)\n\n\t\n\t\t\n\t\tData Curation\n\t\n\nTo ensure homogeneity across our benchmarked datasets, we subsampled the original test set to 500 pairs and renamed the different columns.\n\n\t\n\t\t\n\t\tLoad the dataset\n\t\n\nfrom datasets import load_dataset\nds =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/vidore/docvqa_test_subsampled.","url":"https://huggingface.co/datasets/vidore/docvqa_test_subsampled","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"huggingface_PRBench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tPRBench\n\t\n\nPRBench pairs academic promotion announcements with their supporting\nartefacts (PDFs, factual-accuracy checklists, and promotional figures).\nThis folder was produced with prepare_prbench_dataset.py and is ready\nfor datasets.Dataset.push_to_hub or manual upload.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\nColumn\nType\nDescription\n\n\n\t\t\ntitle\nstring\nPaper title\n\n\narxiv_id\nstring\nArXiv identifier\n\n\nplatform_source\nstring\nPlatform source label\n\n\nid\nstring\nPromotion identifierâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yzweak/huggingface_PRBench.","url":"https://huggingface.co/datasets/yzweak/huggingface_PRBench","creator_name":"zyan","creator_url":"https://huggingface.co/yzweak","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"PIN-14M","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tPIN-14M\n\t\n\nA mini version of \"PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents\"\nPaper: https://arxiv.org/abs/2406.13923\nThis dataset contains 14M samples in PIN format, with around 18.79 TB storage.\nðŸš€ News\n[ 2025.09.04 ] !NEW! ðŸ”¥ We have completed the final version of the PIN-14M dataset and conducted some simple statistics on it.\n[ 2024.12.12 ] !NEW! ðŸ”¥ We have updated the quality signals for all subsets, with the dataset now containing 7.33B tokensâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/PIN-14M.","url":"https://huggingface.co/datasets/m-a-p/PIN-14M","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"MTabVQA-Instruct","keyword":"multimodal","description":"Paper\n\n\t\n\t\t\n\t\tMTabVQA-Instruct Sub-datasets\n\t\n\nThis directory contains multiple MTabVQA-Instruct datasets for visual question answering over tables.\n\n\t\n\t\t\n\t\tDatasets\n\t\n\n\nMTabVQA-Atis-Instruct\nMTabVQA-MiMo-Instruct\nMTabVQA-Multitab-Instruct\nMTabVQA-Spider-Instruct\n\nEach dataset contains a VQA.jsonl file and a table_images directory with the corresponding table images.\n\n\t\n\t\t\n\t\tImportant Note for Multitab-Instruct\n\t\n\nYou must unzip the table_images.zip file in MTabVQA-Multitab-Instruct/ to accessâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mtabvqa/MTabVQA-Instruct.","url":"https://huggingface.co/datasets/mtabvqa/MTabVQA-Instruct","creator_name":"MTabVQA","creator_url":"https://huggingface.co/mtabvqa","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","apache-2.0","10K - 100K","json","Image"],"keywords_longer_than_N":true},
	{"name":"MixBench25-visual","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench25-visual.","url":"https://huggingface.co/datasets/mixed-modality-search/MixBench25-visual","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"PVIT-3M","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tPVIT-3M\n\t\n\nThe paper titled \"Personalized Visual Instruction Tuning\" introduces a novel dataset called PVIT-3M. This dataset is specifically designed for tuning MLLMs in the context of personalized visual instruction tasks. The dataset consists of 3 million image-text pairs that aim to improve MLLMs' abilities to generate responses based on personalized visual inputs, making them more tailored and adaptable to individual user needs and preferences.\nHereâ€™s the PVIT-3M statistics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/PVIT-3M.","url":"https://huggingface.co/datasets/Sterzhang/PVIT-3M","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MindCube","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMindCube: Spatial Mental Modeling from Limited Views\n\t\n\nMindCube is a novel benchmark designed to evaluate how well Vision Language Models (VLMs) can form robust spatial mental models from limited visual views. It comprises 21,154 questions across 3,268 images, assessing capabilities such as cognitive mapping (representing positions), perspective-taking (orientations), and mental simulation (dynamics for \"what-if\" movements). The dataset aims to expose critical gaps in existing VLMs'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MLL-Lab/MindCube.","url":"https://huggingface.co/datasets/MLL-Lab/MindCube","creator_name":"MLL Lab","creator_url":"https://huggingface.co/MLL-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Perception-R1-Dataset","keyword":"multimodal","description":"Paper: arxiv.org/abs/2506.07218\nPlease refer to GitHub repo for detailed usage: https://github.com/tongxiao2002/Perception-R1\n","url":"https://huggingface.co/datasets/tongxiao2002/Perception-R1-Dataset","creator_name":"tongxiao","creator_url":"https://huggingface.co/tongxiao2002","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","Chinese","English","mit"],"keywords_longer_than_N":true},
	{"name":"medical-vision-llm-dataset","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tCombined Medical Vision-Language Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nComprehensive medical vision-language dataset with 4793 samples for vision-based LLM training.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Samples: 4793\nTraining Samples: 3834\nValidation Samples: 959\n\n\n\t\n\t\t\n\t\tModality Distribution\n\t\n\n\nX-ray: 2325 samples\nCT: 1351 samples\nUnknown: 812 samples\nMRI: 231 samples\nUltrasound: 70 samples\nMicroscopy: 2 samples\nEndoscopy: 2 samples\n\n\n\t\n\t\t\n\t\tBody Part Distribution\n\t\n\n\nUnknown:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/robailleo/medical-vision-llm-dataset.","url":"https://huggingface.co/datasets/robailleo/medical-vision-llm-dataset","creator_name":"Robail Yasrab ","creator_url":"https://huggingface.co/robailleo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"OGC_Geotechnie_Compatible_Negatives","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOGC_Geotechnie_Corrected\n\t\n\nCorrected version of racineai/OGC_Geotechnie with proper Image() types and 16 negative image columns for ColPali compatibility.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nid: Unique identifier\nquery: Text query about the document  \nlanguage: Language of the query\nimage: Main document image (corrected Image() type)\nnegative_image_0 to negative_image_15: Negative image columns\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Matchone7/OGC_Geotechnie_Compatible_Negatives.","url":"https://huggingface.co/datasets/Matchone7/OGC_Geotechnie_Compatible_Negatives","creator_name":"NoÃ© BRANDOLINI","creator_url":"https://huggingface.co/Matchone7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"OWMM-Agent-data","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis\n\t\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tðŸš€ Introduction\n\t\n\nThe rapid progress of navigation, manipulation, and vision models has made mobile manipulators capable in many specialized tasks. \nHowever, the open-world mobile manipulation (OWMM) task remains a challenge due to the need for generalization to open-ended instructions and environments, as well as the systematic complexity to integrate high-level decision making withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hhyrhy/OWMM-Agent-data.","url":"https://huggingface.co/datasets/hhyrhy/OWMM-Agent-data","creator_name":"HHY RHY","creator_url":"https://huggingface.co/hhyrhy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10B<n<100B","arxiv:2506.04217","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"teamcraft_data","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tDataset Card for TeamCraft\n\t\n\nThe TeamCraft dataset is designed to develop multi-modal, multi-agent collaboration in Minecraft. It features 55,000 task variants defined by multi-modal prompts and procedurally generated expert demonstrations.\nThis repository contains the data for the validation set and its visualizations. \nTo use the validation set, download TeamCraft-Data-Valid.zip and extract using unzip TeamCraft-Data-Valid.zip.\nIn addition, the training set is available in twoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/teamcraft/teamcraft_data.","url":"https://huggingface.co/datasets/teamcraft/teamcraft_data","creator_name":"TeamCraft","creator_url":"https://huggingface.co/teamcraft","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"MMToM-QA","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMMToM-QA: Multimodal Theory of Mind Question Answering  ðŸ† Outstanding Paper Award at ACL 2024\n\t\n\n[ðŸ Homepage] [ðŸ’»Code] [ðŸ“Paper]\nMMToM-QA is the first multimodal benchmark to evaluate machine Theory of Mind (ToM), the ability to understand people's minds.\nIt systematically evaluates Theory of Mind both on multimodal data and different unimodal data. \nMMToM-QA consists of 600 questions. \nThe questions are categorized into seven types, evaluating belief inference and goal inference inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Chuanyang-Jin/MMToM-QA.","url":"https://huggingface.co/datasets/Chuanyang-Jin/MMToM-QA","creator_name":"Chuanyang Jin","creator_url":"https://huggingface.co/Chuanyang-Jin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["question-answering","English","mit","n<1K","arxiv:2401.08743"],"keywords_longer_than_N":true},
	{"name":"OGC_Energy_Compatible_Negatives","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tOGC_Energy_Corrected\n\t\n\nCorrected version of racineai/OGC_Energy with proper Image() types and 16 negative image columns for ColPali compatibility.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nid: Unique identifier\nquery: Text query about the document  \nlanguage: Language of the query\nimage: Main document image (corrected Image() type)\nnegative_image_0 to negative_image_15: Negative image columns\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"Matchone7/OGC_Energy_Corrected\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Matchone7/OGC_Energy_Compatible_Negatives.","url":"https://huggingface.co/datasets/Matchone7/OGC_Energy_Compatible_Negatives","creator_name":"NoÃ© BRANDOLINI","creator_url":"https://huggingface.co/Matchone7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"PIN-100M","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tPIN-100M\n\t\n\nThe full version of the dataset, related to the paper \"PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents\"\nPaper: https://arxiv.org/abs/2406.13923\nThis dataset contains 100M samples with PIN format.\nPlease note that the required storage space exceeds 150TB!!\nðŸš€ News\n[ 2024.12.20 ] !NEW! ðŸ”¥The currently available version is not the complete version; this project is still ongoing! (It has been released early because we reached the privateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/PIN-100M.","url":"https://huggingface.co/datasets/m-a-p/PIN-100M","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"viexam","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?\n\t\n\n    \n  by \n    Vy Tuong Dang*,\n    An Vo*,\n    Quang Tau, \n    Duc Dm, \n    Daeyoung Kim,\n  \n  \n    *Equal contributionÂ \n    KAIST\n  \n\n\n\n\n\n    \n\n\n\nTLDR: State-of-the-art Vision Language Models (VLMs) demonstrate remarkable capabilities on English multimodal tasks but significantly underperform on Vietnamese educational assessments. ViExam reveals that SOTA VLMs achieve only 57.74% accuracyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anvo25/viexam.","url":"https://huggingface.co/datasets/anvo25/viexam","creator_name":"An Vo","creator_url":"https://huggingface.co/anvo25","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","Vietnamese","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Leopard-Instruct","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tLeopard-Instruct\n\t\n\nPaper | Github | Models-LLaVA | Models-Idefics2\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nLeopard-Instruct is a large instruction-tuning dataset, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. It's been used to train Leopard-LLaVA [checkpoint]  and Leopard-Idefics2 [checkpoint].\n\n\t\n\t\t\n\t\tLoading dataset\n\t\n\n\nto load the dataset without automatically downloading and process the images (Please run the following codes with datasets==2.18.0)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wyu1/Leopard-Instruct.","url":"https://huggingface.co/datasets/wyu1/Leopard-Instruct","creator_name":"Wenhao Yu","creator_url":"https://huggingface.co/wyu1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"CHOICE","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tCHOICE: Benchmarking The Remote Sensing Capabilities of Large Vision-Language Models\n\t\n\n Abstract: The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/An-Xiao/CHOICE.","url":"https://huggingface.co/datasets/An-Xiao/CHOICE","creator_name":"AnXiao","creator_url":"https://huggingface.co/An-Xiao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"fire-exam-base64","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tðŸ”¥ Fire Exam Dataset with Images\n\t\n\nì´ ë°ì´í„°ì…‹ì€ ì†Œë°©ê³µë¬´ì› ì‹œí—˜ ë¬¸ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ëœ ë©€í‹°ëª¨ë‹¬ QA ë°ì´í„°ì…‹ìž…ë‹ˆë‹¤.ê° ìƒ˜í”Œì€ ë¬¸ì œ í…ìŠ¤íŠ¸, ì„ íƒì§€, ì •ë‹µ, ê·¸ë¦¬ê³  ì‹œê° ì •ë³´ë¥¼ ë‹´ì€ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ í¬í•¨í•˜ê³  ìžˆìŠµë‹ˆë‹¤.\n","url":"https://huggingface.co/datasets/taean-yoo/fire-exam-base64","creator_name":"Taean Yoo","creator_url":"https://huggingface.co/taean-yoo","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Korean","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"ScreenSpot-v2","keyword":"multimodal","description":"zonghanHZH/ScreenSpot-v2 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/zonghanHZH/ScreenSpot-v2","creator_name":"Hsieh ZongHan","creator_url":"https://huggingface.co/zonghanHZH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"MUMA-TOM-BENCHMARK","keyword":"multi-modal","description":"\n\t\n\t\t\n\t\tMuMA-ToM: Multi-modal Multi-Agent Theory of Mind   AAAI 2025 (Oral) \n\t\n\n[ðŸ Homepage] [ðŸ’»Code] [ðŸ“Paper]\nMuMA-ToM is the first multi-modal Theory of Mind benchmark designed to evaluate mental reasoning in embodied multi-agent interactions. The benchmark was designed with several key features in mind:\n\nIt is factually correct, concise, and readable.\nIt requires integrating information from multiple modalities to answer the questions.\nIt tests understanding of multi-agent interactionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SCAI-JHU/MUMA-TOM-BENCHMARK.","url":"https://huggingface.co/datasets/SCAI-JHU/MUMA-TOM-BENCHMARK","creator_name":"Social Cognitive AI Lab at JHU","creator_url":"https://huggingface.co/SCAI-JHU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","Text"],"keywords_longer_than_N":true},
	{"name":"MTabVQA-Eval","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tDataset Card for MTabVQA\n\t\n\nPaper\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMTabVQA (Multi-Tabular Visual Question Answering) is a novel benchmark designed to evaluate the ability of Vision-Language Models (VLMs) to perform multi-hop reasoning over multiple tables presented as images. This scenario is common in real-world documents like web pages and PDFs but is critically under-represented in existing benchmarks.\nThe dataset consists of two main parts:\n\nMTabVQA-Eval:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval.","url":"https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval","creator_name":"MTabVQA","creator_url":"https://huggingface.co/mtabvqa","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","apache-2.0","1K - 10K","json","Image"],"keywords_longer_than_N":true},
	{"name":"AMEX-8k","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tAMEX-8K\n\t\n\nThis dataset is a curated 8K-sample subset from the original AMEX dataset, as mentioned in our paper. It serves as part of the training corpus for GUI grounding tasks, specifically capturing mobile app interfaces across diverse platforms and screen densities.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: Sampled from AMEX  \nDomain: Mobile GUI screenshots  \nDiversity: Includes a variety of app types and device form factors  \nUse case: GUI grounding pretraining, especially for mobileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zonghanHZH/AMEX-8k.","url":"https://huggingface.co/datasets/zonghanHZH/AMEX-8k","creator_name":"Hsieh ZongHan","creator_url":"https://huggingface.co/zonghanHZH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"MAmmoTH-VL-Instruct-12M","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tMAmmoTH-VL-Instruct-12M used in MoCa Pre-training\n\t\n\nðŸ  Homepage | ðŸ’» Code | ðŸ¤– MoCa-Qwen25VL-7B | ðŸ¤– MoCa-Qwen25VL-3B | ðŸ“š Datasets | ðŸ“„ Paper\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThis is a VQA style dataset used in the modality-aware continual pre-training of MoCa models. It is adapted from MAmmoTH-VL-Instruct-12M by concatenating prompts and responses.\nThe dataset consists of interleaved multimodal examples. text is a string containing text while imagesare image binaries that can be loadedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/moca-embed/MAmmoTH-VL-Instruct-12M.","url":"https://huggingface.co/datasets/moca-embed/MAmmoTH-VL-Instruct-12M","creator_name":"MoCa","creator_url":"https://huggingface.co/moca-embed","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"PIG_R1","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tPIG_R1: A High-Quality Dataset for Visual Geolocation\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPIG_R1 (Precise Image Geolocation - Release 1) is a large and diverse collection of street-level imagery and associated metadata, meticulously compiled for the task of visual geolocation. This dataset served as the foundational data asset for the research presented in the paper \"GeoLocSFT: Efficient Visual Geolocation via Supervised Fine-Tuning of Multimodal Foundation Models\".\nThe dataset isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/paidaixing/PIG_R1.","url":"https://huggingface.co/datasets/paidaixing/PIG_R1","creator_name":"qiangyi","creator_url":"https://huggingface.co/paidaixing","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["English","mit","ðŸ‡ºðŸ‡¸ Region: US","computer vision","multimodal"],"keywords_longer_than_N":true},
	{"name":"MMT-Bench","keyword":"multimodal","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for MMT-Bench\n\t\n\n\n\nRepository: https://github.com/OpenGVLab/MMT-Bench\nPaper: https://openreview.net/forum?id=R4Ng8zYaiz\nPoint of Contact: Wenqi Shao\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nLarge Vision-Language Models (LVLMs) show significant strides in general-propose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling shortâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMT-Bench.","url":"https://huggingface.co/datasets/OpenGVLab/MMT-Bench","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MIS_Test","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tRethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models\n\t\n\n\nOur paper, code, data, models can be found at MIS.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nOur MIS test set contains three split \"MIS-easy\", \"MIS-hard\", \"MIS-real\".\n{\n  \"question\": \"str\",\n  \"category\": \"str\",\n  \"sub_category\": \"str\",\n  \"image_path1\": \"str\",\n  \"image_path2\": \"str\",\n  \"id\": int\n}\n\n\n\t\n\t\t\n\t\tStatistics\n\t\n\nOur 'MIS-easy' and 'MIS-hard' datasets together contain 2,185 samples across 6 categories and 12â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Tuwhy/MIS_Test.","url":"https://huggingface.co/datasets/Tuwhy/MIS_Test","creator_name":"Yi Ding","creator_url":"https://huggingface.co/Tuwhy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"empathy-affective-datasets","keyword":"multimodal","description":"Empathy and Affective Computing Datasets Summary\nThis repository is a curated summary of existing datasets for empathy and affective computing research. It distinguishes between empathy-focused datasets (directly measuring empathic processes) and general affective computing datasets (emotion recognition, valence/arousal, etc.). This is not a new dataset but a reference guideâ€”please access original datasets via provided links and cite their sources.\n\n\t\n\t\t\n\t\n\t\n\t\tEmpathy and Affective Computingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Limorgu/empathy-affective-datasets.","url":"https://huggingface.co/datasets/Limorgu/empathy-affective-datasets","creator_name":"Limor Ki","creator_url":"https://huggingface.co/Limorgu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["cc-by-4.0","arxiv:1908.11706","arxiv:2501.12345","arxiv:2405.15708","arxiv:1810.02508"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React.","url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ImageNet-Paste","keyword":"multimodal","description":"\n\t\n\t\t\n\t\tImageNet-Paste\n\t\n\nImageNet-Paste is created by pasting in small images of different concepts into each image from the ImageNet validation dataset to probe the impact of concept pairs on multimodal task accuracy in natural images.\n\n\nEach ImageNet validation image is augmented by pasting in a small image of a different concept (accessory_word), and models are tasked with producing the correct ImageNet classification in the presence of the other concept. In our paper, we provide furtherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/helenqu/ImageNet-Paste.","url":"https://huggingface.co/datasets/helenqu/ImageNet-Paste","creator_name":"Helen Qu","creator_url":"https://huggingface.co/helenqu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","imagefolder","Image"],"keywords_longer_than_N":true}
]
;
