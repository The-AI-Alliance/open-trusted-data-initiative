const data_for_modality_multimodal = 
[
	{"name":"robonar","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/robonar/robonar","creator_name":"RoboNar","creator_url":"https://huggingface.co/robonar","description":"\n\t\n\t\t\n\t\tüìá RONAR (RoboNar) Dataset\n\t\n\nüìÑ Paper on arXiv  | üåê Project Website\nRONAR introduces a real-world multimodal dataset paired with natural language narrations for robotic experience grounding. Built on the Stretch SE3 mobile manipulator in real home environments, the dataset supports behavior transparency, risk estimation, and failure recovery for intelligent robotics systems. It underlies the RONAR framework described in the CoRL 2024 paper: \"I Can Tell What I Am Doing: Toward‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robonar/robonar.","first_N":5,"first_N_keywords":["text-generation","summarization","object-detection","robotics","English"],"keywords_longer_than_N":true},
	{"name":"teamcraft_data","keyword":"multi-modal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/teamcraft/teamcraft_data","creator_name":"TeamCraft","creator_url":"https://huggingface.co/teamcraft","description":"\n\t\n\t\t\n\t\tDataset Card for TeamCraft\n\t\n\nThe TeamCraft dataset is designed to develop multi-modal, multi-agent collaboration in Minecraft. It features 55,000 task variants defined by multi-modal prompts and procedurally generated expert demonstrations.\nThis repository contains the data for the validation set and its visualizations. \nTo use the validation set, download TeamCraft-Data-Valid.zip and extract using unzip TeamCraft-Data-Valid.zip.\nIn addition, the training set is available in two‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/teamcraft/teamcraft_data.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"vlsbench","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Foreshhh/vlsbench","creator_name":"XuHao Hu","creator_url":"https://huggingface.co/Foreshhh","description":"üéâ VLSBench has been accpeted to ACL2025 Main Conference, see you in Vienna.\n‚úÖ Update data.json with safety reason and image description for more efficient and reliable evaluaiton.\n\n\t\n\t\t\n\t\tDataset Card for VLSBench\n\t\n\nThis dataset is for paper VLSBench: Unveiling Information Leakage In Multimodal Safety\nYou can check our Paper, Github, Project Page for more information.\ndataset = load_dataset(\"Foreshhh/vlsbench\", split='train') \n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nOur dataset statistics is listed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Foreshhh/vlsbench.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"MMMU-LLM-R1-format","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xDAN-Vision/MMMU-LLM-R1-format","creator_name":"xDAN-RL-Group","creator_url":"https://huggingface.co/xDAN-Vision","description":"\n\t\n\t\t\n\t\tMMMU-LLM-R1 Reformatted Dataset\n\t\n\n","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"AmericanExpress_vision_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/AmericanExpress_vision_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tAMERICAN-EXPRESS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical and financial queries generated from American Express annual reports. It is designed to train and evaluate information retrieval models and improve AI understanding of financial documentation, with a specific focus on the credit card industry, payment processing, and banking services.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/AmericanExpress_vision_dataset.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"SMMILE","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/smmile/SMMILE","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMs have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE.","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ViStoryBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ViStoryBench/ViStoryBench","creator_name":"ViStoryBench","creator_url":"https://huggingface.co/ViStoryBench","description":"\n\t\n\t\t\n\t\tModel Card: ViStoryBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nViStoryBench is a comprehensive benchmark dataset for story visualization. It aims to thoroughly evaluate and advance the performance of story visualization models by providing diverse story types, artistic styles, and detailed annotations. The goal of story visualization is to generate a sequence of visually coherent and content-accurate images based on a given narrative text and character reference images.\nKey features of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ViStoryBench/ViStoryBench.","first_N":5,"first_N_keywords":["text-to-image","human-annotated","machine-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"MMR1-in-context-synthesizing","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing","creator_name":"Lai Wei","creator_url":"https://huggingface.co/WaltonFuture","description":"This dataset is designed for unsupervised post-training of Multi-Modal Large Language Models (MLLMs) focusing on enhancing reasoning capabilities. It contains image-problem-answer triplets, where the problem requires multimodal reasoning to derive the correct answer from the provided image. The dataset is intended for use with the MM-UPT framework described in the accompanying paper.\n\nüêô GitHub Repo: waltonfuture/MM-UPT\nüìú Paper (arXiv): Unsupervised Post-Training for Multi-Modal LLM Reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"SpaCE-10","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Cusyoung/SpaCE-10","creator_name":"ZiYang Gong","creator_url":"https://huggingface.co/Cusyoung","description":"This repository contains the dataset for the paper SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence.\n\n SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence\n\n\nGitHub Repository: https://github.com/Cuzyoung/SpaCE-10\n\n\n\t\n\t\t\n\t\n\t\n\t\tüß† What is SpaCE-10?\n\t\n\nSpaCE-10 is a compositional spatial intelligence benchmark for evaluating Multimodal Large Language Models (MLLMs) in indoor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cusyoung/SpaCE-10.","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MixBench25-visual","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mixed-modality-search/MixBench25-visual","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench25-visual.","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"MixBench2025","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mixed-modality-search/MixBench2025","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench2025.","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"UGC-VideoCap","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/openinterx/UGC-VideoCap","creator_name":"Memories.ai Research","creator_url":"https://huggingface.co/openinterx","description":"\n\t\n\t\t\n\t\tUGC-VideoCaptioner Dataset\n\t\n\nReal-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio-visual content. However, existing video captioning benchmarks and models remain predominantly visual-centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of full-modality datasets and lightweight, capable models hampers progress in fine-grained, multimodal video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openinterx/UGC-VideoCap.","first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2507.11336","üá∫üá∏ Region: US","video-captioning"],"keywords_longer_than_N":true},
	{"name":"HCTQA","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qcri-ai/HCTQA","creator_name":"Artificial Intelligence Research Group, Qatar Computing Research Institute","creator_url":"https://huggingface.co/qcri-ai","description":"\n\t\n\t\t\n\t\tHCT-QA: Human-Centric Tables Question Answering\n\t\n\nHCT-QA is a benchmark dataset designed to evaluate large language models (LLMs) on question answering over complex, human-centric tables (HCTs). These tables often appear in documents such as research papers, reports, and webpages and present significant challenges for traditional table QA due to their non-standard layouts and compositional structure.\nThe dataset includes:\n\n2,188 real-world tables with 9,835 human-annotated QA pairs\n4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qcri-ai/HCTQA.","first_N":5,"first_N_keywords":["question-answering","document-question-answering","visual-question-answering","expert-generated","English"],"keywords_longer_than_N":true},
	{"name":"DORI-Benchmark","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/appledora/DORI-Benchmark","creator_name":"Nazia Tasnim","creator_url":"https://huggingface.co/appledora","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDORI (Discriminative Orientation Reasoning Intelligence) is a comprehensive benchmark designed to evaluate object orientation understanding in multimodal large language models (MLLMs). The benchmark isolates and evaluates orientation perception as a primary capability, offering a systematic assessment framework that spans four essential dimensions of orientation comprehension: frontal alignment, rotational transformations, relative‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/appledora/DORI-Benchmark.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"M3DRS","keyword":"multi-modal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/heig-vd-geo/M3DRS","creator_name":"HEIG-Vd Geomatic","creator_url":"https://huggingface.co/heig-vd-geo","description":"\n\t\n\t\t\n\t\tM3DRS: Multi-Modal Multi-Resolution Remote Sensing Dataset\n\t\n\nThis repository hosts the M3DRS dataset, a comprehensive collection of 5-channel remote sensing images (RGB, NIR, nDSM) from Switzerland, France, and Italy. The dataset is unlabelled and specifically designed to support self-supervised learning tasks. It is part of our submission to the NeurIPS 2025 Datasets and Benchmarks Track. The dataset is organized into three folders, each containing ZIP archives of images grouped by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/heig-vd-geo/M3DRS.","first_N":5,"first_N_keywords":["feature-extraction","English","cc-by-4.0","100B<n<1T","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"csszengarden","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Technologic101/csszengarden","creator_name":"Anthony Chapman","creator_url":"https://huggingface.co/Technologic101","description":"Technologic101/csszengarden dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["feature-extraction","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"labeled-gaits-500k","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Housto4/labeled-gaits-500k","creator_name":"Ivan H","creator_url":"https://huggingface.co/Housto4","description":"Housto4/labeled-gaits-500k dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["time-series-forecasting","tabular-regression","tabular-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"labeled-gaits-instruct-200k","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Housto4/labeled-gaits-instruct-200k","creator_name":"Ivan H","creator_url":"https://huggingface.co/Housto4","description":"Housto4/labeled-gaits-instruct-200k dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["time-series-forecasting","tabular-regression","tabular-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MAmmoTH-VL-Instruct-12M","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/moca-embed/MAmmoTH-VL-Instruct-12M","creator_name":"MoCa","creator_url":"https://huggingface.co/moca-embed","description":"\n\t\n\t\t\n\t\tMAmmoTH-VL-Instruct-12M used in MoCa Pre-training\n\t\n\nüè† Homepage | üíª Code | ü§ñ MoCa-Qwen25VL-7B | ü§ñ MoCa-Qwen25VL-3B | üìö Datasets | üìÑ Paper\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThis is a VQA style dataset used in the modality-aware continual pre-training of MoCa models. It is adapted from MAmmoTH-VL-Instruct-12M by concatenating prompts and responses.\nThe dataset consists of interleaved multimodal examples. text is a string containing text while imagesare image binaries that can be loaded‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/moca-embed/MAmmoTH-VL-Instruct-12M.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"VCRBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pritamqu/VCRBench","creator_name":"Pritam Sarkar","creator_url":"https://huggingface.co/pritamqu","description":"\n\t\n\t\t\n\t\tVCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models\n\t\n\n \n \n \n \n \nAuthors: Pritam Sarkar and Ali Etemad\nThis repository provides the official implementation of VCRBench.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nPlease check our GitHub repo for the details of usage: VCRBench\nfrom dataset import VCRBench\ndataset=VCRBench(question_file=\"data.json\", \n                video_root=\"./\",\n                mode='default', \n                )\n    \nfor sample in dataset:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pritamqu/VCRBench.","first_N":5,"first_N_keywords":["video-text-to-text","visual-question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"CHASM-Covert_Advertisement_on_RedNote","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Jingyi77/CHASM-Covert_Advertisement_on_RedNote","creator_name":"Jingyi","creator_url":"https://huggingface.co/Jingyi77","description":"\n\t\n\t\t\n\t\tRedNote Covert Advertisement Detection Dataset\n\t\n\nThis dataset contains posts from the RedNote platform for covert advertisement detection tasks.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\n\t\n\t\t\nSplit\nPosts\nAd Posts\nNon-Ad Posts\nTotal Images\n\n\n\t\t\nTrain\n3493\n426\n3067\n18543\n\n\nValidation\n499\n57\n442\n2678\n\n\nTest\n1000\n130\n870\n5103\n\n\nTotal\n4992\n613\n4379\n26324\n\n\n\t\n\n\nNote: The viewer shows a small example subset of the data (60 samples) for demonstration purposes. The complete dataset is available via‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jingyi77/CHASM-Covert_Advertisement_on_RedNote.","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"SeePhys","keyword":"multi-modal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SeePhys/SeePhys","creator_name":"AI4Science","creator_url":"https://huggingface.co/SeePhys","description":"\n\t\n\t\t\n\t\tSeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning\n\t\n\nCan AI truly see the Physics? Test your model with the newly released SeePhys Benchmark!\nCovering 2,000 vision-text multimodal physics problems spanning from middle school to doctoral qualification exams, the SeePhys benchmark systematically evaluates LLMs/MLLMs on tasks integrating complex scientific diagrams with theoretical derivations. Experiments reveal that even SOTA models like Gemini-2.5-Pro‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SeePhys/SeePhys.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MM-MathInstruct","keyword":"multi-modal-qa","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MM-MathInstruct","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\n\t\n\t\t\n\t\tMathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning\n\t\n\nRepo: https://github.com/mathllm/MathCoder\nPaper: https://huggingface.co/papers/2505.10557\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MathCoder-VL, a series of open-source large multimodal models (LMMs) specifically tailored for general math problem-solving. We also introduce FigCodifier-8B, an image-to-code model.\n\n\t\n\t\t\nBase Model\nOurs\n\n\n\t\t\nMini-InternVL-Chat-2B-V1-5\nMathCoder-VL-2B\n\n\nInternVL2-8B‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MM-MathInstruct.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"HueManity","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Jayant-Sravan/HueManity","creator_name":"Jayant Sravan Tamarapalli","creator_url":"https://huggingface.co/Jayant-Sravan","description":"\n\t\n\t\t\n\t\tHueManity: A Benchmark for Testing Human-Like Visual Perception in MLLMs\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nHueManity is a benchmark dataset featuring 83,850 images designed to test the fine-grained visual perception of Multimodal Large Language Models (MLLMs). Each image presents a two-character alphanumeric string embedded within Ishihara-style dot patterns, challenging models to perform precise pattern recognition in visually cluttered environments.\nThe dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jayant-Sravan/HueManity.","first_N":5,"first_N_keywords":["question-answering","image-to-text","image-feature-extraction","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"TAMMs","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IceInPot/TAMMs","creator_name":"ÈîÖ‰∏≠ÂÜ∞","creator_url":"https://huggingface.co/IceInPot","description":"\n\t\n\t\t\n\t\tTAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting\n\t\n\nTAMMs is a large-scale dataset derived from the Functional Map of the World (fMoW) dataset, curated to support multimodal and temporal reasoning tasks such as change detection and future prediction.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 37,003 high-quality temporal sequences, each consisting of at least four distinct satellite images of the same location captured at different‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IceInPot/TAMMs.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"fire-exam-base64","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taean-yoo/fire-exam-base64","creator_name":"Taean Yoo","creator_url":"https://huggingface.co/taean-yoo","description":"\n\t\n\t\t\n\t\tüî• Fire Exam Dataset with Images\n\t\n\nÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ ÏÜåÎ∞©Í≥µÎ¨¥Ïõê ÏãúÌóò Î¨∏Ï†úÎ•º Í∏∞Î∞òÏúºÎ°ú Íµ¨ÏÑ±Îêú Î©ÄÌã∞Î™®Îã¨ QA Îç∞Ïù¥ÌÑ∞ÏÖãÏûÖÎãàÎã§.Í∞Å ÏÉòÌîåÏùÄ Î¨∏Ï†ú ÌÖçÏä§Ìä∏, ÏÑ†ÌÉùÏßÄ, Ï†ïÎãµ, Í∑∏Î¶¨Í≥† ÏãúÍ∞Å Ï†ïÎ≥¥Î•º Îã¥ÏùÄ Ïù¥ÎØ∏ÏßÄ ÌååÏùº Í≤ΩÎ°úÎ•º Ìè¨Ìï®ÌïòÍ≥† ÏûàÏäµÎãàÎã§.\n","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Korean","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"MMMG","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMMGBench/MMMG","creator_name":"MMMG","creator_url":"https://huggingface.co/MMMGBench","description":"\n\t\n\t\t\n\t\tüß† MMMG: Massive Multi-Discipline Multi-Tier Knowledge Image Benchmark\n\t\n\n\n  üß¨ Project Page ‚Ä¢\n  üìÇ Code\n\n\nMMMG introduces knowledge image generation as a new frontier in text-to-image research. This benchmark probes the reasoning capabilities of image generation models by challenging them to produce educational and scientific visuals grounded in structured knowledge.\nKnowledge images‚Äîsuch as charts, diagrams, mind maps, and scientific illustrations‚Äîplay a crucial role in human‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMMGBench/MMMG.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"airbus-vision-dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/airbus-vision-dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tAIRBUS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from Airbus technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice at TW3 Partners, a company specialized in Generative AI. Passionate about artificial intelligence and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/airbus-vision-dataset.","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"SolidGeo","keyword":"multi-modal-qa","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HarryYancy/SolidGeo","creator_name":"HarryYancy","creator_url":"https://huggingface.co/HarryYancy","description":"\n\t\n\t\t\n\t\tSolidGeo: Measuring Multimodal Spatial Math Reasoning in Solid Geometry\n\t\n\n[üåê Homepage] [üíª Github]  [ü§ó Huggingface Dataset] \n[üìä Leaderboard ]  [üîç Visualization]  [üìñ Paper]\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nSolidGeo is the first large-scale benchmark specifically designed to evaluate the performance of MLLMs on mathematical reasoning tasks in solid geometry. SolidGeo consists of 3,113 real-world K‚Äì12 and competition-level problems, each paired with visual context and annotated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HarryYancy/SolidGeo.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-classification","English"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React-Single-Image","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Single-Image","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Single-Image.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React-Multi-Images","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Multi-Images","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Multi-Images.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"android_control_test","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/InfiX-ai/android_control_test","creator_name":"InfiX.ai","creator_url":"https://huggingface.co/InfiX-ai","description":"\n\t\n\t\t\n\t\tProcessed Android Control Test Set for InfiGUI-R1 Evaluation\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the processed test set derived from the Android Control dataset by Google Research. It has been specifically prepared for evaluating the performance of our model, InfiGUI-R1.\nThe InfiGUI-R1 model is detailed in our paper:\n\nInfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners\n\nThis dataset facilitates standardized testing and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/InfiX-ai/android_control_test.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K<n<10K","arxiv:2504.14239"],"keywords_longer_than_N":true},
	{"name":"DataSeeds.AI-Sample-Dataset-DSD","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Dataseeds/DataSeeds.AI-Sample-Dataset-DSD","creator_name":"Dataseeds AI","creator_url":"https://huggingface.co/Dataseeds","description":"\n\t\n\t\t\n\t\tDataSeeds.AI Sample Dataset (DSD)\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe DataSeeds.AI Sample Dataset (DSD) is a high-fidelity, human-curated computer vision-ready dataset comprised of 7,772 peer-ranked, fully annotated photographic images, 350,000+ words of descriptive text, and comprehensive metadata. While the DSD is being released under an open source license, a sister dataset of over 10,000 fully annotated and segmented images is available for immediate commercial licensing, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dataseeds/DataSeeds.AI-Sample-Dataset-DSD.","first_N":5,"first_N_keywords":["image-classification","object-detection","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MixBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mixed-modality-search/MixBench","creator_name":"mixed-modality-search","creator_url":"https://huggingface.co/mixed-modality-search","description":"\n\t\n\t\t\n\t\tMixBench: A Benchmark for Mixed Modality Retrieval\n\t\n\nMixBench is a benchmark for evaluating retrieval across text, images, and multimodal documents. It is designed to test how well retrieval models handle queries and documents that span different modalities, such as pure text, pure images, and combined image+text inputs.\nMixBench includes four subsets, each curated from a different data source:\n\nMSCOCO\nGoogle_WIT\nVisualNews\nOVEN\n\nEach subset contains:\n\nqueries.jsonl: each entry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mixed-modality-search/MixBench.","first_N":5,"first_N_keywords":["text-ranking","document-retrieval","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"TreeOfLife-200M","keyword":"multimodal","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imageomics/TreeOfLife-200M","creator_name":"HDR Imageomics Institute","creator_url":"https://huggingface.co/imageomics","description":"\n\t\n\t\t\n\t\tDataset Card for TreeOfLife-200M\n\t\n\nWith nearly 214 million images representing 952,257 taxa across the tree of life, TreeOfLife-200M is the largest and most diverse public ML-ready dataset for computer vision models in biology at release. This dataset combines images and metadata from four core biodiversity data providers: Global Biodiversity Information Facility (GBIF), Encyclopedia of Life (EOL), BIOSCAN-5M, and FathomNet to more than double the number of unique taxa covered by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imageomics/TreeOfLife-200M.","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","English","Latin","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"MathVision","keyword":"multi-modal-qa","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MathVision","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\n\t\n\t\t\n\t\tMeasuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset\n\t\n\n[üíª Github] [üåê Homepage]  [üìä Leaderboard ] [üìä Open Source Leaderboard ] [üîç Visualization] [üìñ Paper]\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\n\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MathLLMs/MathVision\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tüí• News\n\t\n\n\n[2025.05.16] üí• We now support the official open-source leaderboard! üî•üî•üî• Skywork-R1V2-38B is the best open-source model, scoring 49.7% on MATH-Vision. üî•üî•üî•‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MathVision.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","expert-generated"],"keywords_longer_than_N":true},
	{"name":"ScienceQA","keyword":"multi-modal-qa","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/derek-thomas/ScienceQA","creator_name":"Derek Thomas","creator_url":"https://huggingface.co/derek-thomas","description":"\n\t\n\t\t\n\t\tDataset Card Creation Guide\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nLearn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nMulti-modal Multiple Choice\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nEnglish\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nExplore more samples here.\n{'image': Image,\n 'question': 'Which of these states is farthest north?',\n 'choices': ['West Virginia', 'Louisiana', 'Arizona', 'Oklahoma'],\n 'answer': 0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/derek-thomas/ScienceQA.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","other","visual-question-answering","text-classification"],"keywords_longer_than_N":true},
	{"name":"MathVista","keyword":"multi-modal-qa","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AI4Math/MathVista","creator_name":"AI for Math Reasoning","creator_url":"https://huggingface.co/AI4Math","description":"\n\t\n\t\t\n\t\tDataset Card for MathVista\n\t\n\n\nDataset Description\nPaper Information\nDataset Examples\nLeaderboard\nDataset Usage\nData Downloading\nData Format\nData Visualization\nData Source\nAutomatic Evaluation\n\n\nLicense\nCitation\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nMathVista is a consolidated Mathematical reasoning benchmark within Visual contexts. It consists of three newly created datasets, IQTest, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate logical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI4Math/MathVista.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","text-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"Mantis-Instruct","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tMantis-Instruct\n\t\n\nPaper | Website | Github | Models | Demo\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nMantis-Instruct is a fully text-image interleaved multimodal instruction tuning dataset, \ncontaining 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding. \nIt's been used to train Mantis Model families\n\nMantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\nAmong the 14‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Leopard-Instruct","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wyu1/Leopard-Instruct","creator_name":"Wenhao Yu","creator_url":"https://huggingface.co/wyu1","description":"\n\t\n\t\t\n\t\tLeopard-Instruct\n\t\n\nPaper | Github | Models-LLaVA | Models-Idefics2\n\n\t\n\t\t\n\t\tSummaries\n\t\n\nLeopard-Instruct is a large instruction-tuning dataset, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. It's been used to train Leopard-LLaVA [checkpoint]  and Leopard-Idefics2 [checkpoint].\n\n\t\n\t\t\n\t\tLoading dataset\n\t\n\n\nto load the dataset without automatically downloading and process the images (Please run the following codes with datasets==2.18.0)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wyu1/Leopard-Instruct.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"videophy2_test","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/videophysics/videophy2_test","creator_name":"videophysics","creator_url":"https://huggingface.co/videophysics","description":"Project: https://github.com/Hritikbansal/videophy/tree/main/VIDEOPHY2\ncaption: original prompt in the dataset\nvideo_url: generated video (using original prompt or upsampled caption, depending on the video model)\nsa: semantic adherence score (1-5) from human evaluation\npc: physical commonsense score (1-5) from human evaluation\njoint: computed as sa >= 4, pc >= 4\nphysics_rules_followed: list of physics rules followed in the video as judged by human annotators (1)\nphysics_rules_unfollowed: list‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/videophysics/videophy2_test.","first_N":5,"first_N_keywords":["video-classification","mit","1K - 10K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"ScanBot","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ed1son/ScanBot","creator_name":"zhiling chen","creator_url":"https://huggingface.co/ed1son","description":" \n\n\t\n\t\t\n\t\tScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tüß† Dataset Summary\n\t\n\nScanBot is a dataset for instruction-conditioned, high-precision surface scanning with robots. Unlike existing datasets that focus on coarse tasks like grasping or navigation, ScanBot targets industrial laser scanning, where sub-millimeter accuracy and parameter stability are essential. It includes scanning trajectories across 12 objects and 6 task types, each driven by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ed1son/ScanBot.","first_N":5,"first_N_keywords":["robotics","English","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MotionSight","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nkp37/MotionSight","creator_name":"nkp","creator_url":"https://huggingface.co/nkp37","description":"This is the dataset proposed in our paper MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs.\nWe split the dataset into multiple small files, you can recover by cat:\ncat MotionSightDataset_part* > MotionSightDataset.zip\nunzip MotionSightDataset\n\nProject Page | Github\n","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K<n<100K","arxiv:2506.01674"],"keywords_longer_than_N":true},
	{"name":"MTabVQA-Eval","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval","creator_name":"MTabVQA","creator_url":"https://huggingface.co/mtabvqa","description":"\n\t\n\t\t\n\t\tDataset Card for MTabVQA\n\t\n\nPaper\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMTabVQA (Multi-Tabular Visual Question Answering) is a novel benchmark designed to evaluate the ability of Vision-Language Models (VLMs) to perform multi-hop reasoning over multiple tables presented as images. This scenario is common in real-world documents like web pages and PDFs but is critically under-represented in existing benchmarks.\nThe dataset consists of two main parts:\n\nMTabVQA-Eval:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval.","first_N":5,"first_N_keywords":["table-question-answering","apache-2.0","1K - 10K","json","Image"],"keywords_longer_than_N":true},
	{"name":"CoreCognition","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/williamium/CoreCognition","creator_name":"William Li","creator_url":"https://huggingface.co/williamium","description":"\n\t\n\t\t\n\t\tCoreCognition: A Core Knowledge Benchmark for Multi-modal Large Language Models\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCoreCognition is a large-scale benchmark encompassing 12 core knowledge grounded in developmental cognitive science, designed to evaluate the fundamental core abilities of Multi-modal Large Language Models (MLLMs).\nWhile MLLMs demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild remains limited, often falling short on tasks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/williamium/CoreCognition.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Visco-Attack","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/miaozq/Visco-Attack","creator_name":"miaozq","creator_url":"https://huggingface.co/miaozq","description":"\n\t\n\t\t\n\t\tVisCo Attack: Visual Contextual Jailbreak Dataset\n\t\n\nüìÑ arXiv:2507.02844 ¬∑ üíª Code ‚Äì Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\nThis dataset contains the adversarial contexts, prompts, and images from the paper: \"Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\".\n\n\t\n\t\t\n\t\n\t\n\t\t‚ö†Ô∏è Content Warning\n\t\n\nThis dataset contains content that is offensive and/or harmful. It was created for research purposes to study the safety‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/miaozq/Visco-Attack.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MTabVQA-Instruct","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mtabvqa/MTabVQA-Instruct","creator_name":"MTabVQA","creator_url":"https://huggingface.co/mtabvqa","description":"Paper\n\n\t\n\t\t\n\t\tMTabVQA-Instruct Sub-datasets\n\t\n\nThis directory contains multiple MTabVQA-Instruct datasets for visual question answering over tables.\n\n\t\n\t\t\n\t\tDatasets\n\t\n\n\nMTabVQA-Atis-Instruct\nMTabVQA-MiMo-Instruct\nMTabVQA-Multitab-Instruct\nMTabVQA-Spider-Instruct\n\nEach dataset contains a VQA.jsonl file and a table_images directory with the corresponding table images.\n\n\t\n\t\t\n\t\tImportant Note for Multitab-Instruct\n\t\n\nYou must unzip the table_images.zip file in MTabVQA-Multitab-Instruct/ to access‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mtabvqa/MTabVQA-Instruct.","first_N":5,"first_N_keywords":["table-question-answering","apache-2.0","10K - 100K","json","Image"],"keywords_longer_than_N":true},
	{"name":"clevr-math","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dali-does/clevr-math","creator_name":"Adam Dahlgren Lindstr√∂m","creator_url":"https://huggingface.co/dali-does","description":"CLEVR-Math is a dataset for compositional language, visual and mathematical reasoning. CLEVR-Math poses questions about mathematical operations on visual scenes using subtraction and addition, such as \"Remove all large red cylinders. How many objects are left?\". There are also adversarial (e.g. \"Remove all blue cubes. How many cylinders are left?\") and multihop questions (e.g. \"Remove all blue cubes. Remove all small purple spheres. How many objects are left?\").","first_N":5,"first_N_keywords":["visual-question-answering","visual-question-answering","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"vsr_random","keyword":"multimodality","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cambridgeltl/vsr_random","creator_name":"Language Technology Lab @University of Cambridge","creator_url":"https://huggingface.co/cambridgeltl","description":"\n\t\n\t\t\n\t\tVSR: Visual Spatial Reasoning\n\t\n\nThis is the random set of VSR: Visual Spatial Reasoning (TACL 2023) [paper].\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndata_files = {\"train\": \"train.jsonl\", \"dev\": \"dev.jsonl\", \"test\": \"test.jsonl\"}\ndataset = load_dataset(\"cambridgeltl/vsr_random\", data_files=data_files)\n\nNote that the image files still need to be downloaded separately. See data/ for details.\nGo to our github repo for more introductions.\n\n\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find VSR‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cambridgeltl/vsr_random.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"vsr_zeroshot","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cambridgeltl/vsr_zeroshot","creator_name":"Language Technology Lab @University of Cambridge","creator_url":"https://huggingface.co/cambridgeltl","description":"\n\t\n\t\t\n\t\tVSR: Visual Spatial Reasoning\n\t\n\nThis is the zero-shot set of VSR: Visual Spatial Reasoning (TACL 2023) [paper].\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndata_files = {\"train\": \"train.jsonl\", \"dev\": \"dev.jsonl\", \"test\": \"test.jsonl\"}\ndataset = load_dataset(\"cambridgeltl/vsr_zeroshot\", data_files=data_files)\n\nNote that the image files still need to be downloaded separately. See data/ for details.\nGo to our github repo for more introductions.\n\n\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cambridgeltl/vsr_zeroshot.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VisIT-Bench","keyword":"multi-modal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/VisIT-Bench","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n\t\n\t\t\n\t\tDataset Card for VisIT-Bench\n\t\n\n\nDataset Description\nLinks\nDataset Structure\nData Fields\nData Splits\nData Loading\n\n\nLicensing Information\nAnnotations\nConsiderations for Using the Data\nCitation Information\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nVisIT-Bench is a dataset and benchmark for vision-and-language instruction following. The dataset is comprised of image-instruction pairs and corresponding example outputs, spanning a wide range of tasks, from simple object recognition to complex‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/VisIT-Bench.","first_N":5,"first_N_keywords":["crowdsourced","found","original","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"TALI","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Antreas/TALI","creator_name":"Antreas Antoniou","creator_url":"https://huggingface.co/Antreas","description":"\n\t\n\t\t\n\t\tDataset Card for \"TALI\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nTALI is a large-scale, tetramodal dataset designed to facilitate a shift from unimodal and duomodal to tetramodal research in deep learning. It aligns text, video, images, and audio, providing a rich resource for innovative self-supervised learning tasks and multimodal research. TALI enables exploration of how different modalities and data/model scaling affect downstream performance, with the aim of inspiring‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Antreas/TALI.","first_N":5,"first_N_keywords":["zero-shot-classification","cc-by-4.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Tuberculosis_Dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/moukaii/Tuberculosis_Dataset","creator_name":"Zhankai Ye","creator_url":"https://huggingface.co/moukaii","description":"\n\t\n\t\t\n\t\tMultimodal Dataset of Tuberculosis Patients including CT and Clinical Case Reports\n\t\n\nZhankai Ye    \nNetID: zy172\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is curated from the original ‚ÄúThe MultiCaRe Dataset‚Äù to focus on the chest tuberculosis patients. This is a multimodal dataset consisting of lung computed tomography (CT) imaging data and the clinical case records of tuberculosis patients, along with their case keywords, the captions of their CT images, patient_id, gender, and age‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/moukaii/Tuberculosis_Dataset.","first_N":5,"first_N_keywords":["feature-extraction","text-classification","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"airbnb_embeddings","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MongoDB/airbnb_embeddings","creator_name":"MongoDB","creator_url":"https://huggingface.co/MongoDB","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset consists of AirBnB listings with property descriptions, reviews, and other metadata. \nIt also contains text embeddings of the property descriptions as well as image embeddings of the listing image. The text embeddings were created using OpenAI's text-embedding-3-small model and the image embeddings using OpenAI's clip-vit-base-patch32 model available on Hugging Face. \nThe text embeddings have 1536 dimensions, while the image embeddings have 512 dimensions.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MongoDB/airbnb_embeddings.","first_N":5,"first_N_keywords":["question-answering","text-retrieval","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"VL-ICL","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ys-zong/VL-ICL","creator_name":"Yongshuo Zong","creator_url":"https://huggingface.co/ys-zong","description":"\n\t\n\t\t\n\t\tVL-ICL Bench\n\t\n\nVL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning\n[Webpage] [Paper] [Code]\n\n\t\n\t\t\n\t\tImage-to-Text Tasks\n\t\n\nIn all image-to-text tasks image is a list of image paths (typically one item - for interleaved cases there are two items).\n\n\t\n\t\t\n\t\tFast Open-Ended MiniImageNet\n\t\n\nFrozen introduces the task of fast concept binding for MiniImageNet. The benchmark has a fixed structure so only the given support examples can be used for a given‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ys-zong/VL-ICL.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","mit","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"DRGBT603","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zhaodong2061/DRGBT603","creator_name":"zhaodongding","creator_url":"https://huggingface.co/zhaodong2061","description":"zhaodong2061/DRGBT603 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","apache-2.0","100K<n<1M","Image","doi:10.57967/hf/5438"],"keywords_longer_than_N":true},
	{"name":"MultiCaRe_Dataset","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset","creator_name":"Mauro Nievas Offidani","creator_url":"https://huggingface.co/mauro-nievoff","description":"The dataset contains multi-modal data from over 75,000 open access and de-identified case reports, including metadata, clinical cases, image captions and more than 130,000 images. Images and clinical cases belong to different medical specialties, such as oncology, cardiology, surgery and pathology. The structure of the dataset allows to easily map images with their corresponding article metadata, clinical case, captions and image labels. Details of the data structure can be found in the file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"test-public","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taiseimatsuoka/test-public","creator_name":"taisei matsuoka","creator_url":"https://huggingface.co/taiseimatsuoka","description":"\n\t\n\t\t\n\t\tnanoLLaVA - Sub 1B Vision-Language Model\n\t\n\n\n  \n\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nnanoLLaVA is a \"small but mighty\" 1B vision-language model designed to run efficiently on edge devices.\n\nBase LLM: Quyen-SE-v0.1 (Qwen1.5-0.5B)\nVision Encoder: google/siglip-so400m-patch14-384\n\n\n\t\n\t\t\nModel\nVQA v2\nTextVQA\nScienceQA\nPOPE\nMMMU (Test)\nMMMU (Eval)\nGQA\nMM-VET\n\n\n\t\t\nScore\n70.84\n46.71\n58.97\n84.1\n28.6\n30.4\n54.79\n23.9\n\n\n\t\n\t\t\n\t\tTraining Data\n\t\n\nTraining Data will be released later as I am still writing a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/taiseimatsuoka/test-public.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","text","Image"],"keywords_longer_than_N":true},
	{"name":"PIN-14M","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-a-p/PIN-14M","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","description":"\n\t\n\t\t\n\t\tPIN-14M\n\t\n\nA mini version of \"PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents\"\nPaper: https://arxiv.org/abs/2406.13923\nThis dataset contains 14M samples in PIN format, with at least 7.33B tokens.\nüöÄ News\n[ 2024.12.12 ] !NEW! üî• We have updated the quality signals for all subsets, with the dataset now containing 7.33B tokens after Llama3 tokenization.\n[ 2024.12.06 ] !NEW! üî• We have updated the quality signals, enabling a swift assessment of whether a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/PIN-14M.","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"vwp","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tonyhong/vwp","creator_name":"Xudong Hong","creator_url":"https://huggingface.co/tonyhong","description":"\n\t\n\t\t\n\t\tDataset Card for Visual Writing Prompts Dataset (VWP)\n\t\n\nWebsite | Github Repository | arXiv e-Print\n\n\nThe Visual Writing Prompts (VWP) dataset contains almost 2K selected sequences of\nmovie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which are collected via crowdsourcing given the image sequences and up to 5  grounded characters from the corresponding image sequence.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\n\nTACL‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tonyhong/vwp.","first_N":5,"first_N_keywords":["image-to-text","text-generation","monolingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"vwp","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tonyhong/vwp","creator_name":"Xudong Hong","creator_url":"https://huggingface.co/tonyhong","description":"\n\t\n\t\t\n\t\tDataset Card for Visual Writing Prompts Dataset (VWP)\n\t\n\nWebsite | Github Repository | arXiv e-Print\n\n\nThe Visual Writing Prompts (VWP) dataset contains almost 2K selected sequences of\nmovie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which are collected via crowdsourcing given the image sequences and up to 5  grounded characters from the corresponding image sequence.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\n\nTACL‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tonyhong/vwp.","first_N":5,"first_N_keywords":["image-to-text","text-generation","monolingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MAGB","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sherirto/MAGB","creator_name":"Sherirto","creator_url":"https://huggingface.co/Sherirto","description":"\n\t\n\t\t\n\t\tMAGBÔºö A Comprehensive Benchmark for Multimodal Attributed Graphs\n\t\n\nIn many real-world scenarios, graph nodes are associated with multimodal attributes, such as texts and images, resulting in Multimodal Attributed Graphs (MAGs).\nMAGB first provide 5 dataset from E-Commerce and Social Networks. And we evaluate two major paradigms: GNN-as Predictor and VLM-as-Predictor . The datasets are publicly available:\n\n     ü§ó Hugging Face¬†¬†  | ¬†¬†üìë Paper¬†¬†\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüìñ Table of Contents\n\t\n\n\nüìñ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sherirto/MAGB.","first_N":5,"first_N_keywords":["graph-ml","cc-by-4.0","Image","arxiv:2410.09132","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"llava-pretrain-refined-by-data-juicer","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\n\t\n\t\t\n\t\tLLaVA pretrain -- LCS-558k (refined by Data-Juicer)\n\t\n\nA refined version of LLaVA pretrain dataset (LCS-558k) by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Multimodal Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 115MB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 500,380 (Keep ~89.65% from the original dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"DEEPFRUlT_DATASET","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sc890/DEEPFRUlT_DATASET","creator_name":"shangrong chi","creator_url":"https://huggingface.co/sc890","description":"\n\t\n\t\t\n\t\tDeepFruit Dataset\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis dataset contains total of 21,122 fully labeled images, featuring 20 different kinds of fruits. It is structured into an 80% training set (16,899 images) and a 20% testing set (4,223 images), facilitating a ready-to-use framework for model training and evaluation.\nAdditionally, there are two CSV files that label the types of fruits depicted in each image.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe \"DeepFruit\" dataset is a comprehensive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sc890/DEEPFRUlT_DATASET.","first_N":5,"first_N_keywords":["feature-extraction","text-classification","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ArxivCap","keyword":"multi-modal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMInstruction/ArxivCap","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for ArxivCap\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Instances\n\t\n\n\nExample-1 of single (image, caption) pairs\n\n\"......\" stands for omitted parts.\n\n{\n    'src': 'arXiv_src_2112_060/2112.08947', \n    'meta': \n    {\n        'meta_from_kaggle': \n        {\n            'journey': '', \n            'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', \n            'categories': 'cs.ET'\n        }, \n        'meta_from_s2': \n        {\n            'citationCount': 8‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/ArxivCap.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-ArXiv","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-ArXiv","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-ArXiv.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"unusual-objects-unusual-places_text-image","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zer0int/unusual-objects-unusual-places_text-image","creator_name":"zer0int","creator_url":"https://huggingface.co/zer0int","description":"\n\t\n\t\t\n\t\t(Un-)usual objects in (un-)usual places\n\t\n\n\n\t\n\t\t\n\t\tA small Text-Image dataset to confuse, probe (and improve) SOTA (2024) machine vision models.\n\t\n\nTo be continued (with further examples added)...\nExample results from LMSYS ARENA (June 2024):\n\n\n","first_N":5,"first_N_keywords":["English","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"CGTSF","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChengsenWang/CGTSF","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","description":"\n\t\n\t\t\n\t\tCGTSF: Context-Guided Time Series Forecasting\n\t\n\n\n\t\n\t\t\n\t\t‚ú® Introduction\n\t\n\nThe context-guided time series forecasting task entails the transformation of text into time series data. Relevant multimodal datasets are limited. To address these data gaps, we have collected three multimodal datasets that offer valuable resources for future research. The following table summarizes the statistics of these datasets. MSPG comprises 13 months of solar power generation data on 27 photovoltaic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/CGTSF.","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"TSQA","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChengsenWang/TSQA","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","description":"\n\t\n\t\t\n\t\tTSQA: Time Series Question Answering\n\t\n\n\n\t\n\t\t\n\t\t‚ú® Introduction\n\t\n\nIn the time series question answering task, we employ the KernelSynth to generate a variable-length multimodal question and answer pairs based on identifying four generic typical time series features, which aid ChatTime in comprehending the fundamental principles of time series. The following table summarizes the statistics of this dataset. Trend encompasses three categories: upward trend, downward trend, and constant‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/TSQA.","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"ChatTime-1-Pretrain-1M","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Pretrain-1M","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","description":"\n\t\n\t\t\n\t\tChatTime: A Multimodal Time Series Foundation Model\n\t\n\n\n\t\n\t\t\n\t\t‚ú® Introduction\n\t\n\nIn this paper, we innovatively model time series as a foreign language and construct ChatTime, a unified framework for time series and text processing. As an out-of-the-box multimodal time series foundation model, ChatTime provides zero-shot forecasting capability and supports bimodal input/output for both time series and text. We design a series of experiments to verify the superior performance of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Pretrain-1M.","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"ChatTime-1-Finetune-100K","keyword":"multimodality","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Finetune-100K","creator_name":"Chengsen Wang","creator_url":"https://huggingface.co/ChengsenWang","description":"\n\t\n\t\t\n\t\tChatTime: A Multimodal Time Series Foundation Model\n\t\n\n\n\t\n\t\t\n\t\t‚ú® Introduction\n\t\n\nIn this paper, we innovatively model time series as a foreign language and construct ChatTime, a unified framework for time series and text processing. As an out-of-the-box multimodal time series foundation model, ChatTime provides zero-shot forecasting capability and supports bimodal input/output for both time series and text. We design a series of experiments to verify the superior performance of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Finetune-100K.","first_N":5,"first_N_keywords":["time-series-forecasting","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"MMC","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xywang1/MMC","creator_name":"Xiaoyang Wang","creator_url":"https://huggingface.co/xywang1","description":"\n\t\n\t\t\n\t\tMMC: Advancing Multimodal Chart Understanding with LLM Instruction Tuning\n\t\n\nThis repo releases data introduced in our paper MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning.\n\nThe paper was published in NAACL 2024.\nSee our GithHub repo for demo code and more.\n\n\n\t\n\t\t\n\t\n\t\n\t\tHighlights\n\t\n\n\nWe introduce a large-scale MultiModal Chart Instruction (MMC-Instruction) dataset supporting diverse tasks and chart types. Leveraging this data.\nWe also propose a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xywang1/MMC.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-sa-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Set_Eval","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AarushSah/Set_Eval","creator_name":"Aarush Sah","creator_url":"https://huggingface.co/AarushSah","description":"AarushSah/Set_Eval dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Chinese_interactive_novels_3k","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/Chinese_interactive_novels_3k","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\t‰∏≠Êñá‰∫íÂä®Â∞èËØ¥ÁªìÊûÑÂåñËØ≠Êñô\n\t\n\nThis dataset contains uncleaned (!) 3534 structured Chinese interactive novels (‰∏≠Êñá‰∫íÂä®Â∞èËØ¥), accounting for around 0.25B (gpt-3.5) tokens in total.\nAll contents are parsed from certain online sources.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis dataset can be potentially used for LLM training. But be aware that you'd better clean the data yourself to remove undesired low-quality contents.\nEach novel is a dict structured as follows:\nclass Novel:\n    book_title: str\n    book_author: str‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/Chinese_interactive_novels_3k.","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1K - 10K","arrow"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2024-10","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-10","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-10.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-50","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-50","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-50.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-40","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-23","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-23","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-23.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-14","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-14","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-14.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-06","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-06","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-06.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2024-18","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-18","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-18.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"tabfquad_test_subsampled","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/tabfquad_test_subsampled","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings. Using a vision language model (GPT4V), we create additional queries to augment the existing human-annotated ones.\n\n\t\n\t\t\n\t\tData Curation\n\t\n\nTo ensure homogeneity across our benchmarked datasets, we subsampled the original test set to 280 pairs, leaving the rest for training and renaming the different columns.\n\n\t\n\t\t\n\t\tLoad the dataset\n\t\n\nfrom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vidore/tabfquad_test_subsampled.","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","French","English","mit"],"keywords_longer_than_N":true},
	{"name":"docvqa_test_subsampled","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/docvqa_test_subsampled","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the test set taken from the DocVQA dataset. It includes collected images from the UCSF Industry Documents Library. Questions and answers were manually annotated.\nExample of data (see viewer)\n\n\t\n\t\t\n\t\tData Curation\n\t\n\nTo ensure homogeneity across our benchmarked datasets, we subsampled the original test set to 500 pairs and renamed the different columns.\n\n\t\n\t\t\n\t\tLoad the dataset\n\t\n\nfrom datasets import load_dataset\nds =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vidore/docvqa_test_subsampled.","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"II-Bench","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-a-p/II-Bench","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","description":"\n\t\n\t\t\n\t\tII-Bench\n\t\n\nüåê Homepage | ü§ó Paper | üìñ arXiv | ü§ó Dataset | GitHub\n\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nII-Bench comprises 1,222 images, each accompanied by 1 to 3 multiple-choice questions, totaling 1,434 questions. II-Bench encompasses images from six distinct domains: Life, Art, Society, Psychology, Environment and Others. It also features a diverse array of image types, including Illustrations, Memes, Posters, Multi-panel Comics, Single-panel Comics, Logos and Paintings. The detailed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/II-Bench.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_energy_test","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/syntheticDocQA_energy_test","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about Energy that allow ViDoRe to benchmark technical documentation about energy. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('energy'). From these documents, we randomly sampled 1000 pages.\nWe associated these‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_energy_test.","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_healthcare_industry_test","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/syntheticDocQA_healthcare_industry_test","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about the Healthcare Industry that allow ViDoRe to benchmark medical documents. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('healthcare industry'). From these documents, we randomly sampled 1000 pages.\nWe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_healthcare_industry_test.","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_government_reports_test","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/syntheticDocQA_government_reports_test","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about the Government Reports that allow ViDoRe to benchmark administrative/legal documents. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('government reports'). From these documents, we randomly sampled 1000 pages.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_government_reports_test.","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"syntheticDocQA_artificial_intelligence_test","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vidore/syntheticDocQA_artificial_intelligence_test","creator_name":"Vidore","creator_url":"https://huggingface.co/vidore","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is part of a topic-specific retrieval benchmark spanning multiple domains, which evaluates retrieval in more realistic industrial applications. \nIt includes documents about the Artificial Intelligence. \n\n\t\n\t\t\n\t\tData Collection\n\t\n\nThanks to a crawler (see below), we collected 1,000 PDFs from the Internet with the query ('artificial intelligence'). From these documents, we randomly sampled 1000 pages.\nWe associated these with 100 questions and answers‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vidore/syntheticDocQA_artificial_intelligence_test.","first_N":5,"first_N_keywords":["document-question-answering","visual-document-retrieval","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MixtureVitae","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ontocord/MixtureVitae","creator_name":"Ontocord.AI","creator_url":"https://huggingface.co/ontocord","description":"\n\n\t\n\t\t\n\t\tMixtureVitae: A Permissive, High-Performance, Open-Access Pretraining Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nMixtureVitae is an open-source, permissive, high-quality dataset designed for pretraining large language models (LLMs) across a wide variety of modalities, domains, and languages. The goal of MixtureVitae is to accelerate the development of transparent, open-access AI while lowering legal uncertainty around copyright and data provenance. See our blog.\n\nPlease note this dataset is still‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ontocord/MixtureVitae.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","10M - 100M","json","Text"],"keywords_longer_than_N":true},
	{"name":"M3GIA","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Songweii/M3GIA","creator_name":"Wei Song","creator_url":"https://huggingface.co/Songweii","description":"\n\t\n\t\t\n\t\tM3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability\n\t\n\n[üåê Homepage] | ü§ó Dataset | ü§ó Paper | üìñ arXiv | üíª GitHub\nThe evaluation code can be found in üíª GitHub.\n[Abstract]\nAs recent multi-modality large language models (MLLMs) have shown formidable proficiency on various complex tasks, there has been increasing attention on debating whether these models could eventually mirror human intelligence. However, existing benchmarks mainly focus on evaluating‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Songweii/M3GIA.","first_N":5,"first_N_keywords":["English","Chinese","Spanish","French","Portuguese"],"keywords_longer_than_N":true},
	{"name":"MARVEL","keyword":"multi-modal-qa","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kianasun/MARVEL","creator_name":"Kiana Sun","creator_url":"https://huggingface.co/kianasun","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMARVEL is a new comprehensive benchmark dataset that evaluates multi-modal large language models' abstract reasoning abilities in six patterns across five different task configurations, revealing significant performance gaps between humans and SoTA MLLMs.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]\n\t\n\n\nRepository: https://github.com/1171-jpg/MARVEL_AVR\nPaper [optional]: https://arxiv.org/abs/2404.13591\nDemo [optional]:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kianasun/MARVEL.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"ChartMimic","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChartMimic/ChartMimic","creator_name":"ChartMimic","creator_url":"https://huggingface.co/ChartMimic","description":"\n\n ChartMimic: Evaluating LMM‚Äôs Cross-Modal Reasoning Capability via Chart-to-Code Generation \n\n\nThis is the official dataset repository of ChartMimic.\n\nKind Note: ChartMimic has been integrated into VLMEvalKit. Welcome to use ChartMimic through VLMEvalKit! Special thanks to the VLMEvalKit team.\n\n\n\t\n\t\t\n\t\n\t\n\t\t1. Data Overview\n\t\n\nChartMimic aims at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChartMimic/ChartMimic.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"CoMDataset","keyword":"multi-modal-qa","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/qijimrc/CoMDataset","creator_name":"Ji Qi","creator_url":"https://huggingface.co/qijimrc","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWe open-source both the Automatically Synthesized CoM Data and the Manually Annotated CoM-Math Data to facilitate potential research. The automatically synthesized CoM data (i.e., com.jsonl) consists of 84K positive reasoning chains, which was produced by an automated data generation pipeline with an LLM-based (GPT-4) linguistic solving steps generation and a VFMs-based (GroundingDINO, PaddleOCR) visual evidence compensation upon massive public VQA samples. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qijimrc/CoMDataset.","first_N":5,"first_N_keywords":["visual-question-answering","visual-question-answering","expert-generated","found","expert-generated"],"keywords_longer_than_N":true},
	{"name":"muchomusic","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mulab-mir/muchomusic","creator_name":"mulab-mir","creator_url":"https://huggingface.co/mulab-mir","description":"\n\t\n\t\t\n\t\tMuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models\n\t\n\nMuChoMusic is a benchmark designed to evaluate music understanding in multimodal language models focused on audio. It includes 1,187 multiple-choice questions validated by human annotators, based on 644 music tracks from two publicly available music datasets. These questions cover a wide variety of genres and assess knowledge and reasoning across several musical concepts and their cultural and functional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mulab-mir/muchomusic.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","1K - 10K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"tmp","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YANG-Cheng/tmp","creator_name":"Cheng Yang","creator_url":"https://huggingface.co/YANG-Cheng","description":"\n\n ChartMimic: Evaluating LMM‚Äôs Cross-Modal Reasoning Capability via Chart-to-Code Generation \n\n\nThis is the official dataset repository of ChartMimic. \n\n\t\n\t\t\n\t\t1. Data Overview\n\t\n\nChartMimic aims at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.\nChartMimic includes 1000 human-curated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YANG-Cheng/tmp.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-HTML","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"data-juicer-t2v-optimal-data-pool","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/data-juicer-t2v-optimal-data-pool","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\n\t\n\t\t\n\t\tData-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development\n\t\n\n\n\t\n\t\t\n\t\tProject description\n\t\n\nThe emergence of large-scale multi-modal generative models has drastically advanced artificial intelligence, introducing unprecedented levels of performance and functionality. \nHowever, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/data-juicer-t2v-optimal-data-pool.","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MMT-Bench","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenGVLab/MMT-Bench","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for MMT-Bench\n\t\n\n\n\nRepository: https://github.com/OpenGVLab/MMT-Bench\nPaper: https://openreview.net/forum?id=R4Ng8zYaiz\nPoint of Contact: Wenqi Shao\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nLarge Vision-Language Models (LVLMs) show significant strides in general-propose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMT-Bench.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"table-vqa","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cmarkea/table-vqa","creator_name":"Credit Mutuel Arkea","creator_url":"https://huggingface.co/cmarkea","description":"\n\t\n\t\t\n\t\tDataset description\n\t\n\nThe table-vqa Dataset integrates images of tables from the dataset AFTdb (Arxiv Figure Table Database) curated by cmarkea. \nThis dataset consists of pairs of table images and corresponding LaTeX source code, with each image linked to an average of ten questions and answers. Half of the Q&A pairs are in English and the other half in French. These questions and answers were generated using Gemini 1.5 Pro and Claude 3.5 sonnet, making the dataset well-suited for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cmarkea/table-vqa.","first_N":5,"first_N_keywords":["text-generation","text-to-image","image-to-text","table-question-answering","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Mid-Data","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Mid-Data","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","description":"\n\t\n\t\t\n\t\tDataset Card for LLaVA-OneVision\n\t\n\n\nDue to unknow reasons, we are unable to process dataset with large amount into required HF format. So we directly upload the json files and image folders (compressed into tar.gz files).\n\n\nYou can use the following link to directly download and decompress them.\nhttps://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Mid-Data/tree/main/evol_instruct\n\nWe provide the whole details of LLaVA-OneVision Dataset. In this dataset, we include the data splits‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Mid-Data.","first_N":5,"first_N_keywords":["text-generation","Chinese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"pentomino-easy-vsft","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Koshti10/pentomino-easy-vsft","creator_name":"Koshti","creator_url":"https://huggingface.co/Koshti10","description":"\n\t\n\t\t\n\t\tIndividual Module\n\t\n\nThis dataset is created for instruction tuning llava models based on the dataset created here - llava-instruct-mix-vsft\nThis dataset is based on a Pentomino game - More details -> github\n","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","description":"\n ü¶Ñ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n      \n      \n    \n    \n       \n      \n       \n       \n      \n      \n       \n      \n    \n      \n\n\n\n      \n    [ArXiv] | [ü§óHuggingFace] | [Website]\n    \n    \n\n\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\tüî•News\n\t\n\n\nüéñÔ∏è Our work is accepted by ACL2024.\n\nüî• We have release benchmark on [ü§óHuggingFace].\n\nüî• The paper is also available on [ArXiv].\n\nüîÆ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"multi-modal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","description":"\n ü¶Ñ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n      \n      \n    \n    \n       \n      \n       \n       \n      \n      \n       \n      \n    \n      \n\n\n\n      \n    [ArXiv] | [ü§óHuggingFace] | [Website]\n    \n    \n\n\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\tüî•News\n\t\n\n\nüéñÔ∏è Our work is accepted by ACL2024.\n\nüî• We have release benchmark on [ü§óHuggingFace].\n\nüî• The paper is also available on [ArXiv].\n\nüîÆ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"image-textualization","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sterzhang/image-textualization","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","description":"\n\t\n\t\t\n\t\tImage-Textualization Dataset\n\t\n\nExciting to announce the open-sourcing of our Image-Text Matching Dataset, which consists of 220K image-text pairs. We also release fine-grained annotations, which may be helpful for many downstream tasks.\nThis dataset is designed to facilitate research and development in the field of large mutimodal language model, particularly for tasks such as image captioning, visual question answering, and multimodal understanding.\nNote that our framework can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/image-textualization.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"PIN-100M","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-a-p/PIN-100M","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","description":"\n\t\n\t\t\n\t\tPIN-100M\n\t\n\nThe full version of the dataset, related to the paper \"PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents\"\nPaper: https://arxiv.org/abs/2406.13923\nThis dataset contains 100M samples with PIN format.\nPlease note that the required storage space exceeds 150TB!!\nüöÄ News\n[ 2024.12.20 ] !NEW! üî•The currently available version is not the complete version; this project is still ongoing! (It has been released early because we reached the private‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/PIN-100M.","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"MUMA-TOM-BENCHMARK","keyword":"multi-modal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SCAI-JHU/MUMA-TOM-BENCHMARK","creator_name":"Social Cognitive AI Lab at JHU","creator_url":"https://huggingface.co/SCAI-JHU","description":"\n\t\n\t\t\n\t\tMuMA-ToM: Multi-modal Multi-Agent Theory of Mind   AAAI 2025 (Oral) \n\t\n\n[üè†Homepage] [üíªCode] [üìùPaper]\nMuMA-ToM is the first multi-modal Theory of Mind benchmark designed to evaluate mental reasoning in embodied multi-agent interactions. The benchmark was designed with several key features in mind:\n\nIt is factually correct, concise, and readable.\nIt requires integrating information from multiple modalities to answer the questions.\nIt tests understanding of multi-agent interactions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SCAI-JHU/MUMA-TOM-BENCHMARK.","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","Text"],"keywords_longer_than_N":true},
	{"name":"blip3-grounding-50m","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/blip3-grounding-50m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n\t\n\t\t\n\t\tBLIP3-GROUNDING-50M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-GROUNDING-50M dataset is designed to enhance the ability of Vision-Language Models (VLMs) to ground semantic concepts in visual features, which is crucial for tasks like object detection, semantic segmentation, and understanding referring expressions (e.g., \"the object to the left of the dog\"). Traditional datasets often lack the necessary granularity for such tasks, making it challenging for models to accurately localize and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-grounding-50m.","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"MultiBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/juliusbroomfield/MultiBench","creator_name":"Julius Broomfield","creator_url":"https://huggingface.co/juliusbroomfield","description":"\n\t\n\t\t\n\t\tMultiBench: Safety Evaluation Benchmark for Vision-Language Models\n\t\n\nLarge language models have been extensively studied for their vulnerabilities, particularly in the context of adversarial attacks. \nHowever, the emergence of Vision Language Models introduces new modalities of risk that have not yet been thoroughly explored, \nespecially when processing multiple images simultaneously. To address this, we present a new safety evaluation dataset for multimodal LLMs called MultiBench‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/juliusbroomfield/MultiBench.","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"pokemon-gpt4o-captions","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llamafactory/pokemon-gpt4o-captions","creator_name":"LLaMA Factory","creator_url":"https://huggingface.co/llamafactory","description":"Borrowed from: https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions\nYou can use it in LLaMA Factory by specifying dataset: pokemon_cap.\n","first_N":5,"first_N_keywords":["text-generation","question-answering","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"blip3-ocr-200m","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/blip3-ocr-200m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n\t\n\t\t\n\t\tBLIP3-OCR-200M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-OCR-200M dataset is designed to address the limitations of current Vision-Language Models (VLMs) in processing and interpreting text-rich images, such as documents and charts. Traditional image-text datasets often struggle to capture nuanced textual information, which is crucial for tasks requiring complex text comprehension and reasoning. \n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nOCR Integration: The dataset incorporates Optical Character‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-ocr-200m.","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"MMAT-1M","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VIS-MPU-Agent/MMAT-1M","creator_name":"VIS-MPU-Agent","creator_url":"https://huggingface.co/VIS-MPU-Agent","description":"\n\t\n\t\t\n\t\tMMAT-1M Dataset Card\n\t\n\n\n\t\n\t\t\n\t\tDataset details\n\t\n\n\n\t\n\t\t\n\t\tDataset type\n\t\n\nMMAT-1M is a million-scale multimodal agent tuning dataset, built by consolidating subsets of five publicly available multimodal question-answer datasets: Visual CoT, LLaVA-CoT, The Cauldron, TabMWP, and Infoseek. It integrates dynamically generated API calls and Retrieval Augmented Generation (RAG) information through a GPT-4o-powered multi-turn paradigm, with rationales refined via reflection to ensure logical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VIS-MPU-Agent/MMAT-1M.","first_N":5,"first_N_keywords":["question-answering","apache-2.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"enriched-movie-dataset-with-multimodal-embeddings","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ujwal-jibhkate/enriched-movie-dataset-with-multimodal-embeddings","creator_name":"Ujwal Jibhkate","creator_url":"https://huggingface.co/ujwal-jibhkate","description":"\n\t\n\t\t\n\t\tEnriched Movie Dataset with Multimodal Embeddings\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset provides rich metadata for over 44,000 movies, with a primary focus on providing a pre-computed, high-quality multimodal content embedding for each film.\nIt was created by fusing two popular Kaggle datasets: \"The Movies Dataset\" and the \"IMDB Multimodal Vision & NLP Genre Classification\" dataset. It has been further enriched with parsed text features and a unique 512-dimensional vector‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ujwal-jibhkate/enriched-movie-dataset-with-multimodal-embeddings.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_1_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_1_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_1_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_1_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_2_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_2_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_2_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_2_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_3_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_3_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_3_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_3_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_2_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_2_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_2_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_2_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_4_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_4_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_4_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_4_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_7_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_7_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_7_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_7_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_9_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_9_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_9_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_9_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_16_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_16_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_16_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_16_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_2_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_2_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_2_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_2_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_7_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_7_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_7_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_7_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_11_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_11_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_11_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_11_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_16_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_16_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_16_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_16_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Prompt2SceneBench","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench","creator_name":"Bodhisatta Maiti","creator_url":"https://huggingface.co/bodhisattamaiti","description":"\n\t\n\t\t\n\t\tDataset Card for Prompt2SceneBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPrompt2SceneBench is a structured prompt dataset with 12,606 text descriptions designed for evaluating text-to-image models in realistic indoor environments. \nEach prompt describes the spatial arrangement of 1‚Äì4 common household objects on compatible surfaces and in contextually appropriate scenes, sampled using strict object‚Äìsurface‚Äìscene compatibility mappings.\n\nCurated by: Bodhisatta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench.","first_N":5,"first_N_keywords":["text-to-image","question-answering","zero-shot-classification","image-to-text","English"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_CalligraphyFont_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyFont_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_CalligraphyFont_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_CalligraphyFont_en\", split=\"train\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyFont_en.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_CalligraphyFont_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyFont_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_CalligraphyFont_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_CalligraphyFont_ar\", split=\"train\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyFont_ar.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_1_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_1_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_1_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_1_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_2_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_2_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_2_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_2_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_4_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_4_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_4_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_4_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_3_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_3_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_3_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_3_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_2_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_2_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_2_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_2_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_3_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_3_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_3_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_3_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_5_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_5_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_5_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_5_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_7_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_7_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_7_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_7_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_5_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_5_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_5_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_5_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_14_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_14_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_14_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_14_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_4_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_4_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_4_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_4_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_5_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_5_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_5_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_5_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_6_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_6_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_6_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_6_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_8_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_8_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_8_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_8_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_12_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_12_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_12_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_12_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_14_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_14_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_14_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_14_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_15_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_15_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_15_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_15_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"pdf-rag-embed-bench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/gpahal/pdf-rag-embed-bench","creator_name":"Garvit Pahal","creator_url":"https://huggingface.co/gpahal","description":"This is a benchmark dataset for PDF RAG embedding systems.\nSee gpahal/pdf-rag-embed-bench for more details.\n","first_N":5,"first_N_keywords":["question-answering","mit","üá∫üá∏ Region: US","multimodal","multilingual"],"keywords_longer_than_N":true},
	{"name":"marqo-GS-10M","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Marqo/marqo-GS-10M","creator_name":"Marqo","creator_url":"https://huggingface.co/Marqo","description":"\n  \n    \n  \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tMarqo-GS-10M\n\t\n\nThis dataset is our multimodal, fine-grained, ranking Google Shopping dataset, Marqo-GS-10M, followed by our novel training framework: Generalized Contrastive Learning (GCL). GCL aims to improve and measure the ranking performance of information retrieval models, \nespecially for retrieving relevant products given a search query.\nBlog post:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Marqo/marqo-GS-10M.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"YGD-mix","keyword":"multimodal","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/alibabasglab/YGD-mix","creator_name":"Alibaba_Speech_Lab_SG","creator_url":"https://huggingface.co/alibabasglab","description":"A modified version of the Youtube Gesture Dataset. Original data can be downloaded and processed here and here. \nThis dataset is used for Audio-visual speaker extraction conditioned on body gestures in the SEG paper, which the code can be found here. \n","first_N":5,"first_N_keywords":["bsd-3-clause","< 1K","text","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"multi-modal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","description":"\n\n\nüöÄ Project Page | üìú arXiv | üë®‚Äçüíª Github | üèÜ Leaderboard | üìù blog | ü§ó HF Paper | ùïè Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations‚Äô flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-HQ-311K","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tHumanCaption-HQ-311K\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing Agreement\nHumanCaption-HQ-311K: Approximately 311,000 human-related images and their corresponding natural language descriptions.\nCompared to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Eval_Real","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"A newer version of this dataset is available.\nhttps://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real_v1.1\n\n\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"Geoperception","keyword":"multi-modal-qa","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/euclid-multimodal/Geoperception","creator_name":"Euclid Multimodal LLM","creator_url":"https://huggingface.co/euclid-multimodal","description":"Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions\n\n\t\n\t\t\n\t\tDataset Card for Geoperception\n\t\n\nA Benchmark for Low-level Geometric Perception\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nGeoperception is a benchmark focused specifically on accessing model's low-level visual perception ability in 2D geometry.\nIt is sourced from the Geometry-3K corpus, which offers precise logical forms for geometric diagrams, compiled from popular high-school‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/euclid-multimodal/Geoperception.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Ar-MUSA","keyword":"multimodal","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Skhaled/Ar-MUSA","creator_name":"Salma Khaled Ali","creator_url":"https://huggingface.co/Skhaled","description":"\n\t\n\t\t\n\t\tData Directory Structure\n\t\n\nThe Ar-MUSA directory contains annotated datasets organized by batches and annotation teams. Each batch is labeled with a number, and the annotation team is indicated by a letter. The structure is as follows:\nAr-MUSA\n‚îú‚îÄ‚îÄ Annotation 1a\n‚îÇ   ‚îú‚îÄ‚îÄ frames        # Contains the extracted frames for each record\n‚îÇ   ‚îú‚îÄ‚îÄ audios        # Contains the corresponding audio files\n‚îÇ   ‚îú‚îÄ‚îÄ transcripts   # Contains the transcripts of the audio files\n‚îÇ   ‚îî‚îÄ‚îÄ annotations.csv  #‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Skhaled/Ar-MUSA.","first_N":5,"first_N_keywords":["text-classification","audio-classification","image-classification","Arabic","afl-3.0"],"keywords_longer_than_N":true},
	{"name":"IN-Scientific","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-a-p/IN-Scientific","creator_name":"Multimodal Art Projection","creator_url":"https://huggingface.co/m-a-p","description":"\n\t\n\t\t\n\t\tüì• IN-Scientific\n\t\n\nIN-Scientific: An Open Multimodal Interleaved Dataset for Scientific Knowledge Representation\nThis project is a subproject of the üìåPIN project, focusing on the development of the largest scientific document multimodal dataset, which integrates both text and images.\nüìë: https://arxiv.org/abs/2406.13923\nü§ó: https://huggingface.co/datasets/m-a-p/PIN-14M\n\n\t\n\t\t\n\t\n\t\n\t\tDataset statistics\n\t\n\n\n\t\n\t\t\nSource\nContent Images (#)\nContent Images (Size GB)\nDocuments (#)\nDocuments‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/IN-Scientific.","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"RoboMatrix","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WayneMao/RoboMatrix","creator_name":"WayneMao","creator_url":"https://huggingface.co/WayneMao","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis is the fine-tuning dataset used in the paper RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World.\n\n\t\n\t\t\n\t\tSource\n\t\n\nProject Page: https://robo-matrix.github.io/Paper: https://arxiv.org/abs/2412.00171Code: https://github.com/WayneMao/RoboMatrixModel: \n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you find our work helpful, please cite us:\n@article{mao2024robomatrix,\n  title={RoboMatrix: A Skill-centric Hierarchical Framework for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WayneMao/RoboMatrix.","first_N":5,"first_N_keywords":["mit","1K - 10K","Datasets","Croissant","arxiv:2412.00171"],"keywords_longer_than_N":true},
	{"name":"llava-critic-113k","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lmms-lab/llava-critic-113k","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","description":"\n\t\n\t\t\n\t\tDataset Card for LLaVA-Critic-113k\n\t\n\n\nü™ê Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic/\nüì∞ Paper: https://arxiv.org/abs/2410.02712\nü§ó Huggingface Collection: https://huggingface.co/collections/lmms-lab/llava-critic-66fe3ef8c6e586d8435b4af8\nüëã Point of Contact: Tianyi Xiong\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nLLaVA-Critic-113k is a high quality critic instruction-following dataset tailored to follow instructions in complex evaluation setting, providing both‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/llava-critic-113k.","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"PVIT-3M","keyword":"multi-modal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sterzhang/PVIT-3M","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","description":"\n\t\n\t\t\n\t\tPVIT-3M\n\t\n\nThe paper titled \"Personalized Visual Instruction Tuning\" introduces a novel dataset called PVIT-3M. This dataset is specifically designed for tuning MLLMs in the context of personalized visual instruction tasks. The dataset consists of 3 million image-text pairs that aim to improve MLLMs' abilities to generate responses based on personalized visual inputs, making them more tailored and adaptable to individual user needs and preferences.\nHere‚Äôs the PVIT-3M statistics:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/PVIT-3M.","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"ChineseBQB","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/ChineseBQB","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tChinese BQB\n\t\n\nThis is a data reupload of the repository zhaoolee/ChineseBQB, containing 5k+ Chinese stickers\n‰∏≠ÊñáË°®ÊÉÖÂåÖÊï∞ÊçÆÔºåÊù•Ëá™‰∫ézhaoolee/ChineseBQB\n\n","first_N":5,"first_N_keywords":["Chinese","apache-2.0","1K - 10K","arrow","Image"],"keywords_longer_than_N":true},
	{"name":"ColonINST-v1","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai4colonoscopy/ColonINST-v1","creator_name":"ai4colonoscopy","creator_url":"https://huggingface.co/ai4colonoscopy","description":"\n\t\n\t\t\n\t\tColonINST-v1 Data Card\n\t\n\nA large-scale mutlimodal instruction tuning dataset for colonoscopy research. More details refer to our project page: https://github.com/ai4colonoscopy/ColonGPT.\n\n\n\t\n\t\t\n\t\n\t\n\t\tData description\n\t\n\nWe introduce a pioneering instruction tuning dataset for multimodal colonoscopy research, aimed at instructing models to execute user-driven tasks interactively. This dataset comprises of 62 categories, 300K+ colonoscopic images, 128K+ medical captions (GPT-4V)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4colonoscopy/ColonINST-v1.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"NL-Eye","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MorVentura/NL-Eye","creator_name":"Mor Ventura","creator_url":"https://huggingface.co/MorVentura","description":"\n\t\n\t\t\n\t\tNL-Eye Benchmark\n\t\n\nWill a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? \nRecent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. \nNL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MorVentura/NL-Eye.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-10M","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tHumanCaption-10M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing the Agreement\nHumanCaption-10M: a large, diverse, high-quality dataset of human-related images with natural language descriptions (image to text).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"BioTrove-Train","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BGLab/BioTrove-Train","creator_name":"Baskar Group","creator_url":"https://huggingface.co/BGLab","description":"\n\t\n\t\t\n\t\tBioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity\n\t\n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nSee the BioTrove dataset card on HuggingFace to access the main BioTrove dataset (161.9M)\nBioTrove comprises well-processed metadata with full taxa information and URLs pointing to image files. The metadata can be used to filter specific categories, visualize data distribution, and manage imbalance effectively. We provide a collection of software tools‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BGLab/BioTrove-Train.","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","English","mit","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"BioTrove","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BGLab/BioTrove","creator_name":"Baskar Group","creator_url":"https://huggingface.co/BGLab","description":"\n\t\n\t\t\n\t\n\t\n\t\tBioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity\n\t\n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nSee the BioTrove-Train dataset card on HuggingFace to access the samller BioTrove-Train dataset (40M)\nBioTrove comprises well-processed metadata with full taxa information and URLs pointing to image files. The metadata can be used to filter specific categories, visualize data distribution, and manage imbalance effectively. We provide a collection‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BGLab/BioTrove.","first_N":5,"first_N_keywords":["image-classification","zero-shot-classification","English","mit","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"BHM-Bengali-Hateful-Memes","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Eftekhar/BHM-Bengali-Hateful-Memes","creator_name":"Eftekhar Hossain","creator_url":"https://huggingface.co/Eftekhar","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBHM is a novel multimodal dataset for Bengali Hateful Memes detection. The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, \ntailored for two tasks: (i) detecting hateful memes and (ii) detecting the social entities they target (i.e., Individual, Organization, Community, and Society).\n\n\t\n\t\t\n\t\tPaper Information\n\t\n\n\nPaper: https://aclanthology.org/2024.acl-long.454/\nCode:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Eftekhar/BHM-Bengali-Hateful-Memes.","first_N":5,"first_N_keywords":["other","image-classification","image-to-text","Bengali","mit"],"keywords_longer_than_N":true},
	{"name":"LiveXiv","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LiveXiv/LiveXiv","creator_name":"LiveXiv","creator_url":"https://huggingface.co/LiveXiv","description":"LiveXiv - an evolving multi-modal dataset based on ArXiv (ICLR 2025)\nhttps://arxiv.org/abs/2410.10783\n\n\t\n\t\t\n\t\tLiveXiv Leaderboard\n\t\n\n\n\t\n\t\t\nModel\nV0 VQA\nV0 TQA\nV1 VQA\nV1 TQA\nV2 VQA\nV2 TQA\nV3 VQA\nV3 TQA\nV4 VQA\nV4 TQA\n\n\n\t\t\nClaude-Sonnet\n0.75\n0.81\n0.75\n0.84\n0.78\n0.82\n0.78\n0.84\n0.80\n0.78\n\n\nQwen2-VL-7B\n0.67\n0.58\n0.67\n0.60\n0.68\n0.58\n0.69\n0.67\n0.71\n0.53\n\n\nPixtral\n-\n-\n-\n-\n0.73\n0.59\n0.71\n0.39\n0.73\n0.55\n\n\nInternVL2-8B\n0.62\n0.62\n0.62\n0.62\n0.64\n0.60\n0.65\n0.69\n0.67\n0.57\nGPT4o\n0.51\n0.48\n0.60\n0.55\n0.59\n0.49‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LiveXiv/LiveXiv.","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","table-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"polymath","keyword":"multi-modal-qa","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/him1411/polymath","creator_name":"Himanshu Gupta","creator_url":"https://huggingface.co/him1411","description":"\n\t\n\t\t\n\t\tPaper Information\n\t\n\nWe present PolyMATH, a challenging benchmark aimed at evaluating the general cognitive reasoning abilities of MLLMs. \nPolyMATH comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning. \nWe conducted a comprehensive, and quantitative evaluation of 15 MLLMs using four diverse prompting strategies, including Chain-of-Thought‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/him1411/polymath.","first_N":5,"first_N_keywords":["multiple-choice","expert-generated","found","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"rdt-ft-data","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data","creator_name":"Robotics Diffusion Transformer Collaboration","creator_url":"https://huggingface.co/robotics-diffusion-transformer","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card\n\t\n\nThis is the fine-tuning dataset used in the paper RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation.\n\n\t\n\t\t\n\t\n\t\n\t\tSource\n\t\n\n\nProject Page: https://rdt-robotics.github.io/rdt-robotics/\nPaper: https://arxiv.org/pdf/2410.07864\nCode: https://github.com/thu-ml/RoboticsDiffusionTransformer\nModel: https://huggingface.co/robotics-diffusion-transformer/rdt-1b\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nDownload all archive files and use the following command to extract:\ncat‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data.","first_N":5,"first_N_keywords":["mit","arxiv:2410.07864","üá∫üá∏ Region: US","robotics","multimodal"],"keywords_longer_than_N":true},
	{"name":"MMToM-QA","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Chuanyang-Jin/MMToM-QA","creator_name":"Chuanyang Jin","creator_url":"https://huggingface.co/Chuanyang-Jin","description":"\n\t\n\t\t\n\t\tMMToM-QA: Multimodal Theory of Mind Question Answering  üèÜ Outstanding Paper Award at ACL 2024\n\t\n\n[üè†Homepage] [üíªCode] [üìùPaper]\nMMToM-QA is the first multimodal benchmark to evaluate machine Theory of Mind (ToM), the ability to understand people's minds.\nIt systematically evaluates Theory of Mind both on multimodal data and different unimodal data. \nMMToM-QA consists of 600 questions. \nThe questions are categorized into seven types, evaluating belief inference and goal inference in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Chuanyang-Jin/MMToM-QA.","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"StoryFrames","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/StoryFrames","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tThe StoryFrames Dataset\n\t\n\nStoryFrames is a human-annotated dataset created to enhance a model's capability of understanding and reasoning over sequences of images.\nIt is specifically designed for tasks like generating a description for the next scene in a story based on previous visual and textual information.\nThe dataset repurposes the StoryBench dataset, a video dataset originally designed to predict future frames of a video.\nStoryFrames subsamples frames from those videos and pairs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/StoryFrames.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"AstroM3Processed","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AstroMLCore/AstroM3Processed","creator_name":"AstroMLCore","creator_url":"https://huggingface.co/AstroMLCore","description":"\n\t\n\t\t\n\t\tAstroM3Processed\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nAstroM3Processed is a time-series astronomy dataset containing photometry, spectra, and metadata features for variable stars. \nThe dataset was constructed by cross-matching publicly available astronomical datasets, \nprimarily from the ASAS-SN (Shappee et al. 2014) variable star catalog (Jayasinghe et al. 2019) \nand LAMOST spectroscopic survey (Cui et al. 2012), along with data from \nWISE (Wright et al. 2010), GALEX (Morrissey et al. 2007)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AstroMLCore/AstroM3Processed.","first_N":5,"first_N_keywords":["mit","100K - 1M","parquet","Time-series","Datasets"],"keywords_longer_than_N":true},
	{"name":"SMMILE-plusplus","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/smmile/SMMILE-plusplus","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE-plusplus.","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"mmrl","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/csfufu/mmrl","creator_name":"Shawn","creator_url":"https://huggingface.co/csfufu","description":"\n\t\n\t\t\n\t\tüåü ReVisual-R1 (7B) ‚Äî Open-Source Multimodal Reasoner\n\t\n\n\nOne cold-start, two RL stages, endless reasoning power.\n\n\n\n\t\n\t\t\n\t\tüîë Highlights\n\t\n\n\nSOTA on 9 tough benchmarks covering visual‚Äìmath + text reasoning.\n\nThree-Stage SRO Training\n\nText Cold-Start ‚Äî seed deep reflection\nMultimodal RL ‚Äî align vision & logic\nText RL ‚Äî polish fluency & brevity\n\n\nPAD (Prioritized Advantage Distillation) keeps gradients alive.\n\nEfficient-Length Reward = concise, self-reflective CoT.\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüìö‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/csfufu/mmrl.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"MASH","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YRC10/MASH","creator_name":"Ruichen Yao","creator_url":"https://huggingface.co/YRC10","description":"We present a Multiplatform Annotated Dataset for Societal Impact of Hurricane (MASH) that includes 98,662 relevant social media data posts from Reddit, X, TikTok, and YouTube.  In addition, all relevant posts are annotated on three dimensions: Humanitarian Classes, Bias Classes, and Information Integrity Classes in a multi-modal approach that considers both textual and visual content, providing a rich labeled dataset for in-depth analysis. The dataset is also complemented by an Online Analytics Platform that not only allows users to view hurricane-related posts and articles, but also explores high-frequency keywords, user sentiment, and the locations where posts were made. To our best knowledge, MASH is the first large-scale, multi-platform, multimodal, and multi-dimensionally annotated hurricane dataset.  We envision that MASH can contribute to the study of hurricanes' impact on society, such as disaster severity classification, event detections, public sentiment analysis, and bias identification.\n","first_N":5,"first_N_keywords":["text-classification","image-classification","video-classification","expert-annotated","LLM"],"keywords_longer_than_N":true},
	{"name":"textrl","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/csfufu/textrl","creator_name":"Shawn","creator_url":"https://huggingface.co/csfufu","description":"\n\t\n\t\t\n\t\tüåü ReVisual-R1 (7B) ‚Äî Open-Source Multimodal Reasoner\n\t\n\n\nOne cold-start, two RL stages, endless reasoning power.\n\n\n\n\t\n\t\t\n\t\tüîë Highlights\n\t\n\n\nSOTA on 9 tough benchmarks covering visual‚Äìmath + text reasoning.\n\nThree-Stage SRO Training\n\nText Cold-Start ‚Äî seed deep reflection\nMultimodal RL ‚Äî align vision & logic\nText RL ‚Äî polish fluency & brevity\n\n\nPAD (Prioritized Advantage Distillation) keeps gradients alive.\n\nEfficient-Length Reward = concise, self-reflective CoT.\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüìö‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/csfufu/textrl.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MolLangBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ChemFM/MolLangBench","creator_name":"ChemFM","creator_url":"https://huggingface.co/ChemFM","description":"\n\t\n\t\t\n\t\tMolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation\n\t\n\n\nThe MolLangBench paper is available on arXiv:2505.15054.\nThe code for using and evaluating the MolLangBench datasets is provided in this GitHub repository.\n\n\n\n\n\nMolLangBench is a comprehensive benchmark designed to evaluate the fundamental capabilities of AI models in language-prompted molecular structure recognition, editing, and generation.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChemFM/MolLangBench.","first_N":5,"first_N_keywords":["question-answering","text-to-image","image-to-text","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"hippovlog-dataset","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/linyueqian/hippovlog-dataset","creator_name":"Yueqian Lin","creator_url":"https://huggingface.co/linyueqian","description":"\n\t\n\t\t\n\t\tDataset Card for HippoVlog\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHippoVlog is a novel benchmark dataset designed for evaluating Multimodal Memory and Reasoning (MMR) systems. It consists of 25 long-form daily vlogs (682 minutes total) with naturalistic audiovisual content and 1,000 validated multiple-choice question-answer pairs.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset supports the following tasks:\n\nMultimodal Memory and Reasoning (MMR): The primary task involves answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/linyueqian/hippovlog-dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","visual-question-answering","multiple-choice-qa","English"],"keywords_longer_than_N":true},
	{"name":"VisualWebInstruct-Recall","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Recall","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is the dataset recalled from Google Search from the seed images.\n\n\t\n\t\t\n\t\tLinks\n\t\n\nGithub|\nPaper|\nWebsite\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{visualwebinstruct,\n    title={VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},\n    author = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},\n    journal={arXiv preprint arXiv:2503.10582},\n    year={2025}\n}\n\n","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"FrenchBee_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/FrenchBee_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tFRENCHBEE-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from FrenchBee technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice at TW3 Partners, a company specialized in Generative AI. Passionate about artificial intelligence‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/FrenchBee_dataset.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"SPATIAL-v1.0","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Youlln/SPATIAL-v1.0","creator_name":"Lalain Youri","creator_url":"https://huggingface.co/Youlln","description":"\n\t\n\t\t\n\t\tüöÄ Aerospace Knowledge Dataset (VLM)\n\t\n\n\n\t\n\t\t\n\t\tüìå Overview\n\t\n\nThe Aerospace Knowledge Dataset is a large-scale, multi-modal dataset designed for training Vision-Language Models (VLMs) in the aerospace domain. It is built from over 26,000 pages of technical documents, research papers, engineering reports, and mission data from leading space organizations such as NASA, ArianeGroup, SpaceX, ESA, and others.  \nThis dataset is structured in a query + image format, allowing AI models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Youlln/SPATIAL-v1.0.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"CathayPacific_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/CathayPacific_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tCATHAY-PACIFIC-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical queries generated from Cathay Pacific technical documents. It is designed to train and evaluate information retrieval models and improve AI understanding of aerospace technical documentation, with a specific focus on international airline operations in the Asia-Pacific region.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an apprentice at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/CathayPacific_dataset.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"QatarAirways_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/QatarAirways_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tAMERICAN-EXPRESS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical and financial queries generated from American Express annual reports. It is designed to train and evaluate information retrieval models and improve AI understanding of financial documentation, with a specific focus on the credit card industry, payment processing, and banking services.\n\n\t\n\t\t\n\t\tAbout Me\n\t\n\nI'm David Soeiro-Vuong, a third-year Computer Science student working as an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/QatarAirways_dataset.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"NIH-CXR14-BiomedCLIP-Features","keyword":"multi-modal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Yasintuncer/NIH-CXR14-BiomedCLIP-Features","creator_name":"Tun√ßer","creator_url":"https://huggingface.co/Yasintuncer","description":"\n\t\n\t\t\n\t\tNIH-CXR14-BiomedCLIP-Features Dataset\n\t\n\nThis dataset is derived from the NIH Chest X-ray Dataset (NIH-CXR14) and processed using the BiomedCLIP-PubMedBERT_256-vit_base_patch16_224 model from Microsoft. It contains image and text features extracted from chest X-ray images and their corresponding textual findings.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe original NIH-CXR14 dataset comprises 112,120 chest X-ray images with disease labels from 30,805 unique patients. This processed dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Yasintuncer/NIH-CXR14-BiomedCLIP-Features.","first_N":5,"first_N_keywords":["image-classification","text-retrieval","text-classification","image-feature-extraction","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"multimodal_low-resource_language_translation","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qianstats/multimodal_low-resource_language_translation","creator_name":"Qian Shen","creator_url":"https://huggingface.co/qianstats","description":"\n\t\n\t\t\n\t\tDataset Card for Multimodal Low-Resource Language Translation Dataset\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is the dataset for our paper \"From Text to Multi-Modal: Advancing Low-Resource-Language Translation through Synthetic Data Generation and Cross-Modal Alignments\" accepted by the workshop LoResMT 2025 of NAACL 2025\n\nShared by [optional]: Bushi‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qianstats/multimodal_low-resource_language_translation.","first_N":5,"first_N_keywords":["translation","English","Yoruba","Tigrinya","Hausa"],"keywords_longer_than_N":true},
	{"name":"IDKVQA","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ftaioli/IDKVQA","creator_name":"Francesco Taioli","creator_url":"https://huggingface.co/ftaioli","description":"\n\t\n\t\t\n\t\tI Don't Know Visual Question Answering - IDKVQA dataset - ICCV 25\n\t\n\n\n\nWe introduce IDKVQA, an embodied dataset specifically designed and annotated for visual question answering using the agent‚Äôs observations during navigation,\nwhere the answer includes not only Yes and No, but also I don‚Äôt know.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nPlease see our ICCV 25 accepted paper: Collaborative Instance Object Navigation: Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues\nFor more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ftaioli/IDKVQA.","first_N":5,"first_N_keywords":["question-answering","zero-shot-classification","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CHOICE","keyword":"multi-modal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/An-Xiao/CHOICE","creator_name":"AnXiao","creator_url":"https://huggingface.co/An-Xiao","description":"\n\t\n\t\t\n\t\tCHOICE: Benchmarking The Remote Sensing Capabilities of Large Vision-Language Models\n\t\n\n Abstract: The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/An-Xiao/CHOICE.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"unidisc_hq","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aswerdlow/unidisc_hq","creator_name":"Alexander Swerdlow","creator_url":"https://huggingface.co/aswerdlow","description":"This repository contains the dataset used in the paper Unified Multimodal Discrete Diffusion.\nCode: https://github.com/AlexSwerdlow/unidisc\nAdditionally, we release a synthetic dataset available here and the corresponding generation scripts as well as the raw data.\n","first_N":5,"first_N_keywords":["image-text-to-text","mit","10M - 100M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"open-pmc","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vector-institute/open-pmc","creator_name":"Vector Institute","creator_url":"https://huggingface.co/vector-institute","description":"\n\t\n\t\t\n\t\tOPEN-PMC\n\t\n\n\n    \n\n\n\n  Arxiv: Arxiv \n  ¬†¬†¬†¬†|¬†¬†¬†¬†\n Code: Open-PMC Github\n  ¬†¬†¬†¬†|¬†¬†¬†¬†\n Model Checkpoint: Hugging Face\n \n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of image-text pairs extracted from medical papers available on PubMed Central. It has been curated to support research in medical image understanding, particularly in natural language processing (NLP) and computer vision tasks related to medical imagery. The dataset includes:\n\nExtracted images from research articles.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vector-institute/open-pmc.","first_N":5,"first_N_keywords":["English","mit","1M - 10M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"GeoGrid_Bench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bowen-upenn/GeoGrid_Bench","creator_name":"Lauren Jiang","creator_url":"https://huggingface.co/bowen-upenn","description":"\n\t\n\t\t\n\t\tGeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?\n\t\n\n\n\nWe present GeoGrid-Bench, a benchmark designed to evaluate the ability of foundation models to understand geo-spatial data in the grid structure. Geo-spatial datasets pose distinct challenges due to their dense numerical values, strong spatial and temporal dependencies, and unique multimodal representations including tabular data, heatmaps, and geographic visualizations. To assess how foundation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bowen-upenn/GeoGrid_Bench.","first_N":5,"first_N_keywords":["table-question-answering","visual-question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"open-pmc-18m","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vector-institute/open-pmc-18m","creator_name":"Vector Institute","creator_url":"https://huggingface.co/vector-institute","description":"\n\t\n\t\t\n\t\tOPEN-PMC\n\t\n\n\n    \n\n\n  Arxiv: Arxiv \n  ¬†¬†¬†¬†|¬†¬†¬†¬†\n Code: Open-PMC Github\n  ¬†¬†¬†¬†|¬†¬†¬†¬†\n Model Checkpoint: Hugging Face\n \n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of image-text pairs extracted from medical papers available on PubMed Central. It has been curated to support research in medical image understanding, particularly in natural language processing (NLP) and computer vision tasks related to medical imagery. The dataset includes:\n\nExtracted images from research articles.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vector-institute/open-pmc-18m.","first_N":5,"first_N_keywords":["English","mit","10M - 100M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Eval_Synthetic","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Synthetic","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\" (COLM 2025).\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Synthetic.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Train","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Train","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Train.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"Electrohydrodynamics","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Taylor658/Electrohydrodynamics","creator_name":"atayloraerospace","creator_url":"https://huggingface.co/Taylor658","description":"\n\t\n\t\t\n\t\tElectrohydrodynamics in Hall Effect Thrusters Dataset for Mistral-Large-Instruct-2411 Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of 6,000 high fidelity training instances tailored for fine-tuning the Mistral-Large-Instruct-2411 foundation model. It captures theoretical, computational, and experimental aspects of electrohydrodynamics in Hall Effect Thrusters. \n\n\t\n\t\t\n\t\tKey Features:\n\t\n\n\nMultimodal elements: Includes LaTeX equations, code snippets, textual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Taylor658/Electrohydrodynamics.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"geotechnie","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MatteoKhan/geotechnie","creator_name":"Khan Matteo","creator_url":"https://huggingface.co/MatteoKhan","description":"\n\t\n\t\t\n\t\tAbout Me\n\t\n\nAbout Me\nI'm Matteo Khan, a computer science apprentice at TW3 Partners, specializing in Generative AI and NLP. My focus is on creating datasets that improve AI's ability to process complex technical documents.\nYou can connect with me on LinkedIn: Matteo Khan\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tPurpose\n\t\n\nThis dataset is designed to fine-tune models for expertise in geotechnical engineering by generating structured queries from soil mechanics and construction-related‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MatteoKhan/geotechnie.","first_N":5,"first_N_keywords":["English","French","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MIS_Test","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Tuwhy/MIS_Test","creator_name":"Yi Ding","creator_url":"https://huggingface.co/Tuwhy","description":"\n\t\n\t\t\n\t\tRethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models\n\t\n\n\nOur paper, code, data, models can be found at MIS.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nOur MIS test set contains three split \"MIS-easy\", \"MIS-hard\", \"MIS-real\".\n{\n  \"question\": \"str\",\n  \"category\": \"str\",\n  \"sub_category\": \"str\",\n  \"image_path1\": \"str\",\n  \"image_path2\": \"str\",\n  \"id\": int\n}\n\n\n\t\n\t\t\n\t\tStatistics\n\t\n\nOur 'MIS-easy' and 'MIS-hard' datasets together contain 2,185 samples across 6 categories and 12‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tuwhy/MIS_Test.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K<n<10K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"MaCBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jablonkagroup/MaCBench","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","description":"\n\t\n\t\t\n\t\tMaCBench\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\t‚ö†Ô∏è IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tüö´ THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY üö´\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluation results. Please‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","image-to-text","visual-question-answering","language-modeling"],"keywords_longer_than_N":true},
	{"name":"medmax_data","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mint-medmax/medmax_data","creator_name":"mint-medmax","creator_url":"https://huggingface.co/mint-medmax","description":"\n\t\n\t\t\n\t\tMedMax Dataset\n\t\n\n\n\t\n\t\t\n\t\tMixed-Modal Instruction Tuning for Training Biomedical Assistants\n\t\n\nAuthors: Hritik Bansal, Daniel Israel‚Ä†, Siyan Zhao‚Ä†, Shufan Li, Tung Nguyen, Aditya GroverInstitution: University of California, Los Angeles‚Ä† Equal Contribution\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nLarge Language Models (LLMs) and Large Multimodal Models (LMMs) have demonstrated remarkable capabilities in multimodal information integration, opening transformative possibilities‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mint-medmax/medmax_data.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"MM-IQ","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huanqia/MM-IQ","creator_name":"huanqiacai","creator_url":"https://huggingface.co/huanqia","description":"\n\t\n\t\t\n\t\tDataset Card for \"MM-IQ\"\n\t\n\n\nIntroduction\nPaper Information\nDataset Examples\nLeaderboard\nDataset Usage\nData Downloading\nData Format\nAutomatic Evaluation\n\n\nCitation\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nIQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huanqia/MM-IQ.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"AniGamePersonaCaps","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/AniGamePersonaCaps","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tAniGamePersonaCap\n\t\n\nThis multimodal dataset curates a collection of 633,565 beloved anime, manga and game characters from 3,860 Fandom wiki sites, organized across the following components:\n\nImage Modality\n\nVisuals of character figures.\n\n\nText Modality\n\nFandom Wiki Metadata: Meta information about characters from HTML contents.  \nCaptions:  \nVLM-Generated: Descriptions of visual appearance and inferred personality generated by Vision-Language Models (e.g., Qwen-VL-72B-Instruct).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/AniGamePersonaCaps.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","100K - 1M","Image","Text"],"keywords_longer_than_N":true},
	{"name":"LayoutSAM","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","description":"\n\t\n\t\t\n\t\tLayoutSAM Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe LayoutSAM dataset is a large-scale layout dataset derived from the SAM dataset, containing 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a spatial position (i.e., bounding box) and a textual description.\nTraditional layout datasets often exhibit a closed-set and coarse-grained nature, which may limit the model's ability to generate complex attributes such as color, shape, and texture.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tKey‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/LayoutSAM.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"UnLOK-VQA","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vaidehi99/UnLOK-VQA","creator_name":"Vaidehi Patil","creator_url":"https://huggingface.co/vaidehi99","description":"\n\t\n\t\t\n\t\tüìä Dataset: UnLOK-VQA (Unlearning Outside Knowledge VQA)\n\t\n\nPaper: Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation\nCode: https://github.com/Vaidehi99/mmmedit\nLink: Dataset Link\nThis dataset contains approximately 500 entries with the following key attributes:\n\n\"id\": Unique Identifier for each entry\n\"src\": The question whose answer is to be deleted ‚ùì\n\"pred\": The answer to the question meant for deletion ‚ùå\n\"loc\": Related neighborhood questions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vaidehi99/UnLOK-VQA.","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MedXpertQA","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA","creator_name":"TsinghuaC3I","creator_url":"https://huggingface.co/TsinghuaC3I","description":"\n\t\n\t\t\n\t\tDataset Card for MedXpertQA\n\t\n\n\n\nMedXpertQA is a highly challenging and comprehensive benchmark designed to evaluate expert-level medical knowledge and advanced reasoning capabilities. It features both text-based and multimodal question-answering tasks, with the multimodal subset leveraging structured clinical information alongside images.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMedXpertQA comprises 4,460 questions spanning diverse medical specialties, tasks, body systems, and image types. It‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA.","first_N":5,"first_N_keywords":["table-question-answering","question-answering","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"XTD-10","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Haon-Chen/XTD-10","creator_name":"Haonan Chen","creator_url":"https://huggingface.co/Haon-Chen","description":"\n\t\n\t\t\n\t\tXTD Multimodal Multilingual Data With Instruction\n\t\n\nThis dataset contains datasets (with English instruction) used for evaluating the multilingual capability of a multimodal embedding model, including seven languages:\n\nit, es, ru, zh, pl, tr, ko\n\n\n\t\n\t\t\n\t\tDataset Usage\n\t\n\n\nThe instruction on the query side is: \"Retrieve an image of this caption.\"\nThe instruction on the document side is: \"Represent the given image.\"\nEach example contains a query and a set of targets. The first one in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Haon-Chen/XTD-10.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"mmE5-MMEB-hardneg","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","description":"\n\t\n\t\t\n\t\tmmE5 Labeled Data\n\t\n\nThis dataset contains datasets used for the supervised finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\n\nMMEB (with hard negative)\nInfoSeek (from M-BEIR)\nTAT-DQA\nArxivQA\n\nGithub\n\n\t\n\t\t\n\t\n\t\n\t\tImage Preparation\n\t\n\nFirst, you should prepare the images used for training:\n\n\t\n\t\t\n\t\n\t\n\t\tImage Downloads\n\t\n\n\nDownload All Images Used in mmE5:\n\nYou can use the script provided in our source code to download all images used‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg.","first_N":5,"first_N_keywords":["English","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"mmE5-synthetic","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/intfloat/mmE5-synthetic","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","description":"\n\t\n\t\t\n\t\tmmE5 Synthetic Data\n\t\n\nThis dataset contains synthetic datasets used for the finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\n\nClassification\nRetrieval\nVQA\n\nGithub\n\n\t\n\t\t\n\t\n\t\n\t\tImage Preparation\n\t\n\nFirst, you should prepare the images used for training:\n\n\t\n\t\t\n\t\n\t\n\t\tImage Downloads\n\t\n\n\nDownload mmE5-synthetic Images:\n\nRun the following command to download and extract the images only in this dataset.\nmkdir -p images && cd images\nwget‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-synthetic.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"genshin-impact-outfits","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/genshin-impact-outfits","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tGenshin Impact Outfit\n\t\n\nThis is a collection of Genshin Impact character outfits (both wish and in-game version), with outfit description and detailed appearance, parsed from Fandom Wiki.\n","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"LayoutSAM-eval","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","description":"\n\t\n\t\t\n\t\tLayoutSAM-eval Benchmark\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nLayoutSAM-Eval is a comprehensive benchmark for evaluating the quality of Layout-to-Image (L2I) generation models. This benchmark assesses L2I generation quality from two perspectives: region-wise quality (spatial and attribute accuracy) and global-wise quality (visual quality and prompt following). It employs the VLM‚Äôs visual question answering to evaluate spatial and attribute adherence, and utilizes various metrics including IR score‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"VMCBench","keyword":"multi-modal-qa","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/suyc21/VMCBench","creator_name":"Yuchang Su","creator_url":"https://huggingface.co/suyc21","description":"\n\t\n\t\t\n\t\tVMCBench (Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation)\n\t\n\nüåê Homepage | ü§ó Dataset | üìñ arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Introduction\n\t\n\nWe introduce VMCBench: a benchmark that unifies 20 existing visual question answering (VQA) datasets into a consistent multiple-choice format. VMCBench spans a diverse array of visual and linguistic contexts, rigorously testing various model capabilities. By‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/suyc21/VMCBench.","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"Iranian_olympiad_of_informatics_multimodal_questions","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ckodser/Iranian_olympiad_of_informatics_multimodal_questions","creator_name":"Arshia Soltani Moakhar","creator_url":"https://huggingface.co/ckodser","description":"ckodser/Iranian_olympiad_of_informatics_multimodal_questions dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"VisNumBench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GML-FMGroup/VisNumBench","creator_name":"Foundation Model Group at Guangming Laboratory","creator_url":"https://huggingface.co/GML-FMGroup","description":"This dataset is designed for research in Deep Learning for Geometry Problem Solving (DL4GPS) and accompanies the survey paper A Survey of Deep Learning for Geometry Problem Solving. It aims to provide a structured resource for evaluating and training AI models, particularly multimodal large language models (MLLMs), on mathematical reasoning tasks involving geometric contexts.\nThe dataset provides a collection of geometry problems, each consisting of a textual question and a corresponding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GML-FMGroup/VisNumBench.","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"banque_vision","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MatteoKhan/banque_vision","creator_name":"Khan Matteo","creator_url":"https://huggingface.co/MatteoKhan","description":"\n\t\n\t\t\n\t\tüìä Banque_Vision: A Multimodal Dataset for Document Understanding\n\t\n\n\n\t\n\t\t\n\t\tüìå Overview\n\t\n\nBanque_Vision is a multimodal dataset designed for document-based question answering (QA) and information retrieval. It combines textual data and visual document representations, enabling research on how vision models and language models interact for document comprehension.\nüîó Created by: Matteo Khanüéì Affiliation: TW3Partners \nüìç License: MIT  \nüîó Connect with me on LinkedInüîó Dataset on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MatteoKhan/banque_vision.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"mmE5-Synthetic","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/backup-mmE5/mmE5-Synthetic","creator_name":"backup","creator_url":"https://huggingface.co/backup-mmE5","description":"\n\t\n\t\t\n\t\tmmE5 Synthetic Data\n\t\n\n","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Eval_Real_v1.1","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real_v1.1","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\" (COLM 2025).\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real_v1.1.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"SpaceThinker","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/remyxai/SpaceThinker","creator_name":"Remyx AI","creator_url":"https://huggingface.co/remyxai","description":"\n\t\n\t\t\n\t\tSpaceThinker Dataset\n\t\n\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\nTry training a LLaVA-style VLM using the SpaceThinker Dataset\n\n\t\n\t\t\n\t\tEnhanced Quantitative Spatial Reasoning with Test-Time Compute\n\t\n\nThe SpaceThinker dataset is created using VQASynth to synthesize spatial reasoning traces from a subset of images \nin the localized narratives split of the cauldron.\n\n\t\n\t\t\n\t\tData Samples\n\t\n\n\n\t\n\t\t\n\n\n\n\n\n\t\t\nPrompt: How far is the man in the red hat from the pallet of boxes in feet?\nPrompt: How far is the Goal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/remyxai/SpaceThinker.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"VisQuant","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Anas-Mohiuddin-Syed/VisQuant","creator_name":"Syed Anas Mohiuddin","creator_url":"https://huggingface.co/Anas-Mohiuddin-Syed","description":"\nlicense: cc-by-4.0\ndatasets:\n\nvisquant\nlanguage:\nen\ntags:\nvisual-question-answering\nobject-counting\nspatial-reasoning\nsynthetic\nmultimodal\nbenchmark\n\n\n\t\n\t\t\n\t\tVisQuant: A Synthetic Benchmark for Object Counting and Spatial Reasoning\n\t\n\nVisQuant is a synthetic dataset of 100 annotated image scenarios, purpose-built to evaluate AI systems on object counting, spatial layout understanding, and visual question answering (VQA).\nThis dataset is ideal for benchmarking vision-language models (e.g.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Anas-Mohiuddin-Syed/VisQuant.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Emirates_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Davidsv/Emirates_dataset","creator_name":"David Soeiro-Vuong","creator_url":"https://huggingface.co/Davidsv","description":"\n\t\n\t\t\n\t\tEMIRATES-AIRWAYS-TECHNICAL-QUERY-DATASET\n\t\n\nThis dataset contains a structured collection of technical, financial, and sustainability queries generated from Emirates Airways annual and sustainability reports. It is designed to train and evaluate information retrieval models and improve AI understanding of aviation industry documentation, with a specific focus on airline operations, sustainability initiatives, and international business strategies.\n\n\t\n\t\t\n\t\n\t\n\t\tAbout Me\n\t\n\nI'm David‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Davidsv/Emirates_dataset.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"multimodal-genshin-impact","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/multimodal-genshin-impact","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tGenshin Impact Fandom Wiki Multimodal Dataset\n\t\n\nGithub repo here\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset is a comprehensive collection of 22,162 fandom wiki pages for the popular game Genshin Impact.\nThe dataset includes markdown-formatted English content from the wiki, featuring interleaved text, as well as image, video, and audio file links. Additionally, the associated multimodal files (images, videos, and audio) have been downloaded and organized to facilitate the multimodal dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/multimodal-genshin-impact.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","10K<n<100K","Audio","Image"],"keywords_longer_than_N":true},
	{"name":"SmallMinesDS","keyword":"multi-modal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ellaampy/SmallMinesDS","creator_name":"SOA","creator_url":"https://huggingface.co/ellaampy","description":"\n\t\n\t\t\n\t\tSmallMinesDS\n\t\n\nThe gradual expansion of unregularized artisanal small-scale gold mining (ASGM) fuels environmental degradation and poses risk to miners and mining communities. To enforce sustainable mining, support reclamation initiatives and pave the way for understudying the impacts of mining, we present SmallMinesDS, a benchmark dataset for mapping artisanal small-scale gold mining from multi-sensor satellite images. The initial version of the dataset covers five districts in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ellaampy/SmallMinesDS.","first_N":5,"first_N_keywords":["cc-by-sa-4.0","1K - 10K","csv","Image","Text"],"keywords_longer_than_N":true},
	{"name":"FRABench","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SPUH/FRABench","creator_name":"HONG SHIBO","creator_url":"https://huggingface.co/SPUH","description":"\n\t\n\t\t\n\t\tDataset Card for FRABench\n\t\n\n\nü™ê Project Page: https://github.com/ALEX-nlp/FRABench-and-GenEval\nüì∞ Paper: https://arxiv.org/abs/2505.12795\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nFRABench is a large-scale fine-grained pairwise evaluation datasets across four tasks- Natural Language Generation(NLG), Image Understanding(IU), Image Generation(IG), and Interleaved Text-and-Image Generation(ITIG)- comprising 28 sub-tasks and 60.4 pairwise samples with 325k evaluation labels, which is based on our‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SPUH/FRABench.","first_N":5,"first_N_keywords":["English","apache-2.0","arxiv:2505.12795","üá∫üá∏ Region: US","multimodal"],"keywords_longer_than_N":false},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"FinMME","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/luojunyu/FinMME","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","description":"Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, there is a notable lack of effective and specialized multimodal evaluation datasets in the financial domain. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/FinMME.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MoCa-CL-Pairs","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/moca-embed/MoCa-CL-Pairs","creator_name":"MoCa","creator_url":"https://huggingface.co/moca-embed","description":"\n\t\n\t\t\n\t\tMoCa Contrastive Learning Data\n\t\n\nüè† Homepage | üíª Code | ü§ñ MoCa-Qwen25VL-7B | ü§ñ MoCa-Qwen25VL-3B | üìö Datasets | üìÑ Paper\nThis dataset contains datasets used for the supervised finetuning of MoCa (MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings):\n\nMMEB (with hard negative)\nInfoSeek (from M-BEIR)\nTAT-DQA\nArxivQAVisRAG\nViDoRe\nColPali\nE5 text pairs (can not release due to restrictions of Microsoft)\n\n\n\t\n\t\t\n\t\tImage Preparation\n\t\n\nFirst, you‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/moca-embed/MoCa-CL-Pairs.","first_N":5,"first_N_keywords":["English","mit","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"livevqa-benchmark","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fmy666/livevqa-benchmark","creator_name":"fmy666","creator_url":"https://huggingface.co/fmy666","description":"\n\t\n\t\t\n\t\tLiveVQA Benchmark Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nLiveVQA is a comprehensive Visual Question Answering benchmark that evaluates multimodal models across three dynamic domains: News, Academic Papers, and Videos. The dataset features both level1 (basic comprehension) and level2 (advanced reasoning) questions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nid: Unique identifier for each question\nimage: Path to the associated image\nquestion: The question text\noptions: List‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fmy666/livevqa-benchmark.","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"KoLLaVA-v1.5-Instruct-581k","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ko-vlm/KoLLaVA-v1.5-Instruct-581k","creator_name":"korean-vision-language","creator_url":"https://huggingface.co/ko-vlm","description":"\n\t\n\t\t\n\t\tKoLLaVA-v1.5-Instruct-581k\n\t\n\nÌïúÍµ≠Ïñ¥ Vision-Language Î™®Îç∏ÏùÑ ÏúÑÌïú instruction tuning Îç∞Ïù¥ÌÑ∞ÏÖãÏûÖÎãàÎã§.\n\n\t\n\t\t\n\t\tÎç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÎ≥¥\n\t\n\n\nÏ¥ù ÏÉòÌîå Ïàò: 435,093Í∞ú\nÌòïÏãù: ChatML ÌòïÏãù (role: user/assistant, content: ÌÖçÏä§Ìä∏)\nÏù¥ÎØ∏ÏßÄ: COCO + GQA + Visual Genome Îç∞Ïù¥ÌÑ∞ÏÖã\nÏñ∏Ïñ¥: ÌïúÍµ≠Ïñ¥\n\n\n\t\n\t\t\n\t\tÌè¨Ìï®Îêú Îç∞Ïù¥ÌÑ∞ÏÖã\n\t\n\n\nCOCO Îç∞Ïù¥ÌÑ∞: 362,953Í∞ú ÏÉòÌîå\n\nMS COCO 2017 Ïù¥ÎØ∏ÏßÄ Í∏∞Î∞ò\nÌïúÍµ≠Ïñ¥ ÎåÄÌôî Îç∞Ïù¥ÌÑ∞\n\n\nGQA Îç∞Ïù¥ÌÑ∞: 72,140Í∞ú ÏÉòÌîå\n\nGQA (Visual Question Answering) Ïù¥ÎØ∏ÏßÄ Í∏∞Î∞ò\nÌïúÍµ≠Ïñ¥ ÎåÄÌôî Îç∞Ïù¥ÌÑ∞\n\n\nVisual Genome Îç∞Ïù¥ÌÑ∞: Ìè¨Ìï®\n\nVisual Genome Ïù¥ÎØ∏ÏßÄ Í∏∞Î∞ò\nÌïúÍµ≠Ïñ¥ ÎåÄÌôî Îç∞Ïù¥ÌÑ∞\n\n\n\n\n\t\n\t\t\n\t\tÏ†úÏô∏Îêú Îç∞Ïù¥ÌÑ∞ÏÖã\n\t\n\n\nEKVQA Îç∞Ïù¥ÌÑ∞: AI Hub ÎùºÏù¥ÏÑ†Ïä§Î°ú Ïù∏Ìï¥ Í≥µÍ∞ú Î∂àÍ∞Ä‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ko-vlm/KoLLaVA-v1.5-Instruct-581k.","first_N":5,"first_N_keywords":["Korean","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"coco-fastvlm-2k-val2017","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/riddhimanrana/coco-fastvlm-2k-val2017","creator_name":"Riddhiman Rana","creator_url":"https://huggingface.co/riddhimanrana","description":"\n\t\n\t\t\n\t\tDataset Card for COCO FastVLM 2K Val2017 Structured Captions\n\t\n\nThis dataset contains 2,000 high-quality image-text pairs generated from the COCO 2017 validation set using a FastVLM-based vision-language model with structured prompt engineering and automated distillation. Each caption follows a structured 7-point format to describe the visual content in detail, enabling high-fidelity fine-tuning of multimodal models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/riddhimanrana/coco-fastvlm-2k-val2017.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"KC-MMbench","keyword":"multimodal","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kwai-Keye/KC-MMbench","creator_name":"Kwai-Keye","creator_url":"https://huggingface.co/Kwai-Keye","description":"  [üçé Home Page] [üìñ Technical Report] [\\ud83d\\udcca Models] [\\ud83d\\ude80 Demo] \nThis repository contains KC-MMBench, a new benchmark dataset meticulously tailored for real-world short-video scenarios, as presented in the paper \"Kwai Keye-VL Technical Report\". Constructed from Kuaishou short video data, KC-MMBench comprises 6 distinct datasets designed to evaluate the performance of Vision-Language Models (VLMs) like Kwai Keye-VL-8B, Qwen2.5-VL, and InternVL in comprehending dynamic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kwai-Keye/KC-MMbench.","first_N":5,"first_N_keywords":["video-text-to-text","Chinese","English","cc-by-sa-4.0","arxiv:2507.01949"],"keywords_longer_than_N":true},
	{"name":"MSLoRA_CR","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/VentusAislant/MSLoRA_CR","creator_name":"VentusAislant","creator_url":"https://huggingface.co/VentusAislant","description":"\n\t\n\t\t\n\t\tüß¨ Contrastive Regularization with LoRA for Multimodal Biomedical Image Incremental Learning\n\t\n\nLast updated: Jun 26th, 2025Maintainer: @VentusAislantPaper: MSLoRA-CR\n\n\n\t\n\t\t\n\t\tüì¶ Dataset Overview\n\t\n\nThis dataset supports MSLoRA-CR: Contrastive Regularization with LoRA for Multimodal Biomedical Image Incremental Learning.It includes curated annotations (train.json, test.jsonl) for multiple biomedical imaging modalities. The dataset is intended to facilitate incremental learning and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VentusAislant/MSLoRA_CR.","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"OGC_Geotechnie_Compatible_Negatives","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/noebrndl/OGC_Geotechnie_Compatible_Negatives","creator_name":"No√© BRANDOLINI","creator_url":"https://huggingface.co/noebrndl","description":"\n\t\n\t\t\n\t\tOGC_Geotechnie_Corrected\n\t\n\nCorrected version of racineai/OGC_Geotechnie with proper Image() types and 16 negative image columns for ColPali compatibility.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nid: Unique identifier\nquery: Text query about the document  \nlanguage: Language of the query\nimage: Main document image (corrected Image() type)\nnegative_image_0 to negative_image_15: Negative image columns\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/noebrndl/OGC_Geotechnie_Compatible_Negatives.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"OGC_Energy_Compatible_Negatives","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/noebrndl/OGC_Energy_Compatible_Negatives","creator_name":"No√© BRANDOLINI","creator_url":"https://huggingface.co/noebrndl","description":"\n\t\n\t\t\n\t\tOGC_Energy_Corrected\n\t\n\nCorrected version of racineai/OGC_Energy with proper Image() types and 16 negative image columns for ColPali compatibility.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nid: Unique identifier\nquery: Text query about the document  \nlanguage: Language of the query\nimage: Main document image (corrected Image() type)\nnegative_image_0 to negative_image_15: Negative image columns\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"Matchone7/OGC_Energy_Corrected\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/noebrndl/OGC_Energy_Compatible_Negatives.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"french-lot-department-captioned-photos","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NoeFlandre/french-lot-department-captioned-photos","creator_name":"No√© Flandre","creator_url":"https://huggingface.co/NoeFlandre","description":"\n\t\n\t\t\n\t\tLot Department, France Image Dataset\n\t\n\nA collection of high-resolution scenic photographs from the Lot region of France with AI-generated descriptive captions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains scenic photographs from three notable locations in France's Lot department: Rocamadour, Autoire, and Padirac. All images were captured using a Sony A6600 camera and are paired with detailed English captions generated by Mistral AI's Pixtral-Large model.\nKey Features:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/french-lot-department-captioned-photos.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"MathVision_with_difficulty_level","keyword":"multi-modal-qa","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/JierunChen/MathVision_with_difficulty_level","creator_name":"Jierun Chen","creator_url":"https://huggingface.co/JierunChen","description":"\n\t\n\t\t\n\t\tMathVision with difficulty level tags\n\t\n\nThis dataset extends the ü§ó MathVision  benchmark by introducing two additional tags: passrate_for_qwen2.5_vl_7b and difficulty_level_for_qwen2.5_vl_7b. Further details are available in our paper The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"JierunChen/MathVision_with_difficulty_level\")\nprint(dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JierunChen/MathVision_with_difficulty_level.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","expert-generated"],"keywords_longer_than_N":true},
	{"name":"albi-captioned-photos","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NoeFlandre/albi-captioned-photos","creator_name":"No√© Flandre","creator_url":"https://huggingface.co/NoeFlandre","description":"\n\t\n\t\t\n\t\tAlbi, France Image Dataset\n\t\n\nA collection of high-resolution scenic photographs from Albi, France with AI-generated descriptive captions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains scenic photographs from Albi, France, including the city center, the Toulouse Lautrec museum, and the Sainte-C√©cile Cathedral. All images were captured using a Sony A6600 camera and are paired with detailed English captions generated by Mistral AI's Pixtral-Large model.\nKey Features:\n\nHigh-resolution‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/albi-captioned-photos.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_CalligraphyOCR_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyOCR_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_CalligraphyOCR_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_CalligraphyOCR_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_CalligraphyOCR_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_CalligraphyOCR_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_CalligraphyOCR_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_CalligraphyOCR_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Countxy_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Countxy_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Countxy_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Countxy_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Countxy_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Countxy_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Countxy_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Countxy_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_4_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_4_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_4_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_4_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_country_3_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_country_3_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_country_3_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_country_3_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_1_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_1_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_1_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_1_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_4_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_4_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_4_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_4_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_5_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_5_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_5_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_5_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_6_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_6_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_6_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_6_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_7_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_7_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_7_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_7_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_8_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_8_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_8_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_8_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_1_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_1_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_1_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_1_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_4_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_4_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_4_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_4_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_6_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_6_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_6_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_6_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OryxTrain_fanarvisknow_8_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/OryxTrain_fanarvisknow_8_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/OryxTrain_fanarvisknow_8_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/OryxTrain_fanarvisknow_8_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_1_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_1_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_1_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_1_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_2_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_2_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_2_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_2_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_3_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_3_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_3_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_3_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_6_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_6_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_6_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_6_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_8_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_8_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_8_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_8_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_10_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_10_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_10_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_10_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_11_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_11_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_11_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_11_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_12_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_12_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_12_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_12_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_13_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_13_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_13_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_13_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_15_ar","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_15_ar","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_15_ar\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_15_ar\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_1_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_1_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_1_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_1_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_3_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_3_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_3_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_3_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_9_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_9_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_9_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_9_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_10_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_10_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_10_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_10_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PixmoTrain_Caption_13_en","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/PixmoTrain_Caption_13_en","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"\n\t\n\t\t\n\t\tllm-lab/PixmoTrain_Caption_13_en\n\t\n\nThis dataset is part of the OryxTrain collection.\nStored in efficient .parquet format for large-scale instruction-tuning.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach record includes:\n\nmessages: user/assistant dialogue with embedded image prompt\nimages: list of image paths associated with the example\n\nTotal examples: 15352556\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"llm-lab/llm-lab/PixmoTrain_Caption_13_en\", split=\"train\")\nprint(ds[0])\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Resume-Analysis-CoTR","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","description":"\n\t\n\t\t\n\t\tResume Reasoning and Feedback Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains approximately 417 examples designed to facilitate research and development in automated resume analysis and feedback generation. Each data point consists of a user query regarding their resume, a simulated internal analysis (chain-of-thought) performed by an expert persona, and a final, user-facing feedback response derived solely from that analysis.\nThe dataset captures a two-step reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR.","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_eval_analysis_2","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_2","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_2.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_eval_analysis_5","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_5","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_5.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_eval_analysis_6","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_6","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_6.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"OWMM-Agent-data","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hhyrhy/OWMM-Agent-data","creator_name":"HHY RHY","creator_url":"https://huggingface.co/hhyrhy","description":"\n\t\n\t\t\n\t\tOWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis\n\t\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tüöÄ Introduction\n\t\n\nThe rapid progress of navigation, manipulation, and vision models has made mobile manipulators capable in many specialized tasks. \nHowever, the open-world mobile manipulation (OWMM) task remains a challenge due to the need for generalization to open-ended instructions and environments, as well as the systematic complexity to integrate high-level decision making with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hhyrhy/OWMM-Agent-data.","first_N":5,"first_N_keywords":["English","mit","10B<n<100B","arxiv:2506.04217","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"ToxiMol-benchmark","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DeepYoke/ToxiMol-benchmark","creator_name":"DeepYoke","creator_url":"https://huggingface.co/DeepYoke","description":"\n\t\n\t\t\n\t\tToxiMol: A Benchmark for Structure-Level Molecular Detoxification\n\t\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nToxiMol is the first comprehensive benchmark for molecular toxicity repair tailored to general-purpose Multimodal Large Language Models (MLLMs). This is the dataset repository for the paper \"Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?\".\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n\t\n\t\t\n\t\tüß¨ Comprehensive Dataset\n\t\n\n\n560 representative toxic molecules spanning diverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DeepYoke/ToxiMol-benchmark.","first_N":5,"first_N_keywords":["tabular-classification","tabular-regression","multi-class-classification","tabular-single-column-regression","monolingual"],"keywords_longer_than_N":true},
	{"name":"VS-Bench","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zelaix/VS-Bench","creator_name":"Zelai Xu","creator_url":"https://huggingface.co/zelaix","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVS-Bench is a multimodal benchmark for evaluating VLMs in multi-agent environments. We evaluate fourteen state-of-the-art models in eight vision-grounded environments with two complementary dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return.\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@article{xu2025vs,\n  title={VS-Bench: Evaluating VLMs for Strategic Reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zelaix/VS-Bench.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_eval_analysis_3","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_3","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_3.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_eval_analysis_4","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_4","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_4.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"R1-Reward-RL","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","description":"\n  \n\n\n[üìñ arXiv Paper] \n[üìä R1-Reward Code] \n[üìù R1-Reward Model] \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tTraining Multimodal Reward Model Through Stable Reinforcement Learning\n\t\n\nüî• We are proud to open-source R1-Reward, a comprehensive project for improve reward modeling through reinforcement learning. This release includes:\n\nR1-Reward Model: A state-of-the-art (SOTA) multimodal reward model demonstrating substantial gains (Voting@15):\n13.5% improvement on VL Reward-Bench.3.5% improvement on MM-RLHF Reward-Bench.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"vpi-bench","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VPI-Bench/vpi-bench","creator_name":"VPI Bench","creator_url":"https://huggingface.co/VPI-Bench","description":"\n\t\n\t\t\n\t\tDataset Card for VPI-Bench\n\t\n\n\n\n\nVPI-Bench is a benchmark dataset of testcases and web platforms used to evaluate the robustness of computer-use and browser-use agents under visual prompt injection attacks.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\nLanguage(s) (NLP): English\nLicense: Creative Commons Attribution 4.0\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\n\n\nRepository: VPI-Bench\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\n\n\t\n\t\t\n\t\tDirect Use\n\t\n\n\n\n\nBenchmarking the Attempted Rate (AR) and Success Rate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VPI-Bench/vpi-bench.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"nesteo-prototype","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nesteo-datasets/nesteo-prototype","creator_name":"NestEO Datasets","creator_url":"https://huggingface.co/nesteo-datasets","description":"\n\t\n\t\t\n\t\tNestEO: Modular and Hierarchical EO Dataset Framework\n\t\n\nNestEO is a hierarchical, resolution-aligned, UTM-based nested grid dataset framework supporting general-purpose, multi-scale multimodal Earth Observation workflows. Built from diverse EO sources and enriched with metadata for landcover, climate zones, and population, it enables scalable, representative and progressive sampling for AI4EO.\nGrid Levels: 120000m, 12000m, 2400m, 1200m, 600m, 300m, 150mGrid Metadata: ESA WorldCover‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nesteo-datasets/nesteo-prototype.","first_N":5,"first_N_keywords":["image-segmentation","image-classification","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"NOAH-mini","keyword":"multi-modal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mutakabbirCarleton/NOAH-mini","creator_name":"Mutakabbir","creator_url":"https://huggingface.co/mutakabbirCarleton","description":"\n\t\n\t\t\n\t\tMOAH mini\n\t\n\nThe dataset prest here is a very samll sample of NOAH dataset.\nIn the original dataset each satellite image is ~650MB with 234,089 images present in 11 bands.\nIt is not feasible to upload the complete dataset. \nA sample of the dataset across diffrent modalities can be seen in the figure below:\n\nThe diffrence between NOAH and NOAH mini is hilighted in the figure below.\nEach subplot is a band of Landsat 8 in NOAH.\nThe region hilighted in red is the region available in NOAH‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mutakabbirCarleton/NOAH-mini.","first_N":5,"first_N_keywords":["image-to-image","mit","< 1K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_length_angle","keyword":"multi-modal-qa","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_length_angle","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_length_angle.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"KokushiMD-10","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/humanalysis-square/KokushiMD-10","creator_name":"Tako AI","creator_url":"https://huggingface.co/humanalysis-square","description":"\n\t\n\t\t\n\t\tKokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nKokushiMD-10 is the first comprehensive multimodal benchmark constructed from ten Japanese national healthcare licensing examinations. This dataset addresses critical gaps in existing medical AI evaluation by providing a linguistically grounded, multimodal, and multi-profession assessment framework for large language models (LLMs) in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/humanalysis-square/KokushiMD-10.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Japanese","English","mit"],"keywords_longer_than_N":true},
	{"name":"android_control_train","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/InfiX-ai/android_control_train","creator_name":"InfiX.ai","creator_url":"https://huggingface.co/InfiX-ai","description":"\n\t\n\t\t\n\t\tProcessed Android Control Training Set\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis repository contains the processed training set derived from the Android Control dataset by Google Research.\nThe data processing methodology is identical to that used for our corresponding test set, which can be found at Reallm-Labs/android_control_test.\n\n\t\n\t\t\n\t\n\t\n\t\tData Content and Image Extraction\n\t\n\nImportant Note: Due to the large size of the dataset, this repository contains only the processed text files.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/InfiX-ai/android_control_train.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"ScienceOlympiad","keyword":"multimodal","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ByteDance-Seed/ScienceOlympiad","creator_name":"ByteDance Seed","creator_url":"https://huggingface.co/ByteDance-Seed","description":"\n\t\n\t\t\n\t\tScienceOlympiad: Challenging AI with Olympiad-Level Multimodal Science Problems\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe ScienceOlympiad dataset is a meticulously curated benchmark designed to test the limits of current AI models in scientific reasoning. It comprises elite, competition-level problems in physics and chemistry. Addressing the need for more diverse and realistic challenges, ScienceOlympiad introduces multimodal integration as a key dimension. Unlike purely text-based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ByteDance-Seed/ScienceOlympiad.","first_N":5,"first_N_keywords":["cc0-1.0","< 1K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"PhysUniBench","keyword":"multimodal","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrismaX/PhysUniBench","creator_name":"PrismaX","creator_url":"https://huggingface.co/PrismaX","description":"\n\t\n\t\t\n\t\tPhysUniBench\n\t\n\nAn Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models\nPaper: https://arxiv.org/abs/2506.17667\nRepository: https://github.com/PrismaX-Team/PhysUniBenchmark\nProject page: https://prismax-team.github.io/PhysUniBenchmark/\nPhysUniBench is the first large-scale multimodal physics benchmark specifically designed for undergraduate-level understanding, reasoning, and problem-solving. It provides a valuable testbed for advancing multimodal large language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrismaX/PhysUniBench.","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Mobile-R1","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PG23/Mobile-R1","creator_name":"Qihang Ai","creator_url":"https://huggingface.co/PG23","description":"\n\t\n\t\t\n\t\tDataset Card for Mobile-R1\n\t\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nimages/: All the screenshots\ndata.jsonl: The trajectory data\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nAll screenshots are stored in the images/ directory.\nWe describe the structure of a single trajectory entry from the file data.jsonl, which contains the full interaction trajectories and action history.\n\napp_name: String. The name of the mobile application (e.g., \"Èó≤È±º\" / Xianyu) where the task is performed.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PG23/Mobile-R1.","first_N":5,"first_N_keywords":["Chinese","apache-2.0","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"EEE-Bench","keyword":"multi-modal-qa","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/afdsafas/EEE-Bench","creator_name":"Ming Li","creator_url":"https://huggingface.co/afdsafas","description":"\n\t\n\t\t\n\t\tEEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark\n\t\n\n\n\t\n\t\t\n\t\tIntroduction:\n\t\n\nEEE-Bench is a multimodal benchmark designed to evaluate the practical engineering capabilities of large multimodal models (LMMs), using electrical and electronics engineering (EEE) as the domain focus. It comprises 2,860 carefully curated problems across 10 core subdomains, including analog circuits and control systems, featuring complex visual inputs such as abstract‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/afdsafas/EEE-Bench.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"MindCube","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MLL-Lab/MindCube","creator_name":"MLL Lab","creator_url":"https://huggingface.co/MLL-Lab","description":"\n\t\n\t\t\n\t\tMindCube: Spatial Mental Modeling from Limited Views\n\t\n\nMindCube is a novel benchmark designed to evaluate how well Vision Language Models (VLMs) can form robust spatial mental models from limited visual views. It comprises 21,154 questions across 3,268 images, assessing capabilities such as cognitive mapping (representing positions), perspective-taking (orientations), and mental simulation (dynamics for \"what-if\" movements). The dataset aims to expose critical gaps in existing VLMs'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MLL-Lab/MindCube.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"central-florida-native-plants","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/deepearth/central-florida-native-plants","creator_name":"DeepEarth","creator_url":"https://huggingface.co/deepearth","description":"\n\t\n\t\t\n\t\tDeepEarth Central Florida Native Plants Dataset v0.2.0\n\t\n\n\n\t\n\t\t\n\t\tüåø Dataset Summary\n\t\n\nA comprehensive multimodal dataset featuring 33,665 observations of 232 native plant species from Central Florida. This dataset combines citizen science observations with state-of-the-art vision and language embeddings for advancing multimodal self-supervised ecological intelligence research.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nüåç Spatiotemporal Coverage: Complete GPS coordinates and timestamps for all‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/deepearth/central-florida-native-plants.","first_N":5,"first_N_keywords":["image-classification","feature-extraction","zero-shot-classification","English","mit"],"keywords_longer_than_N":true},
	{"name":"japanese-humor-evaluation","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation","creator_name":"Miyamoto Ryoto","creator_url":"https://huggingface.co/iammytoo","description":"\n\t\n\t\t\n\t\tJapanese Multimodal Humor Evaluation Dataset\n\t\n\nThis dataset combines two Japanese humor datasets for evaluating the funniness of responses to prompts (odai).\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset merges:\n\nbokete dataset: Image prompts with text responses\nkeitai dataset: Text prompts with text responses\n\nAll scores are normalized to a 0-4 scale for consistency.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nodai_id: Unique identifier for the prompt\nodai_type: Type of prompt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation.","first_N":5,"first_N_keywords":["Japanese","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"japanese-humor-evaluation-v2","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation-v2","creator_name":"Miyamoto Ryoto","creator_url":"https://huggingface.co/iammytoo","description":"\n\t\n\t\t\n\t\tJapanese Multimodal Humor Evaluation Dataset (v2)\n\t\n\nÁîªÂÉè/„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„ÅäÈ°å„Å´ÂØæ„Åô„ÇãÈù¢ÁôΩ„ÅÑÂõûÁ≠î„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇboketeÔºàÁîªÂÉè‚Üí„ÉÜ„Ç≠„Çπ„ÉàÔºâ„Å®keitaiÔºà„ÉÜ„Ç≠„Çπ„Éà‚Üí„ÉÜ„Ç≠„Çπ„ÉàÔºâ„ÇíÁµ±Âêà„ÄÇ\n\n\t\n\t\t\n\t\t‰Ωø„ÅÑÊñπ\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"iammytoo/japanese-humor-evaluation-v2\")\n\n\n\t\n\t\t\n\t\t„Éá„Éº„ÇøÊßãÈÄ†\n\t\n\n\nodai_type: 'image' or 'text'\nimage: ÁîªÂÉè„ÅäÈ°åÔºàtext„Çø„Ç§„Éó„Åß„ÅØNoneÔºâ\nodai: „ÉÜ„Ç≠„Çπ„Éà„ÅäÈ°åÔºàimage„Çø„Ç§„Éó„Åß„ÅØNoneÔºâ\nresponse: ÂõûÁ≠î„ÉÜ„Ç≠„Çπ„Éà\nscore: 0-4„ÅÆÊ≠£Ë¶èÂåñ„Çπ„Ç≥„Ç¢\n\n\n\t\n\t\t\n\t\t„ÇΩ„Éº„Çπ\n\t\n\n\nYANS-official/ogiri-bokete\nYANS-official/ogiri-keitai\n\n","first_N":5,"first_N_keywords":["text-generation","image-to-text","Japanese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"viet-cultural-vqa","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Dangindev/viet-cultural-vqa","creator_name":"Nguyen Hai Dang","creator_url":"https://huggingface.co/Dangindev","description":"\n\t\n\t\t\n\t\tVietnamese Cultural VQA Dataset\n\t\n\nüáªüá≥ B·ªô d·ªØ li·ªáu VQA VƒÉn h√≥a Vi·ªát Nam - T·∫≠p d·ªØ li·ªáu ƒë·∫ßu ti√™n v·ªÅ VQA ƒëa ph∆∞∆°ng th·ª©c v·ªõi kh·∫£ nƒÉng gi·∫£i th√≠ch vƒÉn h√≥a Vi·ªát Nam.\n\n\t\n\t\t\n\t\tüìä Th·ªëng k√™ Dataset\n\t\n\n\nüìù T·ªïng s·ªë m·∫´u: 28,484\n‚ùì T·ªïng s·ªë c√¢u h·ªèi: 135,645\nüè∑Ô∏è S·ªë danh m·ª•c: 12 danh m·ª•c vƒÉn h√≥a\nüéØ Lo·∫°i VQA: Few-shot learning v·ªõi gi·∫£i th√≠ch vƒÉn h√≥a\nüåç Ng√¥n ng·ªØ: Ti·∫øng Vi·ªát\n\n\n\t\n\t\t\n\t\tüé≠ Danh m·ª•c VƒÉn h√≥a\n\t\n\n\n\t\n\t\t\nDanh m·ª•c\nTi·∫øng Vi·ªát\nM√¥ t·∫£\n\n\n\t\t\nam_thuc\n·∫®m th·ª±c\nM√≥n ƒÉn, ƒë·ªì u·ªëng truy·ªÅn th·ªëng\n\n\ndoi_song_hang_ngay‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dangindev/viet-cultural-vqa.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","Vietnamese","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"Perception-R1-Dataset","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tongxiao2002/Perception-R1-Dataset","creator_name":"tongxiao","creator_url":"https://huggingface.co/tongxiao2002","description":"Paper: arxiv.org/abs/2506.07218\nPlease refer to GitHub repo for detailed usage: https://github.com/tongxiao2002/Perception-R1\n","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","Chinese","English","mit"],"keywords_longer_than_N":true},
	{"name":"TreeVGR-SFT-35K","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","description":"\n\t\n\t\t\n\t\tTreeBench: Traceable Evidence Enhanced Visual Grounded Reasoning Benchmark\n\t\n\nThis repository contains TreeBench, a diagnostic benchmark dataset proposed in the paper Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTreeBench is designed to holistically evaluate \"thinking with images\" capabilities by dynamically referencing visual regions. It is built on three core principles:\n\nFocused visual perception of subtle targets in complex scenes.\nTraceable‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Grammer_dataset","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/csfufu/Grammer_dataset","creator_name":"Shawn","creator_url":"https://huggingface.co/csfufu","description":"\n\t\n\t\t\n\t\tüåü ReVisual-R1 (7B) ‚Äî Open-Source Multimodal Reasoner\n\t\n\n\nOne cold-start, two RL stages, endless reasoning power.\n\n\n\n\t\n\t\t\n\t\tüîë Highlights\n\t\n\n\nSOTA on 9 tough benchmarks covering visual‚Äìmath + text reasoning.\n\nThree-Stage SRO Training\n\nText Cold-Start ‚Äî seed deep reflection\nMultimodal RL ‚Äî align vision & logic\nText RL ‚Äî polish fluency & brevity\n\n\nPAD (Prioritized Advantage Distillation) keeps gradients alive.\n\nEfficient-Length Reward = concise, self-reflective CoT.\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüìö‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/csfufu/Grammer_dataset.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"s2lcd","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/neuronelab/s2lcd","creator_name":"NeuRoNeLab","creator_url":"https://huggingface.co/neuronelab","description":"\n\t\n\t\t\n\t\tSentinel-2 Land-cover Captioning Dataset\n\t\n\nThe Sentinel-2 Land-cover Captioning Dataset (S2LCD) is a newly proposed dataset specifically designed for deep learning research on remote sensing image captioning. It comprises 1533 image patches, each of size 224 √ó 224 pixels, derived from Sentinel-2 L2A images. The dataset ensures a diverse representation of land cover and land use types in temperate regions, including forests, mountains, agricultural lands, and urban areas, each one with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neuronelab/s2lcd.","first_N":5,"first_N_keywords":["zero-shot-classification","image-classification","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MM-MathInstruct-longest-20k-solutions-with-images","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/penfever/MM-MathInstruct-longest-20k-solutions-with-images","creator_name":"Benjamin Feuer","creator_url":"https://huggingface.co/penfever","description":"\n\t\n\t\t\n\t\tMM-MathInstruct Longest 20K Solutions with Images\n\t\n\nThis dataset contains the top 20,000 samples from MathLLMs/MM-MathInstruct selected by solution length, filtered to include only samples with valid images.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tSource\n\t\n\nThis dataset is derived from MathLLMs/MM-MathInstruct by selecting the 20,000 samples with the longest solution text.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nApache 2.0 (inherited from source dataset)\n","first_N":5,"first_N_keywords":["visual-question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"vlaa-thinking-grpo","keyword":"multimodal","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/penfever/vlaa-thinking-grpo","creator_name":"Benjamin Feuer","creator_url":"https://huggingface.co/penfever","description":"\n\t\n\t\t\n\t\tVLAA-Thinking-SFT-126K\n\t\n\nLarge-scale vision-language dataset with 126K instruction-following samples featuring chain-of-thought reasoning\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains vision-language samples with instruction-following conversations. Each sample includes:\n\nimage: PIL Image object\nquestion: Question or instruction text\nanswer or gt: Response with thinking process (SFT dataset) or ground truth answer (GRPO dataset)\ncaption: Image caption (may be empty for some‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/penfever/vlaa-thinking-grpo.","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"ImageNet-Paste","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/helenqu/ImageNet-Paste","creator_name":"Helen Qu","creator_url":"https://huggingface.co/helenqu","description":"\n\t\n\t\t\n\t\tImageNet-Paste\n\t\n\nImageNet-Paste is created by pasting in small images of different concepts into each image from the ImageNet validation dataset to probe the impact of concept pairs on multimodal task accuracy in natural images.\n\n\nEach ImageNet validation image is augmented by pasting in a small image of a different concept (accessory_word), and models are tasked with producing the correct ImageNet classification in the presence of the other concept. In our paper, we provide further‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/helenqu/ImageNet-Paste.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"MusiXQA","keyword":"multimodal","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/puar-playground/MusiXQA","creator_name":"Jian Chen","creator_url":"https://huggingface.co/puar-playground","description":"\n\t\n\t\t\n\t\tMusiXQA üéµ\n\t\n\nMusiXQA is a multimodal dataset for evaluating and training music sheet understanding systems. Each data sample is composed of:\n\nA scanned music sheet image (.png)\nIts corresponding MIDI file (.mid)\nA structured annotation (from metadata.json)\nQuestion‚ÄìAnswer (QA) pairs targeting musical structure, semantics, and optical music recognition (OMR)\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüìÇ Dataset Structure\n\t\n\nMusiXQA/\n‚îú‚îÄ‚îÄ images.tar             # PNG files of music sheets (e.g., 0000000.png)\n‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/puar-playground/MusiXQA.","first_N":5,"first_N_keywords":["English","mit","arxiv:2506.23009","üá∫üá∏ Region: US","music"],"keywords_longer_than_N":true},
	{"name":"MathVista_with_difficulty_level","keyword":"multi-modal-qa","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JierunChen/MathVista_with_difficulty_level","creator_name":"Jierun Chen","creator_url":"https://huggingface.co/JierunChen","description":"\n\t\n\t\t\n\t\tMathVista with difficulty level tags\n\t\n\nThis dataset extends the ü§ó MathVista testmini benchmark by introducing two additional tags: passrate_for_qwen2.5_vl_7b and difficulty_level_for_qwen2.5_vl_7b. Further details are available in our paper  The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"JierunChen/MathVista_with_difficulty_level\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JierunChen/MathVista_with_difficulty_level.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","text-classification","multiple-choice-qa"],"keywords_longer_than_N":true}
]
;
