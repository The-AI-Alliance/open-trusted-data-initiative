const data_for_modality_pretraining = 
[
	{"name":"TemplateGSM","keyword":"pretraining","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/TemplateGSM","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tTraining and Evaluating Language Models with Template-based Data Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tTemplateGSM Dataset\\n\\t\\n\\nThe TemplateGSM dataset is a large-scale collection of over 7 million (with potential for unlimited generation) grade school math problems, each paired with both code-based and natural language solutions.  Designed to advance mathematical reasoning in language models, this dataset presents a diverse range of challenges to assess and improve model capabilities in solving‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/TemplateGSM.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10M - 100M","Tabular"],"keywords_longer_than_N":true},
	{"name":"Bhandara","keyword":"pretrain","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Tensoic/Bhandara","creator_name":"Tensoic AI","creator_url":"https://huggingface.co/Tensoic","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tA Pretraining Hindi Dataset for Diverse Indian NLP Tasks\\n\\t\\n\\nThis dataset contains over 12,000 rows and 7 million words of text specifically generated for pretraining NLP models on Hindi language tasks. It was created using the Bard API, ensuring high-quality and diverse content.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Feature: Rich India-Specific Data\\n\\t\\n\\nA distinguishing characteristic of this dataset is its inclusion of a substantial amount of content related to India. This makes it valuable for training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tensoic/Bhandara.","first_N":5,"first_N_keywords":["text-generation","Hindi","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"llava-pretrain-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLLaVA pretrain -- LCS-558k (refined by Data-Juicer)\\n\\t\\n\\nA refined version of LLaVA pretrain dataset (LCS-558k) by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Multimodal Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 115MB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 500,380 (Keep ~89.65% from the original dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"yoruba-proverbs-parallel-corpora","keyword":"pretrain","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mxronga/yoruba-proverbs-parallel-corpora","creator_name":"Maro jos","creator_url":"https://huggingface.co/mxronga","description":"Parralel corpora for yoruba to english.\\nSource: http://yoruba.unl.edu/yoruba1.html\\n","first_N":5,"first_N_keywords":["Yoruba","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"CCNet","keyword":"pretraining","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/JorgeeGF/CCNet","creator_name":"Jorge Gallego Feliciano","creator_url":"https://huggingface.co/JorgeeGF","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCCNet Reproduced Split (4M rows, 3.7B Tokens (Mistral tokenizer))\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is a reproduced subset of the larger CCNet dataset, tailored specifically to facilitate easier access and processing for researchers needing high-quality, web-crawled text data for natural language processing tasks. The CCNet dataset leverages data from the Common Crawl, a non-profit organization that crawls the web and freely provides its archives to the public. This subset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JorgeeGF/CCNet.","first_N":5,"first_N_keywords":["English","mit","1M - 10M","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Arabic-OpenHermes-2.5","keyword":"distillation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Arabic-OpenHermes-2.5","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Arabic-OpenHermes-2.5\\\"\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources & Infos\\n\\t\\n\\n\\nData Origin: Derived from the original OpenHermes dataset : teknium/OpenHermes-2.5.\\nLanguages: Modern Standard Arabic (MSA)\\nApplications: Language Modeling\\nMaintainer: Marwa El Kamil & Mohammed Machrouh\\nLicense: Apache-2.0\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nArabic-OpenHermes-2.5 is a carefully curated dataset extracted / translated from the OpenHermes-2.5 collection provided by teknium.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPurpose‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Arabic-OpenHermes-2.5.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"wiki-yo","keyword":"pretrain","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mxronga/wiki-yo","creator_name":"Maro jos","creator_url":"https://huggingface.co/mxronga","description":"Wikipedia Yoruba dump 2024\\n","first_N":5,"first_N_keywords":["text-generation","Yoruba","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"nvidia_steer_yo","keyword":"pretrain","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mxronga/nvidia_steer_yo","creator_name":"Maro jos","creator_url":"https://huggingface.co/mxronga","description":"Yoruba translation of the Nvidia steer dataset\\n","first_N":5,"first_N_keywords":["Yoruba","apache-2.0","1K - 10K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"cultura-x-deduped-yoruba","keyword":"pretrain","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mxronga/cultura-x-deduped-yoruba","creator_name":"Maro jos","creator_url":"https://huggingface.co/mxronga","description":"Deduplicated and extended cultura-x yoruba dataset\\n","first_N":5,"first_N_keywords":["Yoruba","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"BioCite","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mkhalifa/BioCite","creator_name":"Muhammad Khalifa","creator_url":"https://huggingface.co/mkhalifa","description":"This is the synthetic dataset used for pretraining in the paper Source-Aware Training Enables Knowledge Attribution in Language Models\\n. \\nStats (number of tokens is computed based on the TinyLLaMa tokenizer):\\n\\n\\t\\n\\t\\t\\n\\nSize\\n\\n\\n\\t\\t\\nPretraining\\n\\n\\n\\n#documents\\n100K\\n\\n\\n#facts/sents\\n408K\\n\\n\\n#tokens\\n5.7M\\n\\n\\navg. sents per doc\\n4.1\\n\\n\\navg. tokens per doc\\n56.9\\n\\n\\nInstruction tuning\\n\\n\\n\\n#examples\\n186K\\n\\n\\n#tokens\\n3.1M\\n\\n\\n\\t\\n\\n","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Document-Translation-en-es","keyword":"distillation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Iker/Document-Translation-en-es","creator_name":"Iker Garc√≠a-Ferrero","creator_url":"https://huggingface.co/Iker","description":"This dataset contains 10533 news articles from ELiRF/dacsa translated from Spanish to English using GPT-3.5-turbo. The dataset is intended to be used for training a model to translate text from English to Spanish and vicerversa. The dataset is also usefull to evaluate document level machine translation models.\\nWe use the following prompt\\n\\ndef get_conversation(text: str, id: int) -> str:\\n    messages = {\\n        \\\"custom_id\\\": str(id),\\n        \\\"method\\\": \\\"POST\\\",\\n        \\\"url\\\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Iker/Document-Translation-en-es.","first_N":5,"first_N_keywords":["translation","English","Spanish","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-Spanish","keyword":"distillation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Iker/OpenHermes-2.5-Spanish","creator_name":"Iker Garc√≠a-Ferrero","creator_url":"https://huggingface.co/Iker","description":"\\nteknium/OpenHermes-2.5 dataset translated to Spanish using the Iker/TowerInstruct-13B-v0.1-EN2ES model. This dataset has a total of 1 Million High-Quality instructions in Spanish!!\\nThe original dataset can be found here: https://hf.co/datasets/teknium/OpenHermes-2.5\\nI have also added the following datasets:\\n\\nIker/Document-Translation-en-es\\nIker/InstructTranslation-EN-ES\\nHelsinki-NLP/opus-100 (en-es, only a few examples to reach 1 million instructions)\\nprojecte-aina/RAG_Multilingual(es only‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Iker/OpenHermes-2.5-Spanish.","first_N":5,"first_N_keywords":["text-generation","Spanish","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"MMLU_SyntheticData","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Ashmal/MMLU_SyntheticData","creator_name":"Ashmal Vayani","creator_url":"https://huggingface.co/Ashmal","description":"Dataset\\nMMLU_SyntheticData is generated using GPT-4. The aim of generating this dataset was to generate a similar dataset to MMLU. \\nNote: Please note that this is not the translated version of MMLU, it's an entirely independent dataset.\\nSubjects\\nIt has the data of the following subjects:\\n\\nHumanities ['Islamic Studies', 'Law', 'History', 'Philosophy']\\nLanguage ['Arabic Language', 'Arabic Language (General)', 'Arabic Language (Grammar)']\\nOther ['General Knowledge', 'Management', 'Driving Test']‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ashmal/MMLU_SyntheticData.","first_N":5,"first_N_keywords":["text-classification","Arabic","apache-2.0","1K<n<10K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"ArabiBoost","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Ashmal/ArabiBoost","creator_name":"Ashmal Vayani","creator_url":"https://huggingface.co/Ashmal","description":"Dataset\\nThe ArabiBoost is the Instruction-tuning or fine-tuning dataset (5.7k samples) for MMLU_SyntheticData is generated using GPT-3.5 Turbo.\\nNote: Please note that this is not the translated version of MMLU, it's an entirely independent dataset.\\nSubjects\\nIt has the data of the following subjects:\\n\\nHumanities ['Islamic Studies', 'Law', 'History', 'Philosophy']\\nLanguage ['Arabic Language', 'Arabic Language (General)', 'Arabic Language (Grammar)']\\nOther ['General Knowledge', 'Management'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ashmal/ArabiBoost.","first_N":5,"first_N_keywords":["text-classification","Arabic","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Copernicus-Pretrain","keyword":"pretrain","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wangyi111/Copernicus-Pretrain","creator_name":"Yi Wang","creator_url":"https://huggingface.co/wangyi111","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Copernicus-Pretrain\\n\\t\\n\\n\\n\\nA large-scale EO pretraining dataset with 18.7M aligned images covering all major Sentinel missions (S1,2,3,5P).\\nOfficially named Copernicus-Pretrain, also referred to as SSL4EO-S (\\\"S\\\" means Sentinel), as an extension of SSL4EO-S12 to the whole Sentinel series.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nCopernicus-Pretrain contains 18.7M aligned imagery from all major Sentinel missions in operation (Sentinel-1 SAR, Sentinel-2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wangyi111/Copernicus-Pretrain.","first_N":5,"first_N_keywords":["cc-by-4.0","10M<n<100M","Geospatial","üá∫üá∏ Region: US","earth-observation"],"keywords_longer_than_N":true},
	{"name":"LIBRA","keyword":"long-context","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ai-forever/LIBRA","creator_name":"ai-forever","creator_url":"https://huggingface.co/ai-forever","description":"\\n\\t\\n\\t\\t\\n\\t\\tLIBRA: Long Input Benchmark for Russian Analysis\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nLIBRA (Long Input Benchmark for Russian Analysis) is designed to evaluate the capabilities of large language models (LLMs) in understanding and processing long texts in Russian. This benchmark includes 21 datasets adapted for different tasks and complexities. The tasks are divided into four complexity groups and allow evaluation across various context lengths ranging from 4k up to 128k tokens.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai-forever/LIBRA.","first_N":5,"first_N_keywords":["Russian","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-zh","keyword":"distillation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ldwang/OpenHermes-2.5-zh","creator_name":"ldwang","creator_url":"https://huggingface.co/ldwang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"OpenHermes-2.5-zh\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources & Infos\\n\\t\\n\\n\\nData Origin: Derived from the original OpenHermes dataset : teknium/OpenHermes-2.5.\\nLanguages: Chinese\\nApplications: Language Modeling\\nLicense: Apache-2.0\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nOpenHermes-2.5-zh is a dataset translated from the OpenHermes-2.5 collection provided by teknium.\\n","first_N":5,"first_N_keywords":["Chinese","apache-2.0","100K<n<1M","üá∫üá∏ Region: US","Synthetic"],"keywords_longer_than_N":true},
	{"name":"TinyDialogues","keyword":"pretraining","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/styfeng/TinyDialogues","creator_name":"Steven Feng","creator_url":"https://huggingface.co/styfeng","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for TinyDialogues\\n\\t\\n\\nTinyDialogues dataset collected as part of the EMNLP 2024 paper \\\"Is Child-Directed Speech Effective Training Data for Language Models?\\\" by Steven Y. Feng, Noah D. Goodman, and Michael C. Frank. For more details, please see Appendices A-C in our paper.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nRepository: https://github.com/styfeng/TinyDialogues\\nPaper: https://aclanthology.org/2024.emnlp-main.1231/\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nFinal training and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/styfeng/TinyDialogues.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Nebo-T1-Russian","keyword":"distillation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kristaller486/Nebo-T1-Russian","creator_name":"Kristaller486","creator_url":"https://huggingface.co/kristaller486","description":"\\n\\t\\n\\t\\t\\n\\t\\tRussian Description (English below)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tUPD: Dataset reuploaded, correct_format column added\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tNebo-T1-Russian\\n\\t\\n\\n(–í–µ—Ä–æ—è—Ç–Ω–æ) –ø–µ—Ä–≤—ã–π \\\"longCoT\\\" –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —á–µ—Ä–µ–∑ Deeseek-R1\\n\\n–ü–æ–¥—Å–∫–∞–∑–∫–∏ –≤–∑—è—Ç—ã –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ Sky-T1 –∏ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã —á–µ—Ä–µ–∑ Llama3.3-70B\\n–û—Ç–≤–µ—Ç—ã –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ Deeseek-R1 (685B)\\n16.4K —Å—ç–º–ø–ª–æ–≤ –≤ —Ü–µ–ª–æ–º, ‚âà12.4K —Ç–æ–ª—å–∫–æ —Å —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º (–≤ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ª–∏–±–æ –æ—Ç–≤–µ—Ç, –ª–∏–±–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)\\n–Ø–∑—ã–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Ä–∞–∑–º–µ—á–µ–Ω—ã‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kristaller486/Nebo-T1-Russian.","first_N":5,"first_N_keywords":["text-generation","Russian","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"next-kmer-prediction","keyword":"long-context","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GenerTeam/next-kmer-prediction","creator_name":"Gener Team","creator_url":"https://huggingface.co/GenerTeam","description":"\\n\\t\\n\\t\\t\\n\\t\\tNext K-mer Prediction\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tAbouts\\n\\t\\n\\nThe Next K-mer Prediction task is a zero-shot evaluation method introduced in the GENERator paper to assess the quality of pretrained models. It involves inputting a sequence segment into the model and having it predict the next K base pairs. The predicted sequence is then compared to the actual sequence to assess accuracy.\\n\\nSequence: The input sequence has a maximum length of 96k base pairs (bp). You can control the number of input tokens by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GenerTeam/next-kmer-prediction.","first_N":5,"first_N_keywords":["text-generation","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"SUnsET","keyword":"long-context","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/dwright37/SUnsET","creator_name":"Dustin Wright","creator_url":"https://huggingface.co/dwright37","description":"\\n\\t\\n\\t\\t\\n\\t\\tSUnsET Dataset\\n\\t\\n\\nThe Summaries with Unstructured Evidence Text (SUnsET) dataset from the paper Unstructured Evidence Attribution for Long Context Query Focused Summarization\\nOur paper explores the problem of unstructured evidence extraction for long context query focused summarization. Here, a model must generate a summary from a long context given a query,\\nand use inline citations to free text spans in the context for support. Evidence has no fixed level of granularity. We found that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwright37/SUnsET.","first_N":5,"first_N_keywords":["summarization","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2-Pause1","keyword":"long context","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1","creator_name":"James Begin","creator_url":"https://huggingface.co/JamesBegin","description":"\\n\\t\\n\\t\\t\\n\\t\\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\\n\\t\\n\\nüåê Project Page: https://longbench2.github.io\\nüíª Github Repo: https://github.com/THUDM/LongBench\\nüìö Arxiv Paper: https://arxiv.org/abs/2412.15204\\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"ChouBun","keyword":"long context","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SakanaAI/ChouBun","creator_name":"Sakana AI","creator_url":"https://huggingface.co/SakanaAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChouBun\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nChouBun is a benchmark for assessing LLMs' performance in long-context tasks in the Japanese language.\\nIt is created and introduced in the paper An Evolved Universal Transformer Memory.\\nThe benchmark includes documents from multiple websites and synthetic question-answer pairs generated by GPT-4 variants and Claude-3.5-Sonnet.\\nThe current version of ChouBun contains 2 task categories -- extractive QA and abstractive summarization -- and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SakanaAI/ChouBun.","first_N":5,"first_N_keywords":["question-answering","summarization","Japanese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SimpleStories","keyword":"distillation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lennart-finke/SimpleStories","creator_name":"Lennart Finke","creator_url":"https://huggingface.co/lennart-finke","description":"\\n\\t\\n\\t\\t\\n\\t\\tüìòüìï SimpleStories üìôüìó\\n\\t\\n\\nThis dataset is a collection of short stories generated by gpt-4o-mini (+ other models, soon). To see how this dataset was generated, or to generate some stories yourself, head over to this repository.\\nIf you'd like to commission other languages or story formats, feel free to send mail.\\nSimpleStories is an iteration upon TinyStories by Eldan and Li, and can likewise be used for distillation to very small language models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeatures\\n\\t\\n\\n\\nStory‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lennart-finke/SimpleStories.","first_N":5,"first_N_keywords":["text-generation","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"LongWriter-6k-English","keyword":"long context","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/themex1380/LongWriter-6k-English","creator_name":"themex","creator_url":"https://huggingface.co/themex1380","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongWriter-6k-English\\n\\t\\n\\nLongWriter-6k-English is a filtered version of the LongWriter-6k dataset, containing only the English-language samples. This dataset includes 2,299 instances of long-form text, ranging from 2,000 to 32,000 words, designed to train large language models (LLMs) to handle extended output contexts.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nLanguages: English\\nData Size: 2,299 samples\\nOutput Length: 2,000 to 32,000 words per sample\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource\\n\\t\\n\\nThis dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/themex1380/LongWriter-6k-English.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"gener-tasks","keyword":"long-context","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GenerTeam/gener-tasks","creator_name":"Gener Team","creator_url":"https://huggingface.co/GenerTeam","description":"\\n\\t\\n\\t\\t\\n\\t\\tGener Tasks\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tAbouts\\n\\t\\n\\nThe Gener Tasks currently includes 2 subtasks:\\n\\nThe gene classification task assesses the model's ability to understand short to medium-length sequences. It includes six different gene types and control samples drawn from non-gene regions, with balanced sampling from six distinct eukaryotic taxonomic groups in RefSeq. The classification goal is to predict the gene type. \\nThe taxonomic classification task is designed to assess the model's comprehension‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GenerTeam/gener-tasks.","first_N":5,"first_N_keywords":["text-classification","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"LooGLE","keyword":"long context","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bigai-nlco/LooGLE","creator_name":"BIGAI NLCo","creator_url":"https://huggingface.co/bigai-nlco","description":"\\n\\t\\n\\t\\t\\n\\t\\tüìúIntroduction\\n\\t\\n\\nLooGLE is a comprehensive evaluation benchmark for LLM long context understanding which contains up-to-date  (all after 2022) and extremely long realistic documents (over 24k tokens per document, many of which exceed 100k words) and 6,000 newly generated questions spanning diverse domains and categories. Details statistics of our dataset can be seen in the table below.\\nShort and long dependency tasks  LooGLE is composed of 7 major tasks to evaluate LLMs' ability to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bigai-nlco/LooGLE.","first_N":5,"first_N_keywords":["question-answering","summarization","text-generation","fill-mask","English"],"keywords_longer_than_N":true},
	{"name":"the-pile-pubmed-central-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/the-pile-pubmed-central-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Pile -- PubMed Central (refined by Data-Juicer)\\n\\t\\n\\nA refined version of PubMed Central dataset in The Pile by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 83G).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 2,694,860 (Keep ~86.96% from the original dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-pubmed-central-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"pretraining","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/StackMathQA","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tStackMathQA\\n\\t\\n\\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\\n\\n\\t\\n\\t\\t\\n\\t\\tConfigs\\n\\t\\n\\nconfigs:\\n- config_name: stackmathqa1600k\\n  data_files: data/stackmathqa1600k/all.jsonl\\n  default: true\\n- config_name: stackmathqa800k\\n  data_files:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/StackMathQA.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"AutoMathText","keyword":"pretraining","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/AutoMathText","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tAutoMathText\\n\\t\\n\\nAutoMathText is an extensive and carefully curated dataset encompassing around 200 GB of mathematical texts. It's a compilation sourced from a diverse range of platforms including various websites, arXiv, and GitHub (OpenWebMath, RedPajama, Algebraic Stack). This rich repository has been autonomously selected (labeled) by the state-of-the-art open-source language model, Qwen-72B. Each piece of content in the dataset is assigned a score lm_q1q2_score within the range of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/AutoMathText.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"pretraining","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/StackMathQA","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tStackMathQA\\n\\t\\n\\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tConfigs\\n\\t\\n\\nconfigs:\\n- config_name: stackmathqa1600k\\n  data_files: data/stackmathqa1600k/all.jsonl\\n  default: true\\n- config_name: stackmathqa800k\\n  data_files:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/StackMathQA.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"webnovel-chinese","keyword":"pretrain","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wdndev/webnovel-chinese","creator_name":"Dongnian","creator_url":"https://huggingface.co/wdndev","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tÁÆÄ‰ªã\\n\\t\\n\\nÊêúÈõÜÁΩëÁªú‰∏äÁöÑÁΩëÊñáÂ∞èËØ¥ÔºåÊ∏ÖÊ¥óÔºåÂàÜÂâ≤ÂêéÔºåÁî®‰∫éËÆ≠ÁªÉÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÂÖ±ËÆ°9000Êú¨Â∑¶Âè≥ÔºåÂ§ßÁ∫¶9BÂ∑¶Âè≥token„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t‰ΩøÁî®\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tÊ†ºÂºèËØ¥Êòé\\n\\t\\n\\nÈááÁî®jsonlÊ†ºÂºèÂ≠òÂÇ®ÔºåÂàÜ‰∏∫‰∏â‰∏™Â≠óÊÆµÔºö\\n\\ntitle ÔºöÂ∞èËØ¥ÂêçÁß∞\\nchapterÔºöÁ´†ËäÇ\\ntextÔºöÊ≠£ÊñáÂÜÖÂÆπ\\n\\nÁ§∫‰æãÔºö\\n{\\\"title\\\": \\\"ÊñóÁ†¥ËãçÁ©π\\\", \\\"chapter\\\": \\\" Á¨¨‰∏ÄÁ´† Èô®ËêΩÁöÑÂ§©Êâç\\\", \\\"text\\\": \\\"‚ÄúÊñó‰πãÂäõÔºå‰∏âÊÆµÔºÅ‚Äù\\\\nÊúõÁùÄÊµãÈ™åÈ≠îÁü≥Á¢ë‰∏äÈù¢Èó™‰∫ÆÂæóÁîöËá≥Êúâ‰∫õÂà∫ÁúºÁöÑ‰∫î‰∏™Â§ßÂ≠óÔºåÂ∞ëÂπ¥Èù¢Êó†Ë°®ÊÉÖÔºåÂîáËßíÊúâÁùÄ‰∏ÄÊäπËá™Âò≤ÔºåÁ¥ßÊè°ÁöÑÊâãÊéåÔºåÂõ†‰∏∫Â§ßÂäõÔºåËÄåÂØºËá¥Áï•ÂæÆÂ∞ñÈîêÁöÑÊåáÁî≤Ê∑±Ê∑±ÁöÑÂà∫Ëøõ‰∫ÜÊéåÂøÉ‰πã‰∏≠ÔºåÂ∏¶Êù•‰∏ÄÈòµÈòµÈíªÂøÉÁöÑÁñºÁóõ‚Ä¶‚Ä¶\\\\n‚ÄúËêßÁÇéÔºåÊñó‰πãÂäõÔºå‰∏âÊÆµÔºÅÁ∫ßÂà´Ôºö‰ΩéÁ∫ßÔºÅ‚ÄùÊµãÈ™åÈ≠îÁü≥Á¢ë‰πãÊóÅÔºå‰∏Ä‰Ωç‰∏≠Âπ¥Áî∑Â≠êÔºåÁúã‰∫Ü‰∏ÄÁúºÁ¢ë‰∏äÊâÄÊòæÁ§∫Âá∫Êù•ÁöÑ‰ø°ÊÅØÔºåËØ≠Ê∞îÊº†ÁÑ∂ÁöÑÂ∞Ü‰πãÂÖ¨Â∏É‰∫ÜÂá∫Êù•‚Ä¶‚Ä¶\\\\n\\\"}\\n\\n","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"korean_parallel_sentences_v1.1","keyword":"distillation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lemon-mint/korean_parallel_sentences_v1.1","creator_name":"Lemon Mint","creator_url":"https://huggingface.co/lemon-mint","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Korean Parallel Sentences Ver 1.1\\n\\t\\n\\nThis dataset card provides information about the Korean Parallel Sentences Ver 1.1 dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Korean Parallel Sentences Ver 1.1 dataset is a collection of parallel sentences in Korean and English.\\nAlthough the factual accuracy of the data is not guaranteed, it has been designed to ensure accurate and consistent translation style between English and Korean.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lemon-mint/korean_parallel_sentences_v1.1.","first_N":5,"first_N_keywords":["translation","text2text-generation","Korean","English","mit"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-Filtered","keyword":"distillation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Replete-AI/OpenHermes-2.5-Filtered","creator_name":"Replete-AI","creator_url":"https://huggingface.co/Replete-AI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the teknium/OpenHermes-2.5 dataset with 2,697 censored lines removed using my uncensored code found bellow.\\n\\t\\n\\n\\nhttps://huggingface.co/datasets/Replete-AI/data_processing_code\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThank you teknium for the original dataset, you can find it bellow.\\n\\t\\n\\n\\nhttps://huggingface.co/datasets/teknium/OpenHermes-2.5\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the same version of Open-Hermes-2.5 that was used in code_bagel_hermes-2.5 found bellow:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Replete-AI/OpenHermes-2.5-Filtered.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"open-genome","keyword":"long context","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LongSafari/open-genome","creator_name":"LongSafari","creator_url":"https://huggingface.co/LongSafari","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset organization\\n\\t\\n\\nThe OpenGenome dataset is organized in 2 stages, where stage 1 has context length 8k and stage 2 has context length 131k.  Each stage has their own datasplits.\\n- stage1\\n  - train\\n  - validation\\n  - test\\n\\n- stage2\\n  - train\\n  - validation\\n  - test\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstructions to download\\n\\t\\n\\nYou can load a dataset using HF's API, with an example below.\\nfrom datasets import load_dataset\\n\\nstage1_data = load_dataset(\\\"LongSafari/open-genome\\\", 'stage1')\\n\\n# access just‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LongSafari/open-genome.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"LLama-405B-Logits","keyword":"distillation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/LLama-405B-Logits","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLlama-405B-Logits Dataset\\n\\t\\n\\nThe Llama-405B-Logits Dataset is a curated subset of logits extracted from the Llama-405B model, created to distill high-performance language models such as Arcee AI's SuperNova using DistillKit. This dataset was also instrumental in the training of the groundbreaking INTELLECT-1 model, demonstrating the effectiveness of leveraging distilled knowledge for enhancing model performance.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbout the Dataset\\n\\t\\n\\nThis dataset contains a carefully‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/LLama-405B-Logits.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LongWriter-6k","keyword":"long context","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongWriter-6k","creator_name":"Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University","creator_url":"https://huggingface.co/THUDM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongWriter-6k\\n\\t\\n\\n\\n  ü§ó [LongWriter Dataset]  ‚Ä¢ üíª [Github Repo] ‚Ä¢ üìÉ [LongWriter Paper] \\n\\n\\nLongWriter-6k dataset contains 6,000 SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese). The data can support training LLMs to extend their maximum output window size to 10,000+ words.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAll Models\\n\\t\\n\\nWe open-sourced the following list of models trained on LongWriter-6k:\\n\\n\\t\\n\\t\\t\\nModel\\nHuggingface Repo\\nDescription\\n\\n\\n\\t\\t\\nLongWriter-glm4-9b\\nü§ó‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongWriter-6k.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LongCite-45k","keyword":"long context","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongCite-45k","creator_name":"Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University","creator_url":"https://huggingface.co/THUDM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongCite-45k\\n\\t\\n\\n\\n  ü§ó [LongCite Dataset]  ‚Ä¢ üíª [Github Repo] ‚Ä¢ üìÉ [LongCite Paper] \\n\\n\\nLongCite-45k dataset contains 44,600 long-context QA instances paired with sentence-level citations (both English and Chinese, up to 128,000 words). The data can support training long-context LLMs to generate response and fine-grained citations within a single output.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Example\\n\\t\\n\\nEach instance in LongCite-45k consists of an instruction, a long context (divided into sentences), a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongCite-45k.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"frames-benchmark","keyword":"long-context","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/frames-benchmark","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFRAMES: Factuality, Retrieval, And reasoning MEasurement Set\\n\\t\\n\\nFRAMES is a comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.\\nOur paper with details and experiments is available on arXiv: https://arxiv.org/abs/2409.12941.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Overview\\n\\t\\n\\n\\n824 challenging multi-hop questions requiring information from 2-15 Wikipedia articles\\nQuestions span diverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/frames-benchmark.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"sys-novel-cleaned","keyword":"pretrain","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Orion-zhen/sys-novel-cleaned","creator_name":"Orion","creator_url":"https://huggingface.co/Orion-zhen","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSIS-Novel\\n\\t\\n\\nÂ∞Üa686d380/sis-novel‰∏≠ÁöÑÊñáÊú¨ËøõË°å‰∫ÜÂàùÊ≠•ÁöÑÊ∏ÖÊ¥ó, ÂΩ¢Êàê‰∫ÜÁ∫ØÊñáÊú¨Êï∞ÊçÆÈõÜ, ÂèØÁî®‰∫éÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ\\n","first_N":5,"first_N_keywords":["text-generation","Chinese","gpl-3.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LongReward-10k","keyword":"long context","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongReward-10k","creator_name":"Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University","creator_url":"https://huggingface.co/THUDM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongReward-10k\\n\\t\\n\\n\\n  üíª [Github Repo] ‚Ä¢ üìÉ [LongReward Paper] \\n\\n\\nLongReward-10k dataset contains 10,000 long-context QA instances (both English and Chinese, up to 64,000 words). \\nThe sft split contains SFT data generated by GLM-4-0520, following the self-instruct method in LongAlign. Using this split, we supervised fine-tune two models: LongReward-glm4-9b-SFT and LongReward-llama3.1-8b-SFT, which are based on GLM-4-9B and Meta-Llama-3.1-8B, respectively. \\nThe dpo_glm4_9b and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongReward-10k.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2","keyword":"long context","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongBench-v2","creator_name":"Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University","creator_url":"https://huggingface.co/THUDM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\\n\\t\\n\\nüåê Project Page: https://longbench2.github.io\\nüíª Github Repo: https://github.com/THUDM/LongBench\\nüìö Arxiv Paper: https://arxiv.org/abs/2412.15204\\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongBench-v2.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"multimodal_textbook","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DAMO-NLP-SG/multimodal_textbook","creator_name":"Language Technology Lab at Alibaba DAMO Academy","creator_url":"https://huggingface.co/DAMO-NLP-SG","description":"\\n\\t\\n\\t\\t\\n\\t\\tMultimodal-Textbook-6.5M\\n\\t\\n\\n    \\n\\n\\n  \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is for \\\"2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining\\\", containing 6.5M images interleaving with 0.8B text from instructional videos.\\n\\nIt contains pre-training corpus using interleaved image-text format. Specifically, our multimodal-textbook includes 6.5M keyframesextracted from instructional videos, interleaving with 0.8B ASR texts.\\nAll the images and text are extracted from online‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DAMO-NLP-SG/multimodal_textbook.","first_N":5,"first_N_keywords":["text-generation","summarization","English","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"multimodal-genshin-impact","keyword":"pretrain","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/multimodal-genshin-impact","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGenshin Impact Fandom Wiki Multimodal Dataset\\n\\t\\n\\nGithub repo here\\n\\n\\t\\n\\t\\t\\n\\t\\tDescription\\n\\t\\n\\nThis dataset is a comprehensive collection of 22,162 fandom wiki pages for the popular game Genshin Impact.\\nThe dataset includes markdown-formatted English content from the wiki, featuring interleaved text, as well as image, video, and audio file links. Additionally, the associated multimodal files (images, videos, and audio) have been downloaded and organized to facilitate the multimodal dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/multimodal-genshin-impact.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","10K<n<100K","Audio","Image"],"keywords_longer_than_N":true},
	{"name":"regularization-architecture","keyword":"preservation-loss-training","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-architecture","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArchitecture Regularization Images\\n\\t\\n\\nA collection of regularization & class instance datasets of architecture for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\\n","first_N":5,"first_N_keywords":["mit","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"regularization-castle","keyword":"preservation-loss-training","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-castle","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCastle Regularization Images\\n\\t\\n\\nA collection of regularization & class instance datasets of castles for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\\n","first_N":5,"first_N_keywords":["mit","1K - 10K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"regularization-horse","keyword":"preservation-loss-training","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-horse","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHorse Regularization Images\\n\\t\\n\\nA collection of regularization & class instance datasets of horses for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-creature","keyword":"preservation-loss-training","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-creature","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCreature Regularization Images\\n\\t\\n\\nA collection of regularization & class instance datasets of creatures for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\\n","first_N":5,"first_N_keywords":["mit","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"regularization-forest","keyword":"preservation-loss-training","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-forest","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tForest Regularization Images\\n\\t\\n\\nA collection of regularization & class instance datasets of forests for the Stable Diffusion 1.5 model to use for DreamBooth prior preservation loss training.\\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-space","keyword":"preservation-loss-training","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-space","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSpace Regularization Images\\n\\t\\n\\nA collection of regularization & class instance datasets of space for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-tiger","keyword":"preservation-loss-training","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-tiger","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTiger Regularization Images\\n\\t\\n\\nA collection of regularization & class instance datasets of tigers for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\\n","first_N":5,"first_N_keywords":["mit","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"regularization-landscape","keyword":"preservation-loss-training","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-landscape","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLandscape Regularization Images\\n\\t\\n\\nA collection of regularization & class instance datasets of landscapes for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-man","keyword":"preservation-loss-training","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-man","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMan Regularization Images\\n\\t\\n\\nA collection of regularization & class instance datasets of men for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-woman","keyword":"preservation-loss-training","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-woman","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWoman Regularization Images\\n\\t\\n\\nA collection of regularization & class instance datasets of women for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"DSIR-filtered-pile-50M","keyword":"pretraining","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/stanford-crfm/DSIR-filtered-pile-50M","creator_name":"Stanford CRFM","creator_url":"https://huggingface.co/stanford-crfm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for DSIR-filtered-pile-50M\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a subset of The Pile, selected via the DSIR data selection method. The target distribution for DSIR is the Wikipedia and BookCorpus2 subsets of The Pile.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish (EN)\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nA train set is provided (51.2M examples) in jsonl format.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n{\\\"contents\\\": \\\"Hundreds of soul music enthusiasts from the United Kingdom plan to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stanford-crfm/DSIR-filtered-pile-50M.","first_N":5,"first_N_keywords":["text-generation","fill-mask","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"astro-classification-redshifts","keyword":"pretraining","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/helenqu/astro-classification-redshifts","creator_name":"Helen Qu","creator_url":"https://huggingface.co/helenqu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAstroClassification and Redshifts Datasets\\n\\t\\n\\n\\n\\nThis dataset was used for the AstroClassification and Redshifts introduced in Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations. This is a dataset of simulated astronomical time-series (e.g., supernovae, active galactic nuclei), and the task is to classify the object type (AstroClassification) or predict the object's redshift (Redshifts).\\n\\nRepository: https://github.com/helenqu/connect-later\\nPaper: will‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/helenqu/astro-classification-redshifts.","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"the-pile-philpaper-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/the-pile-philpaper-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Pile -- PhilPaper (refined by Data-Juicer)\\n\\t\\n\\nA refined version of PhilPaper dataset in The Pile by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 1.7GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 29,117 (Keep ~88.82% from the original dataset)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRefining‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-philpaper-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-arxiv-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-arxiv-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama -- ArXiv (refined by Data-Juicer)\\n\\t\\n\\nA refined version of ArXiv dataset in RedPajama by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 85GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 1,655,259 (Keep ~95.99% from the original dataset)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRefining‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-arxiv-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-cc-2023-06-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-cc-2023-06-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama -- CommonCrawl-2023-06 (refined by Data-Juicer)\\n\\t\\n\\nA refined version of CommonCrawl-2023-06 dataset in RedPajama by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 310GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 50,643,699 (Keep ~45.46% from the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-cc-2023-06-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-cc-2022-05-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-cc-2022-05-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama -- CommonCrawl-2022-05 (refined by Data-Juicer)\\n\\t\\n\\nA refined version of CommonCrawl-2022-05 dataset in RedPajama by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 265GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 42,648,496 (Keep ~45.34% from the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-cc-2022-05-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-stack-code-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-stack-code-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama & TheStack -- Github Code (refined by Data-Juicer)\\n\\t\\n\\nA refined version of Github Code dataset in RedPajama & TheStack by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 232GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 49,279,344 (Keep ~52.09% from the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-stack-code-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"the-pile-europarl-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/the-pile-europarl-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Pile -- EuroParl (refined by Data-Juicer)\\n\\t\\n\\nA refined version of EuroParl dataset in The Pile by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 2.2GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 61,601 (Keep ~88.23% from the original dataset)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRefining‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-europarl-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-pile-stackexchange-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-pile-stackexchange-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama & The Pile -- StackExchange (refined by Data-Juicer)\\n\\t\\n\\nA refined version of StackExchange dataset in RedPajama & The Pile by Data-Juicer. Removing some \\\"bad\\\" samples from the original merged dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 71GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 26,309,203\\t (Keep ~57.89% from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-pile-stackexchange-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-wiki-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-wiki-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama -- Wikipedia (refined by Data-Juicer)\\n\\t\\n\\nA refined version of Wikipedia dataset in RedPajama by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 68GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 26,990,659 (Keep ~90.47% from the original dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-wiki-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-cc-2021-04-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-cc-2021-04-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama -- CommonCrawl-2021-04 (refined by Data-Juicer)\\n\\t\\n\\nA refined version of CommonCrawl-2021-04 dataset in RedPajama by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 284GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 44,724,752 (Keep ~45.23% from the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-cc-2021-04-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-cc-2020-05-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-cc-2020-05-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama -- CommonCrawl-2020-05 (refined by Data-Juicer)\\n\\t\\n\\nA refined version of CommonCrawl-2020-05 dataset in RedPajama by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 297GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 42,612,596 (Keep ~46.90% from the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-cc-2020-05-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-cc-2019-30-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-cc-2019-30-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama -- CommonCrawl-2019-30 (refined by Data-Juicer)\\n\\t\\n\\nA refined version of CommonCrawl-2019-30 dataset in RedPajama by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 240GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 36,557,283 (Keep ~45.08% from the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-cc-2019-30-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-book-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-book-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama -- Book (refined by Data-Juicer)\\n\\t\\n\\nA refined version of Book dataset in RedPajama by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 91GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 195,983 (Keep ~95.51% from the original dataset)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRefining Recipe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-book-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"redpajama-c4-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/redpajama-c4-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRedPajama -- C4 (refined by Data-Juicer)\\n\\t\\n\\nA refined version of C4 dataset in RedPajama by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 832GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 344,491,171 (Keep ~94.42% from the original dataset)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRefining Recipe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-c4-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"the-pile-hackernews-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/the-pile-hackernews-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Pile -- HackerNews (refined by Data-Juicer)\\n\\t\\n\\nA refined version of HackerNews dataset in The Pile by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 1.8G).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 371,331 (Keep ~99.55% from the original dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-hackernews-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"the-pile-freelaw-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/the-pile-freelaw-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Pile -- FreeLaw (refined by Data-Juicer)\\n\\t\\n\\nA refined version of FreeLaw dataset in The Pile by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 45GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 2,942,612 (Keep ~82.61% from the original dataset)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRefining‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-freelaw-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"the-pile-uspto-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/the-pile-uspto-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Pile -- USPTO (refined by Data-Juicer)\\n\\t\\n\\nA refined version of USPTO dataset in The Pile by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 18G).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 4,516,283 (Keep ~46.77% from the original dataset)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRefining Recipe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-uspto-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"the-pile-pubmed-abstracts-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/the-pile-pubmed-abstracts-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Pile -- PubMed Abstracts (refined by Data-Juicer)\\n\\t\\n\\nA refined version of PubMed Abstracts dataset in The Pile by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 24G).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 371,331 (Keep ~99.55% from the original dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-pubmed-abstracts-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"the-pile-nih-refined-by-data-juicer","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/the-pile-nih-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Pile -- NIHExPorter (refined by Data-Juicer)\\n\\t\\n\\nA refined version of NIHExPorter dataset in The Pile by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to pretrain a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 2.0G).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 858,492 (Keep ~91.36% from the original dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-nih-refined-by-data-juicer.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"M4LE","keyword":"long context","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wckwan/M4LE","creator_name":"Cyrus Kwan","creator_url":"https://huggingface.co/wckwan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nM4LE is a Multi-ability, Multi-range, Multi-task, bilingual benchmark for long-context evaluation. We categorize long-context understanding into five distinct abilities by considering whether it is required to identify single or multiple spans in long contexts based on explicit or semantic hints. Specifically, these abilities are explicit single-span, semantic single-span, explicit multiple-span, semantic multiple-span, and global. Different from previous‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wckwan/M4LE.","first_N":5,"first_N_keywords":["question-answering","translation","summarization","text-classification","text-retrieval"],"keywords_longer_than_N":true},
	{"name":"Marathon","keyword":"long context","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Lemoncoke/Marathon","creator_name":"Lei Zhang","creator_url":"https://huggingface.co/Lemoncoke","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Marathon\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRelease\\n\\t\\n\\n\\n[2024/05/15] üî• Marathon is accepted by ACL 2024 Main Conference.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMarathon benchmark is a new long-context multiple-choice benchmark, mainly based on LooGLE, with some original data from LongBench. The context length can reach up to 200K+. Marathon benchmark comprises six tasks: Comprehension and Reasoning, Multiple Information Retrieval, Timeline Reorder, Computation, Passage Retrieval, and Short‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Lemoncoke/Marathon.","first_N":5,"first_N_keywords":["question-answering","open-domain-qa","no-annotation","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"longwriter-6k-filtered","keyword":"long context","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lenML/longwriter-6k-filtered","creator_name":"len","creator_url":"https://huggingface.co/lenML","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongWriter-6k-Filtered\\n\\t\\n\\n\\n  ü§ñ [LongWriter Dataset]  ‚Ä¢ üíª [Github Repo] ‚Ä¢ üìÉ [LongWriter Paper] ‚Ä¢ üìÉ [Tech report]\\n\\n\\nlongwriter-6k-filtered dataset contains 666 filtered examples SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese) based on LongWriter-6k.The data can support training LLMs to extend their maximum output window size to 10,000+ words with low computational cost.\\nThe tech report is available at Minimum Tuning to Unlock Long‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lenML/longwriter-6k-filtered.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Pretraining_Dataset","keyword":"pretraining","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LukeAsh/Pretraining_Dataset","creator_name":"Lukasz Boruszko","creator_url":"https://huggingface.co/LukeAsh","description":"LukeAsh/Pretraining_Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"SimpleStories-JA","keyword":"distillation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lennart-finke/SimpleStories-JA","creator_name":"Lennart Finke","creator_url":"https://huggingface.co/lennart-finke","description":"\\n\\t\\n\\t\\t\\n\\t\\tüìòüìï SimpleStories üìôüìó\\n\\t\\n\\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅgpt-4o-mini„Å´„Çà„Å£„Å¶ÁîüÊàê„Åï„Çå„ÅüÁü≠Á∑®Â∞èË™¨„ÅßÂá∫Êù•„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇÁîüÊàêÊñπÊ≥ï„ÇÑ„ÄÅËá™ÂàÜ„ÅßÁâ©Ë™û„ÇíÁîüÊàê„Åô„ÇãÊñπÊ≥ï„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ„Åì„Å°„Çâ„ÅÆ„É™„Éù„Ç∏„Éà„É™„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑ„ÄÇ\\n‰ªñ„ÅÆË®ÄË™û„ÇÑÁâ©Ë™ûÂΩ¢Âºè„ÅÆÂà∂‰Ωú„ÇíÂ∏åÊúõ„Åï„Çå„ÇãÂ†¥Âêà„ÅØ„ÄÅ„É°„Éº„É´„Å´„Å¶„ÅäÂïè„ÅÑÂêà„Çè„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ\\nSimpleStories„ÅØ„ÄÅElden„Å®Li„Å´„Çà„ÇãTinyStories„ÅÆÊîπËâØÁâà„Åß„Åô„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\tÁâπÂæ¥\\n\\t\\n\\n\\nÁâ©Ë™û„ÅÆÊ≥®ÈáàÊÉÖÂ†±Ôºàtheme„ÄÅtopic„ÄÅstyle„Å™„Å©Ôºâ\\nÂ§öÊßòÊÄß„ÅÆÈ´ò„Åï\\n2024Âπ¥„ÅÆ„É¢„Éá„É´„Å´„Çà„Å£„Å¶ÁîüÊàê\\nNLP„ÅÆ„Éá„Éº„Çø„ÅåÁî®ÊÑè„Åó„Å¶„ÅÑ„Çã„Åü„ÇÅ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Åó„ÇÑ„Åô„ÅÑ\\n‰ª•‰∏ã„ÅÆË®ÄË™ûÁâà„ÅåÂà©Áî®ÂèØËÉΩÔºö\\nËã±Ë™û\\nÊó•Êú¨Ë™û\\n‰ªñ„Å´„ÇÇËøΩÂä†‰∫àÂÆö\\n\\n\\n\\n\\nThis dataset is a collection of short stories generated by gpt-4o-mini (+ other models, soon). To see how this dataset was generated, or to generate some stories‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lennart-finke/SimpleStories-JA.","first_N":5,"first_N_keywords":["text-generation","Japanese","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-multihiertt","keyword":"long-context","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-multihiertt","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tAveniBench: MultiHiertt\\n\\t\\n\\nMultiHiertt split used in the AveniBench.\\n\\n\\t\\n\\t\\t\\n\\t\\tLicense\\n\\t\\n\\nThis dataset is made available under the MIT license.\\n\\n\\t\\n\\t\\t\\n\\t\\tCitation\\n\\t\\n\\nAveniBench\\nTDB\\n\\nMultiHiertt\\n@inproceedings{zhao-etal-2022-multihiertt,\\n    title = \\\"{M}ulti{H}iertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data\\\",\\n    author = \\\"Zhao, Yilun  and\\n      Li, Yunxiang  and\\n      Li, Chenying  and\\n      Zhang, Rui\\\",\\n    booktitle = \\\"Proceedings of the 60th Annual Meeting of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-multihiertt.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true}
]
;
