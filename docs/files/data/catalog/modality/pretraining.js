const data_for_modality_pretraining = 
[
	{"name":"LongWriter-6k","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongWriter-6k\n\t\n\n\n  ðŸ¤— [LongWriter Dataset]  â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongWriter Paper] \n\n\nLongWriter-6k dataset contains 6,000 SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese). The data can support training LLMs to extend their maximum output window size to 10,000+ words.\n\n\t\n\t\t\n\t\n\t\n\t\tAll Models\n\t\n\nWe open-sourced the following list of models trained on LongWriter-6k:\n\n\t\n\t\t\nModel\nHuggingface Repo\nDescription\n\n\n\t\t\nLongWriter-glm4-9b\nðŸ¤—â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zai-org/LongWriter-6k.","url":"https://huggingface.co/datasets/zai-org/LongWriter-6k","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LongCite-45k","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongCite-45k\n\t\n\n\n  ðŸ¤— [LongCite Dataset]  â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongCite Paper] \n\n\nLongCite-45k dataset contains 44,600 long-context QA instances paired with sentence-level citations (both English and Chinese, up to 128,000 words). The data can support training long-context LLMs to generate response and fine-grained citations within a single output.\n\n\t\n\t\t\n\t\n\t\n\t\tData Example\n\t\n\nEach instance in LongCite-45k consists of an instruction, a long context (divided into sentences), a userâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zai-org/LongCite-45k.","url":"https://huggingface.co/datasets/zai-org/LongCite-45k","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"mmBERT-midtraining-data","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Mid-training Data\n\t\n\n\n\n\n\n\nPhase 2 of 3: High-quality mid-training data mixture (600B tokens) with context extension to 8192 tokens.\n\nThis dataset contains the mid-training phase data used to train all mmBERT encoder models. This phase focuses on higher quality data sources and extends the context length from 1024 to 8192 tokens. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.\n\n\t\n\t\t\n\t\tðŸ“Š Data Composition\n\t\n\n\n\t\n\t\t\nData Sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-midtraining-data.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-midtraining-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"regularization-architecture","keyword":"preservation-loss-training","description":"\n\t\n\t\t\n\t\tArchitecture Regularization Images\n\t\n\nA collection of regularization & class instance datasets of architecture for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","url":"https://huggingface.co/datasets/3ee/regularization-architecture","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"pretraining","description":"\n\n\n\t\n\t\t\n\t\tStackMathQA\n\t\n\nStackMathQA: A Curated Collection of 2 Million Mathematical Questions and Answers Sourced from Stack Exchange\n\n\n\n\n\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\n\n\t\t\n\t\n\t\tConfigs\n\t\n\nconfigs:\n- config_name: stackmathqa1600kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/StackMathQA.","url":"https://huggingface.co/datasets/math-ai/StackMathQA","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"the-pile-europarl-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tThe Pile -- EuroParl (refined by Data-Juicer)\n\t\n\nA refined version of EuroParl dataset in The Pile by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 2.2GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 61,601 (Keep ~88.23% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefining Recipeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-europarl-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/the-pile-europarl-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-arxiv-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama -- ArXiv (refined by Data-Juicer)\n\t\n\nA refined version of ArXiv dataset in RedPajama by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 85GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 1,655,259 (Keep ~95.99% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefining Recipeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-arxiv-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-arxiv-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"the-pile-hackernews-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tThe Pile -- HackerNews (refined by Data-Juicer)\n\t\n\nA refined version of HackerNews dataset in The Pile by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 1.8G).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 371,331 (Keep ~99.55% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefiningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-hackernews-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/the-pile-hackernews-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"frames-benchmark","keyword":"long-context","description":"\n\t\n\t\t\n\t\tFRAMES: Factuality, Retrieval, And reasoning MEasurement Set\n\t\n\nFRAMES is a comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.\nOur paper with details and experiments is available on arXiv: https://arxiv.org/abs/2409.12941.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\n824 challenging multi-hop questions requiring information from 2-15 Wikipedia articles\nQuestions span diverse topicsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google/frames-benchmark.","url":"https://huggingface.co/datasets/google/frames-benchmark","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\n\t\n\nðŸŒ Project Page: https://longbench2.github.io\nðŸ’» Github Repo: https://github.com/THUDM/LongBench\nðŸ“š Arxiv Paper: https://arxiv.org/abs/2412.15204\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zai-org/LongBench-v2.","url":"https://huggingface.co/datasets/zai-org/LongBench-v2","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"CCNet","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tCCNet Reproduced Split (4M rows, 3.7B Tokens (Mistral tokenizer))\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is a reproduced subset of the larger CCNet dataset, tailored specifically to facilitate easier access and processing for researchers needing high-quality, web-crawled text data for natural language processing tasks. The CCNet dataset leverages data from the Common Crawl, a non-profit organization that crawls the web and freely provides its archives to the public. This subset contains 4â€¦ See the full description on the dataset page: https://huggingface.co/datasets/JorgeeGF/CCNet.","url":"https://huggingface.co/datasets/JorgeeGF/CCNet","creator_name":"Jorge Gallego Feliciano","creator_url":"https://huggingface.co/JorgeeGF","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1M - 10M","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"human2locoman","keyword":"pretraining","description":"\n  Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining\n\n\n  \n\n\n  Yaru Niu1,*Â Â Â \n  Yunzhe Zhang1,*Â Â Â \n  Mingyang Yu1Â Â Â \n  Changyi Lin1Â Â Â \n  Chenhao Li1Â Â Â \n  Yikai Wang1\n  \n  Yuxiang Yang2Â Â Â \n  Wenhao Yu2Â Â Â \n  Tingnan Zhang2Â Â Â \n  Zhenzhen Li3Â Â Â \n  Jonathan Francis1,3Â Â Â \n  Bingqing Chen3Â Â Â \n  \n  Jie Tan2Â Â Â \n  Ding Zhao1Â Â Â \n  \n  1Carnegie Mellon UniversityÂ Â Â \n  2Google DeepMindÂ Â Â \n  3Bosch Center for AIÂ Â Â \n  \n  *Equal contributions\n\n    Robotics: Science and Systemsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/chrisyrniu/human2locoman.","url":"https://huggingface.co/datasets/chrisyrniu/human2locoman","creator_name":"Yaru Niu","creator_url":"https://huggingface.co/chrisyrniu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","English","mit","< 1K","Video"],"keywords_longer_than_N":true},
	{"name":"SDO","keyword":"pretraining","description":"\n\n\t\n\t\t\n\t\tðŸŒž SDO ML-Ready Dataset: AIA and HMI Level-1.5\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset provides machine learning (ML)-ready solar data curated from NASAâ€™s Solar Dynamics Observatory (SDO), covering observations from May 13, 2010, to July 31, 2024. It includes Level-1.5 processed data from:\n\nAtmospheric Imaging Assembly (AIA): \nHelioseismic and Magnetic Imager (HMI):\n\nThe dataset is designed to facilitate large-scale ML applications in heliophysics, such as solar activity forecastingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/harshinde/SDO.","url":"https://huggingface.co/datasets/harshinde/SDO","creator_name":"Harsh","creator_url":"https://huggingface.co/harshinde","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["mit","n>1T","ðŸ‡ºðŸ‡¸ Region: US","Helio","Foundation_model"],"keywords_longer_than_N":true},
	{"name":"TemplateGSM","keyword":"pretraining","description":"\n\n\n\t\n\t\t\n\t\tTemplateMath: Template-based Data Generation (TDG)\n\t\n\n\n\n\n\n\n\n\nThis is the official repository for the paper \"Training and Evaluating Language Models with Template-based Data Generation\", published at the ICLR 2025 DATA-FM Workshop.\nOur work introduces Template-based Data Generation (TDG), a scalable paradigm to address the critical data bottleneck in training LLMs for complex reasoning tasks. We use TDG to create TemplateGSM, a massive dataset designed to unlock the next level ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/TemplateGSM.","url":"https://huggingface.co/datasets/math-ai/TemplateGSM","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10M - 100M","Tabular"],"keywords_longer_than_N":true},
	{"name":"ImplicitPersona","keyword":"long-context","description":"\n\t\n\t\t\n\t\tPersonaMem-v2: Implicit Personas\n\t\n\nPersonalization is becoming the next milestone of artificial super-intelligence. AI cannot always satisfy every user, especially on tasks with subjective goals, but personalization offers a path toward pluralistic alignment. PersonaMem-v2 (temporary) is the new state-of-the-art LLM-personalization dataset focusing on implicit personas in LLMs, where userâ€“chatbot conversations implicitly indicate user preferences. For example, a user mightâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bowen-upenn/ImplicitPersona.","url":"https://huggingface.co/datasets/bowen-upenn/ImplicitPersona","creator_name":"Lauren Jiang","creator_url":"https://huggingface.co/bowen-upenn","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"TheBlueScrubs-v1-fixed","keyword":"pretraining","description":"\n\t\n\t\t\n\t\topenmed-community/TheBlueScrubs-v1-fixed\n\t\n\n\n\t\n\t\t\n\t\tWhat is this?\n\t\n\nTheBlueScrubs-v1-fixed is a maintenance fork of the upstream TheBlueScrubs/TheBlueScrubs-v1 train split that resolves a schema bug in the meta column.In the original train files, some rows serialized meta incorrectly (appearing as the literal string \"dict\"). This fork re-exports the entire train split without meta column, preserving text field and values.\n\nDocument count: 11,080,331 texts (train)  \nTokens (upstreamâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/TheBlueScrubs-v1-fixed.","url":"https://huggingface.co/datasets/openmed-community/TheBlueScrubs-v1-fixed","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"Pretraining_Dataset","keyword":"pretraining","description":"LukeAsh/Pretraining_Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/LukeAsh/Pretraining_Dataset","creator_name":"Lukasz Boruszko","creator_url":"https://huggingface.co/LukeAsh","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"mmBERT-data-decay-cont","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-data-decay-cont.","url":"https://huggingface.co/datasets/orionweller/mmBERT-data-decay-cont","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"DBpediaOntoTrain","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tðŸ§  DBpediaOntoTrain: A Quality-Segmented Ontology Dataset for LLM Pretraining\n\t\n\n\n\t\n\t\t\n\t\tðŸ“˜ Overview\n\t\n\nDBpediaOntoTrain is a dataset of 1,766 OWL ontologies in Turtle format, extracted from DBpedia Archivo and prepared for continual pretraining of Large Language Models (LLMs) in ontology generation and completion tasks.\nEach ontology is analyzed using a set of semantic quality metrics, tokenized using the LLaMA 3.2 tokenizer, and sorted by Quality Score (QS). The dataset includesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gplsi/DBpediaOntoTrain.","url":"https://huggingface.co/datasets/gplsi/DBpediaOntoTrain","creator_name":"GPLSI UA","creator_url":"https://huggingface.co/gplsi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","ðŸ‡ºðŸ‡¸ Region: US","ontology"],"keywords_longer_than_N":true},
	{"name":"ChouBun","keyword":"long context","description":"\n\t\n\t\t\n\t\tChouBun\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nChouBun is a benchmark for assessing LLMs' performance in long-context tasks in the Japanese language.\nIt is created and introduced in the paper An Evolved Universal Transformer Memory.\nThe benchmark includes documents from multiple websites and synthetic question-answer pairs generated by GPT-4 variants and Claude-3.5-Sonnet.\nThe current version of ChouBun contains 2 task categories -- extractive QA and abstractive summarization -- and 4 tasksâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SakanaAI/ChouBun.","url":"https://huggingface.co/datasets/SakanaAI/ChouBun","creator_name":"Sakana AI","creator_url":"https://huggingface.co/SakanaAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","Japanese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"high-quality-text","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tHigh Quality Text Dataset\n\t\n\nA curated collection of English-language texts for AI training and research.\n\n\t\n\t\t\n\t\tSources\n\t\n\n\nHuggingFaceFW/fineweb-edu  \nopenbmb/Ultra-FineWeb  \nZyphra/Zyda-2\nEssentialAI/eai-taxonomy-stem-w-dclm-100b-sample\nm-a-p/FineFineWeb\n\nEach dataset was processed as follows:\n\nSplit into approximately 2â€‰000-token chunks using the LLaMA 3.1 tokenizer.  \nCleaned by normalizing spaces, punctuation, and characters, and replacing emails and phone numbers withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/high-quality-text.","url":"https://huggingface.co/datasets/agentlans/high-quality-text","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","English","odc-by","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"nvidia_steer_yo","keyword":"pretrain","description":"Yoruba translation of the Nvidia steer dataset\n","url":"https://huggingface.co/datasets/mxronga/nvidia_steer_yo","creator_name":"Maro jos","creator_url":"https://huggingface.co/mxronga","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Yoruba","apache-2.0","1K - 10K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"dclm_20b","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tDCLM used in MoCa Pre-training\n\t\n\nðŸ  Homepage | ðŸ’» Code | ðŸ¤– MoCa-Qwen25VL-7B | ðŸ¤– MoCa-Qwen25VL-3B | ðŸ“š Datasets | ðŸ“„ Paper\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThis is a text pre-training dataset used in the modality-aware continual pre-training of MoCa models. It is adapted from DCLM and randomly downsampled to ~20B tokens.\nThe dataset consists of text examples. text is a string containing text while images are left blank intentionally since there is no image available.\n\t\n\t\t\n\t\tCitationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/moca-embed/dclm_20b.","url":"https://huggingface.co/datasets/moca-embed/dclm_20b","creator_name":"MoCa","creator_url":"https://huggingface.co/moca-embed","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10M - 100M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"cds-annotation","keyword":"long-context","description":"\n\t\n\t\t\n\t\tProkaryotic Genome CDS Annotation Summary\n\t\n\n\n\t\n\t\t\n\t\tAbouts\n\t\n\nIn this repository, we present the evaluation datasets for prokaryotic genome annotation tasked. Through comprehensive evaluations, GENERanno-cds-annotator achieves superior accuracy compared to traditional HMM-based methods (e.g., GLIMMER3, GeneMarkS2, Prodigal) and recent LLM-based approaches (e.g., GeneLM), while demonstrating exceptional generalization ability on archaeal genomes.\n","url":"https://huggingface.co/datasets/GenerTeam/cds-annotation","creator_name":"Gener Team","creator_url":"https://huggingface.co/GenerTeam","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"indic_reasoning","keyword":"distillation","description":"\n\t\n\t\t\n\t\tIndic Reasoning\n\t\n\nThe Indic Reasoning Dataset (~500M tokens, 592k examples) is a high-quality, large-scale open-source resource created using advanced distillation techniques. It is designed to train and evaluate reasoning-capable AI systems with a strong emphasis on complex reasoning, structured chain-of-thought (CoT), and culturally relevant content.\nThis domain-rich corpus integrates Indian cultural, legal, historical, philosophical, and social contexts with global knowledgeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/169Pi/indic_reasoning.","url":"https://huggingface.co/datasets/169Pi/indic_reasoning","creator_name":"169Pi","creator_url":"https://huggingface.co/169Pi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"pi-llm","keyword":"long-context","description":"\n\n\t\n\t\t\n\t\tPI-LLM Bench: The Core Retrieval Challenge Behind MRCR\n\t\n\n\nICML 2025 Long-Context Foundation Models Workshop Accepted.\n\nA simple context interference evaluation.\n\nUpdate: This dataset is integrated into Moonshot AI(Kimi)'s internal benchmarking framework for assessing ** tracking capacity and context interference in LLM/agents**.\nUpdate:Sept.6-mergerd into Moonshot/Kimi AI's internal eval tools and under review by a xAI(Grok)'s' eval team\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tTL;DR\n\t\n\nWe identify a taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/giantfish-fly/pi-llm.","url":"https://huggingface.co/datasets/giantfish-fly/pi-llm","creator_name":"c.p. wang","creator_url":"https://huggingface.co/giantfish-fly","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","Tabular"],"keywords_longer_than_N":true},
	{"name":"testDBpedia","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tðŸ§  DBpediaOntoTrain: A Quality-Segmented Ontology Dataset for LLM Pretraining\n\t\n\n\n\t\n\t\t\n\t\tðŸ“˜ Overview\n\t\n\nDBpediaOntoTrain is a dataset of 1,766 OWL ontologies in Turtle format, extracted from DBpedia Archivo and prepared for continual pretraining of Large Language Models (LLMs) in ontology generation and completion tasks.\nEach ontology is analyzed using a set of semantic quality metrics, tokenized using the LLaMA 3.2 tokenizer, and sorted by Quality Score (QS). The dataset includesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/miquelCanal/testDBpedia.","url":"https://huggingface.co/datasets/miquelCanal/testDBpedia","creator_name":"Miquel Canal ","creator_url":"https://huggingface.co/miquelCanal","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","ðŸ‡ºðŸ‡¸ Region: US","ontology"],"keywords_longer_than_N":true},
	{"name":"DCLM_German","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tDCLM German Dataset\n\t\n\nThis dataset contains German language data processed for LLM pretraining, filtered using FastText language detection.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the entire dataset\ndataset = load_dataset(\"faidrap/DCLM_German\")\n\n# Stream for large datasets (recommended)\ndataset = load_dataset(\"faidrap/DCLM_German\", streaming=True)\n\n# Access the data\nfor example in dataset['train']:\n    print(example['text'][:100])  # Print first 100 charsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/faidrap/DCLM_German.","url":"https://huggingface.co/datasets/faidrap/DCLM_German","creator_name":"Faidra Patsatzi","creator_url":"https://huggingface.co/faidrap","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","monolingual","original","German"],"keywords_longer_than_N":true},
	{"name":"ettin-pretraining-data","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tEttin Pre-training Data\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse pre-training data mixture (1.7T tokens) used to train the Ettin model suite.\n\nThis dataset contains the pre-training phase data used to train all Ettin encoder and decoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.\n\n\t\n\t\t\n\t\tðŸ“Š Data Composition\n\t\n\n\n\t\n\t\t\nData Source\nTokens (B)\nPercentage\nDescription\n\n\n\t\t\nDCLM\n837.2\n49.1%\nHigh-quality web crawl data\n\n\nCC Head\n356.6â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/ettin-pretraining-data.","url":"https://huggingface.co/datasets/jhu-clsp/ettin-pretraining-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","text-classification","English","mit"],"keywords_longer_than_N":true},
	{"name":"Nebo-T1-Russian","keyword":"distillation","description":"\n\t\n\t\t\n\t\tRussian Description (English below)\n\t\n\n\n\t\n\t\t\n\t\tUPD: Dataset reuploaded, correct_format column added\n\t\n\n\n\t\n\t\t\n\t\tNebo-T1-Russian\n\t\n\n(Ð’ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾) Ð¿ÐµÑ€Ð²Ñ‹Ð¹ \"longCoT\" Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð´Ð»Ñ Ñ€ÑƒÑÑÐºÐ¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ°, ÑÐ¾Ð·Ð´Ð°Ð½Ð½Ñ‹Ð¹ Ñ‡ÐµÑ€ÐµÐ· Deeseek-R1\n\nÐŸÐ¾Ð´ÑÐºÐ°Ð·ÐºÐ¸ Ð²Ð·ÑÑ‚Ñ‹ Ð¸Ð· Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° Sky-T1 Ð¸ Ð¿ÐµÑ€ÐµÐ²ÐµÐ´ÐµÐ½Ñ‹ Ñ‡ÐµÑ€ÐµÐ· Llama3.3-70B\nÐžÑ‚Ð²ÐµÑ‚Ñ‹ Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Deeseek-R1 (685B)\n16.4K ÑÑÐ¼Ð¿Ð»Ð¾Ð² Ð² Ñ†ÐµÐ»Ð¾Ð¼, â‰ˆ12.4K Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ Ñ€ÑƒÑÑÐºÐ¸Ð¼ ÑÐ·Ñ‹ÐºÐ¾Ð¼ (Ð² Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð»Ð¸Ð±Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚, Ð»Ð¸Ð±Ð¾ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð½Ð° Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¾Ð¼)\nÐ¯Ð·Ñ‹ÐºÐ¸ Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ðµ Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÑÑ… Ñ€Ð°Ð·Ð¼ÐµÑ‡ÐµÐ½Ñ‹â€¦ See the full description on the dataset page: https://huggingface.co/datasets/kristaller486/Nebo-T1-Russian.","url":"https://huggingface.co/datasets/kristaller486/Nebo-T1-Russian","creator_name":"Kristaller486","creator_url":"https://huggingface.co/kristaller486","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\n\t\n\nðŸŒ Project Page: https://longbench2.github.io\nðŸ’» Github Repo: https://github.com/THUDM/LongBench\nðŸ“š Arxiv Paper: https://arxiv.org/abs/2412.15204\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongBench-v2.","url":"https://huggingface.co/datasets/THUDM/LongBench-v2","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"LongFinanceQA","keyword":"long-context","description":"\n\t\n\t\t\n\t\tDataset Card for LongFinanceQA\n\t\n\nLongFinanceQA dataset is designed to generate practical long-context QA pairs with reasoning steps to effectively analyze long content. It consists of 46,457 long-context QA pairs.\nPaper and more resources: [arXiv] [Project Website]\n\n\t\n\t\t\n\t\tIntended Uses\n\t\n\nThis dataset is used for academic research purposes only.\n\n\t\n\t\t\n\t\tData Sample Demo\n\t\n\nBelow is a sample from the dataset:\n{\n    \"id\": \"id_000000\",\n    \"doc\": \"docs/doc_0800.txt\",\n    \"question\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jylins/LongFinanceQA.","url":"https://huggingface.co/datasets/jylins/LongFinanceQA","creator_name":"Jingyang Lin","creator_url":"https://huggingface.co/jylins","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","Chinese","apache-2.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-zh","keyword":"distillation","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for \"OpenHermes-2.5-zh\"\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the original OpenHermes dataset : teknium/OpenHermes-2.5.\nLanguages: Chinese\nApplications: Language Modeling\nLicense: Apache-2.0\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOpenHermes-2.5-zh is a dataset translated from the OpenHermes-2.5 collection provided by teknium.\n","url":"https://huggingface.co/datasets/ldwang/OpenHermes-2.5-zh","creator_name":"ldwang","creator_url":"https://huggingface.co/ldwang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Chinese","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US","Synthetic"],"keywords_longer_than_N":true},
	{"name":"ettin-decay-data","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tEttin Decay Phase Data\n\t\n\n\n\n\n\n\nPhase 3 of 3: Premium data sources for final training phase (100B tokens) following the ProLong recipe.\n\nThis dataset contains the decay phase data used to train all Ettin encoder and decoder models. This final phase uses premium data sources with emphasis on long-form content and educational materials. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nThe large language model (LLM)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/ettin-decay-data.","url":"https://huggingface.co/datasets/jhu-clsp/ettin-decay-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","arxiv:2507.11412","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"LIBRA","keyword":"long-context","description":"\n\t\n\t\t\n\t\tLIBRA: Long Input Benchmark for Russian Analysis\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nLIBRA (Long Input Benchmark for Russian Analysis) is designed to evaluate the capabilities of large language models (LLMs) in understanding and processing long texts in Russian. This benchmark includes 21 datasets adapted for different tasks and complexities. The tasks are divided into four complexity groups and allow evaluation across various context lengths ranging from 4k up to 128k tokens.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai-forever/LIBRA.","url":"https://huggingface.co/datasets/ai-forever/LIBRA","creator_name":"ai-forever","creator_url":"https://huggingface.co/ai-forever","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Russian","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-1.5-Instruct-Data","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-1.5 Instruction Data\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset, LLaVA-OneVision-1.5-Instruct, was collected and integrated during the development of LLaVA-OneVision-1.5. LLaVA-OneVision-1.5 is a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. This meticulously curated 22M instruction dataset (LLaVA-OneVision-1.5-Instruct) is part of a comprehensive andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mvp-lab/LLaVA-OneVision-1.5-Instruct-Data.","url":"https://huggingface.co/datasets/mvp-lab/LLaVA-OneVision-1.5-Instruct-Data","creator_name":"Mobile Vision Perception Lab","creator_url":"https://huggingface.co/mvp-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2509.23661","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Basic-Math_TR","keyword":"pretraining","description":"Ba2han/Basic-Math_TR dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Ba2han/Basic-Math_TR","creator_name":"Batuhan S","creator_url":"https://huggingface.co/Ba2han","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Turkish","mit","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"regularization-landscape","keyword":"preservation-loss-training","description":"\n\t\n\t\t\n\t\tLandscape Regularization Images\n\t\n\nA collection of regularization & class instance datasets of landscapes for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","url":"https://huggingface.co/datasets/3ee/regularization-landscape","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-castle","keyword":"preservation-loss-training","description":"\n\t\n\t\t\n\t\tCastle Regularization Images\n\t\n\nA collection of regularization & class instance datasets of castles for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","url":"https://huggingface.co/datasets/3ee/regularization-castle","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"redpajama-wiki-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama -- Wikipedia (refined by Data-Juicer)\n\t\n\nA refined version of Wikipedia dataset in RedPajama by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 68GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 26,990,659 (Keep ~90.47% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefiningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-wiki-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-wiki-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"the-pile-pubmed-abstracts-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tThe Pile -- PubMed Abstracts (refined by Data-Juicer)\n\t\n\nA refined version of PubMed Abstracts dataset in The Pile by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 24G).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 371,331 (Keep ~99.55% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-pubmed-abstracts-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/the-pile-pubmed-abstracts-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p2-fineweb2-remaining","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P2\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P2 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p2-fineweb2-remaining.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p2-fineweb2-remaining","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","English","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p1-fineweb2-langs","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P1\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P1 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","feature-extraction","multilingual","mit","arxiv:2509.06888"],"keywords_longer_than_N":true},
	{"name":"webnovel-chinese","keyword":"pretrain","description":"\n\t\n\t\t\n\t\tç®€ä»‹\n\t\n\næœé›†ç½‘ç»œä¸Šçš„ç½‘æ–‡å°è¯´ï¼Œæ¸…æ´—ï¼Œåˆ†å‰²åŽï¼Œç”¨äºŽè®­ç»ƒå¤§è¯­è¨€æ¨¡åž‹ï¼Œå…±è®¡9000æœ¬å·¦å³ï¼Œå¤§çº¦9Bå·¦å³tokenã€‚\n\n\t\n\t\t\n\t\tä½¿ç”¨\n\t\n\n\n\t\n\t\t\n\t\tæ ¼å¼è¯´æ˜Ž\n\t\n\né‡‡ç”¨jsonlæ ¼å¼å­˜å‚¨ï¼Œåˆ†ä¸ºä¸‰ä¸ªå­—æ®µï¼š\n\ntitle ï¼šå°è¯´åç§°\nchapterï¼šç« èŠ‚\ntextï¼šæ­£æ–‡å†…å®¹\n\nç¤ºä¾‹ï¼š\n{\"title\": \"æ–—ç ´è‹ç©¹\", \"chapter\": \" ç¬¬ä¸€ç«  é™¨è½çš„å¤©æ‰\", \"text\": \"â€œæ–—ä¹‹åŠ›ï¼Œä¸‰æ®µï¼â€\\næœ›ç€æµ‹éªŒé­”çŸ³ç¢‘ä¸Šé¢é—ªäº®å¾—ç”šè‡³æœ‰äº›åˆºçœ¼çš„äº”ä¸ªå¤§å­—ï¼Œå°‘å¹´é¢æ— è¡¨æƒ…ï¼Œå”‡è§’æœ‰ç€ä¸€æŠ¹è‡ªå˜²ï¼Œç´§æ¡çš„æ‰‹æŽŒï¼Œå› ä¸ºå¤§åŠ›ï¼Œè€Œå¯¼è‡´ç•¥å¾®å°–é”çš„æŒ‡ç”²æ·±æ·±çš„åˆºè¿›äº†æŽŒå¿ƒä¹‹ä¸­ï¼Œå¸¦æ¥ä¸€é˜µé˜µé’»å¿ƒçš„ç–¼ç—›â€¦â€¦\\nâ€œè§ç‚Žï¼Œæ–—ä¹‹åŠ›ï¼Œä¸‰æ®µï¼çº§åˆ«ï¼šä½Žçº§ï¼â€æµ‹éªŒé­”çŸ³ç¢‘ä¹‹æ—ï¼Œä¸€ä½ä¸­å¹´ç”·å­ï¼Œçœ‹äº†ä¸€çœ¼ç¢‘ä¸Šæ‰€æ˜¾ç¤ºå‡ºæ¥çš„ä¿¡æ¯ï¼Œè¯­æ°”æ¼ ç„¶çš„å°†ä¹‹å…¬å¸ƒäº†å‡ºæ¥â€¦â€¦\\n\"}\n\n","url":"https://huggingface.co/datasets/wdndev/webnovel-chinese","creator_name":"Dongnian","creator_url":"https://huggingface.co/wdndev","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"dParallel_Dream_Distill_Data","keyword":"distillation","description":"\n\t\n\t\t\n\t\tdParallel-Dream-Distill Dataset:\n\t\n\nThis dataset is used for the certainty-forcing distillation process in dParallel. We use prompts from publicly available training datasets and let the pretrained model generate its own responses as training data. For LLaDA-8B-Instruct, we sample prompts from the GSM8K, PRM12K training set, and part of the Numina-Math dataset. We generate target trajectories using a semi-autoregressive strategy with a sequence length of 256 and block length of 32. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zigeng/dParallel_Dream_Distill_Data.","url":"https://huggingface.co/datasets/Zigeng/dParallel_Dream_Distill_Data","creator_name":"Zigeng Chen","creator_url":"https://huggingface.co/Zigeng","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-man","keyword":"preservation-loss-training","description":"\n\t\n\t\t\n\t\tMan Regularization Images\n\t\n\nA collection of regularization & class instance datasets of men for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","url":"https://huggingface.co/datasets/3ee/regularization-man","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"simpleqa-verified","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tSimpleQA Verified\n\t\n\n\n\t\n\t\t\n\t\tA 1,000-prompt factuality benchmark from Google DeepMind and Google Research, designed to reliably evaluate LLM parametric knowledge.\n\t\n\nâ–¶ SimpleQA Verified Leaderboard on Kaggleâ–¶ Technical Reportâ–¶ Evaluation Starter Code\n\n\t\n\t\t\n\t\n\t\n\t\tBenchmark\n\t\n\nSimpleQA Verified is a 1,000-prompt benchmark for reliably evaluating Large Language Models (LLMs) on short-form factuality \nand parametric knowledge. The authors from Google DeepMind and Google Research build onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/google/simpleqa-verified.","url":"https://huggingface.co/datasets/google/simpleqa-verified","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"experimental-pretrain-1b","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tDataset Card for Experimental Pretraining Dataset 1B\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA meticulously curated 1 billion token dataset optimized for experimental pretraining of small language models. This dataset represents a balanced mixture of the highest quality educational content (60%), mathematical reasoning (30%), and Python code (10%), specifically designed for rapid experimentation and research in language model training.\n\nCurated by: Yxanulâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yxanul/experimental-pretrain-1b.","url":"https://huggingface.co/datasets/Yxanul/experimental-pretrain-1b","creator_name":"David Franco","creator_url":"https://huggingface.co/Yxanul","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"MeetingBank-transcript-de","keyword":"long-context","description":"This dataset consists of transcripts from the MeetingBank dataset. \nOverview\nMeetingBank, a benchmark dataset created from the city councils of 6 major U.S. cities to supplement existing datasets.\nIt contains 1,366 meetings with over 3,579 hours of video, as well as transcripts, PDF documents of meeting minutes, agenda, and other metadata.\nOn average, a council meeting is 2.6 hours long and its transcript contains over 28k tokens, making it a valuable testbed for meeting summarizers and forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlioLeuchtmann/MeetingBank-transcript-de.","url":"https://huggingface.co/datasets/AlioLeuchtmann/MeetingBank-transcript-de","creator_name":"Alio Leuchtmann","creator_url":"https://huggingface.co/AlioLeuchtmann","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","summarization","text-generation","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ruler-long-data","keyword":"long-context","description":"\n\t\n\t\t\n\t\tRULER-long (tturing)\n\t\n\nThis dataset contains longer generated samples produced with the RULER benchmark configuration for testing.Size: ~36 GB\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\nds = load_dataset(\"USER/ri-ruler-generated\")\n\n","url":"https://huggingface.co/datasets/tturing/ruler-long-data","creator_name":"Hz Tang","creator_url":"https://huggingface.co/tturing","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","long-context","evaluation","ruler"],"keywords_longer_than_N":false},
	{"name":"SimpleStories-JA","keyword":"distillation","description":"\n\t\n\t\t\n\t\tðŸ“˜ðŸ“• SimpleStories ðŸ“™ðŸ“—\n\t\n\nã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€gpt-4o-miniã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸçŸ­ç·¨å°èª¬ã§å‡ºæ¥ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚ç”Ÿæˆæ–¹æ³•ã‚„ã€è‡ªåˆ†ã§ç‰©èªžã‚’ç”Ÿæˆã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€ã“ã¡ã‚‰ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ã”è¦§ãã ã•ã„ã€‚\nä»–ã®è¨€èªžã‚„ç‰©èªžå½¢å¼ã®åˆ¶ä½œã‚’å¸Œæœ›ã•ã‚Œã‚‹å ´åˆã¯ã€ãƒ¡ãƒ¼ãƒ«ã«ã¦ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚\nSimpleStoriesã¯ã€Eldenã¨Liã«ã‚ˆã‚‹TinyStoriesã®æ”¹è‰¯ç‰ˆã§ã™ã€‚\n\n\t\n\t\t\n\t\tç‰¹å¾´\n\t\n\n\nç‰©èªžã®æ³¨é‡ˆæƒ…å ±ï¼ˆthemeã€topicã€styleãªã©ï¼‰\nå¤šæ§˜æ€§ã®é«˜ã•\n2024å¹´ã®ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ç”Ÿæˆ\nNLPã®ãƒ‡ãƒ¼ã‚¿ãŒç”¨æ„ã—ã¦ã„ã‚‹ãŸã‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã‚„ã™ã„\nä»¥ä¸‹ã®è¨€èªžç‰ˆãŒåˆ©ç”¨å¯èƒ½ï¼š\nè‹±èªž\næ—¥æœ¬èªž\nä»–ã«ã‚‚è¿½åŠ äºˆå®š\n\n\n\n\nThis dataset is a collection of short stories generated by gpt-4o-mini (+ other models, soon). To see how this dataset was generated, or to generate some storiesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SimpleStories/SimpleStories-JA.","url":"https://huggingface.co/datasets/SimpleStories/SimpleStories-JA","creator_name":"The SimpleStories Project","creator_url":"https://huggingface.co/SimpleStories","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Japanese","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"SimpleStories","keyword":"distillation","description":"\n\t\n\t\t\n\t\tðŸ“˜ðŸ“• SimpleStories ðŸ“™ðŸ“—\n\t\n\nSimpleStories is a dataset of >2 million model-generated short stories. It was made to train small, interpretable language models on it. The generation process is open-source: To see how the dataset was generated, or to generate some stories yourself, head over to this repository.\nIf you'd like to commission other languages or story formats, feel free to send mail.\nWhen using SimpleStories in your work, please cite the SimpleStories paper:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SimpleStories/SimpleStories.","url":"https://huggingface.co/datasets/SimpleStories/SimpleStories","creator_name":"The SimpleStories Project","creator_url":"https://huggingface.co/SimpleStories","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"SUnsET","keyword":"long-context","description":"\n\t\n\t\t\n\t\tSUnsET Dataset\n\t\n\nThe Summaries with Unstructured Evidence Text (SUnsET) dataset from the paper Unstructured Evidence Attribution for Long Context Query Focused Summarization\nOur paper explores the problem of unstructured evidence extraction for long context query focused summarization. Here, a model must generate a summary from a long context given a query,\nand use inline citations to free text spans in the context for support. Evidence has no fixed level of granularity. We found thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dwright37/SUnsET.","url":"https://huggingface.co/datasets/dwright37/SUnsET","creator_name":"Dustin Wright","creator_url":"https://huggingface.co/dwright37","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["summarization","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"prokaryotic-gener-tasks","keyword":"long-context","description":"\n\t\n\t\t\n\t\tAbouts\n\t\n\nIn this repository, we present Prokaryotic Gener Tasks, a suite of biologically meaningful benchmark tasks in the prokaryotic domain. \n","url":"https://huggingface.co/datasets/GenerTeam/prokaryotic-gener-tasks","creator_name":"Gener Team","creator_url":"https://huggingface.co/GenerTeam","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","mit","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"mmBERT-midtraining-data","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Mid-training Data\n\t\n\n\n\n\n\n\nPhase 2 of 3: High-quality mid-training data mixture (600B tokens) with context extension to 8192 tokens.\n\nThis dataset contains the mid-training phase data used to train all mmBERT encoder models. This phase focuses on higher quality data sources and extends the context length from 1024 to 8192 tokens. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.\n\n\t\n\t\t\n\t\tðŸ“Š Data Composition\n\t\n\n\n\t\n\t\t\nData Sourceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-midtraining-data.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-midtraining-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretraining-data-chunk0","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk0.","url":"https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk0","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"korean_parallel_sentences_v1.1","keyword":"distillation","description":"\n\t\n\t\t\n\t\tDataset Card for Korean Parallel Sentences Ver 1.1\n\t\n\nThis dataset card provides information about the Korean Parallel Sentences Ver 1.1 dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Korean Parallel Sentences Ver 1.1 dataset is a collection of parallel sentences in Korean and English.\nAlthough the factual accuracy of the data is not guaranteed, it has been designed to ensure accurate and consistent translation style between English and Korean.\n\nCurated by:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lemon-mint/korean_parallel_sentences_v1.1.","url":"https://huggingface.co/datasets/lemon-mint/korean_parallel_sentences_v1.1","creator_name":"Lemon Mint","creator_url":"https://huggingface.co/lemon-mint","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","text2text-generation","Korean","English","mit"],"keywords_longer_than_N":true},
	{"name":"mmBERT-decay-data","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tMMBERT Decay Phase Data\n\t\n\n\n\n\n\n\nPhase 3 of 3: Annealed language learning decay phase (100B tokens) with massive multilingual expansion to 1833 languages.\n\n\n\t\n\t\n\t\n\t\tðŸ“Š Data Composition\n\t\n\nNOTE: there are multiple decay data mixtures: this mixture described below is the Decay-Cont mixture. However, the data in this repository is the Decay-Eng. If you are interested in the others, please let me know so I can prioritize it.\n\t\n\t\t\nData Source\nTokens (B)\nPercentage\nDescription\n\n\n\t\t\nFineWeb2â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-decay-data.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-decay-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"LongPage","keyword":"long-context","description":"\n\n\t\n\t\t\n\t\tOverview ðŸš€ðŸ“š\n\t\n\nThe first comprehensive dataset for training AI models to write complete novels with sophisticated reasoning.\nðŸ§  Hierarchical Reasoning Architecture â€” Multi-layered planning traces including character archetypes, story arcs, world rules, and scene breakdowns. A complete cognitive roadmap for long-form narrative construction.\nðŸ“– Complete Novel Coverage â€” From 40,000 to 600,000+ tokens per book, spanning novellas to epic series with consistent quality throughout.\nâš¡â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Pageshift-Entertainment/LongPage.","url":"https://huggingface.co/datasets/Pageshift-Entertainment/LongPage","creator_name":"Pageshift-Entertainment","creator_url":"https://huggingface.co/Pageshift-Entertainment","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","text2text-generation","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"Arabic-OpenHermes-2.5","keyword":"distillation","description":"\n\t\n\t\t\n\t\tDataset Card for \"Arabic-OpenHermes-2.5\"\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the original OpenHermes dataset : teknium/OpenHermes-2.5.\nLanguages: Modern Standard Arabic (MSA)\nApplications: Language Modeling\nMaintainer: Marwa El Kamil & Mohammed Machrouh\nLicense: Apache-2.0\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nArabic-OpenHermes-2.5 is a carefully curated dataset extracted / translated from the OpenHermes-2.5 collection provided by teknium.\n\n\t\n\t\n\t\n\t\tPurposeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Arabic-OpenHermes-2.5.","url":"https://huggingface.co/datasets/2A2I/Arabic-OpenHermes-2.5","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"mmBERT-data-decay-all","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-data-decay-all.","url":"https://huggingface.co/datasets/orionweller/mmBERT-data-decay-all","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"LongWriter-6k-English","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongWriter-6k-English\n\t\n\nLongWriter-6k-English is a filtered version of the LongWriter-6k dataset, containing only the English-language samples. This dataset includes 2,299 instances of long-form text, ranging from 2,000 to 32,000 words, designed to train large language models (LLMs) to handle extended output contexts.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nLanguages: English\nData Size: 2,299 samples\nOutput Length: 2,000 to 32,000 words per sample\n\n\n\t\n\t\t\n\t\tSource\n\t\n\nThis dataset is derived fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/themex1380/LongWriter-6k-English.","url":"https://huggingface.co/datasets/themex1380/LongWriter-6k-English","creator_name":"themex","creator_url":"https://huggingface.co/themex1380","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ArabiBoost","keyword":"pretraining","description":"Dataset\nThe ArabiBoost is the Instruction-tuning or fine-tuning dataset (5.7k samples) for MMLU_SyntheticData is generated using GPT-3.5 Turbo.\nNote: Please note that this is not the translated version of MMLU, it's an entirely independent dataset.\nSubjects\nIt has the data of the following subjects:\n\nHumanities ['Islamic Studies', 'Law', 'History', 'Philosophy']\nLanguage ['Arabic Language', 'Arabic Language (General)', 'Arabic Language (Grammar)']\nOther ['General Knowledge', 'Management'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ashmal/ArabiBoost.","url":"https://huggingface.co/datasets/Ashmal/ArabiBoost","creator_name":"Ashmal Vayani","creator_url":"https://huggingface.co/Ashmal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","Arabic","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2-Pause1","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\n\t\n\nðŸŒ Project Page: https://longbench2.github.io\nðŸ’» Github Repo: https://github.com/THUDM/LongBench\nðŸ“š Arxiv Paper: https://arxiv.org/abs/2412.15204\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1.","url":"https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1","creator_name":"James Begin","creator_url":"https://huggingface.co/JamesBegin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"bb-tt-3-pretrain","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tBiomed-FR-v3 High-Quality Pretraining Dataset\n\t\n\nThis dataset contains French biomedical text annotated with 20 different classification and regression tasks using the rntc/biomed-fr-v2-classifier model.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal samples: 2,782,686\nTotal columns: 41\nAnnotation tasks: 25\nLanguage: French\nDomain: Biomedical/Clinical\nFilter criteria: Filtered for pretraining_suitable >= 0.0 (94.6% of data)\n\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nâœ… Complete annotation coverage: All 20 tasks fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rntc/bb-tt-3-pretrain.","url":"https://huggingface.co/datasets/rntc/bb-tt-3-pretrain","creator_name":"Rian Touchent","creator_url":"https://huggingface.co/rntc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","French","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-Spanish","keyword":"distillation","description":"\nteknium/OpenHermes-2.5 dataset translated to Spanish using the Iker/TowerInstruct-13B-v0.1-EN2ES model. This dataset has a total of 1 Million High-Quality instructions in Spanish!!\nThe original dataset can be found here: https://hf.co/datasets/teknium/OpenHermes-2.5\nI have also added the following datasets:\n\nIker/Document-Translation-en-es\nIker/InstructTranslation-EN-ES\nHelsinki-NLP/opus-100 (en-es, only a few examples to reach 1 million instructions)\nprojecte-aina/RAG_Multilingual(es onlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Iker/OpenHermes-2.5-Spanish.","url":"https://huggingface.co/datasets/Iker/OpenHermes-2.5-Spanish","creator_name":"Iker GarcÃ­a-Ferrero","creator_url":"https://huggingface.co/Iker","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Spanish","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p3-others","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P3\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P3 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p3-others.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p3-others","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","English","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"LLama-405B-Logits","keyword":"distillation","description":"\n\t\n\t\t\n\t\tLlama-405B-Logits Dataset\n\t\n\nThe Llama-405B-Logits Dataset is a curated subset of logits extracted from the Llama-405B model, created to distill high-performance language models such as Arcee AI's SuperNova using DistillKit. This dataset was also instrumental in the training of the groundbreaking INTELLECT-1 model, demonstrating the effectiveness of leveraging distilled knowledge for enhancing model performance.\n\n\t\n\t\t\n\t\n\t\n\t\tAbout the Dataset\n\t\n\nThis dataset contains a carefullyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/LLama-405B-Logits.","url":"https://huggingface.co/datasets/arcee-ai/LLama-405B-Logits","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MMLU_SyntheticData","keyword":"pretraining","description":"Dataset\nMMLU_SyntheticData is generated using GPT-4. The aim of generating this dataset was to generate a similar dataset to MMLU. \nNote: Please note that this is not the translated version of MMLU, it's an entirely independent dataset.\nSubjects\nIt has the data of the following subjects:\n\nHumanities ['Islamic Studies', 'Law', 'History', 'Philosophy']\nLanguage ['Arabic Language', 'Arabic Language (General)', 'Arabic Language (Grammar)']\nOther ['General Knowledge', 'Management', 'Driving Test']â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ashmal/MMLU_SyntheticData.","url":"https://huggingface.co/datasets/Ashmal/MMLU_SyntheticData","creator_name":"Ashmal Vayani","creator_url":"https://huggingface.co/Ashmal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","Arabic","apache-2.0","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"distilled-roleplay","keyword":"distillation","description":"\n\t\n\t\t\n\t\tDistilled Roleplay\n\t\n\nOverview:This dataset consists of short roleplaying conversations generated by prompting various large language models (LLMs) one message at a time. Each conversation begins with a system prompt describing a scenario from a second-person point of view. The dialogue then alternates between first-person user inputs and AI responses in second person.\n\nNumber of roleplay sessions: 100 per model  \nMessages per session: 10 (excluding the system prompt)\n\nModels Included:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/distilled-roleplay.","url":"https://huggingface.co/datasets/agentlans/distilled-roleplay","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"BioCite","keyword":"pretraining","description":"This is the synthetic dataset used for pretraining in the paper Source-Aware Training Enables Knowledge Attribution in Language Models\n. \nStats (number of tokens is computed based on the TinyLLaMa tokenizer):\n\n\t\n\t\t\n\nSize\n\n\n\t\t\nPretraining\n\n\n\n#documents\n100K\n\n\n#facts/sents\n408K\n\n\n#tokens\n5.7M\n\n\navg. sents per doc\n4.1\n\n\navg. tokens per doc\n56.9\n\n\nInstruction tuning\n\n\n\n#examples\n186K\n\n\n#tokens\n3.1M\n\n\n\t\n\n","url":"https://huggingface.co/datasets/mkhalifa/BioCite","creator_name":"Muhammad Khalifa","creator_url":"https://huggingface.co/mkhalifa","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"deepseek-math-dataset","keyword":"distillation","description":"\n\t\n\t\t\n\t\tDeepSeek Math Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe DeepSeek Math Dataset is a high-quality dataset designed for distilling smaller mathematical reasoning models. It is derived from the official DeepSeek-Prover-V1 dataset and tailored to improve the efficiency of lightweight models while preserving strong mathematical problem-solving capabilities.\nThis dataset has been used to distill Qwen2.5-1.5B, achieving impressive performance on mathematical reasoning tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/madaibaba/deepseek-math-dataset.","url":"https://huggingface.co/datasets/madaibaba/deepseek-math-dataset","creator_name":"Alex Hou","creator_url":"https://huggingface.co/madaibaba","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"llava-pretrain-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tLLaVA pretrain -- LCS-558k (refined by Data-Juicer)\n\t\n\nA refined version of LLaVA pretrain dataset (LCS-558k) by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Multimodal Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 115MB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 500,380 (Keep ~89.65% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"S1_QFFT","keyword":"distillation","description":"\n\t\n\t\t\n\t\tðŸ“˜ S1â€“QFFT\n\t\n\nS1â€“QFFT is a question-free version of the original simplescaling/s1K-1.1 dataset, designed for QFFT training workflows.\n\n\t\n\t\t\n\t\tðŸ” Description\n\t\n\nThis dataset discards the original questions and any system instructions, keeping only the reasoning completions as supervision. It is especially useful for models that aim to learn when and how to think, rather than just how to answer.\nThe dataset is fully converted into a format compatible with LLaMA-Factory training.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/lwl-uestc/S1_QFFT.","url":"https://huggingface.co/datasets/lwl-uestc/S1_QFFT","creator_name":"Wanlong Liu","creator_url":"https://huggingface.co/lwl-uestc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Primus-FineWeb","keyword":"pretraining","description":"\nâ­ Please download the dataset from here.\n\n\n\t\n\t\t\n\t\tPRIMUS: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training\n\t\n\n\n\t\n\t\t\n\t\tðŸ¤— Primus-FineWeb\n\t\n\nThe Primus-FineWeb dataset is constructed by filtering cybersecurity-related text from FineWeb, a refined version of Common Crawl. We began by leveraging Primus-Seed, a high-quality dataset of manually curated cybersecurity text, as positive samples. We then sampled ten times the amount of data from FineWeb as negative samplesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/trend-cybertron/Primus-FineWeb.","url":"https://huggingface.co/datasets/trend-cybertron/Primus-FineWeb","creator_name":"Trend Cybertron (Trend Micro)","creator_url":"https://huggingface.co/trend-cybertron","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":null,"first_N":5,"first_N_keywords":["text-generation","English","odc-by","1M<n<10M","arxiv:2502.11191"],"keywords_longer_than_N":true},
	{"name":"multimodal_textbook","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tMultimodal-Textbook-6.5M\n\t\n\n    \n\n\n  \n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThis dataset is for \"2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining\", containing 6.5M images interleaving with 0.8B text from instructional videos.\n\nIt contains pre-training corpus using interleaved image-text format. Specifically, our multimodal-textbook includes 6.5M keyframesextracted from instructional videos, interleaving with 0.8B ASR texts.\nAll the images and text are extracted from onlineâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DAMO-NLP-SG/multimodal_textbook.","url":"https://huggingface.co/datasets/DAMO-NLP-SG/multimodal_textbook","creator_name":"Language Technology Lab at Alibaba DAMO Academy","creator_url":"https://huggingface.co/DAMO-NLP-SG","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","summarization","English","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"TinyDialogues","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tDataset Card for TinyDialogues\n\t\n\nTinyDialogues dataset collected as part of the EMNLP 2024 paper \"Is Child-Directed Speech Effective Training Data for Language Models?\" by Steven Y. Feng, Noah D. Goodman, and Michael C. Frank. For more details, please see Appendices A-C in our paper.\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository: https://github.com/styfeng/TinyDialogues\nPaper: https://aclanthology.org/2024.emnlp-main.1231/\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nFinal training and validation dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/styfeng/TinyDialogues.","url":"https://huggingface.co/datasets/styfeng/TinyDialogues","creator_name":"Steven Feng","creator_url":"https://huggingface.co/styfeng","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-1.5-Instruct-Data","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-1.5 Instruction Data\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset, LLaVA-OneVision-1.5-Instruct, was collected and integrated during the development of LLaVA-OneVision-1.5. LLaVA-OneVision-1.5 is a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. This meticulously curated 22M instruction dataset (LLaVA-OneVision-1.5-Instruct) is part of a comprehensive andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Instruct-Data.","url":"https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Instruct-Data","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2509.23661","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"redpajama-book-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama -- Book (refined by Data-Juicer)\n\t\n\nA refined version of Book dataset in RedPajama by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 91GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 195,983 (Keep ~95.51% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefining Recipe\n\t\n\n#â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-book-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-book-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"DSIR-filtered-pile-50M","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tDataset Card for DSIR-filtered-pile-50M\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a subset of The Pile, selected via the DSIR data selection method. The target distribution for DSIR is the Wikipedia and BookCorpus2 subsets of The Pile.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nEnglish (EN)\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nA train set is provided (51.2M examples) in jsonl format.\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\"contents\": \"Hundreds of soul music enthusiasts from the United Kingdom plan to make their way toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stanford-crfm/DSIR-filtered-pile-50M.","url":"https://huggingface.co/datasets/stanford-crfm/DSIR-filtered-pile-50M","creator_name":"Stanford CRFM","creator_url":"https://huggingface.co/stanford-crfm","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"longbench-v2","keyword":"long context","description":"\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{bai2024longbench2,\n  title={LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks},\n  author={Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},\n  journal={arXiv preprint arXiv:2412.15204},\n  year={2024}\n}\n\n","url":"https://huggingface.co/datasets/jannalu/longbench-v2","creator_name":"Janna","creator_url":"https://huggingface.co/jannalu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"regularization-space","keyword":"preservation-loss-training","description":"\n\t\n\t\t\n\t\tSpace Regularization Images\n\t\n\nA collection of regularization & class instance datasets of space for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","url":"https://huggingface.co/datasets/3ee/regularization-space","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-horse","keyword":"preservation-loss-training","description":"\n\t\n\t\t\n\t\tHorse Regularization Images\n\t\n\nA collection of regularization & class instance datasets of horses for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","url":"https://huggingface.co/datasets/3ee/regularization-horse","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"the-pile-freelaw-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tThe Pile -- FreeLaw (refined by Data-Juicer)\n\t\n\nA refined version of FreeLaw dataset in The Pile by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 45GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 2,942,612 (Keep ~82.61% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefining Recipeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-freelaw-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/the-pile-freelaw-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-cc-2021-04-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama -- CommonCrawl-2021-04 (refined by Data-Juicer)\n\t\n\nA refined version of CommonCrawl-2021-04 dataset in RedPajama by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 284GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 44,724,752 (Keep ~45.23% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-cc-2021-04-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-cc-2021-04-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"the-pile-nih-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tThe Pile -- NIHExPorter (refined by Data-Juicer)\n\t\n\nA refined version of NIHExPorter dataset in The Pile by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 2.0G).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 858,492 (Keep ~91.36% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefiningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-nih-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/the-pile-nih-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"M4LE","keyword":"long context","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nM4LE is a Multi-ability, Multi-range, Multi-task, bilingual benchmark for long-context evaluation. We categorize long-context understanding into five distinct abilities by considering whether it is required to identify single or multiple spans in long contexts based on explicit or semantic hints. Specifically, these abilities are explicit single-span, semantic single-span, explicit multiple-span, semantic multiple-span, and global. Different from previous long-contextâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wckwan/M4LE.","url":"https://huggingface.co/datasets/wckwan/M4LE","creator_name":"Cyrus Kwan","creator_url":"https://huggingface.co/wckwan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","translation","summarization","text-classification","text-retrieval"],"keywords_longer_than_N":true},
	{"name":"astro-classification-redshifts","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tAstroClassification and Redshifts Datasets\n\t\n\n\n\nThis dataset was used for the AstroClassification and Redshifts introduced in Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations. This is a dataset of simulated astronomical time-series (e.g., supernovae, active galactic nuclei), and the task is to classify the object type (AstroClassification) or predict the object's redshift (Redshifts).\n\nRepository: https://github.com/helenqu/connect-later\nPaper: will beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/helenqu/astro-classification-redshifts.","url":"https://huggingface.co/datasets/helenqu/astro-classification-redshifts","creator_name":"Helen Qu","creator_url":"https://huggingface.co/helenqu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"DAPO-Math-17k-Qwen3-235B-A22B-Thinking-2507-rejection-distill","keyword":"distillation","description":"\n\t\n\t\t\n\t\tDAPO-Math-17k-Qwen3-235B-A22B-Thinking-2507-rejection-distill\n\t\n\nA high-quality Chain-of-Thought (CoT) dataset generated using Qwen/Qwen3-235B-A22B-Thinking-2507 with rejection sampling on BytedTsinghua-SIA/DAPO-Math-17k. This dataset is ideal for SFT distillation training to improve mathematical reasoning capabilities of models.\nThe dataset format is compatible with LLaMA-Factory for efficient SFT training.\n\n\t\n\t\n\t\n\t\tFiles\n\t\n\n\ndapo_distill_boxed.json: Single sampling subsetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yang-Zhou/DAPO-Math-17k-Qwen3-235B-A22B-Thinking-2507-rejection-distill.","url":"https://huggingface.co/datasets/Yang-Zhou/DAPO-Math-17k-Qwen3-235B-A22B-Thinking-2507-rejection-distill","creator_name":"YANG ZHOU","creator_url":"https://huggingface.co/Yang-Zhou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"muld_character-archetype-classification","keyword":"long-context","description":"\n\t\n\t\t\n\t\tMulD: Movie Character Type Classification\n\t\n\nthis is the Movie Character Types task from MuLD:\n\nTask: Classify characters as Hero/Protagonist or Villain/Antagonist\nData: Movie scripts matched with Wikipedia plot summaries\nMethod: Amazon Turk annotation based on plot summaries\nAverage length: ~45,000 tokens\nChallenge: Character role understanding from full narrative context\n\n@inproceedings{hudson-al-moubayed-2022-muld,\n    title = \"{M}u{LD}: The Multitask Long Document Benchmark\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/muld_character-archetype-classification.","url":"https://huggingface.co/datasets/pszemraj/muld_character-archetype-classification","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","English","odc-by","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"cuad-deepseek","keyword":"distillation","description":"\n\t\n\t\t\n\t\tCUAD-DeepSeek: Enhanced Legal Contract Understanding Dataset\n\t\n\nCUAD-DeepSeek is an enhanced version of the Contract Understanding Atticus Dataset (CUAD), enriched with expert rationales and reasoning traces provided by the DeepSeek language model. This dataset aims to improve legal contract analysis by providing not just classifications but detailed explanations for why specific clauses belong to particular legal categories.\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose and Scope\n\t\n\nLegal contract review isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zenml/cuad-deepseek.","url":"https://huggingface.co/datasets/zenml/cuad-deepseek","creator_name":"ZenML","creator_url":"https://huggingface.co/zenml","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","theatticusproject/cuad","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p1-fineweb2-langs","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P1\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P1 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p1-fineweb2-langs","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","feature-extraction","multilingual","mit","arxiv:2509.06888"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p2-fineweb2-remaining","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P2\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P2 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p2-fineweb2-remaining.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p2-fineweb2-remaining","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","English","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"cybersec-knowledge-foundation","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tIoT & Cybersecurity Knowledge Graph\n\t\n\nThis dataset is a unified knowledge graph constructed from various Hugging Face datasets focusing on IoT and Cybersecurity domains. It integrates structured and semi-structured data to provide a comprehensive view of network activities, device behaviors, cyberattacks, and building automation systems.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe primary goal of this dataset is to facilitate research and development in areas such as:\n\nIoT Security:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RDLTechworks/cybersec-knowledge-foundation.","url":"https://huggingface.co/datasets/RDLTechworks/cybersec-knowledge-foundation","creator_name":"mosaÃ¯que","creator_url":"https://huggingface.co/RDLTechworks","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1M - 10M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"InfiMM-WebMath-40B","keyword":"pretrain","description":"\n\t\n\t\t\n\t\tInfiMM-WebMath-40B Dataset\n\t\n\nArXiv| PDF\nThis dataset is also discussed in the survey paper A Survey of Deep Learning for Geometry Problem Solving.\nThe accompanying reading list/code for the survey can be found at: https://github.com/majianz/gps-survey\nInfiMM-WebMath-40B is a large-scale, open-source multimodal dataset specifically designed for mathematical reasoning tasks. It incorporates both text and images, extracted from web documents, to advance the pre-training of Multimodalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Infi-MM/InfiMM-WebMath-40B.","url":"https://huggingface.co/datasets/Infi-MM/InfiMM-WebMath-40B","creator_name":"InfiMM","creator_url":"https://huggingface.co/Infi-MM","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","odc-by","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"pretrain","keyword":"pretrain","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"UR5\",\n    \"total_episodes\": 120,\n    \"total_frames\": 17880,\n    \"total_tasks\":4,\n    \"total_videos\": 0,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 30,\n    \"splits\": {\n        \"train\": \"0:120\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/GAOTHU/pretrain.","url":"https://huggingface.co/datasets/GAOTHU/pretrain","creator_name":"é«˜æ¢“æ¶µ","creator_url":"https://huggingface.co/GAOTHU","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"mixed-pretrain-3b","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tMixed Pretraining Dataset (3B Tokens)\n\t\n\nA carefully curated and mixed pretraining dataset containing 3 billion tokens from high-quality educational, mathematical, and programming sources.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset combines three high-quality data sources with specific proportions optimized for language model pretraining:\n\n60% FineWeb-Edu (1.8B tokens): High-quality educational web content\n30% Mathematics (900M tokens): Mathematical problems, solutions, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yxanul/mixed-pretrain-3b.","url":"https://huggingface.co/datasets/Yxanul/mixed-pretrain-3b","creator_name":"David Franco","creator_url":"https://huggingface.co/Yxanul","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"mga-fineweb-edu","keyword":"pretrain","description":"\n\t\n\t\t\n\t\tMassive Genre-Audience Augment Fineweb-Edu Corpus\n\t\n\nThis dataset is a synthetic pretraining corpus described in paper Reformulation for Pretraining Data Augmentation.\n\nOverview of synthesis framework. Our method expands the original corpus through a two-stage synthesis process. \nEach document is reformulated to 5 new documents, achieving 3.9Ã— token number expansion while maintaining diversity through massive (genre, audience) pairs.\n\nWe build MGACorpus based on SmolLM Corpusâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ByteDance-Seed/mga-fineweb-edu.","url":"https://huggingface.co/datasets/ByteDance-Seed/mga-fineweb-edu","creator_name":"ByteDance Seed","creator_url":"https://huggingface.co/ByteDance-Seed","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","English","odc-by","100M - 1B","parquet"],"keywords_longer_than_N":true},
	{"name":"Collective-Corpus","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tðŸ§  Collective Corpus â€” Universal Pretraining + Finetuning Dataset (500B+ Tokens)\n\t\n\n\n\n\nCollective-Corpus is a massive-scale, multi-domain dataset designed to train Transformer-based language models from scratch and finetune them across a wide variety of domains â€” all in one place.\n\n\t\n\t\n\t\n\t\tðŸ“š Dataset Scope\n\t\n\nThis dataset aims to cover the full LLM lifecycle, from raw pretraining to domain-specialized finetuning.\n\t\n\t\t\n\t\t1. Pretraining Corpus\n\t\n\n\nLarge-scale, diverse multilingual textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dignity045/Collective-Corpus.","url":"https://huggingface.co/datasets/dignity045/Collective-Corpus","creator_name":"Dhiraj","creator_url":"https://huggingface.co/dignity045","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","text-classification","summarization","question-answering"],"keywords_longer_than_N":true},
	{"name":"fineweb-100_128k","keyword":"long context","description":"\n\t\n\t\t\n\t\tBEE-spoke-data/fineweb-100_128k\n\t\n\n100 documents from HuggingFaceFW/fineweb that are 128,000 GPT-4 tiktoken tokens or more.\n","url":"https://huggingface.co/datasets/BEE-spoke-data/fineweb-100_128k","creator_name":"BEEspoke Data","creator_url":"https://huggingface.co/BEE-spoke-data","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","feature-extraction","HuggingFaceFW/fineweb","English","odc-by"],"keywords_longer_than_N":true},
	{"name":"open-genome","keyword":"long context","description":"\n\t\n\t\t\n\t\tDataset organization\n\t\n\nThe OpenGenome dataset is organized in 2 stages, where stage 1 has context length 8k and stage 2 has context length 131k.  Each stage has their own datasplits.\n- stage1\n  - train\n  - validation\n  - test\n\n- stage2\n  - train\n  - validation\n  - test\n\n\n\t\n\t\t\n\t\tInstructions to download\n\t\n\nYou can load a dataset using HF's API, with an example below.\nfrom datasets import load_dataset\n\nstage1_data = load_dataset(\"LongSafari/open-genome\", 'stage1')\n\n# access just theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LongSafari/open-genome.","url":"https://huggingface.co/datasets/LongSafari/open-genome","creator_name":"LongSafari","creator_url":"https://huggingface.co/LongSafari","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"Copernicus-Pretrain","keyword":"pretrain","description":"\n\t\n\t\t\n\t\tDataset Card for Copernicus-Pretrain\n\t\n\n\n\nCopernicus-Pretrain is a large-scale EO pretraining dataset with 18.7M aligned images covering all major Sentinel missions (S1,2,3,5P).\nOfficially named Copernicus-Pretrain, also referred to as SSL4EO-S (\"S\" means Sentinel), as an extension of SSL4EO-S12 to the whole Sentinel series.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\nCopernicus-Pretrain contains 18.7M aligned imagery from all major Sentinel missions in operation (Sentinel-1 SAR, Sentinel-2â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wangyi111/Copernicus-Pretrain.","url":"https://huggingface.co/datasets/wangyi111/Copernicus-Pretrain","creator_name":"Yi Wang","creator_url":"https://huggingface.co/wangyi111","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-feature-extraction","cc-by-4.0","10M<n<100M","Geospatial"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretraining-data-chunk2","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk2.","url":"https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk2","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"LongCite-45k","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongCite-45k\n\t\n\n\n  ðŸ¤— [LongCite Dataset]  â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongCite Paper] \n\n\nLongCite-45k dataset contains 44,600 long-context QA instances paired with sentence-level citations (both English and Chinese, up to 128,000 words). The data can support training long-context LLMs to generate response and fine-grained citations within a single output.\n\n\t\n\t\t\n\t\n\t\n\t\tData Example\n\t\n\nEach instance in LongCite-45k consists of an instruction, a long context (divided into sentences), a userâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongCite-45k.","url":"https://huggingface.co/datasets/THUDM/LongCite-45k","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LongRewardBench","keyword":"long-context","description":"\n\t\n\t\t\n\t\tðŸ“œ LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling\n\t\n\n\nPaper: https://arxiv.org/pdf/2510.06915code:  https://github.com/LCM-Lab/LongRM\n\n\nModels:  \n\nðŸ¤– Generative RM: LCM_group/LongReward_Qwen3-8B  \nðŸ” Discriminative RM: LCM_group/LongReward_Skywork-Reward-V2-Llama-3.1-8B\n\nPushing the limits of reward modeling beyond 128K tokens â€” with memory-efficient training and a new benchmark for long-context reward model.\n\n\n\t\t\n\t\tIntroduction\n\t\n\nLong-RewardBench is the firstâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LCM-Lab/LongRewardBench.","url":"https://huggingface.co/datasets/LCM-Lab/LongRewardBench","creator_name":"Long-Context Model Laboratory","creator_url":"https://huggingface.co/LCM-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Bhandara","keyword":"pretrain","description":"\n\t\n\t\t\n\t\tA Pretraining Hindi Dataset for Diverse Indian NLP Tasks\n\t\n\nThis dataset contains over 12,000 rows and 7 million words of text specifically generated for pretraining NLP models on Hindi language tasks. It was created using the Bard API, ensuring high-quality and diverse content.\n\n\t\n\t\t\n\t\tKey Feature: Rich India-Specific Data\n\t\n\nA distinguishing characteristic of this dataset is its inclusion of a substantial amount of content related to India. This makes it valuable for training modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Tensoic/Bhandara.","url":"https://huggingface.co/datasets/Tensoic/Bhandara","creator_name":"Tensoic AI","creator_url":"https://huggingface.co/Tensoic","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Hindi","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"core-sdo","keyword":"pretraining","description":"\n\n\t\n\t\t\n\t\tML-Ready Multi-Modal Image Dataset from SDO\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset provides machine learning (ML)-ready solar data curated from NASAâ€™s Solar Dynamics Observatory (SDO), covering observations from May 13, 2010, to Dec 31, 2024. It includes Level-1.5 processed data from: Atmospheric Imaging Assembly (AIA)\nand Helioseismic and Magnetic Imager (HMI). \nThe dataset is designed to facilitate large-scale learning applications in heliophysics, such as space weather forecastingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nasa-ibm-ai4science/core-sdo.","url":"https://huggingface.co/datasets/nasa-ibm-ai4science/core-sdo","creator_name":"NASA-IBM AI4Science","creator_url":"https://huggingface.co/nasa-ibm-ai4science","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US","Helio","Pretraining"],"keywords_longer_than_N":true},
	{"name":"TheBlueScrubs-v1-fixed","keyword":"pretraining","description":"\n\t\n\t\t\n\t\topenmed-community/TheBlueScrubs-v1-fixed\n\t\n\n\n\t\n\t\t\n\t\tWhat is this?\n\t\n\nTheBlueScrubs-v1-fixed is a maintenance fork of the upstream TheBlueScrubs/TheBlueScrubs-v1 train split that resolves a schema bug in the meta column.In the original train files, some rows serialized meta incorrectly (appearing as the literal string \"dict\"). This fork re-exports the entire train split without meta column, preserving text field and values.\n\nDocument count: 11,080,331 texts (train)  \nTokens (upstreamâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/TheBlueScrubs-v1-fixed.","url":"https://huggingface.co/datasets/openmed-community/TheBlueScrubs-v1-fixed","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretrain-p3-others","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Pre-training Data P3\n\t\n\n\n\n\n\n\nPhase 1 of 3: Diverse multilingual pre-training data mixture (trained for 2.3T tokens) used to train the mmBERT model suite.\n\nNOTE: this is only P3 of the pre-training data due to HF limits, you need to download and combine all three into one folderThis dataset contains the pre-training phase data used to train all mmBERT encoder models. The data is provided in MDS format ready for use with Composer and the ModernBERT training repository.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p3-others.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p3-others","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","English","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"coreference-challenge","keyword":"long-context","description":"\n\n\t\n\t\t\n\t\tPI-LLM Bench: The Core Retrieval Challenge Behind MRCR\n\t\n\n\nICML 2025 Long-Context Foundation Models Workshop Accepted.\n\nA simple context interference evaluation.\n\nUpdate: This dataset is integrated into Moonshot AI(Kimi)'s internal benchmarking framework for assessing ** tracking capacity and context interference in LLM/agents**.\nUpdate:Sept.6-mergerd into Moonshot/Kimi AI's internal eval tools and under review by a xAI(Grok)'s' eval team\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tTL;DR\n\t\n\nWe identify a taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/giantfish-fly/coreference-challenge.","url":"https://huggingface.co/datasets/giantfish-fly/coreference-challenge","creator_name":"c.p. wang","creator_url":"https://huggingface.co/giantfish-fly","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"the-pile-pubmed-central-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tThe Pile -- PubMed Central (refined by Data-Juicer)\n\t\n\nA refined version of PubMed Central dataset in The Pile by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 83G).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 2,694,860 (Keep ~86.96% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-pubmed-central-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/the-pile-pubmed-central-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"regularization-woman","keyword":"preservation-loss-training","description":"\n\t\n\t\t\n\t\tWoman Regularization Images\n\t\n\nA collection of regularization & class instance datasets of women for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","url":"https://huggingface.co/datasets/3ee/regularization-woman","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-forest","keyword":"preservation-loss-training","description":"\n\t\n\t\t\n\t\tForest Regularization Images\n\t\n\nA collection of regularization & class instance datasets of forests for the Stable Diffusion 1.5 model to use for DreamBooth prior preservation loss training.\n","url":"https://huggingface.co/datasets/3ee/regularization-forest","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-creature","keyword":"preservation-loss-training","description":"\n\t\n\t\t\n\t\tCreature Regularization Images\n\t\n\nA collection of regularization & class instance datasets of creatures for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","url":"https://huggingface.co/datasets/3ee/regularization-creature","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"mbpp-longcontext","keyword":"long-context","description":"\n\t\n\t\t\n\t\tMBPP Long-Context Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nMBPP Long-Context is a benchmark dataset that combines coding problems from the MBPP (Mostly Basic Python Problems) dataset with long-context distractors from BABILong. This dataset evaluates code generation performance under long-context conditions, testing whether models can maintain coding ability with stuffed context.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Fields\n\t\n\nEach sample contains:\n\n\t\n\t\t\n\t\n\t\n\t\tOriginal MBPP Fieldsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jannalu/mbpp-longcontext.","url":"https://huggingface.co/datasets/jannalu/mbpp-longcontext","creator_name":"Janna","creator_url":"https://huggingface.co/jannalu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","arrow"],"keywords_longer_than_N":true},
	{"name":"redpajama-c4-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama -- C4 (refined by Data-Juicer)\n\t\n\nA refined version of C4 dataset in RedPajama by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 832GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 344,491,171 (Keep ~94.42% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefining Recipe\n\t\n\n#â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-c4-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-c4-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-pile-stackexchange-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama & The Pile -- StackExchange (refined by Data-Juicer)\n\t\n\nA refined version of StackExchange dataset in RedPajama & The Pile by Data-Juicer. Removing some \"bad\" samples from the original merged dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 71GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 26,309,203\t (Keep ~57.89% from theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-pile-stackexchange-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-pile-stackexchange-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-stack-code-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama & TheStack -- Github Code (refined by Data-Juicer)\n\t\n\nA refined version of Github Code dataset in RedPajama & TheStack by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 232GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 49,279,344 (Keep ~52.09% from the originalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-stack-code-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-stack-code-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-cc-2022-05-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama -- CommonCrawl-2022-05 (refined by Data-Juicer)\n\t\n\nA refined version of CommonCrawl-2022-05 dataset in RedPajama by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 265GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 42,648,496 (Keep ~45.34% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-cc-2022-05-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-cc-2022-05-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"LongReward-10k","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongReward-10k\n\t\n\n\n  ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongReward Paper] \n\n\nLongReward-10k dataset contains 10,000 long-context QA instances (both English and Chinese, up to 64,000 words). \nThe sft split contains SFT data generated by GLM-4-0520, following the self-instruct method in LongAlign. Using this split, we supervised fine-tune two models: LongReward-glm4-9b-SFT and LongReward-llama3.1-8b-SFT, which are based on GLM-4-9B and Meta-Llama-3.1-8B, respectively. \nThe dpo_glm4_9b andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zai-org/LongReward-10k.","url":"https://huggingface.co/datasets/zai-org/LongReward-10k","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"radon-test-long_context","keyword":"long-context","description":"\n\t\n\t\t\n\t\tradon-test-long_context\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nLong context test dataset for RADON model evaluation with extended text samples\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tLoad Dataset\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MagistrTheOne/radon-test-long_context\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tUse with RADON Model\n\t\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load RADON model\nmodel = AutoModelForCausalLM.from_pretrained(\"MagistrTheOne/RadonSAI\")\ntokenizer =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagistrTheOne/radon-test-long_context.","url":"https://huggingface.co/datasets/MagistrTheOne/radon-test-long_context","creator_name":"Maga","creator_url":"https://huggingface.co/MagistrTheOne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","original","Russian","English"],"keywords_longer_than_N":true},
	{"name":"regularization-tiger","keyword":"preservation-loss-training","description":"\n\t\n\t\t\n\t\tTiger Regularization Images\n\t\n\nA collection of regularization & class instance datasets of tigers for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","url":"https://huggingface.co/datasets/3ee/regularization-tiger","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"mmBERT-decay-data","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tMMBERT Decay Phase Data\n\t\n\n\n\n\n\n\nPhase 3 of 3: Annealed language learning decay phase (100B tokens) with massive multilingual expansion to 1833 languages.\n\n\n\t\n\t\n\t\n\t\tðŸ“Š Data Composition\n\t\n\nNOTE: there are multiple decay data mixtures: this mixture described below is the Decay-Cont mixture. However, the data in this repository is the Decay-Eng. If you are interested in the others, please let me know so I can prioritize it.\n\t\n\t\t\nData Source\nTokens (B)\nPercentage\nDescription\n\n\n\t\t\nFineWeb2â€¦ See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/mmBERT-decay-data.","url":"https://huggingface.co/datasets/jhu-clsp/mmBERT-decay-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tStackMathQA\n\t\n\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\n\n\t\n\t\t\n\t\tConfigs\n\t\n\nconfigs:\n- config_name: stackmathqa1600k\n  data_files: data/stackmathqa1600k/all.jsonl\n  default: true\n- config_name: stackmathqa800k\n  data_files:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/StackMathQA.","url":"https://huggingface.co/datasets/agicorp/StackMathQA","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"the-pile-philpaper-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tThe Pile -- PhilPaper (refined by Data-Juicer)\n\t\n\nA refined version of PhilPaper dataset in The Pile by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 1.7GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 29,117 (Keep ~88.82% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefiningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-philpaper-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/the-pile-philpaper-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-cc-2019-30-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama -- CommonCrawl-2019-30 (refined by Data-Juicer)\n\t\n\nA refined version of CommonCrawl-2019-30 dataset in RedPajama by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 240GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 36,557,283 (Keep ~45.08% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-cc-2019-30-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-cc-2019-30-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"the-pile-uspto-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tThe Pile -- USPTO (refined by Data-Juicer)\n\t\n\nA refined version of USPTO dataset in The Pile by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 18G).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 4,516,283 (Keep ~46.77% from the original dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tRefining Recipe\n\t\n\n#â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/the-pile-uspto-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/the-pile-uspto-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-cc-2020-05-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama -- CommonCrawl-2020-05 (refined by Data-Juicer)\n\t\n\nA refined version of CommonCrawl-2020-05 dataset in RedPajama by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 297GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 42,612,596 (Keep ~46.90% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-cc-2020-05-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-cc-2020-05-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"redpajama-cc-2023-06-refined-by-data-juicer","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tRedPajama -- CommonCrawl-2023-06 (refined by Data-Juicer)\n\t\n\nA refined version of CommonCrawl-2023-06 dataset in RedPajama by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 310GB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 50,643,699 (Keep ~45.46% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/redpajama-cc-2023-06-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/redpajama-cc-2023-06-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Marathon","keyword":"long context","description":"\n\t\n\t\t\n\t\tDataset Card for Marathon\n\t\n\n\n\t\n\t\t\n\t\tRelease\n\t\n\n\n[2024/05/15] ðŸ”¥ Marathon is accepted by ACL 2024 Main Conference.\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMarathon benchmark is a new long-context multiple-choice benchmark, mainly based on LooGLE, with some original data from LongBench. The context length can reach up to 200K+. Marathon benchmark comprises six tasks: Comprehension and Reasoning, Multiple Information Retrieval, Timeline Reorder, Computation, Passage Retrieval, and Short Dependencyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Lemoncoke/Marathon.","url":"https://huggingface.co/datasets/Lemoncoke/Marathon","creator_name":"Lei Zhang","creator_url":"https://huggingface.co/Lemoncoke","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","open-domain-qa","no-annotation","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"dParallel_LLaDA_Distill_Data","keyword":"distillation","description":"\n\t\n\t\t\n\t\tdParallel-LLaDA-Distill Dataset:\n\t\n\nThis dataset is used for the certainty-forcing distillation process in dParallel. We use prompts from publicly available training datasets and let the pretrained model generate its own responses as training data. For LLaDA-8B-Instruct, we sample prompts from the GSM8K, PRM12K training set, and part of the Numina-Math dataset. We generate target trajectories using a semi-autoregressive strategy with a sequence length of 256 and block length of 32. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zigeng/dParallel_LLaDA_Distill_Data.","url":"https://huggingface.co/datasets/Zigeng/dParallel_LLaDA_Distill_Data","creator_name":"Zigeng Chen","creator_url":"https://huggingface.co/Zigeng","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"mmBERT-data-midtraining","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-data-midtraining.","url":"https://huggingface.co/datasets/orionweller/mmBERT-data-midtraining","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"Document-Translation-en-es","keyword":"distillation","description":"This dataset contains 10533 news articles from ELiRF/dacsa translated from Spanish to English using GPT-3.5-turbo. The dataset is intended to be used for training a model to translate text from English to Spanish and vicerversa. The dataset is also usefull to evaluate document level machine translation models.\nWe use the following prompt\n\ndef get_conversation(text: str, id: int) -> str:\n    messages = {\n        \"custom_id\": str(id),\n        \"method\": \"POST\",\n        \"url\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Iker/Document-Translation-en-es.","url":"https://huggingface.co/datasets/Iker/Document-Translation-en-es","creator_name":"Iker GarcÃ­a-Ferrero","creator_url":"https://huggingface.co/Iker","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","English","Spanish","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"longbench-v2","keyword":"long context","description":"\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{bai2024longbench2,\n  title={LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks},\n  author={Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},\n  journal={arXiv preprint arXiv:2412.15204},\n  year={2024}\n}\n\n","url":"https://huggingface.co/datasets/recursal/longbench-v2","creator_name":"recursal","creator_url":"https://huggingface.co/recursal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-multihiertt","keyword":"long-context","description":"\n\t\n\t\t\n\t\tAveniBench: MultiHiertt\n\t\n\nMultiHiertt split used in the AveniBench.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is made available under the MIT license.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nAveniBench\nTDB\n\nMultiHiertt\n@inproceedings{zhao-etal-2022-multihiertt,\n    title = \"{M}ulti{H}iertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data\",\n    author = \"Zhao, Yilun  and\n      Li, Yunxiang  and\n      Li, Chenying  and\n      Zhang, Rui\",\n    booktitle = \"Proceedings of the 60th Annual Meeting ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-multihiertt.","url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-multihiertt","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ArabicText-Large","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tArabicText-Large: High-Quality Arabic Corpus for LLM Training\n\t\n\n\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nArabicText-Large is a comprehensive, high-quality Arabic text corpus comprising 743,288 articles with over 244 million words, specifically curated for Large Language Model (LLM) training and fine-tuning. This dataset represents one of the largest publicly available Arabic text collections for machine learning research.\nThis corpus addresses the critical shortage of high-quality Arabic NLPâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jr23xd23/ArabicText-Large.","url":"https://huggingface.co/datasets/Jr23xd23/ArabicText-Large","creator_name":"Jaber","creator_url":"https://huggingface.co/Jr23xd23","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","fill-mask","text-classification","Arabic","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MileBench","keyword":"long-context","description":"\n\t\n\t\t\n\t\tMileBench\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs. \nThis benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation. \nWe establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMsâ€™ long-context adaptation capacity and their ability to completetasks in long-context scenarios\n \n\nToâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/MileBench.","url":"https://huggingface.co/datasets/FreedomIntelligence/MileBench","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","text-generation","image-to-text","video-classification"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretraining-data-chunk1","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk1.","url":"https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk1","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"smclm-10M-sentence-corpus","keyword":"pretraining","description":"@ARTICLE{11068992,\n  author={PereÅ‚kiewicz, MichaÅ‚ and Dadas, SÅ‚awomir and PoÅ›wiata, RafaÅ‚},\n  journal={IEEE Access}, \n  title={SMCLM: Semantically Meaningful Causal Language Modeling for Autoregressive Paraphrase Generation}, \n  year={2025},\n  volume={},\n  number={},\n  pages={1-1},\n  keywords={Semantics;Transformers;Training;Encoding;Decoding;Syntactics;Measurement;Computational modeling;Autoencoders;Adaptation models;Autoregressive models;paraphrase generation;text embeddings;semanticallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mmichall/smclm-10M-sentence-corpus.","url":"https://huggingface.co/datasets/mmichall/smclm-10M-sentence-corpus","creator_name":"MichaÅ‚ PereÅ‚kiewicz","creator_url":"https://huggingface.co/mmichall","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","text","Text"],"keywords_longer_than_N":true},
	{"name":"OpenHermes-2.5-Filtered","keyword":"distillation","description":"\n\t\n\t\t\n\t\tThis is the teknium/OpenHermes-2.5 dataset with 2,697 censored lines removed using my uncensored code found bellow.\n\t\n\n\nhttps://huggingface.co/datasets/Replete-AI/data_processing_code\n\n\n\t\n\t\t\n\t\tThank you teknium for the original dataset, you can find it bellow.\n\t\n\n\nhttps://huggingface.co/datasets/teknium/OpenHermes-2.5\n\n\n\t\n\t\t\n\t\tThis is the same version of Open-Hermes-2.5 that was used in code_bagel_hermes-2.5 found bellow:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Replete-AI/OpenHermes-2.5-Filtered.","url":"https://huggingface.co/datasets/Replete-AI/OpenHermes-2.5-Filtered","creator_name":"Replete-AI","creator_url":"https://huggingface.co/Replete-AI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"mmBERT-pretraining-data-chunk3","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk3.","url":"https://huggingface.co/datasets/orionweller/mmBERT-pretraining-data-chunk3","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"LongReward-10k","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongReward-10k\n\t\n\n\n  ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongReward Paper] \n\n\nLongReward-10k dataset contains 10,000 long-context QA instances (both English and Chinese, up to 64,000 words). \nThe sft split contains SFT data generated by GLM-4-0520, following the self-instruct method in LongAlign. Using this split, we supervised fine-tune two models: LongReward-glm4-9b-SFT and LongReward-llama3.1-8b-SFT, which are based on GLM-4-9B and Meta-Llama-3.1-8B, respectively. \nThe dpo_glm4_9b andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongReward-10k.","url":"https://huggingface.co/datasets/THUDM/LongReward-10k","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ida-reasoning-model","keyword":"distillation","description":"\n\t\n\t\t\n\t\tIDA Reasoning Model\n\t\n\nThis model was trained using Imitation, Distillation, and Amplification (IDA) on multiple reasoning datasets.\n\n\t\n\t\t\n\t\tTraining Details\n\t\n\n\nTeacher Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\nStudent Model: Qwen/Qwen3-1.7B\nDatasets: 4 reasoning datasets\nTotal Samples: 600\nTraining Method: IDA (Iterative Distillation and Amplification)\n\n\n\t\n\t\t\n\t\tDatasets Used\n\t\n\n\ngsm8k\nHuggingFaceH4/MATH-500\nMuskumPillerum/General-Knowledge\nSAGI-1/reasoningData_200kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ziadrone/ida-reasoning-model.","url":"https://huggingface.co/datasets/ziadrone/ida-reasoning-model","creator_name":"drone","creator_url":"https://huggingface.co/ziadrone","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"LongWriter-6k","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongWriter-6k\n\t\n\n\n  ðŸ¤— [LongWriter Dataset]  â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongWriter Paper] \n\n\nLongWriter-6k dataset contains 6,000 SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese). The data can support training LLMs to extend their maximum output window size to 10,000+ words.\n\n\t\n\t\t\n\t\n\t\n\t\tAll Models\n\t\n\nWe open-sourced the following list of models trained on LongWriter-6k:\n\n\t\n\t\t\nModel\nHuggingface Repo\nDescription\n\n\n\t\t\nLongWriter-glm4-9b\nðŸ¤—â€¦ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongWriter-6k.","url":"https://huggingface.co/datasets/THUDM/LongWriter-6k","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"animepics","keyword":"pretrain","description":"\n\t\n\t\t\n\t\tAnimePics\n\t\n\nThis dataset is a pure image dataset in WebDataset format, designed for pre-training anime-style models. \nIt can also be used to evaluate the effect of additional pre-training on backbones trained primarily on real-world images when applied to datasets of a completely different nature. \nThe dataset is designed to be continuously updated, leveraging the features of WebDataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe animepics dataset is a large-scaleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zenless-lab/animepics.","url":"https://huggingface.co/datasets/zenless-lab/animepics","creator_name":"Zenless Lab","creator_url":"https://huggingface.co/zenless-lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-feature-extraction","mit","100M<n<1B","ðŸ‡ºðŸ‡¸ Region: US","anime"],"keywords_longer_than_N":true},
	{"name":"next-kmer-prediction","keyword":"long-context","description":"\n\t\n\t\t\n\t\tNext K-mer Prediction\n\t\n\n\n\t\n\t\t\n\t\tAbouts\n\t\n\nThe Next K-mer Prediction task is a zero-shot evaluation method introduced in the GENERator paper to assess the quality of pretrained models. It involves inputting a sequence segment into the model and having it predict the next K base pairs. The predicted sequence is then compared to the actual sequence to assess accuracy.\n\nSequence: The input sequence has a maximum length of 96k base pairs (bp). You can control the number of input tokens byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GenerTeam/next-kmer-prediction.","url":"https://huggingface.co/datasets/GenerTeam/next-kmer-prediction","creator_name":"Gener Team","creator_url":"https://huggingface.co/GenerTeam","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"DCLM-10B-Qwen2-binidx","keyword":"distillation","description":"This repository contains the DCLM-10B-Qwen2-binidx dataset, a large-scale text corpus used as training data for the RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale protocol.\nRADLADS proposes a method for rapidly converting traditional softmax attention transformers into efficient linear attention decoder models. This dataset is crucial for the distillation process, enabling the conversion of large language models like Qwen2.5 into linear attention variants withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/recursal/DCLM-10B-Qwen2-binidx.","url":"https://huggingface.co/datasets/recursal/DCLM-10B-Qwen2-binidx","creator_name":"recursal","creator_url":"https://huggingface.co/recursal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","apache-2.0","arxiv:2505.03005","ðŸ‡ºðŸ‡¸ Region: US","linear-attention"],"keywords_longer_than_N":true},
	{"name":"wiki-yo","keyword":"pretrain","description":"Wikipedia Yoruba dump 2024\n","url":"https://huggingface.co/datasets/mxronga/wiki-yo","creator_name":"Maro jos","creator_url":"https://huggingface.co/mxronga","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","Yoruba","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"cultura-x-deduped-yoruba","keyword":"pretrain","description":"Deduplicated and extended cultura-x yoruba dataset\n","url":"https://huggingface.co/datasets/mxronga/cultura-x-deduped-yoruba","creator_name":"Maro jos","creator_url":"https://huggingface.co/mxronga","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Yoruba","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"DBpediaOntoTrain","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tðŸ§  DBpediaOntoTrain: A Quality-Segmented Ontology Dataset for LLM Pretraining\n\t\n\n\n\t\n\t\t\n\t\tðŸ“˜ Overview\n\t\n\nDBpediaOntoTrain is a dataset of 1,766 OWL ontologies in Turtle format, extracted from DBpedia Archivo and prepared for continual pretraining of Large Language Models (LLMs) in ontology generation and completion tasks.\nEach ontology is analyzed using a set of semantic quality metrics, tokenized using the LLaMA 3.2 tokenizer, and sorted by Quality Score (QS). The dataset includesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/miquelCanal/DBpediaOntoTrain.","url":"https://huggingface.co/datasets/miquelCanal/DBpediaOntoTrain","creator_name":"Miquel Canal ","creator_url":"https://huggingface.co/miquelCanal","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","ðŸ‡ºðŸ‡¸ Region: US","ontology"],"keywords_longer_than_N":true},
	{"name":"longwriter-6k-filtered","keyword":"long context","description":"\n\t\n\t\t\n\t\tLongWriter-6k-Filtered\n\t\n\n\n  ðŸ¤– [LongWriter Dataset]  â€¢ ðŸ’» [Github Repo] â€¢ ðŸ“ƒ [LongWriter Paper] â€¢ ðŸ“ƒ [Tech report]\n\n\nlongwriter-6k-filtered dataset contains 666 filtered examples SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese) based on LongWriter-6k.The data can support training LLMs to extend their maximum output window size to 10,000+ words with low computational cost.\nThe tech report is available at Minimum Tuning to Unlock Long Outputâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lenML/longwriter-6k-filtered.","url":"https://huggingface.co/datasets/lenML/longwriter-6k-filtered","creator_name":"len","creator_url":"https://huggingface.co/lenML","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"LLMs-First-Task","keyword":"long-context","description":"\nSuper easy task for humans  that All SOTA LLM fail to retrieve the correct answer from context. Including SOTA models: GPT5, Grok4, DeepSeek, Gemini 2.5PRO, Mistral, Llama4...etc \n\nICML 2025 Long-Context Foundation Models Workshop Accepted.(https://arxiv.org/abs/2506.08184)\nUpdate: This dataset is integrated into Moonshot AI(Kimi)'s internal benchmarking framework for assessing ** tracking capacity and context interference in LLM/agents**.\nUpdate:Sept.6-mergerd into Moonshot/Kimi AI'sâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/giantfish-fly/LLMs-First-Task.","url":"https://huggingface.co/datasets/giantfish-fly/LLMs-First-Task","creator_name":"c.p. wang","creator_url":"https://huggingface.co/giantfish-fly","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"SDO_training","keyword":"pretraining","description":"\n\n\t\n\t\t\n\t\tðŸŒž SDO ML-Ready Dataset: AIA and HMI Level-1.5\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset provides machine learning (ML)-ready solar data curated from NASAâ€™s Solar Dynamics Observatory (SDO), covering observations from May 13, 2010, to July 31, 2024. It includes Level-1.5 processed data from:\n\nAtmospheric Imaging Assembly (AIA): \nHelioseismic and Magnetic Imager (HMI):\n\nThe dataset is designed to facilitate large-scale ML applications in heliophysics, such as solar activity forecastingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nasa-ibm-ai4science/SDO_training.","url":"https://huggingface.co/datasets/nasa-ibm-ai4science/SDO_training","creator_name":"NASA-IBM AI4Science","creator_url":"https://huggingface.co/nasa-ibm-ai4science","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","n>1T","ðŸ‡ºðŸ‡¸ Region: US","Helio","Foundation_model"],"keywords_longer_than_N":true},
	{"name":"pi-llm-bench","keyword":"long-context","description":"\n\n\t\n\t\t\n\t\tPI-LLM Bench: The Core Retrieval Challenge Behind MRCR\n\t\n\nICML 2025 Long-Context Foundation Models Workshop Accepted.\nA simple context interference evaluation.\n\nAdoption (Aug 31, 2025): This dataset is integrated into a top-5 open-weight model companyâ€™s internal benchmarking framework for assessing ** tracking capacity and context interference in agents**.\nUpdate:Sept.6-mergerd into Moonshot/Kimi AI's internal eval tools and under review by a leading properiety model's eval teamâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cog2ai/pi-llm-bench.","url":"https://huggingface.co/datasets/Cog2ai/pi-llm-bench","creator_name":"Cog2 AI","creator_url":"https://huggingface.co/Cog2ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"gener-tasks","keyword":"long-context","description":"\n\t\n\t\t\n\t\tGener Tasks\n\t\n\n\n\t\n\t\t\n\t\tAbouts\n\t\n\nThe Gener Tasks currently includes 2 subtasks:\n\nThe gene classification task assesses the model's ability to understand short to medium-length sequences. It includes six different gene types and control samples drawn from non-gene regions, with balanced sampling from six distinct eukaryotic taxonomic groups in RefSeq. The classification goal is to predict the gene type. \nThe taxonomic classification task is designed to assess the model's comprehensionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GenerTeam/gener-tasks.","url":"https://huggingface.co/datasets/GenerTeam/gener-tasks","creator_name":"Gener Team","creator_url":"https://huggingface.co/GenerTeam","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"en-fr-debut-kit","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tDÃ©but Kit\n\t\n\n\n  English\nA dataset for training English-French bilingual chatbots\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nDÃ©but Kit is a comprehensive dataset designed to facilitate the development of English-French bilingual chatbots. It covers three crucial stages of model development:\n\nPretraining\nSupervised Fine-Tuning (SFT)\nDirect Preference Optimization (DPO)\n\nEach stage features a balanced mix of English and French content, ensuring robust bilingual capabilities.\n\n\t\n\t\t\n\t\tDataset Detailsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/en-fr-debut-kit.","url":"https://huggingface.co/datasets/agentlans/en-fr-debut-kit","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","French","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mmBERT-data-decay-eng","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tmmBERT Training Data (Ready-to-Use)\n\t\n\n\n\n\n\n\nComplete Training Dataset: Pre-randomized and ready-to-use multilingual training data (3T tokens) for encoder model pre-training.\n\nThis dataset is part of the complete, pre-shuffled training data used to train the mmBERT encoder models. Unlike the individual phase datasets, this version is ready for immediate use but the mixture cannot be modified easily. The data is provided in decompressed MDS format ready for use with ModernBERT's Composerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/orionweller/mmBERT-data-decay-eng.","url":"https://huggingface.co/datasets/orionweller/mmBERT-data-decay-eng","creator_name":"Orion Weller","creator_url":"https://huggingface.co/orionweller","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","mit","arxiv:2509.06888","ðŸ‡ºðŸ‡¸ Region: US","pretraining"],"keywords_longer_than_N":true},
	{"name":"MAmmoTH-VL-Instruct-12M","keyword":"pretraining","description":"\n\t\n\t\t\n\t\tMAmmoTH-VL-Instruct-12M used in MoCa Pre-training\n\t\n\nðŸ  Homepage | ðŸ’» Code | ðŸ¤– MoCa-Qwen25VL-7B | ðŸ¤– MoCa-Qwen25VL-3B | ðŸ“š Datasets | ðŸ“„ Paper\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThis is a VQA style dataset used in the modality-aware continual pre-training of MoCa models. It is adapted from MAmmoTH-VL-Instruct-12M by concatenating prompts and responses.\nThe dataset consists of interleaved multimodal examples. text is a string containing text while imagesare image binaries that can be loadedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/moca-embed/MAmmoTH-VL-Instruct-12M.","url":"https://huggingface.co/datasets/moca-embed/MAmmoTH-VL-Instruct-12M","creator_name":"MoCa","creator_url":"https://huggingface.co/moca-embed","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"yoruba-proverbs-parallel-corpora","keyword":"pretrain","description":"Parralel corpora for yoruba to english.\nSource: http://yoruba.unl.edu/yoruba1.html\n","url":"https://huggingface.co/datasets/mxronga/yoruba-proverbs-parallel-corpora","creator_name":"Maro jos","creator_url":"https://huggingface.co/mxronga","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Yoruba","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true}
]
;
