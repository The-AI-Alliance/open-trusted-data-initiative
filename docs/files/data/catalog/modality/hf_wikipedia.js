const data_for_modality_wikipedia = 
[
	{"name":"OpenDataGen-factuality-en-v0.1","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thoddnn/OpenDataGen-factuality-en-v0.1","creator_name":"Thomas","creator_url":"https://huggingface.co/thoddnn","description":"This synthetic dataset was generated using the Open DataGen Python library. (https://github.com/thoddnn/open-datagen)\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMethodology:\\n\\t\\n\\n\\nRetrieve random article content from the HuggingFace Wikipedia English dataset.\\nConstruct a Chain of Thought (CoT) to generate a Multiple Choice Question (MCQ).\\nUtilize a Large Language Model (LLM) to score the results then filter it.\\n\\nAll these steps are prompted in the 'template.json' file located in the specified code folder.\\nCode:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thoddnn/OpenDataGen-factuality-en-v0.1."},
	{"name":"SinhalaWikipediaArticles","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KanishkaRandunu/SinhalaWikipediaArticles","creator_name":"Kanishka Randunu ","creator_url":"https://huggingface.co/KanishkaRandunu","description":"KanishkaRandunu/SinhalaWikipediaArticles dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"WikiFactDiff","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Orange/WikiFactDiff","creator_name":"Orange","creator_url":"https://huggingface.co/Orange","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWikiFactDiff: A Realistic Dataset for Atomic Factual Knowledge Update\\n\\t\\n\\nWikiFactDiff is a dataset designed as a resource to perform realistic factual updates within language models and to evaluate them post-update.\\nAvailable datasets:\\n\\n20210104-20230227_legacy: The recommended WikiFactDiff dataset (its creation process is in the paper)\\n20210104-20230227: An improves version of WikiFactDiff in terms of verbalization quality (Work still in progress.. DO NOT USE IT)\\ntriple_verbs:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Orange/WikiFactDiff."},
	{"name":"kazqad-retrieval","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/issai/kazqad-retrieval","creator_name":"Institute of Smart Systems and Artificial Intelligence, Nazarbayev University","creator_url":"https://huggingface.co/issai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for KazQAD-Retrieval\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nKazQAD is a Kazakh open-domain Question Answering Dataset\\nthat can be used in both reading comprehension and full ODQA settings, as well as for information retrieval experiments.\\nThis repository contains only the collection and relevance judgments for information retrieval task.\\nShort answers and data for the reading comprehension task (extractive QA) can be found here.\\nKazQAD contains just under 6,000 unique‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/issai/kazqad-retrieval."},
	{"name":"Detect-Egyptian-Wikipedia-Articles","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SaiedAlshahrani/Detect-Egyptian-Wikipedia-Articles","creator_name":"Saied Alshahrani","creator_url":"https://huggingface.co/SaiedAlshahrani","description":" Detect Egyptian Wikipedia Template-translated Articles \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description:\\n\\t\\n\\nWe release the heuristically filtered, manually processed, and automatically classified Egyptian Arabic Wikipedia articles dataset. This dataset was used to develop a web-based detection system to automatically identify the template-translated articles on the Egyptian Arabic Wikipedia edition. The system is called Egyptian Arabic Wikipedia Scanner and is hosted on Hugging Face Spaces, here:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SaiedAlshahrani/Detect-Egyptian-Wikipedia-Articles."},
	{"name":"sql-create-context-thai","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/saksornr/sql-create-context-thai","creator_name":"Saksorn Ruangtanusak","creator_url":"https://huggingface.co/saksornr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from sql-create-context.\\n@misc{b-mc2_2023_sql-create-context,\\n  title   = {sql-create-context Dataset},\\n  author  = {b-mc2}, \\n  year    = {2023},\\n  url     = {https://huggingface.co/datasets/b-mc2/sql-create-context},\\n  note    = {This dataset was created by modifying data from the following sources: \\\\cite{zhongSeq2SQL2017, yu2018spider}.},\\n}\\n\\n"},
	{"name":"Wikipedia-it-Trame-di-Film","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/scribis/Wikipedia-it-Trame-di-Film","creator_name":"Fabio Martines","creator_url":"https://huggingface.co/scribis","description":"Collection of plots of historical films and adventure films from Italian Wikipedia (April 2024)\\nRaccolta di trame di film storici e film di avventura da Wikipedia italiana (Aprile 2024)\\n"},
	{"name":"ParaNames","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bltlab/ParaNames","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bltlab/ParaNames."},
	{"name":"day_in_history","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/santhosh/day_in_history","creator_name":"Santhosh Thottingal","creator_url":"https://huggingface.co/santhosh","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDay in History\\n\\t\\n\\nThis is a dataset prepared out of wikipedia pages https://en.wikipedia.org/wiki/Category:Days_of_the_year.\\nHistoric events are mapped against each date with reference if available.\\nHere is a demo app using this dataset: https://huggingface.co/spaces/santhosh/day_in_history\\nScript used for parsing wiki pages: https://github.com/santhoshtr/day-in-history. Please use that repository for tracking issues regarding adding more languages, data cleanup etc.\\n"},
	{"name":"PUGG","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPUGG: KBQA, MRC, IR Dataset for Polish\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\\n\\nKBQA (Knowledge Base Question Answering)\\nMRC (Machine Reading Comprehension)\\nIR (Information Retrieval)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPaper\\n\\t\\n\\nFor more detailed information, please refer to our research paper titled:\\n\\\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\\\" \\nAuthored by:\\n\\nAlbert Sawczyn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG."},
	{"name":"PUGG","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPUGG: KBQA, MRC, IR Dataset for Polish\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\\n\\nKBQA (Knowledge Base Question Answering)\\nMRC (Machine Reading Comprehension)\\nIR (Information Retrieval)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPaper\\n\\t\\n\\nFor more detailed information, please refer to our research paper titled:\\n\\\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\\\" \\nAuthored by:\\n\\nAlbert Sawczyn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG."},
	{"name":"NQ-256-24-gpt-4o-2024-05-13-803084","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NQ-256-24-gpt-4o-2024-05-13-803084","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNQ-256-24-gpt-4o-2024-05-13-803084 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"question answering dataset search\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NQ-256-24-gpt-4o-2024-05-13-803084 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NQ-256-24-gpt-4o-2024-05-13-803084."},
	{"name":"VCR-wiki-zh-hard","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard."},
	{"name":"VCR-wiki-zh-easy","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy."},
	{"name":"VCR-wiki-en-hard","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard."},
	{"name":"VCR-wiki-en-easy","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy."},
	{"name":"VCR-wiki-zh-hard-test-100","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test-100","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test-100."},
	{"name":"VCR-wiki-zh-hard-test-500","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test-500","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test-500."},
	{"name":"VCR-wiki-zh-hard-test","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test."},
	{"name":"VCR-wiki-zh-easy-test","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test."},
	{"name":"VCR-wiki-zh-easy-test-500","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test-500","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test-500."},
	{"name":"VCR-wiki-zh-easy-test-100","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test-100","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test-100."},
	{"name":"VCR-wiki-en-easy-test-100","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test-100","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test-100."},
	{"name":"VCR-wiki-en-easy-test-500","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test-500","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test-500."},
	{"name":"VCR-wiki-en-easy-test","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test."},
	{"name":"VCR-wiki-en-hard-test","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test."},
	{"name":"VCR-wiki-en-hard-test-500","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test-500","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test-500."},
	{"name":"VCR-wiki-en-hard-test-100","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test-100","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\\n\\t\\n\\nüè† Paper | üë©üèª‚Äçüíª GitHub | ü§ó Huggingface Datasets | üìè Evaluation with lmms-eval\\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test-100."},
	{"name":"wikidata_triple_en","keyword":"wikidata","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RJZ/wikidata_triple_en","creator_name":"JZ","creator_url":"https://huggingface.co/RJZ","description":"RJZ/wikidata_triple_en dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"PUGG_KBQA","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_KBQA","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPUGG: KBQA, MRC, IR Dataset for Polish\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\\n\\nKBQA (Knowledge Base Question Answering)\\nMRC (Machine Reading Comprehension)\\nIR (Information Retrieval)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPaper\\n\\t\\n\\nFor more detailed information, please refer to our research paper titled:\\n\\\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\\\" \\nAuthored by:\\n\\nAlbert Sawczyn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_KBQA."},
	{"name":"PUGG_MRC","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_MRC","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPUGG: KBQA, MRC, IR Dataset for Polish\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\\n\\nKBQA (Knowledge Base Question Answering)\\nMRC (Machine Reading Comprehension)\\nIR (Information Retrieval)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPaper\\n\\t\\n\\nFor more detailed information, please refer to our research paper titled:\\n\\\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\\\" \\nAuthored by:\\n\\nAlbert Sawczyn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_MRC."},
	{"name":"PUGG_IR","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_IR","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\\n\\t\\n\\t\\t\\n\\t\\tPUGG: KBQA, MRC, IR Dataset for Polish\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDescription\\n\\t\\n\\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\\n\\nKBQA (Knowledge Base Question Answering)\\nMRC (Machine Reading Comprehension)\\nIR (Information Retrieval)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tPaper\\n\\t\\n\\nFor more detailed information, please refer to our research paper titled:\\n\\\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\\\" \\nAuthored by:\\n\\nAlbert Sawczyn\\nKatsiaryna‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_IR."},
	{"name":"PUGG_KBQA","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_KBQA","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPUGG: KBQA, MRC, IR Dataset for Polish\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\\n\\nKBQA (Knowledge Base Question Answering)\\nMRC (Machine Reading Comprehension)\\nIR (Information Retrieval)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPaper\\n\\t\\n\\nFor more detailed information, please refer to our research paper titled:\\n\\\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\\\" \\nAuthored by:\\n\\nAlbert Sawczyn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_KBQA."},
	{"name":"tydi_xor_rc","keyword":"extended|wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/coastalcph/tydi_xor_rc","creator_name":"CoAStaL NLP Group","creator_url":"https://huggingface.co/coastalcph","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"tydi_xor_rc\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTyDi QA is a question answering dataset covering 11 typologically diverse languages. \\nXORQA is an extension of the original TyDi QA dataset to also include unanswerable questions, where context documents are only in English but questions are in 7 languages.\\nXOR-AttriQA contains annotated attribution data for a sample of XORQA.\\nThis dataset is a combined and simplified version of the Reading Comprehension data from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/coastalcph/tydi_xor_rc."},
	{"name":"PUGG_KG","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_KG","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPUGG: KBQA, MRC, IR Dataset for Polish\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis repository contains the knowledge graph dedicated for \\nthe KBQA (Knowledge Base Question Answering) task within the PUGG dataset. This repository does not \\ncontain directly any task, but it provides the knowledge graph that can be used to solve the KBQA \\ntask from the PUGG dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGraphs\\n\\t\\n\\nWe provide sampled versions of the knowledge graph based on Wikidata:\\n\\nWikidata1H: A subgraph created by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_KG."},
	{"name":"PUGG_KG","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_KG","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPUGG: KBQA, MRC, IR Dataset for Polish\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis repository contains the knowledge graph dedicated for \\nthe KBQA (Knowledge Base Question Answering) task within the PUGG dataset. This repository does not \\ncontain directly any task, but it provides the knowledge graph that can be used to solve the KBQA \\ntask from the PUGG dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGraphs\\n\\t\\n\\nWe provide sampled versions of the knowledge graph based on Wikidata:\\n\\nWikidata1H: A subgraph created by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_KG."},
	{"name":"German-RAG-SFT-Alpaca-HESSIAN-AI","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-SFT-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-SFT (Supervised Fine-Tuning) Alpaca-Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SFT Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-SFT-Alpaca-HESSIAN-AI."},
	{"name":"German-RAG-SFT-ShareGPT-HESSIAN-AI","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-SFT-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-SFT (Supervised Fine-Tuning) Share-GPT Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SFT Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-SFT-ShareGPT-HESSIAN-AI."},
	{"name":"10k_wiki_summary","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ambrosfitz/10k_wiki_summary","creator_name":"Christopher Smith","creator_url":"https://huggingface.co/ambrosfitz","description":"ambrosfitz/10k_wiki_summary dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"test-big-dataset","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huggingface/test-big-dataset","creator_name":"Hugging Face","creator_url":"https://huggingface.co/huggingface","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Danish WIT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGoogle presented the Wikipedia Image Text (WIT) dataset in July\\n2021, a dataset which contains\\nscraped images from Wikipedia along with their descriptions. WikiMedia released\\nWIT-Base in September\\n2021,\\nbeing a modified version of WIT where they have removed the images with empty\\n\\\"reference descriptions\\\", as well as removing images where a person's face covers more\\nthan 10% of the image surface, along with inappropriate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huggingface/test-big-dataset."},
	{"name":"wiki-talks","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lflage/wiki-talks","creator_name":"Lucas Fonseca Lage","creator_url":"https://huggingface.co/lflage","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWiki-Talks\\n\\t\\n\\nThe Wiki-Talks dataset is a collection of conversational threads extracted from the talk pages on Wikipedia.\\nThis dataset captures collaborative dialogue, discussion patterns, and consensus-building among Wikipedia contributors.\\nIt is useful for NLP research focused on dialogue, sentiment analysis, and community dynamics.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDetails\\n\\t\\n\\nCurrently due to PyArrow incompatibility to the long recursive structures in the dataset there is an intrinsic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lflage/wiki-talks."},
	{"name":"German-RAG-DPO-Alpaca-HESSIAN-AI","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-DPO-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-DPO Alpaca Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe DPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge graph where Question-Answer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-DPO-Alpaca-HESSIAN-AI."},
	{"name":"German-RAG-DPO-ShareGPT-HESSIAN-AI","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-DPO-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-DPO Share-GPT Format\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe DPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge graph where‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-DPO-ShareGPT-HESSIAN-AI."},
	{"name":"wikipedia-paragraph-titles","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/wikipedia-paragraph-titles","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWikipedia Paragraphs and AI-Generated Titles Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains pairs of Wikipedia paragraphs and their corresponding AI-generated titles. It is designed to facilitate research and development in natural language processing tasks, particularly in the areas of text summarization, title generation, and topic modeling.\\nThe dataset combines human-written content from Wikipedia with machine-generated titles, providing a unique resource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/wikipedia-paragraph-titles."},
	{"name":"AnimeMangaCharacters-247K","keyword":"wiki","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/AnimeMangaCharacters-247K","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAnime Manga Characters Dataset\\n\\t\\n\\nThis dataset is a metafile containing information about 247,034 anime and manga characters sourced from 2,372 fandom wiki sites. Each entry represents a character along with its associated metadata. The dataset has been deduplicated based on the url field to avoid redundancy, although a single character may still have multiple associated URLs.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPotential Applications\\n\\t\\n\\n\\nMultimodal Data Creation: Use the URLs to download the respective‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/AnimeMangaCharacters-247K."},
	{"name":"Bharat_NanoDBPedia_bn","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bn","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Bengali version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bn."},
	{"name":"Bharat_NanoDBPedia_hi","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hi","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Hindi version of the NanoDBPedia dataset, specifically adapted for information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hi."},
	{"name":"Bharat_NanoDBPedia_hne","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Chhattisgarhi version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hne."},
	{"name":"Bharat_NanoDBPedia_ksd","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksd","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Kashmiri (Devanagari script) version of the NanoDBPedia dataset, specifically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksd."},
	{"name":"Bharat_NanoDBPedia_mag","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mag","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Magahi version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mag."},
	{"name":"Bharat_NanoDBPedia_ml","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ml","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Malayalam version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ml."},
	{"name":"Bharat_NanoDBPedia_mni","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mni","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Manipuri version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mni."},
	{"name":"Bharat_NanoDBPedia_ne","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Nepali version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ne."},
	{"name":"Bharat_NanoDBPedia_or","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_or","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Odia (Oriya) version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_or."},
	{"name":"Bharat_NanoDBPedia_ta","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ta","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Tamil version of the NanoDBPedia dataset, specifically adapted for information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ta."},
	{"name":"chatjsonsql","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rishi2903/chatjsonsql","creator_name":"Rishabh Mekala","creator_url":"https://huggingface.co/rishi2903","description":"\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names, column‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rishi2903/chatjsonsql."},
	{"name":"wikibio","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/marco-stranisci/wikibio","creator_name":"Marco Stranisci","creator_url":"https://huggingface.co/marco-stranisci","description":"\\n\\t\\n\\t\\t\\n\\t\\tWikiBio @ ACL 2023\\n\\t\\n\\nThis is the repository of the WikiBio corpus, which is described in the following paper:\\n\\nWikibio: a Semantic Resource for the Intersectional Analysis of Biographical Events\\n\\nPlease use this reference to cite our work\\n\\n@inproceedings{stranisci-etal-2023-wikibio,    title = \\\"{W}iki{B}io: a Semantic Resource for the Intersectional Analysis of Biographical Events\\\",    author = \\\"Stranisci, Marco Antonio  and\\n      Damiano, Rossana  and\\n      Mensa, Enrico  and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marco-stranisci/wikibio."},
	{"name":"Bills","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zli12321/Bills","creator_name":"LZX","creator_url":"https://huggingface.co/zli12321","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThis repository contains benchmark datasets for evaluating Large Language Model (LLM)-based topic discovery methods and comparing them against traditional topic models.  These datasets provide a valuable resource for researchers studying topic modeling and LLM capabilities in this domain.  The work is described in the following paper: Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs.  Original data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zli12321/Bills."},
	{"name":"PUGG_IR-qrels","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_IR-qrels","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\\n\\t\\n\\t\\t\\n\\t\\tPUGG: KBQA, MRC, IR Dataset for Polish\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDescription\\n\\t\\n\\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\\n\\nKBQA (Knowledge Base Question Answering)\\nMRC (Machine Reading Comprehension)\\nIR (Information Retrieval)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tPaper\\n\\t\\n\\nFor more detailed information, please refer to our research paper titled:\\n\\\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\\\" \\nAuthored by:\\n\\nAlbert Sawczyn\\nKatsiaryna‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_IR-qrels."},
	{"name":"enwiki_anchor_pos_negative_490K_qwen2-5_en","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CCRss/enwiki_anchor_pos_negative_490K_qwen2-5_en","creator_name":"CCR","creator_url":"https://huggingface.co/CCRss","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tenwiki_anchor_pos_negative_490K Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains 495,470 entries derived from English Wikipedia, meticulously crafted for training and evaluating embedding models. It's particularly suited for fine-tuning models using techniques like MultipleNegativesRankingLoss.\\nImportant Note: Each 'positive' text in this dataset is a chunk of information from a Wikipedia article, limited to a maximum of 512 tokens (\\\"tokenizer_class\\\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CCRss/enwiki_anchor_pos_negative_490K_qwen2-5_en."},
	{"name":"wikipedia-paragraph-keywords","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/wikipedia-paragraph-keywords","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWikipedia Paragraph and Keyword Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains 10,693 paragraphs extracted from English Wikipedia articles, along with corresponding search-engine style keywords for each paragraph. It is designed to support tasks such as text summarization, keyword extraction, and information retrieval.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is structured as a collection of JSON objects, each representing a single paragraph with its associated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/wikipedia-paragraph-keywords."},
	{"name":"wikipedia-paragraph-keywords","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/wikipedia-paragraph-keywords","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWikipedia Paragraph and Keyword Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains 10,693 paragraphs extracted from English Wikipedia articles, along with corresponding search-engine style keywords for each paragraph. It is designed to support tasks such as text summarization, keyword extraction, and information retrieval.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is structured as a collection of JSON objects, each representing a single paragraph with its associated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/wikipedia-paragraph-keywords."},
	{"name":"lez_wiki_20240920","keyword":"wiki","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/leks-forever/lez_wiki_20240920","creator_name":"Lezghian Community","creator_url":"https://huggingface.co/leks-forever","description":"leks-forever/lez_wiki_20240920 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"PangeaBench-tydiqa","keyword":"extended|wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/neulab/PangeaBench-tydiqa","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"tydiqa\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\\nin the world. It contains language phenomena that would not be found‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neulab/PangeaBench-tydiqa."},
	{"name":"German-RAG-LLM-EASY-BENCHMARK","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-LLM-EASY-BENCHMARK","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG-LLM-EASY-BENCHMARK\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tGerman-RAG - German Retrieval Augmented Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis German-RAG-LLM-BENCHMARK represents a specialized collection for evaluating language models with a focus on source citation, time difference stating in RAG-specific tasks.\\nTo evaluate models compatible with OpenAI-Endpoints you can refer to our Github Repo: https://github.com/avemio-digital/German-RAG-LLM-EASY-BENCHMARK/\\nMost of the Subsets are synthetically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-LLM-EASY-BENCHMARK."},
	{"name":"text-sft","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/text-sft","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tText Questions and Answers Dataset\\n\\t\\n\\nThis dataset is a combination of multiple datasets.\\nThe text has been cleaned and formatted into a consistent JSONL structure, containing questions and answers derived from the text.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nTotal entries: 41‚Äâ977\\nFrom agentlans/wikipedia-paragraph-sft: 21‚Äâ810\\nFrom agentlans/finewebedu-sft: 10‚Äâ168\\nFrom Cosmopedia: 9‚Äâ999\\n\\n\\n\\nThe Cosmopedia dataset's web_samples_v2 train split was processed using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/text-sft."},
	{"name":"GreekWikipedia","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IMISLab/GreekWikipedia","creator_name":"IMIS Lab UPatras","creator_url":"https://huggingface.co/IMISLab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGreekWikipedia\\n\\t\\n\\nA Greek abstractive summarization dataset collected from the Greek part of Wikipedia, which contains 93,432 articles, their titles and summaries.\\nThis dataset has been used to train our best-performing model GreekWiki-umt5-base as part of our upcoming research article:Giarelis, N., Mastrokostas, C., & Karacapilidis, N. (2024) Greek Wikipedia: A Study on Abstractive Summarization.For information about dataset creation, limitations etc. see the original article.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IMISLab/GreekWikipedia."},
	{"name":"squad","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rajpurkar/squad","creator_name":"Pranav R","creator_url":"https://huggingface.co/rajpurkar","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SQuAD\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\\nSQuAD 1.1 contains 100,000+ question-answer pairs on 500+ articles.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nQuestion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rajpurkar/squad."},
	{"name":"sql-create-context","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/b-mc2/sql-create-context","creator_name":"brianm","creator_url":"https://huggingface.co/b-mc2","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/b-mc2/sql-create-context."},
	{"name":"bbaw_egyptian","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/phiwi/bbaw_egyptian","creator_name":"Wiesenbach","creator_url":"https://huggingface.co/phiwi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"bbaw_egyptian\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset comprises parallel sentences of hieroglyphic encodings, transcription and translation as used in the paper Multi-Task Modeling of Phonographic Languages: Translating Middle Egyptian Hieroglyph. The data triples are extracted from the digital corpus of Egyptian texts compiled by the project \\\"Strukturen und Transformationen des Wortschatzes der √§gyptischen Sprache\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/phiwi/bbaw_egyptian."},
	{"name":"wiki_toxic","keyword":"wikipedia","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic","creator_name":"OxAI Safety Hub Active Learning with Large Language Models Labs Team","creator_url":"https://huggingface.co/OxAISH-AL-LLM","description":"Jigsaw Toxic Comment Challenge dataset. This dataset was the basis of a Kaggle competition run by Jigsaw"},
	{"name":"tamil_sentences_sample","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AnanthZeke/tamil_sentences_sample","creator_name":"Ananth","creator_url":"https://huggingface.co/AnanthZeke","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"tamil_combined_sentences\\\"\\n\\t\\n\\nMore Information needed\\n"},
	{"name":"TALI","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Antreas/TALI","creator_name":"Antreas Antoniou","creator_url":"https://huggingface.co/Antreas","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"TALI\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbstract\\n\\t\\n\\nTALI is a large-scale, tetramodal dataset designed to facilitate a shift from unimodal and duomodal to tetramodal research in deep learning. It aligns text, video, images, and audio, providing a rich resource for innovative self-supervised learning tasks and multimodal research. TALI enables exploration of how different modalities and data/model scaling affect downstream performance, with the aim‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Antreas/TALI."},
	{"name":"sql-create-context-instruction","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction","creator_name":"Spartak Bughdaryan","creator_url":"https://huggingface.co/bugdaryan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is built upon SQL Create Context, which in turn was constructed using data from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-SQL LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-SQL datasets. The CREATE TABLE statement can often‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction."},
	{"name":"qald_9_plus","keyword":"wikidata","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/casey-martin/qald_9_plus","creator_name":"Casey","creator_url":"https://huggingface.co/casey-martin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQALD-9-plus Dataset Description\\n\\t\\n\\nQALD-9-plus is the dataset for Knowledge Graph Question Answering (KGQA) based on well-known QALD-9.\\nQALD-9-plus enables to train and test KGQA systems over DBpedia and Wikidata using questions in 9 different languages: English, German, Russian, French, Armenian, Belarusian, Lithuanian, Bashkir, and Ukrainian.\\nSome of the questions have several alternative writings in particular languages which enables to evaluate the robustness of KGQA systems‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/casey-martin/qald_9_plus."},
	{"name":"kazqad","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/issai/kazqad","creator_name":"Institute of Smart Systems and Artificial Intelligence, Nazarbayev University","creator_url":"https://huggingface.co/issai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for KazQAD\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nKazQAD is a Kazakh open-domain Question Answering Dataset\\nthat can be used in both reading comprehension and full ODQA settings, as well as for information retrieval experiments.\\nThis repository contains only the data for the reading comprehension task (extractive QA).\\nCollection and relevance judgments for information retrieval can be found here.\\nThe main dataset (subset kazqad) contains just under 6,000 unique questions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/issai/kazqad."},
	{"name":"chinese_moegirl_wiki_corpus_raw","keyword":"wiki","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/chinese_moegirl_wiki_corpus_raw","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChinese Moegirl ACG Corpus (Raw Data)\\n\\t\\n\\nMoegirl ÊòØ‰∏™‰∏≠Êñá‰∫åÊ¨°ÂÖÉ wiki ÁΩëÁ´ô\\nÊú¨È°πÁõÆÂØπ 20230814 wiki dump for wiki-zh.moegirl.org.cn Âè™ËøõË°å‰∫ÜÁÆÄÂçïÁöÑÊï∞ÊçÆÊ†ºÂºèÂ§ÑÁêÜÔºàxml -> jsonl datasetÔºâÔºåÂêéÁª≠Â¶ÇÊÉ≥‰Ωú‰∏∫ LLM È¢ÑËÆ≠ÁªÉËØ≠ÊñôÔºåÂä°ÂøÖËøõË°åÂêÑÁßçÊñáÊú¨Ê∏ÖÊ¥ó„ÄÇ\\nÁÆÄÂçï‰ΩøÁî®Ê≠£ÂàôÁªôÊØèÊù°Êï∞ÊçÆÂ¢ûÂä†‰∫Ü tagÔºõÁõ¥Êé•ËøáÊª§ÊéâÊâÄÊúâÂ∏¶Êúâ \\\"#REDIRECT\\\" ÂÜÖÂÆπÁöÑÈáçÂÆöÂêëÊù°ÁõÆ„ÄÇ\\nMoegirl is a well-known Chinese wiki website for ACG.\\nThis datasets is a raw text version of the 20230814 wiki dump for wiki-zh.moegirl.org.cn reformatted into jsonl dataset. You must perform further data processing for LLM (continual) pretraining.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/chinese_moegirl_wiki_corpus_raw."},
	{"name":"structured-wikipedia","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wikimedia/structured-wikipedia","creator_name":"Wikimedia","creator_url":"https://huggingface.co/wikimedia","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Wikimedia Structured Wikipedia\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nEarly beta release of pre-parsed English and French Wikipedia articles including infoboxes. Inviting feedback.\\nThis dataset contains all articles of the English and French language editions of Wikipedia, pre-parsed and outputted as structured JSON files with a consistent schema (JSONL compressed as zip). Each JSON line holds the content of one full Wikipedia article‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wikimedia/structured-wikipedia."},
	{"name":"MultiSimV2","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MichaelR207/MultiSimV2","creator_name":"Michael Ryan","creator_url":"https://huggingface.co/MichaelR207","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MultiSim Benchmark\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe MultiSim benchmark is a growing collection of text simplification datasets targeted at sentence simplification in several languages.  Currently, the benchmark spans 12 languages.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\n\\nSentence Simplification\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"MichaelR207/MultiSimV2\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this benchmark, please cite our‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MichaelR207/MultiSimV2."},
	{"name":"Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AmanPriyanshu/Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens","creator_name":"Aman Priyanshu","creator_url":"https://huggingface.co/AmanPriyanshu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDynamic Topic Modeling Dataset: RedPajama-1T SubSample (100k samples, 1k tokens)\\n\\t\\n\\n\\n  üìùCheck out the Blog Post\\n\\n\\nThis dataset represents a curated subset of the RedPajama-1T Sample dataset, specifically processed for dynamic topic modeling applications. It contains 100,000 \\nsamples from the original dataset, with each document limited to the first 1,024 tokens for consistent processing.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Overview\\n\\t\\n\\n\\nName:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AmanPriyanshu/Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens."},
	{"name":"noob-wiki","keyword":"wiki","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Laxhar/noob-wiki","creator_name":"Laxhar Dream Lab","creator_url":"https://huggingface.co/Laxhar","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNoob SDXL Wiki\\n\\t\\n\\nThis is the WIKI database for Noob SDXL Models.\\n"},
	{"name":"RUwiki","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DataSynGen/RUwiki","creator_name":"DataGen","creator_url":"https://huggingface.co/DataSynGen","description":"\\n\\t\\n\\t\\t\\n\\t\\t–î–∞—Ç–∞—Å–µ—Ç –∏–∑ 1000 —Å—Ç–∞—Ç–µ–π —Å —Ä—É—Å—Å–∫–æ–π –≤–∏–∫–∏–ø–µ–¥–∏–∏\\n\\t\\n\\n\\n–ë—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è ruWiki-web-scraper\\n–ö–∞–∂–¥–∞—è —Å—Ç–∞—Ç—å—è –∑–∞–∫–ª—é—á–µ–Ω–∞ –≤ —Ç–µ–≥–∏ <s_text> </s_text>\\n\\n"},
	{"name":"RUwiki","keyword":"wiki","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DataSynGen/RUwiki","creator_name":"DataGen","creator_url":"https://huggingface.co/DataSynGen","description":"\\n\\t\\n\\t\\t\\n\\t\\t–î–∞—Ç–∞—Å–µ—Ç –∏–∑ 1000 —Å—Ç–∞—Ç–µ–π —Å —Ä—É—Å—Å–∫–æ–π –≤–∏–∫–∏–ø–µ–¥–∏–∏\\n\\t\\n\\n\\n–ë—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è ruWiki-web-scraper\\n–ö–∞–∂–¥–∞—è —Å—Ç–∞—Ç—å—è –∑–∞–∫–ª—é—á–µ–Ω–∞ –≤ —Ç–µ–≥–∏ <s_text> </s_text>\\n\\n"},
	{"name":"wikidata","keyword":"wikidata","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/philippesaade/wikidata","creator_name":"Philippe Saad√©","creator_url":"https://huggingface.co/philippesaade","description":"\\n\\t\\n\\t\\t\\n\\t\\tWikidata Entities Connected to Wikipedia\\n\\t\\n\\nThis dataset is a multilingual, JSON-formatted version of the Wikidata dump from September 18, 2024. It only includes Wikidata entities that are connected to a Wikipedia page in any language.\\nA total of 112,467,802 entities are included in the original data dump, of which 30,072,707 are linked to a Wikipedia page (26.73% of all entities have at least one Wikipedia sitelink).\\n\\nCurated by: Jonathan Fraine & Philippe Saad√©, Wikimedia Deutschland‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/philippesaade/wikidata."},
	{"name":"wikipedia_quality_wikirank","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank","creator_name":"W≈Çodzimierz Lewoniewski","creator_url":"https://huggingface.co/lewoniewski","description":"Datasets with WikiRank quality score as of 1 August 2024 for 47 million Wikipedia articles in 55 language versions (also simplified version for each language in separate files).\\nThe WikiRank quality score is a metric designed to assess the overall quality of a Wikipedia article. Although its specific algorithm can vary depending on the implementation, the score typically combines several key features of the Wikipedia article.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy It‚Äôs Important\\n\\t\\n\\n\\nEnhances Trust: For readers and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank."},
	{"name":"ropes","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenai/ropes","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ROPES\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nROPES (Reasoning Over Paragraph Effects in Situations) is a QA dataset which tests a system's ability to apply knowledge from a passage of text to a new situation. A system is presented a background passage containing a causal or qualitative relation(s) (e.g., \\\"animal pollinators increase efficiency of fertilization in flowers\\\"), a novel situation that uses this background, and questions that require reasoning about effects‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/ropes."},
	{"name":"wiki-entity-similarity","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Exr0n/wiki-entity-similarity","creator_name":"exr0n","creator_url":"https://huggingface.co/Exr0n","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWiki Entity Similarity\\n\\t\\n\\nUsage:\\nfrom datasets import load_dataset\\n\\ncorpus = load_dataset('Exr0n/wiki-entity-similarity', '2018thresh20corpus', split='train')\\nassert corpus[0] == {'article': 'A1000 road', 'link_text': 'A1000', 'is_same': 1}\\n\\npairs = load_dataset('Exr0n/wiki-entity-similarity', '2018thresh20pairs', split='train')\\nassert corpus[0] == {'article': 'Rhinobatos', 'link_text': 'Ehinobatos beurleni', 'is_same': 1}\\nassert len(corpus) == 4_793_180\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCorpus‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Exr0n/wiki-entity-similarity."},
	{"name":"tydiqa","keyword":"extended|wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google-research-datasets/tydiqa","creator_name":"Google Research Datasets","creator_url":"https://huggingface.co/google-research-datasets","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"tydiqa\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\\nin the world. It contains language phenomena that would not be found‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google-research-datasets/tydiqa."},
	{"name":"few-nerd","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DFKI-SLT/few-nerd","creator_name":"Speech and Language Technology, DFKI","creator_url":"https://huggingface.co/DFKI-SLT","description":"Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, \\nwhich contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities \\nand 4,601,223 tokens. Three benchmark tasks are built, one is supervised: Few-NERD (SUP) and the \\nother two are few-shot: Few-NERD (INTRA) and Few-NERD (INTER)."},
	{"name":"medwiki","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mvarma/medwiki","creator_name":"Maya Varma","creator_url":"https://huggingface.co/mvarma","description":"MedWiki is a large-scale sentence dataset collected from Wikipedia with medical entity (UMLS) annotations. This dataset is intended for pretraining."},
	{"name":"catalan_textual_corpus","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/catalan_textual_corpus","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Catalan Textual Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Catalan Textual Corpus is a 1760-million-token web corpus of Catalan built from several sources.\\nIt consists of 1,758,388,896 tokens, 73,172,152 sentences, and 12,556,365 documents. Documents are separated by single new lines. These boundaries have been preserved as long as the license allowed it.\\nThis work is licensed under a Creative Commons Attribution Share Alike 4.0 International license.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/catalan_textual_corpus."},
	{"name":"WikiConvert","keyword":"extended|wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/usc-isi/WikiConvert","creator_name":"USC Information Sciences Institute","creator_url":"https://huggingface.co/usc-isi","description":"Language Modelling with Cardinal Number Annotations."},
	{"name":"wit_base","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wikimedia/wit_base","creator_name":"Wikimedia","creator_url":"https://huggingface.co/wikimedia","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for WIT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWikimedia's version of the Wikipedia-based Image Text (WIT) Dataset, a large multimodal multilingual dataset.\\nFrom the official blog post:\\n\\nThe core training data is taken from the Wikipedia Image-Text (WIT) Dataset, a large curated set of more than 37 million image-text associations extracted from Wikipedia articles in 108 languages that was recently released by Google Research.\\nThe WIT dataset offers extremely valuable data about the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wikimedia/wit_base."},
	{"name":"answerable_tydiqa","keyword":"extended|wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/copenlu/answerable_tydiqa","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"answerable-tydiqa\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTyDi QA is a question answering dataset covering 11 typologically diverse languages. \\nAnswerable TyDi QA is an extension of the GoldP subtask of the original TyDi QA dataset to also include unanswertable questions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset contains a train and a validation set, with 116067 and 13325 examples, respectively. Access them with\\nfrom datasets import load_dataset\\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/answerable_tydiqa."},
	{"name":"tydiqa_copenlu","keyword":"extended|wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/copenlu/tydiqa_copenlu","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"tydiqa\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\\nin the world. It contains language phenomena that would not be found‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/tydiqa_copenlu."},
	{"name":"machine-paraphrase-dataset","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jpwahle/machine-paraphrase-dataset","creator_name":"Jan Philip Wahle","creator_url":"https://huggingface.co/jpwahle","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Machine Paraphrase Dataset (MPC)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Machine Paraphrase Corpus (MPC) consists of ~200k examples of original, and paraphrases using two online paraphrasing tools.\\nIt uses two paraphrasing tools (SpinnerChief, SpinBot) on three source texts (Wikipedia, arXiv, student theses).\\nThe examples are not aligned, i.e., we sample different paragraphs for originals and paraphrased versions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to use it\\n\\t\\n\\nYou can load the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jpwahle/machine-paraphrase-dataset."},
	{"name":"autoencoder-paraphrase-dataset","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jpwahle/autoencoder-paraphrase-dataset","creator_name":"Jan Philip Wahle","creator_url":"https://huggingface.co/jpwahle","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Machine Paraphrase Dataset (MPC)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Autoencoder Paraphrase Corpus (APC) consists of ~200k examples of original, and paraphrases using three neural language models.\\nIt uses three models (BERT, RoBERTa, Longformer) on three source texts (Wikipedia, arXiv, student theses).\\nThe examples are aligned, i.e., we sample the same paragraphs for originals and paraphrased versions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to use it\\n\\t\\n\\nYou can load the dataset using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jpwahle/autoencoder-paraphrase-dataset."},
	{"name":"autoregressive-paraphrase-dataset","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jpwahle/autoregressive-paraphrase-dataset","creator_name":"Jan Philip Wahle","creator_url":"https://huggingface.co/jpwahle","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for [Dataset Name]\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCuration Rationale\\n\\t\\n\\n[More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jpwahle/autoregressive-paraphrase-dataset."},
	{"name":"docee-event-classification","keyword":"wiki","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fkdosilovic/docee-event-classification","creator_name":"Filip Karlo Do≈°iloviƒá","creator_url":"https://huggingface.co/fkdosilovic","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for DocEE Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nDocEE dataset is an English-language dataset containing more than 27k news and Wikipedia articles. Dataset is primarily annotated and collected for large-scale document-level event extraction.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\n\\ntitle: TODO\\ntext: TODO\\nevent_type: TODO\\ndate: TODO\\nmetadata: TODO\\n\\nNote: this repo contains only event detection portion of the dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits\\n\\t\\n\\nThe dataset has 2 splits: train and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fkdosilovic/docee-event-classification."},
	{"name":"TyDiP","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Genius1237/TyDiP","creator_name":"Genius1237","creator_url":"https://huggingface.co/Genius1237","description":"The TyDiP dataset is a dataset of requests in conversations between wikipedia editors\\nthat have been annotated for politeness. The splits available below consists of only\\nrequests from the top 25 percentile (polite) and bottom 25 percentile (impolite) of\\npoliteness scores. The English train set and English test set that are\\nadapted from the Stanford Politeness Corpus, and test data in 9 more languages\\n(Hindi, Korean, Spanish, Tamil, French, Vietnamese, Russian, Afrikaans, Hungarian) \\nwas annotated by us."},
	{"name":"skquad","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TUKE-DeutscheTelekom/skquad","creator_name":"TUKE and DTSS cooperation","creator_url":"https://huggingface.co/TUKE-DeutscheTelekom","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SkQuAD\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSK-QuAD is the first QA dataset for the Slovak language.\\nIt is manually annotated, so it has no distortion caused by\\nmachine translation. The dataset is thematically diverse ‚Äì it\\ndoes not overlap with SQuAD ‚Äì it brings new knowledge.\\nIt passed the second round of annotation ‚Äì each question\\nand the answer were seen by at least two annotators.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\n\\nQuestion answering\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TUKE-DeutscheTelekom/skquad."},
	{"name":"qa_squad","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lmqg/qa_squad","creator_name":"LMQG","creator_url":"https://huggingface.co/lmqg","description":"SQuAD with the train/validation/test split used in SQuAD QG"},
	{"name":"da-wit","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexandrainst/da-wit","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Danish WIT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGoogle presented the Wikipedia Image Text (WIT) dataset in July\\n2021, a dataset which contains\\nscraped images from Wikipedia along with their descriptions. WikiMedia released\\nWIT-Base in September\\n2021,\\nbeing a modified version of WIT where they have removed the images with empty\\n\\\"reference descriptions\\\", as well as removing images where a person's face covers more\\nthan 10% of the image surface, along with inappropriate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexandrainst/da-wit."},
	{"name":"danish-wit","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/severo/danish-wit","creator_name":"Sylvain Lesage","creator_url":"https://huggingface.co/severo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Danish WIT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGoogle presented the Wikipedia Image Text (WIT) dataset in July\\n2021, a dataset which contains\\nscraped images from Wikipedia along with their descriptions. WikiMedia released\\nWIT-Base in September\\n2021,\\nbeing a modified version of WIT where they have removed the images with empty\\n\\\"reference descriptions\\\", as well as removing images where a person's face covers more\\nthan 10% of the image surface, along with inappropriate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/severo/danish-wit."},
	{"name":"scandi-wiki","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexandrainst/scandi-wiki","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","description":"ScandiWiki is a parsed and deduplicated version of the Danish, Norwegian Bokm√•l,\\nNorwegian Nynorsk, Swedish, Icelandic and Faroese Wikipedia corpora, as of January\\n2023."},
	{"name":"qa_squadshifts_synthetic","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lmqg/qa_squadshifts_synthetic","creator_name":"LMQG","creator_url":"https://huggingface.co/lmqg","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"lmqg/qa_squadshifts_synthetic\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis is a synthetic QA dataset generated with fine-tuned QG models over lmqg/qa_squadshifts, made for question-answering based evaluation (QAE) for question generation model proposed by Zhang and Bansal, 2019.\\nThe test split is the original validation set of lmqg/qa_squadshifts, where the model should be evaluate on.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\nquestion-answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lmqg/qa_squadshifts_synthetic."},
	{"name":"cmu_wiki_qa","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sedthh/cmu_wiki_qa","creator_name":"Richard Nagyfi","creator_url":"https://huggingface.co/sedthh","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"cmu_wiki_qa\\\"\\n\\t\\n\\nA filtered / cleaned version of the http://www.cs.cmu.edu/~ark/QA-data/ Q&A dataset, which provides manually-generated factoid questions from Wikipedia articles.\\nAcknowledgments\\nThese data were collected by Noah Smith, Michael Heilman, Rebecca Hwa, Shay Cohen, Kevin Gimpel, and many students at Carnegie Mellon University and the University of Pittsburgh between 2008 and 2010.\\nTheir research project was supported by NSF IIS-0713265 (to Smith), an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sedthh/cmu_wiki_qa."},
	{"name":"faquad-nli","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ruanchaves/faquad-nli","creator_name":"Ruan Chaves Rodrigues","creator_url":"https://huggingface.co/ruanchaves","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for FaQuAD-NLI\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nFaQuAD is a Portuguese reading comprehension dataset that follows the format of the Stanford Question Answering Dataset (SQuAD). It is a pioneer Portuguese reading comprehension dataset using the challenging format of SQuAD. The dataset aims to address the problem of abundant questions sent by academics whose answers are found in available institutional documents in the Brazilian higher education system. It consists of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ruanchaves/faquad-nli."},
	{"name":"latvian-text","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RaivisDejus/latvian-text","creator_name":"Raivis Dejus","creator_url":"https://huggingface.co/RaivisDejus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLatvian text dataset\\n\\t\\n\\nData set of latvian language texts. Intended for use in AI tool development, like speech recognition or spellcheckers\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData sources used\\n\\t\\n\\n\\nLatvian Wikisource articles - https://wikisource.org/wiki/Category:Latvian\\nLiterary works of Rainis - https://repository.clarin.lv/repository/xmlui/handle/20.500.12574/41\\nLatvian Wikipedia articles - https://huggingface.co/datasets/joelito/EU_Wikipedias\\nEuropean Parliament Proceedings Parallel Corpus -‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RaivisDejus/latvian-text."},
	{"name":"squad-sk","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TUKE-DeutscheTelekom/squad-sk","creator_name":"TUKE and DTSS cooperation","creator_url":"https://huggingface.co/TUKE-DeutscheTelekom","description":"        Slovak translation of Standford Question Answering Dataset"},
	{"name":"rebel-dataset-de","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mingaflo/rebel-dataset-de","creator_name":"Florian","creator_url":"https://huggingface.co/mingaflo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for German REBEL Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is the German version of Babelscape/rebel-dataset. It has been generated using CROCODILE.\\nThe Wikipedia Version is from November 2022. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n\\nGerman\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n{\\\"docid\\\": \\\"9400003\\\",\\n \\\"title\\\": \\\"Odin-Gletscher\\\",\\n \\\"uri\\\": \\\"Q7077818\\\",\\n \\\"text\\\": \\\"Der Odin-Gletscher ist ein kleiner Gletscher im ostantarktischen Viktorialand. Er flie√üt von den Westh√§ngen des Mount‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mingaflo/rebel-dataset-de."},
	{"name":"NewQA","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/badokorach/NewQA","creator_name":"brenda Adokorach","creator_url":"https://huggingface.co/badokorach","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"squad\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nMore Information Needed\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nMore Information Needed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/badokorach/NewQA."},
	{"name":"rebel-dataset-de","keyword":"wikidata","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mingaflo/rebel-dataset-de","creator_name":"Florian","creator_url":"https://huggingface.co/mingaflo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for German REBEL Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is the German version of Babelscape/rebel-dataset. It has been generated using CROCODILE.\\nThe Wikipedia Version is from November 2022. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n\\nGerman\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n{\\\"docid\\\": \\\"9400003\\\",\\n \\\"title\\\": \\\"Odin-Gletscher\\\",\\n \\\"uri\\\": \\\"Q7077818\\\",\\n \\\"text\\\": \\\"Der Odin-Gletscher ist ein kleiner Gletscher im ostantarktischen Viktorialand. Er flie√üt von den Westh√§ngen des Mount‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mingaflo/rebel-dataset-de."},
	{"name":"sql-create-context-copy","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/philschmid/sql-create-context-copy","creator_name":"Philipp Schmid","creator_url":"https://huggingface.co/philschmid","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFork of b-mc2/sql-create-context\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/philschmid/sql-create-context-copy."},
	{"name":"wikipedia-zh-mnbvc","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wanng/wikipedia-zh-mnbvc","creator_name":"wangjunjie","creator_url":"https://huggingface.co/wanng","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tzhwiki-mnbvc\\n\\t\\n\\nÂàÜÈ°πÁõÆÔºöÁà¨ÂèñÂπ∂Â§ÑÁêÜ‰∏≠ÊñáÁª¥Âü∫ÁôæÁßëËØ≠Êñô\\nÊï∞ÊçÆÊó∂Èó¥Ôºö202302-202305 ÔºàÊåÅÁª≠Êõ¥Êñ∞Ôºâ\\n‰∏ªÈ°πÁõÆÔºöMNBVC(Massive Never-ending BT Vast Chinese corpus)Ë∂ÖÂ§ßËßÑÊ®°‰∏≠ÊñáËØ≠ÊñôÈõÜ https://github.com/esbatmop/MNBVC\\nËØ•È°πÁõÆÊ∏ÖÊ¥óÊµÅÁ®ã‰∏ªË¶ÅÂèÇËÄÉÔºöhttps://kexue.fm/archives/4176/comment-page-1\\nÂπ∂‰∏î‰ΩøÁî®ÁªÑÂëòÂºÄÂèëÁöÑÂéªÈáçÂ∑•ÂÖ∑ËøõË°åÊï∞ÊçÆÊ†ºÂºèÂåñ„ÄÇ\\nÊÄªË°åÊï∞ÔºàÊ†∑Êú¨Ôºâ: 10,754,146\\n‰∏Ä‰∏™Á§∫‰æãÔºö\\n{\\n  \\\"Êñá‰ª∂Âêç\\\": \\\"cleaned/zhwiki-20230420/folder_0/723712.txt\\\",\\n  \\\"ÊòØÂê¶ÂæÖÊü•Êñá‰ª∂\\\": false,\\n  \\\"ÊòØÂê¶ÈáçÂ§çÊñá‰ª∂\\\": false,\\n  \\\"Êñá‰ª∂Â§ßÂ∞è\\\": 558,\\n  \\\"simhash\\\": 14363740497821204542,\\n  \\\"ÊúÄÈïøÊÆµËêΩÈïøÂ∫¶\\\": 142,\\n  \\\"ÊÆµËêΩÊï∞\\\": 6,\\n  \\\"ÂéªÈáçÊÆµËêΩÊï∞\\\": 6,\\n  \\\"‰ΩéË¥®ÈáèÊÆµËêΩÊï∞\\\": 0,\\n  \\\"ÊÆµËêΩ\\\": [\\n    {‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wanng/wikipedia-zh-mnbvc."},
	{"name":"wikipedia-1k-cohere-openai-embeddings","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KShivendu/wikipedia-1k-cohere-openai-embeddings","creator_name":"Kumar Shivendu","creator_url":"https://huggingface.co/KShivendu","description":"Smaller version of https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings that includes Cohere as well as OpenAI embeddings (text-embedding-ada-002)\\n100k version of this dataset will be released soon. \\n"},
	{"name":"wikipedia_tw","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jslin09/wikipedia_tw","creator_name":"Chun-Hsien Lin","creator_url":"https://huggingface.co/jslin09","description":"Ë¶ÅÊêûËá™Â∑±ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåÊúÄÂü∫Êú¨ÁöÑÂü∫Êú¨ÔºåÂ∞±ÊòØÈúÄË¶Å‰∏ÄÂ§ßÂ†ÜÊñáÂ≠óË≥áÊñôÔºåÂæû Common Crawl ‰∏äÈ†≠ÊäìÂõû‰æÜÊÖ¢ÊÖ¢Ê∏ÖÊ¥óÊòØ‰∏ÄÊ¢ùË∑ØÔºåÊ∏ÖÊ¥óÁ∂≠Âü∫ÁôæÁßëÁ∂≤Á´ôÁöÑÈÄ±ÊúüÊÄß‰∏ãËºâÊ™î‰πüÊòØ‰∏ÄÂÄãÊñπÊ≥ï„ÄÇÊú¨Ë≥áÊñôÈõÜÊòØËß£ÊûêËá™Á∂≠Âü∫ÁôæÁßëÊñº 20240420 ÁôºÂ∏ÉÁöÑÁπÅÈ´î‰∏≠ÊñáÁâàÊâìÂåÖÊ™î bz2 Ê™îÊ°àÁöÑÂÖßÂÆπÔºåÂú®Ëß£ÊûêÂá∫ÊâÄÈúÄÂÖßÂÆπÂæåÔºåÂà©Áî® wikitextparser ÁßªÈô§ Wiki Ê®ôË®ò„ÄÇËß£ÊûêÂæå‰øùÁïôÁöÑÊ¨Ñ‰ΩçÊúâÂÖ©ÂÄãÔºöÊ¢ùÁõÆÂêçÁ®±ÔºàtitleÔºâÔºåÊ¢ùÁõÆÂÖßÂÆπÔºàpage articleÔºâ„ÄÇ\\nÂéüÂßãÁöÑÊâìÂåÖÊ™îÊ¢ùÁõÆÂÖßÂÆπÁ∞°ÁπÅÊ∑∑ÈõúÔºåÊâÄ‰ª•ÊúâÂà©Áî® OpenCC ÈÄ≤Ë°åÁ∞°ËΩâÁπÅËôïÁêÜ„ÄÇ\\n\\nÂéüÂßãÁ∏ΩÊ¢ùÁõÆÊï∏: 4,451,426 Ê¢ùÁõÆ„ÄÇ\\nÂÖ®ÈÉ® 4,451,426 ÂÄãÊ¢ùÁõÆÊ®ôÈ°å„ÄÇ\\nÁÑ°Ê≥ïËá™ÂãïÂéªÊ®ôË®òÁöÑÊ¢ùÁõÆÊï∏: 3,035,750\\nÊúâÂÖßÂÆπÁöÑÊ¢ùÁõÆÊï∏: 1,415,676\\n\\nÂõ†ÁÇ∫Êú¨Ë≥áÊñôÈõÜÂÖßÂÆπÈæêÂ§ßÔºåË¶ÅÂ°ûÈÄ≤‰∏ÄËà¨ÁöÑÂÄã‰∫∫ÈõªËÖ¶‰∏≠ÈÄ≤Ë°åË®àÁÆóÔºåÊÅêÊÄïÊúÉÊúâË≥áÊ∫ê‰∏çË∂≥ÁöÑÊÉÖÂΩ¢„ÄÇÂª∫Ë≠∞‰ΩøÁî®parquetÊ†ºÂºè‰∏ãËºâ‰ΩøÁî®„ÄÇ\\nË≥áÊñôÈõÜÁï∂‰∏≠Êúâ‰∏çÂ∞ëÂÖßÂÆπÁÇ∫ #REDIRECT ÁöÑÊ¢ùÁõÆÂ∑≤Á∂ìÂòóË©¶ÁßªÈô§ÔºåÂ¶ÇÊûúÁßªÈô§ÁöÑ‰∏ç‰πæÊ∑®ÔºåÂ∞±Á≠â‰ª•ÂæåÊúâÁ©∫Êé®Âá∫‰øÆÊ≠£ÁâàÂÜç‰æÜÊ∏ÖÊ¥ó‰∫Ü„ÄÇ\\n"},
	{"name":"wikidata-en-descriptions","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/derenrich/wikidata-en-descriptions","creator_name":"Daniel Erenrich","creator_url":"https://huggingface.co/derenrich","description":"derenrich/wikidata-en-descriptions dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"wikidata-en-descriptions","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/derenrich/wikidata-en-descriptions","creator_name":"Daniel Erenrich","creator_url":"https://huggingface.co/derenrich","description":"derenrich/wikidata-en-descriptions dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"squad","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lhoestq/squad","creator_name":"Quentin Lhoest","creator_url":"https://huggingface.co/lhoestq","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."},
	{"name":"music-wiki","keyword":"wiki","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/seungheondoh/music-wiki","creator_name":"seungheon.doh","creator_url":"https://huggingface.co/seungheondoh","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"music-wiki\\\"\\n\\t\\n\\nüìöüéµ Introducing music-wiki \\nüìäüé∂ Our data collection process unfolds as follows: \\n\\nStarting with a seed page from Wikipedia's music section, we navigate through a referenced page graph, employing recursive crawling up to a depth of 20 levels.\\nSimultaneously, tapping into the rich MusicBrainz dump, we encounter a staggering 11 million unique music entities spanning 10 distinct categories. These entities serve as the foundation for utilizing the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/seungheondoh/music-wiki."},
	{"name":"Korean_Wikipedia_Dataset_for_GPT2_August_2022","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/eaglewatch/Korean_Wikipedia_Dataset_for_GPT2_August_2022","creator_name":"Yongwoo Jeong","creator_url":"https://huggingface.co/eaglewatch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for korean_wikipedia_dataset_for_GPT2\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nEntire Korean language Wikipedia data for GPT-2 training as of August 1st, 2022.\\nemail: oscar.eaglewatch@gmail.com\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis is to make a pre-trained GPT-2 Korean model\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nKorean\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nTrain wikipedia article count: 334420\\nvalidation wikipedia article count: 83605\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eaglewatch/Korean_Wikipedia_Dataset_for_GPT2_August_2022."},
	{"name":"OneOS","keyword":"wikipedia","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wasertech/OneOS","creator_name":"Danny Waser","creator_url":"https://huggingface.co/wasertech","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOneOS Dataset\\n\\t\\n\\nThe OneOS dataset is a collection of text data for the OneOS project. It consists of a large number of text samples that can be used for training and evaluating natural language processing models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nNumber of Samples: 13,068\\nLicense: CC0*\\nLanguage: English, French\\n\\n  * Only unlicensed sentences generated manually fall under CreativeCommon-0. Sentences already licensed under different terms, such as nl2bash or samantha-data, remain‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wasertech/OneOS."},
	{"name":"wikianc","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cyanic-selkie/wikianc","creator_name":"cyanic-selkie","creator_url":"https://huggingface.co/cyanic-selkie","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WikiAnc\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe WikiAnc dataset is an automatically generated dataset from Wikipedia (all languages) and Wikidata dumps (August, 2023). \\nThe code for generating the dataset can be found here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\n\\nwikificiation: The dataset can be used to train a model for Wikification.\\nnamed-entity-linking: The dataset can be used to train a model for Named Entity Linking.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe text in the dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cyanic-selkie/wikianc."},
	{"name":"wikitoxic","keyword":"wikipedia","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pietrolesci/wikitoxic","creator_name":"Pietro Lesci","creator_url":"https://huggingface.co/pietrolesci","description":"This is the same dataset as OxAISH-AL-LLM/wiki_toxic.\\nThe only differences are\\n\\nAddition of a unique identifier, uid\\n\\nAddition of the indices, that is 3 columns with the embeddings of 3 different sentence-transformers\\n\\nall-mpnet-base-v2\\nmulti-qa-mpnet-base-dot-v1\\nall-MiniLM-L12-v2\\n\\n\\nRenaming of the label column to labels for easier compatibility with the transformers library\\n\\n\\n"},
	{"name":"wikianc","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cyanic-selkie/wikianc","creator_name":"cyanic-selkie","creator_url":"https://huggingface.co/cyanic-selkie","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WikiAnc\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe WikiAnc dataset is an automatically generated dataset from Wikipedia (all languages) and Wikidata dumps (August, 2023). \\nThe code for generating the dataset can be found here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\n\\nwikificiation: The dataset can be used to train a model for Wikification.\\nnamed-entity-linking: The dataset can be used to train a model for Named Entity Linking.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe text in the dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cyanic-selkie/wikianc."},
	{"name":"sql-parsed","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VishalCh/sql-parsed","creator_name":"Vishal Choudhary","creator_url":"https://huggingface.co/VishalCh","description":"VishalCh/sql-parsed dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"sql-create-context-id","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/detakarang/sql-create-context-id","creator_name":"Gede Putra Nugraha","creator_url":"https://huggingface.co/detakarang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is a fork from sql-create-context \\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/detakarang/sql-create-context-id."},
	{"name":"squad-augmented-v2","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/christti/squad-augmented-v2","creator_name":"Christoph Timmermann","creator_url":"https://huggingface.co/christti","description":"christti/squad-augmented-v2 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"pandas-create-context","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hiltch/pandas-create-context","creator_name":"Or Hiltch","creator_url":"https://huggingface.co/hiltch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is built from sql-create-context, which in itself builds from WikiSQL and Spider.\\nI have used GPT4 to translate the SQL schema into pandas DataFrame schem initialization statements and to translate the SQL queries into pandas queries. \\nThere are 862 examples of natural language queries, pandas DataFrame creation statements, and pandas query answering the question using the DataFrame creation statement as context. This dataset was built with text-to-pandas‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hiltch/pandas-create-context."},
	{"name":"entity_popularity","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/masaki-sakata/entity_popularity","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEntity Popularity Dataset\\n\\t\\n\\nThis dataset contains information for about 26,000 entities, including the Wikipedia article title, QID, and the annual article view count for the year 2021. \\nThe annual article view count can be considered as an indicator of the popularity of a entity.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThis dataset is composed in English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"masaki-sakata/entity_popularity\\\")[\\\"en\\\"]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/entity_popularity."},
	{"name":"Bharat_NanoDBPedia_as","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_as","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Assamese version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_as."},
	{"name":"Bharat_NanoDBPedia_awa","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_awa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Awadhi version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_awa."},
	{"name":"Bharat_NanoDBPedia_bho","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bho","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Bhojpuri version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bho."},
	{"name":"Bharat_NanoDBPedia_gu","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_gu","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Gujarati version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_gu."},
	{"name":"Bharat_NanoDBPedia_kn","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_kn","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Kannada version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_kn."},
	{"name":"Bharat_NanoDBPedia_ksa","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Kashmiri (Arabic script) version of the NanoDBPedia dataset, specifically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksa."},
	{"name":"Bharat_NanoDBPedia_mai","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Maithili version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mai."},
	{"name":"Bharat_NanoDBPedia_mr","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mr","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Marathi version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mr."},
	{"name":"Bharat_NanoDBPedia_pa","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_pa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Punjabi version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_pa."},
	{"name":"Bharat_NanoDBPedia_sa","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_sa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Sanskrit version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_sa."},
	{"name":"Bharat_NanoDBPedia_te","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_te","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Telugu version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_te."},
	{"name":"Bharat_NanoDBPedia_ur","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ur","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\\n\\t\\n\\t\\t\\n\\t\\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis particular dataset is the Urdu version of the NanoDBPedia dataset, specifically adapted for information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ur."},
	{"name":"UniHGKR_Date_Text_Pairs","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs","creator_name":"Dehai Min","creator_url":"https://huggingface.co/ZhishanQ","description":"See description and preview to understand the content and structure of this corpus.\\nThis dataset is from our paper: UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers.\\nPlease see our github repository UniHGKR to know how to use this dataset and its format.\\nIf you find this resource useful in your research, please consider giving a like and citation.\\n@article{min2024unihgkr,\\n  title={UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers},\\n  author={Min, Dehai‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs."},
	{"name":"UniHGKR_Date_Text_Pairs","keyword":"wiki","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs","creator_name":"Dehai Min","creator_url":"https://huggingface.co/ZhishanQ","description":"See description and preview to understand the content and structure of this corpus.\\nThis dataset is from our paper: UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers.\\nPlease see our github repository UniHGKR to know how to use this dataset and its format.\\nIf you find this resource useful in your research, please consider giving a like and citation.\\n@article{min2024unihgkr,\\n  title={UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers},\\n  author={Min, Dehai‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs."},
	{"name":"UniHGKR_Date_Text_Pairs","keyword":"wikidata","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs","creator_name":"Dehai Min","creator_url":"https://huggingface.co/ZhishanQ","description":"See description and preview to understand the content and structure of this corpus.\\nThis dataset is from our paper: UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers.\\nPlease see our github repository UniHGKR to know how to use this dataset and its format.\\nIf you find this resource useful in your research, please consider giving a like and citation.\\n@article{min2024unihgkr,\\n  title={UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers},\\n  author={Min, Dehai‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs."}
]
;
