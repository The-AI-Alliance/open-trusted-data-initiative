var data_for_multiple_choice = 
[
	{"name":"UHGEvalDataset","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Ki-Seki/UHGEvalDataset","creator_name":"Shichao Song","creator_url":"https://huggingface.co/Ki-Seki","description":"The dataset sourced from https://github.com/IAAR-Shanghai/UHGEval\\n"},
	{"name":"tinyTruthfulQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tinyBenchmarks/tinyTruthfulQA","creator_name":"tinyBenchmarks","creator_url":"https://huggingface.co/tinyBenchmarks","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\ttinyTruthfulQA\\n\\t\\n\\nWelcome to tinyTruthfulQA! This dataset serves as a concise version of the truthfulQA dataset, offering a subset of 100 data points selected from the original compilation. \\ntinyTruthfulQA is designed to enable users to efficiently estimate the performance of a large language model (LLM) with reduced dataset size, saving computational resources \\nwhile maintaining the essence of the truthfulQA evaluation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeatures\\n\\t\\n\\n\\nCompact Dataset: With only 100 data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tinyBenchmarks/tinyTruthfulQA."},
	{"name":"PCA-Bench-V1","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PCA-Bench/PCA-Bench-V1","creator_name":"PCA-Bench","creator_url":"https://huggingface.co/PCA-Bench","description":"PCA-Bench\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPCA-Bench is an innovative benchmark for evaluating and locating errors in Multimodal LLMs when conducting embodied decision making tasks, specifically focusing on perception, cognition, and action.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRelease\\n\\t\\n\\n\\n[2024.02.15] PCA-Bench-V1 is released. We release the open and closed track data in huggingface. We also set an online leaderboard  accepting users' submission.\\n[2023.12.15] PCA-EVAL is accepted to Foundation Model for Decision Making Workshop‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PCA-Bench/PCA-Bench-V1."},
	{"name":"CIDAR-MCQ-100","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arbml/CIDAR-MCQ-100","creator_name":"Arabic Machine Learning ","creator_url":"https://huggingface.co/arbml","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"CIDAR-MCQ-100\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCIDAR-MCQ-100\\n\\t\\n\\nCIDAR-MCQ-100 contains 100 multiple-choice questions and answers about the Arabic culture. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìö Datasets Summary\\n\\t\\n\\n\\n  \\nName\\nExplanation\\n\\n\\nCIDAR \\n10,000 instructions and responses in Arabic\\n\\n\\nCIDAR-EVAL-100 \\n100 instructions to evaluate LLMs on cultural relevance\\n\\n\\nCIDAR-MCQ-100 \\n100 Multiple choice questions and answers to evaluate LLMs on cultural relevance \\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\nCategory\\nCIDAR-EVAL-100‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arbml/CIDAR-MCQ-100."},
	{"name":"truthful_qa_context","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/portkey/truthful_qa_context","creator_name":"Portkey AI","creator_url":"https://huggingface.co/portkey","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for truthful_qa_context\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTruthfulQA Context is an extension of the TruthfulQA benchmark, specifically designed to enhance its utility for models that rely on Retrieval-Augmented Generation (RAG). This version includes the original questions and answers from TruthfulQA, along with the added context text directly associated with each question. This additional context aims to provide immediate reference material for models, making it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/portkey/truthful_qa_context."},
	{"name":"xstorycloze_ca","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/xstorycloze_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for xstorycloze_ca\\n\\t\\n\\n\\n\\nxstorycloze_ca is a question answering dataset in Catalan, professionally translated from the English StoryCloze dataset (Spring 2016 version), used  to create its multilingual version XStoryCloze.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nxstorycloze_ca (Multilingual Story Cloze Test - Catalan) is based on multiple-choice narrative completions. The dataset consists of 360 instances in the train split and 1510 instances‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/xstorycloze_ca."},
	{"name":"SeaExam","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SeaLLMs/SeaExam","creator_name":"SeaLLMs - Language Models for Southeast Asian Languages","creator_url":"https://huggingface.co/SeaLLMs","description":"\\nCheck the üèÜ leaderboard constructed with this dataset and the corresponding üë®üèª‚Äçüíª evaluation code.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSeaExam dataset\\n\\t\\n\\nThe SeaExam dataset aims to evaluate Large Language Models (LLMs) on a diverse set of Southeast Asian (SEA) languages including English, Chinese, Indonesian, Thai, and Vietnamese. \\nOur goal is to ensure a fair and consistent comparison across different LLMs on those languages while mitigating the risk of data contamination. \\nIt consists of the following two‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SeaLLMs/SeaExam."},
	{"name":"piqa_ca","keyword":"multiple-choice","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/piqa_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for piqa_ca\\n\\t\\n\\n\\n\\npiqa_ca is a multiple choice question answering dataset in Catalan that has been professionally translated from the PIQA  validation set in English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\npiqa_ca (Physical Interaction Question Answering - Catalan) is designed to evaluate physical commonsense reasoning using question-answer triplets based on everyday situations. It includes 1838 instances in the validation split. Each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/piqa_ca."},
	{"name":"siqa_ca","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/siqa_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for siqa_ca\\n\\t\\n\\n\\n\\nsiqa_ca is a multiple choice question answering dataset in Catalan that has been professionally translated from the SIQA \\nvalidation set in English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nsiqa_ca (Social Interaction Question Answering - Catalan) is designed to evaluate social commonsense intelligence using multiple choice question-answer instances based on reasoning about people‚Äôs actions and their \\nsocial implications. It‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/siqa_ca."},
	{"name":"openbookqa_ca","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/openbookqa_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for openbookqa_ca\\n\\t\\n\\n\\n\\nopenbookqa_ca is a question answering dataset in Catalan, professionally translated from the main version of the OpenBookQA dataset in English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nopenbookqa_ca (Open Book Question Answering - Catalan) is designed to simulate open book exams and assess human-like understanding of a subject. The dataset comprises 500 instances in the validation split and another 500 instances in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/openbookqa_ca."},
	{"name":"arc-c-okapi-eval-es","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alvarobartt/arc-c-okapi-eval-es","creator_name":"Alvaro Bartolome","creator_url":"https://huggingface.co/alvarobartt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tARC-Challenge translated to Spanish\\n\\t\\n\\nThis dataset was generated by the Natural Language Processing Group of the University of Oregon, where they used the\\noriginal ARC-Challenge dataset in English and translated it into different languages using ChatGPT.\\nThis dataset only contains the Spanish translation, but the following languages are also covered within the original\\nsubsets posted by the University of Oregon at http://nlp.uoregon.edu/download/okapi-eval/datasets/.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alvarobartt/arc-c-okapi-eval-es."},
	{"name":"truthfull_qa-tr","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/malhajar/truthfull_qa-tr","creator_name":"Mohamad Alhajar","creator_url":"https://huggingface.co/malhajar","description":"This Dataset is part of a series of datasets aimed at advancing Turkish LLM Developments by establishing rigid Turkish benchmarks to evaluate the performance of LLM's Produced in the Turkish Language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for truthful_qa-tr\\n\\t\\n\\nmalhajar/truthful_qa-tr is a translated version of truthful_qa aimed specifically to be used in the OpenLLMTurkishLeaderboard \\nDeveloped by: Mohamad Alhajar \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTruthfulQA is a benchmark to measure whether a language model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malhajar/truthfull_qa-tr."},
	{"name":"mmlu-tr","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/malhajar/mmlu-tr","creator_name":"Mohamad Alhajar","creator_url":"https://huggingface.co/malhajar","description":"This Dataset is part of a series of datasets aimed at advancing Turkish LLM Developments by establishing rigid Turkish benchmarks to evaluate the performance of LLM's Produced in the Turkish Language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for mmlu-tr\\n\\t\\n\\nmalhajar/mmlu-tr is a translated version of mmlu aimed specifically to be used in the OpenLLMTurkishLeaderboard \\nMMLU (hendrycks_test on huggingface) without auxiliary train. It is much lighter (7MB vs 162MB) and faster than the original implementation, in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malhajar/mmlu-tr."},
	{"name":"truthful_qa_tr","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Atilla00/truthful_qa_tr","creator_name":"Atilla Karaahmetoƒülu","creator_url":"https://huggingface.co/Atilla00","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\n\\\"truthful_qa\\\" translated to Turkish.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\ndataset = load_dataset('Atilla00/truthful_qa_tr', 'generation')\\ndataset = load_dataset('Atilla00/truthful_qa_tr', 'multiple_choice')\\n\\n"},
	{"name":"EthioEmo","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Tadesse/EthioEmo","creator_name":"Tadesse Destaw Belay","creator_url":"https://huggingface.co/Tadesse","description":"Tadesse/EthioEmo dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"mmlu_tr-v0.2","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/malhajar/mmlu_tr-v0.2","creator_name":"Mohamad Alhajar","creator_url":"https://huggingface.co/malhajar","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for mmlu_tr-v0.2\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nmalhajar/mmlu_tr-v0.2 is an enhanced version of the original mmlu-tr dataset, specifically developed for use in the OpenLLMTurkishLeaderboard v0.2. This iteration of the dataset has been translated into Turkish using advanced language models like GPT-4, with English text provided for cross-checking to ensure accuracy and reliability. The dataset is tailored to assist in evaluating the performance of Turkish language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malhajar/mmlu_tr-v0.2."},
	{"name":"MARVEL","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kianasun/MARVEL","creator_name":"Kiana Sun","creator_url":"https://huggingface.co/kianasun","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nMARVEL is a new comprehensive benchmark dataset that evaluates multi-modal large language models' abstract reasoning abilities in six patterns across five different task configurations, revealing significant performance gaps between humans and SoTA MLLMs.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\nRepository: https://github.com/1171-jpg/MARVEL_AVR\\nPaper [optional]: https://arxiv.org/abs/2404.13591\\nDemo [optional]:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kianasun/MARVEL."},
	{"name":"arc_ca","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/arc_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for arc_ca\\n\\t\\n\\n\\n\\narc_ca is a question answering dataset in Catalan, professionally translated from the Easy and Challenge versions of the ARC dataset in English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\narc_ca (AI2 Reasoning Challenge - Catalan) is based on multiple-choice science questions at elementary school level. The dataset consists of 2950 instances in the Easy version (570 in the test and 2380 instances in the validation split) and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/arc_ca."},
	{"name":"GroundCocoa","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/harsh147/GroundCocoa","creator_name":"Harsh Kohli","creator_url":"https://huggingface.co/harsh147","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nGroundCocoa is a benchmark to evaluate conditional and compositional reasoning in large language models through a flight-booking task presented in multiple-choice format.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\nThe test set consists of 4849 samples consisting of 728 unique user requirements. User requirements may be repeated with varying options. In additon, we also provide a small validation set that may be used for certain parameter tuning. It consists of 52‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/harsh147/GroundCocoa."},
	{"name":"race-qa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/klaylouis1932/race-qa","creator_name":"Klay Louis","creator_url":"https://huggingface.co/klaylouis1932","description":"klaylouis1932/race-qa dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"XCOPA-eu","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/XCOPA-eu","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for XCOPA-eu\\n\\t\\n\\n\\nPoint of Contact: hitz@ehu.eus\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nXCOPA-eu is the professional translation to Basque of the English COPA dataset (Roemmmele et al., 2011),\\nin the spirit of the XCOPA effort (Ponti et al., 2020). \\nCOPA is a dataset of causal commmonsense reasoning that focuses on cause-effect relationships between a\\npremise and two choices.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n\\neu-ES\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/XCOPA-eu."},
	{"name":"truthful_qa-cs","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CIIRC-NLP/truthful_qa-cs","creator_name":"NLP Team at the Czech Institute of Informatics, Robotics and Cybernetics","creator_url":"https://huggingface.co/CIIRC-NLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCzech TruthfulQA\\n\\t\\n\\nThis is a Czech translation of the original TruthfulQA dataset, created using the WMT 21 En-X model. \\nOnly the multiple-choice variant of the dataset is included.\\nThe translation was completed for use within the Czech-Bench evaluation framework. \\nThe script used for translation can be reviewed here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nOriginal dataset:\\n@misc{lin2021truthfulqa,\\n    title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},\\n    author={Stephanie Lin and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CIIRC-NLP/truthful_qa-cs."},
	{"name":"arc-cs","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CIIRC-NLP/arc-cs","creator_name":"NLP Team at the Czech Institute of Informatics, Robotics and Cybernetics","creator_url":"https://huggingface.co/CIIRC-NLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCzech AI2 Reasoning Challenge\\n\\t\\n\\nThis is a Czech translation of the original ARC dataset, created using the WMT 21 En-X model.\\nThe translation was completed for use within the Czech-Bench evaluation framework. \\nThe script used for translation can be reviewed here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nOriginal dataset:\\n@article{allenai:arc,\\n      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and\\n                    Ashish Sabharwal and Carissa Schoenick and Oyvind‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CIIRC-NLP/arc-cs."},
	{"name":"mmlu-cs","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CIIRC-NLP/mmlu-cs","creator_name":"NLP Team at the Czech Institute of Informatics, Robotics and Cybernetics","creator_url":"https://huggingface.co/CIIRC-NLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCzech MMLU\\n\\t\\n\\nThis is a Czech translation of the original MMLU dataset, created using the WMT 21 En-X model. \\nThe 'auxiliary_train' subset is not included.\\nThe translation was completed for use within the Czech-Bench evaluation framework. \\nThe script used for translation can be reviewed here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nOriginal dataset:\\n@article{hendryckstest2021,\\n  title={Measuring Massive Multitask Language Understanding},\\n  author={Dan Hendrycks and Collin Burns and Steven Basart‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CIIRC-NLP/mmlu-cs."},
	{"name":"winograd_th","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pakphum/winograd_th","creator_name":"phakphum artkaew","creator_url":"https://huggingface.co/pakphum","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tA collection of Thai Winograd Schemas\\n\\t\\n\\nWe present a collection of Winograd Schemas in the Thai language. These schemas are adapted from the original set of English Winograd Schemas proposed by Levesque et al., which was based on Ernest Davis's collection.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Translation\\n\\t\\n\\nTwo professional translators, who were native Thai speakers fluent in English and had experience translating from English to Thai, were hired. In a pilot translation phase, one native speaker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pakphum/winograd_th."},
	{"name":"kmZQBkk558WWAGV","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/israel/kmZQBkk558WWAGV","creator_name":"Israel Abebe Azime","creator_url":"https://huggingface.co/israel","description":"israel/kmZQBkk558WWAGV dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"PubmedQA-Mixtral-CoT","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/PubmedQA-Mixtral-CoT","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for pubmedqa-cot\\n\\t\\n\\n\\n\\nSynthetically enhanced responses to the pubmedqa dataset using mixtral.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nTo increase the quality of answers from the training splits of the PubMedQA dataset, we leverage Mixtral-8x7B to generate Chain of Thought(CoT) answers. We create a custom prompt for the dataset, along with a\\nhand-crafted list of few-shot examples. For a multichoice answer, we ask the model to rephrase and explain the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/PubmedQA-Mixtral-CoT."},
	{"name":"MedMCQA-Mixtral-CoT","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/MedMCQA-Mixtral-CoT","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for medmcqa-cot\\n\\t\\n\\n\\n\\nSynthetically enhanced responses to the medmcqa dataset using mixtral.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nTo increase the quality of answers from the training splits of the MedMCQA dataset, we leverage Mixtral-8x7B to generate Chain of Thought(CoT) answers. We create a custom prompt for the dataset, along with a\\nhand-crafted list of few-shot examples. For a multichoice answer, we ask the model to rephrase and explain the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedMCQA-Mixtral-CoT."},
	{"name":"MedQA-Mixtral-CoT","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/MedQA-Mixtral-CoT","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for medqa-cot\\n\\t\\n\\n\\n\\nSynthetically enhanced responses to the medqa dataset using mixtral.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nTo increase the quality of answers from the training splits of the MedQA dataset, we leverage Mixtral-8x7B to generate Chain of Thought(CoT) answers. We create a custom prompt for the dataset, along with a\\nhand-crafted list of few-shot examples. For a multichoice answer, we ask the model to rephrase and explain the question‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedQA-Mixtral-CoT."},
	{"name":"nifty-rl","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/raeidsaqur/nifty-rl","creator_name":"Raeid Saqur","creator_url":"https://huggingface.co/raeidsaqur","description":"\\n  \\n    The News-Informed Financial Trend Yield (NIFTY) Dataset. \\n\\n\\nThe News-Informed Financial Trend Yield (NIFTY) Dataset. Details of the dataset, including data procurement and filtering can be found in the paper here: https://arxiv.org/abs/2405.09747.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìã Table of Contents\\n\\t\\n\\n\\nüß© NIFTY Dataset\\nüìã Table of Contents\\nüìñ Usage\\nDownloading the dataset\\nDataset structure\\n\\n\\nLarge Language Models \\n‚úçÔ∏è Contributing\\nüìù Citing\\nüôè Acknowledgements\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìñ Usage\\n\\t\\n\\nDownloading and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raeidsaqur/nifty-rl."},
	{"name":"Sailcompass_data","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sail/Sailcompass_data","creator_name":"Sea AI Lab","creator_url":"https://huggingface.co/sail","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSailCompass: Towards Reproducible and Robust Evaluation for Southeast Asian Languages\\n\\t\\n\\nThis repository provides the dataset for evaluation SEA large language model.\\n\\nProject Website: sailorllm.github.io\\nCodebase: https://github.com/sail-sg/sailcompass\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAcknowledgment\\n\\t\\n\\nThanks to the contributors of the opencompass.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCiting this work\\n\\t\\n\\nIf you use this repository or sailor models, please cite\\n@misc{sailcompass,\\n      title={SailCompass: Towards Reproducible‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sail/Sailcompass_data."},
	{"name":"MMEvalPro","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MM-Diagnose/MMEvalPro","creator_name":"MM-Diagnose","creator_url":"https://huggingface.co/MM-Diagnose","description":"MMEvalPro\\n\\n\\n  \\n    \\n  ¬†¬†\\n   \\n    \\n  ¬†¬†\\n  \\n    \\n  \\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MMEvalPro\\n\\t\\n\\nWe create MMEvalPro for more accurate and efficent evaluation for Large Multimodal Models. It is designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question from existing benchmarks, human annotators augment it by creating one perception question and one knowledge anchor question through a meticulous annotation process.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MM-Diagnose/MMEvalPro."},
	{"name":"MUIRBENCH","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH","creator_name":"MUIRBENCH","creator_url":"https://huggingface.co/MUIRBENCH","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding\\n\\t\\n\\nüåê Homepage | üìñ Paper | üíª Evaluation \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntro\\n\\t\\n\\nMuirBench is a benchmark containing 11,264 images and 2,600 multiple-choice questions, providing robust evaluation on 12 multi-image understanding tasks.\\n\\nMuirBench evaluates on a comprehensive range of 12 multi-image understanding abilities, e.g. geographic understanding, diagram understanding, visual retrieval, ..., etc, while prior benchmarks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH."},
	{"name":"belebele_gl","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/proxectonos/belebele_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for belebele_gl\\n\\t\\n\\nBelebele is a multiple-choice machine reading comprehension (MRC) dataset. The original dataset includes 122 language variants, with this dataset we include Galician language.\\nIt is composed of 900 items translated and adapted to Galician language from the Spanish version. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nRepository: Proxecto N√ìS at HuggingFace\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUses\\n\\t\\n\\n\\nIt can be used to evaluate Galician language models.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/belebele_gl."},
	{"name":"openbookqa_gl","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/proxectonos/openbookqa_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for OpenBookQA_gl\\n\\t\\n\\nopenbookqa_gl is a question answering dataset in Galician, translated from the OpenBookQA dataset in English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nopenbookqa_gl is designed to simulate open book exams and assess human-like understanding of a subject. The dataset comprises 500 instances in the validation split and another 500 instances in the test split. Each instance contains a question stem, four possible choices, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/openbookqa_gl."},
	{"name":"truthfulqa_gl","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/proxectonos/truthfulqa_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for TruthfulQA_gl\\n\\t\\n\\n\\n\\nTruthfulQA_gl is the Galician version of the TruthfulQA dataset.\\nThis dataset is used to measure the truthfulness of a language model when generating answers to questions. It includes questions from different categories that some humans would answer wrongly due to false beliefs or misconceptions.\\nNote that this version includes only the generation split.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nRepository: Proxecto N√ìS at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/truthfulqa_gl."},
	{"name":"USQA","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Baron-GG/USQA","creator_name":"Anonymity","creator_url":"https://huggingface.co/Baron-GG","description":"Baron-GG/USQA dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"commonsense-embodied-affordance","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Ayush8120/commonsense-embodied-affordance","creator_name":"Ayush Agrawal","creator_url":"https://huggingface.co/Ayush8120","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPlease refer here for the documentation\\n\\t\\n\\n"},
	{"name":"ECN-QA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/raidium/ECN-QA","creator_name":"Raidium","creator_url":"https://huggingface.co/raidium","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tModel Card for Raidium ECN-QA\\n\\t\\n\\nThe dataset is introduced in the paper \\\"Efficient Medical Question Answering with Knowledge-Augmented Question Generation\\\".\\nPaper: https://arxiv.org/abs/2405.14654\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset contains medical questions of different types. It was built from passed ECN exams (french medical examination) and questions created by FreeCN.\\nThe questions can be:\\n\\nIQ (individual question) containing a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raidium/ECN-QA."},
	{"name":"multiple-choice-questions","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mateus-hamade/multiple-choice-questions","creator_name":"Mateus Hamade","creator_url":"https://huggingface.co/mateus-hamade","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuest√µes de M√∫ltipla Escolha - Base de dados (PT-BR)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContextualiza√ß√£o\\n\\t\\n\\nEste reposit√≥rio cont√©m uma base de dados (data.json) com quest√µes de m√∫ltipla escolha, a qual foi utilizada principalmente no desenvolvimento de modelos de recupera√ß√£o de informa√ß√£o.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescri√ß√£o do conjunto de dados\\n\\t\\n\\nO conjunto de dados √© composto por quest√µes de m√∫ltipla escolha, abrangendo uma variedade de temas dentro da √°rea da Ci√™ncia da Computa√ß√£o. Cada quest√£o √© estruturada‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mateus-hamade/multiple-choice-questions."},
	{"name":"FRoG","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GAIR/FRoG","creator_name":"SII - GAIR","creator_url":"https://huggingface.co/GAIR","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nFRoG is a fuzzy reasoning benchmark of generalized quantifiers to evaluate the fuzzy reasoning abilities of a model. The questions in FRoG are collected from real-world math word problem benchmarks GSM8K and MathQA and the generalized quantifier that is used to introduce fuzziness come from QuRe.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSample Data\\n\\t\\n\\n{\\n\\\"id\\\": 1,\\n\\\"question\\\": \\\"john and ingrid pay [MASK] and 40 % tax annually , respectively . if john makes $ 60000 and ingrid makes $ 72000 , what‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GAIR/FRoG."},
	{"name":"xstorycloze_gl","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/proxectonos/xstorycloze_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for xstorycloze_gl\\n\\t\\n\\n\\n\\nxstorycloze_gl is a question answering dataset in Galician, translated from the English StoryCloze dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nxstorycloze_gl is based on multiple-choice narrative completions. The dataset consists of 360 instances in the train split and 1511 instances in the test split. Each instance contains a story stem, divided in 4 sentences, 2 possible completions, and the number indicating the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/xstorycloze_gl."},
	{"name":"teleia","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gonzmart/teleia","creator_name":"Gonzalo","creator_url":"https://huggingface.co/gonzmart","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSpanish Language Benchmark for Artificial Intelligence Models (TELEIA)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAuthors and Affiliations\\n\\t\\n\\nMarina Mayor-Rocher1 , Nina Melero2,3 , Elena Merino-G√≥mez4 , Miguel Gonz√°lez2 , Raquel Ferrando2 , Javier Conde2 and Pedro Reviriego2\\n\\nUniversidad Aut√≥noma de Madrid\\nUniversidad Polit√©cnica de Madrid\\nNew York University\\nUniversidad de Valladolid\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset contains test questions to evaluate LLMs in Spanish\\n\\nTELEIA_Cervantes_AVE.xlsx: test‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gonzmart/teleia."},
	{"name":"PIQA-eu","keyword":"multiple-choice","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/PIQA-eu","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for PIQA-eu\\n\\t\\n\\n\\nPoint of Contact: hitz@ehu.eus\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nPIQA-eu is the professional translation to Basque of the PIQA's \\n(Bisk et al., 2020) validation partition. \\nPIQA is a commonsense QA benchmark for naive physics reasoning focusing on how we interact with everyday\\nobjects in everyday situations.\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguages\\n\\t\\n\\n\\neu-ES\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nPIQA-eu examples look like this:\\n{‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/PIQA-eu."},
	{"name":"CRAFT-BioQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-BioQA","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCRAFT-BioQA\\n\\t\\n\\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated biology question-answering data proved highly beneficial.\\n\\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-BioQA."},
	{"name":"CRAFT-MedQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-MedQA","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCRAFT-MedQA\\n\\t\\n\\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated medicine question-answering data proved highly beneficial.\\n\\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-MedQA."},
	{"name":"CRAFT-CommonSenseQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-CommonSenseQA","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCRAFT-CommonSenseQA\\n\\t\\n\\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated commonsense question-answering data proved highly beneficial.\\n\\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-CommonSenseQA."},
	{"name":"famma","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/weaverbirdllm/famma","creator_name":"weaverbird_llm","creator_url":"https://huggingface.co/weaverbirdllm","description":"\\n\\t\\n\\t\\t\\n\\t\\tIntroduction\\n\\t\\n\\nFAMMA is a multi-modal financial Q&A benchmark dataset. The questions encompass three heterogeneous image types - tables, charts and text & math screenshots - and span eight subfields in finance, comprehensively covering topics across major asset classes. Additionally, all the questions are categorized by three difficulty levels ‚Äî easy, medium, and hard - and are available in three languages ‚Äî English, Chinese, and French. Furthermore, the questions are divided into two‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weaverbirdllm/famma."},
	{"name":"teleia","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/migonsa/teleia","creator_name":"Miguel Gonz√°lez","creator_url":"https://huggingface.co/migonsa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSpanish Language Benchmark for Artificial Intelligence Models (TELEIA)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAuthors and Affiliations\\n\\t\\n\\nMarina Mayor-Rocher1 , Nina Melero2,3 , Elena Merino-G√≥mez4 , Miguel Gonz√°lez2 , Raquel Ferrando2 , Javier Conde2 and Pedro Reviriego2\\n\\nUniversidad Aut√≥noma de Madrid\\nUniversidad Polit√©cnica de Madrid\\nNew York University\\nUniversidad de Valladolid\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe dataset contains test questions to evaluate LLMs in Spanish\\n\\nTELEIA_Cervantes_AVE: test‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/migonsa/teleia."},
	{"name":"MME-RealWorld","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yifanzhang114/MME-RealWorld","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","description":"\\n2024.11.14 üåü MME-RealWorld now has a lite version (50 samples per task) for inference acceleration, which is also supported by VLMEvalKit and Lmms-eval.\\n2024.10.27 üåü LLaVA-OV currently ranks first on our leaderboard, but its overall accuracy remains below 55%, see our leaderboard for the detail.\\n2024.09.03 üåü MME-RealWorld is now supported in the VLMEvalKit and Lmms-eval repository, enabling one-click evaluation‚Äîgive it a try!\\\" \\n2024.08.20 üåü We are very proud to launch MME-RealWorld, which‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/MME-RealWorld."},
	{"name":"bento","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cindermond/bento","creator_name":"Hongyu Zhao","creator_url":"https://huggingface.co/cindermond","description":"This dataset is based on MMLU, FLAN, Big Bench Hard and AgiEval English.\\nThe non-\\\"reduced\\\" benchmark is the original benchmark, except for FLAN, which is a sampled version. \\nThe \\\"reduced\\\" benchmark only contains a few representative tasks in the original ones, such that the performance on the \\\"reduced\\\" benchmark can serve as an approximation to the performance on the original ones.\\n"},
	{"name":"uhura-truthfulqa","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/masakhane/uhura-truthfulqa","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Uhura-TruthfulQA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTruthfulQA is a widely recognized safety benchmark designed to measure the truthfulness of language model outputs across 38 categories, including health, law, finance, and politics. The English version of the benchmark originates from TruthfulQA: Measuring How Models Mimic Human Falsehoods (Lin et al., 2022) and consists of 817 questions in both multiple-choice and generation formats, targeting common‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/uhura-truthfulqa."},
	{"name":"big_bench_hard","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Joschka/big_bench_hard","creator_name":"Joschka Braun","creator_url":"https://huggingface.co/Joschka","description":"All rights and obligations of the dataset are with original authors of the paper/dataset. \\nI have merely made this dataset with a MIT licence available on HuggingFace.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBIG-Bench Hard Dataset\\n\\t\\n\\nThis repository contains a copy of the BIG-Bench Hard dataset.\\nSmall edits to the formatting of the dataset are made to integrate it into the Inspect Evals repository, a community contributed LLM \\nevaulations for Inspect AI a framework by the UK AI Safety Institute.\\nThe BIG-Bench Hard‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Joschka/big_bench_hard."},
	{"name":"MCEval8K","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iszhaoxin/MCEval8K","creator_name":"XIN ZHAO","creator_url":"https://huggingface.co/iszhaoxin","description":"iszhaoxin/MCEval8K dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"mmlu-translated-kk","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kz-transformers/mmlu-translated-kk","creator_name":"Kaz-Transformers","creator_url":"https://huggingface.co/kz-transformers","description":"\\n\\t\\n\\t\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, please cite:\\n@misc{horde_mmlu_kk2024,\\n  author = {Beksultan Sagyndyk, Sanzhar Murzakhmetov, Sanzhar Umbet, Kirill Yakunin},\\n  title = {MMLU on kazakh language: Translated MMLU Benchmark},\\n  year = {2024},\\n  url = {https://huggingface.co/datasets/mmlu-translated-kk},\\n  note = {Available on Hugging Face}\\n}\\n\\n"},
	{"name":"FinShibainu","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aiqwe/FinShibainu","creator_name":"Jay Lee","creator_url":"https://huggingface.co/aiqwe","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFinShibainu Datset Card\\n\\t\\n\\n\\ngithub: https://github.com/aiqwe/FinShibainu\\nmodel: https://huggingface.co/aiqwe/FinShibainu\\n\\nKRX LLM Í≤ΩÏßÑÎåÄÌöå Î¶¨ÎçîÎ≥¥ÎìúÏóêÏÑú Ïö∞ÏàòÏÉÅÏùÑ ÏàòÏÉÅÌïú shibainu24 Î™®Îç∏Ïùò Îç∞Ïù¥ÌÑ∞ÏÖã RepositoryÏûÖÎãàÎã§.Î™®Îç∏Ïóê ÎåÄÌïú ÎÇ¥Ïö©ÏùÄ https://huggingface.co/aiqwe/FinShibainuÎ•º Ï∞∏Ï°∞Ìï¥Ï£ºÏÑ∏Ïöî.Îç∞Ïù¥ÌÑ∞ÏÖã ÏàòÏßë Î∞è ÌïôÏäµÏóê Í¥ÄÎ†®Îêú ÏΩîÎìúÎäî https://github.com/aiqwe/FinShibainuÏóê ÏûêÏÑ∏ÌïòÍ≤å Í≥µÍ∞úÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDPO\\n\\t\\n\\nPreferenceÏùò AÎäî answer_A, BÎäî answer_B Ïª¨ÎüºÏûÖÎãàÎã§.\\n\\nanswer_A: ReferenceÏôÄ ÏßàÎ¨∏ÏùÑ Ìï®Íªò Ï†úÍ≥µÎ∞õÏùÄ gpt ÎãµÎ≥Ä. ReferenceÏóê ÏùòÏ°¥Ï†ÅÏù¥Í≥† ÏßßÏßÄÎßå Ï†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï®\\nanswer_B: ReferenceÏóÜÏù¥ ÏßàÎ¨∏Îßå‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aiqwe/FinShibainu."},
	{"name":"aya-mm-exams-spanish-medical","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/amayuelas/aya-mm-exams-spanish-medical","creator_name":"Alfonso Amayuelas","creator_url":"https://huggingface.co/amayuelas","description":"Medical Spanish Exams for the Multimodal Aya Exams Projects. \\n  Questions available in file: data.json \\n  Images stored in: /images\\nOriginal data and file available here: link\\n"},
	{"name":"aya-mm-exams-spanish-nursing","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/amayuelas/aya-mm-exams-spanish-nursing","creator_name":"Alfonso Amayuelas","creator_url":"https://huggingface.co/amayuelas","description":"Nursing Spanish Exams for the Multimodal Aya Exams Projects.\\nQuestions available in file: data.json\\nImages stored in: /images\\nOriginal data and file available here: link\\n"},
	{"name":"TruthfulQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rahmanidashti/TruthfulQA","creator_name":"Hossein A. (Saeed) Rahmani","creator_url":"https://huggingface.co/rahmanidashti","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for TruthfulQA\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 790 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rahmanidashti/TruthfulQA."},
	{"name":"VisOnlyQA_metadata","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_metadata","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\\n\\t\\n\\t\\t\\n\\t\\tVisOnlyQA\\n\\t\\n\\nThis repository contains the code and data for the paper \\\"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\\\".\\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4 categories of scientific figures. We also provide a training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_metadata."},
	{"name":"dutch-central-exam-mcq","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jjzha/dutch-central-exam-mcq","creator_name":"Mike Zhang","creator_url":"https://huggingface.co/jjzha","description":"Multiple Choice Questions of the Dutch Central Exam 1999-2024\\n\\nWhat?\\n\\nThis dataset contains only multiple choice questions from the Dutch Central Exam (High School level). From Wikipedia:\\nThe Eindexamen (Dutch pronunciation: [Àà…õiÃØnt…õksam…ôn]) or centraal examen (CE) is the matriculation exam in the Netherlands, which takes place in a student's final year of high school education (voortgezet onderwijs; \\\"continued education\\\"). The exam is regulated by the Dutch Secondary Education Act[1] and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jjzha/dutch-central-exam-mcq."},
	{"name":"MRAG-Bench","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mragbenchanonymous/MRAG-Bench","creator_name":"mragbenchanonymous","creator_url":"https://huggingface.co/mragbenchanonymous","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tICLR 2025 Submision 9148\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is an anonymous repo for openreview https://openreview.net/forum?id=Usklli4gMc\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset contains the following fields:\\n\\n\\t\\n\\t\\t\\nField Name\\nDescription\\n\\n\\n\\t\\t\\nid\\nUnique identifier for the example\\n\\n\\naspect\\nAspect type for the example\\n\\n\\nscenario\\nThe type of scenario associated with the entry\\n\\n\\nimage\\nContains‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mragbenchanonymous/MRAG-Bench."},
	{"name":"AV_Odyssey_Bench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench","creator_name":"AV-Odyssey Bench","creator_url":"https://huggingface.co/AV-Odyssey","description":"Official dataset for the paper \\\"AV-Odyssey: Can Your Multimodal LLMs Really Understand Audio-Visual Information?\\\".\\nüåü For more details, please refer to the project page with data examples: https://av-odyssey.github.io/.\\n[üåê Webpage] [üìñ Paper] [ü§ó Huggingface AV-Odyssey Dataset] [ü§ó Huggingface Deaftest Dataset] [üèÜ Leaderboard]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüî• News\\n\\t\\n\\n\\n2024.11.24 üåü We release AV-Odyssey, the first-ever comprehensive evaluation benchmark to explore whether MLLMs really understand audio-visual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench."},
	{"name":"uhura-arc-easy","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/masakhane/uhura-arc-easy","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Uhura-Arc-Easy\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nUhura-ARC-Easy is a widely recognized scientific question answering benchmark composed of multiple-choice science questions derived from grade-school examinations that test various styles of knowledge and reasoning. \\nThe original English version of the benchmark originates from Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge (Clark et al., 2018) and is divided into \\\"Challenge\\\" and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/uhura-arc-easy."},
	{"name":"MMMU_Pro","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMMU/MMMU_Pro","creator_name":"MMMU","creator_url":"https://huggingface.co/MMMU","description":"\\n\\t\\n\\t\\t\\n\\t\\tMMMU-Pro (A More Robust Multi-discipline Multimodal Understanding Benchmark)\\n\\t\\n\\nüåê Homepage | üèÜ Leaderboard | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîîNews\\n\\t\\n\\n\\nüõ†Ô∏èüõ†Ô∏è [2025-03-08] Fixed mismatch between inner image labels and shuffled options in Vision and Standard (10 options) settings. (test_Chemistry_5,94,147,216,314,345,354,461,560,570; test_Materials_450; test_Pharmacy_198; validation_Chemistry_12,26,29; validation_Materials_10,28; validation_Psychology_1)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMMU/MMMU_Pro."},
	{"name":"LongBench-v2-Pause1","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1","creator_name":"James Begin","creator_url":"https://huggingface.co/JamesBegin","description":"\\n\\t\\n\\t\\t\\n\\t\\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\\n\\t\\n\\nüåê Project Page: https://longbench2.github.io\\nüíª Github Repo: https://github.com/THUDM/LongBench\\nüìö Arxiv Paper: https://arxiv.org/abs/2412.15204\\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1."},
	{"name":"LiveXiv","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LiveXiv/LiveXiv","creator_name":"LiveXiv","creator_url":"https://huggingface.co/LiveXiv","description":"LiveXiv - an evolving multi-modal dataset based on ArXiv\\nhttps://arxiv.org/abs/2410.10783\\n"},
	{"name":"mc-translation","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/mc-translation","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"This dataset contains professional human translations from OpenAI's MMMLU dataset, repurposed to train translation models that can help translate future evaluation datasets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy This Dataset?\\n\\t\\n\\nTranslation of evaluation benchmarks is a critical but challenging task. While automated translations may introduce errors or biases, professional human translations are expensive and time-consuming. This dataset leverages existing professional translations (MMMLU) to train specialized‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/efederici/mc-translation."},
	{"name":"SemiEvol","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/luojunyu/SemiEvol","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThe SemiEvol dataset is part of the broader work on semi-supervised fine-tuning for Large Language Models (LLMs). The dataset includes labeled and unlabeled data splits designed to enhance the reasoning capabilities of LLMs through a bi-level knowledge propagation and selection framework, as proposed in the paper SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/SemiEvol."},
	{"name":"hellasigma","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pszemraj/hellasigma","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\thellasigma\\n\\t\\n\\n\\nThis is an initial proof of concept and only contains 190 examples. Still, it seems to be able to tease out differences especially in 7b+ models. I've run some initial evals here\\n\\nMany evaluation datasets focus on a single correct answer to see if the model is \\\"smart.\\\" What about when there's no right answer? HellaSigma is an \\\"eval\\\" dataset to probe at what your model's personality type may be. Is it a Sigma, or not?\\nThis dataset contains generic scenarios and a list‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/hellasigma."},
	{"name":"cultural_evaluation-kalahi","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aisingapore/cultural_evaluation-kalahi","creator_name":"AI Singapore","creator_url":"https://huggingface.co/aisingapore","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKalahi\\n\\t\\n\\nKalahi evaluates the ability of LLMs to generate responses relevant to Filipino culture in terms of shared knowledge and ethics. This dataset contains a MCQ-compatible version of the Kalahi dataset that is used in SEA-HELM.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nKalahi is designed for evaluating Filipino cultural representations in instruction-tuned large language models (LLMs). It is part of the SEA-HELM leaderboard from AI Singapore.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aisingapore/cultural_evaluation-kalahi."},
	{"name":"testing_arc_easy_de","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TDN007/testing_arc_easy_de","creator_name":"Nguyen","creator_url":"https://huggingface.co/TDN007","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\ttesting_arc_easy_de\\n\\t\\n\\nThis is a copy of the dataset openGPT-X/arcx. \\n"},
	{"name":"Taiwan-Curlture-MCQ","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aqweteddy/Taiwan-Curlture-MCQ","creator_name":"aqweteddy","creator_url":"https://huggingface.co/aqweteddy","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTW-Curlture-MCQ\\n\\t\\n\\nTW-Curlture-MCQ ÊòØ‰∏ÄÂÄãË©ïÈáèÂè∞ÁÅ£ÊñáÂåñÁöÑÈÅ∏ÊìáÈ°åË≥áÊñôÈõÜÔºå‰∏ªË¶Å‰æÜËá™‰ª•‰∏ãÂÖ©ÂÄãË≥áÊñôÈõÜÁöÑÁØ©ÈÅ∏ËàáÊï¥ÂêàÔºö\\n\\nTMLU\\nTMMLU+\\n\\n‰ª•‰∫∫Â∑•ÊåëÈÅ∏ËàáÂè∞ÁÅ£ÊñáÂåñÁõ∏ÈóúÁöÑÁßëÁõÆÂæåÔºåÂÜçÁî± gpt-4o-mini Âà§Êñ∑ÂïèÈ°åÊòØÂê¶ËàáÂè∞ÁÅ£ÊñáÂåñÁõ∏ÈóúÔºåÂÖ± 3828 È°å„ÄÇ\\n"},
	{"name":"MapEval-Textual","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MapEval/MapEval-Textual","creator_name":"MapEval","creator_url":"https://huggingface.co/MapEval","description":"\\n\\t\\n\\t\\t\\n\\t\\tMapEval-Textual\\n\\t\\n\\nMapEval-Textual is created using MapQaTor.\\n\\n\\t\\n\\t\\t\\n\\t\\tUsage\\n\\t\\n\\nfrom datasets import load_dataset\\n\\n# Load dataset\\nds = load_dataset(\\\"MapEval/MapEval-Textual\\\", name=\\\"benchmark\\\")\\n\\n# Generate better prompts\\nfor item in ds[\\\"test\\\"]:\\n    # Start with a clear task description\\n    prompt = (\\n        \\\"You are a highly intelligent assistant. \\\"\\n        \\\"Based on the given context, answer the multiple-choice question by selecting the correct option.\\\\n\\\\n\\\"\\n        \\\"Context:\\\\n\\\" +‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MapEval/MapEval-Textual."},
	{"name":"Inst-It-Bench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Bench","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\\n\\t\\n\\t\\t\\n\\t\\tInst-It Bench\\n\\t\\n\\nHomepage | Code | Paper | arXiv\\nInst-It Bench is a fine-grained multimodal benchmark for evaluating LMMs at the instance-Level, which is introduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning.\\n\\nSize: 1,000 image QAs and 1,000 video QAs\\nSplits: Image split and Video split\\nEvaluation Formats: Open-Ended and Multiple-Choice\\n\\n\\n\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nExisting multimodal benchmarks primarily focus on global‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Bench."},
	{"name":"CHOICE","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/An-Xiao/CHOICE","creator_name":"AnXiao","creator_url":"https://huggingface.co/An-Xiao","description":"\\n\\t\\n\\t\\t\\n\\t\\tCHOICE: Benchmarking The Remote Sensing Capabilities of Large Vision-Language Models\\n\\t\\n\\n Abstract: The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/An-Xiao/CHOICE."},
	{"name":"arc_challenge_de_fixed","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TDN007/arc_challenge_de_fixed","creator_name":"Nguyen","creator_url":"https://huggingface.co/TDN007","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tARC Challenge DE fixed\\n\\t\\n\\nThis is a copy of the dataset LeoLM/ArcChallenge_de which is a German translation of the original allenai/ai2_arc. \\nBug fixesWe copied it to fix the numeric labels 1,2,3,4 into A,B,C,D. \\nThanksThe translation of the arc_challenge was done by Bjoern: https://github.com/bjoernpl/GermanBenchmark\\n"},
	{"name":"VisOnlyQA_Eval_Synthetic","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Synthetic","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVisOnlyQA\\n\\t\\n\\nThis repository contains the code and data for the paper \\\"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\\\".\\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4 categories of scientific figures. We also provide a training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Synthetic."},
	{"name":"VisOnlyQA_Train","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Train","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVisOnlyQA\\n\\t\\n\\nThis repository contains the code and data for the paper \\\"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\\\".\\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4 categories of scientific figures. We also provide a training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Train."},
	{"name":"Tin_hoc_mcq_extended","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kamisaiko/Tin_hoc_mcq_extended","creator_name":"NGUYEN VIET TRUNG","creator_url":"https://huggingface.co/kamisaiko","description":"kamisaiko/Tin_hoc_mcq_extended dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"VisOnlyQA_Eval_Real","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVisOnlyQA\\n\\t\\n\\nThis repository contains the code and data for the paper \\\"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\\\".\\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4 categories of scientific figures. We also provide a training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real."},
	{"name":"univ_exams_finnish","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/readd/univ_exams_finnish","creator_name":"Perttu Isotalo","creator_url":"https://huggingface.co/readd","description":"readd/univ_exams_finnish dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"danish-citizenzhip-test-mcq","keyword":"multiple-choice","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tellarin-ai/danish-citizenzhip-test-mcq","creator_name":"Tellarin.ai","creator_url":"https://huggingface.co/tellarin-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"danish-citizen-test-mcq\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset covers Danish tests for both citizenship (\\\"indf√∏dsretspr√∏ven\\\") and permanent residence (\\\"medborgerskabspr√∏ven\\\"), from 2016-2024.\\nData follows the Aya Expedition format for global exams. Only unique questions between exams are kept.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe dataset is available in Danish (da).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nAn example from the dataset looks as follows.\\n{\\n\\\"language\\\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tellarin-ai/danish-citizenzhip-test-mcq."},
	{"name":"mmevalpro","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMEVAL/mmevalpro","creator_name":"MMEVAL","creator_url":"https://huggingface.co/MMEVAL","description":"MMEvalPro\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MMEvalPro\\n\\t\\n\\nWe create MMEvalPro for more accurate and efficent evaluation for Large Multimodal Models. It is designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question from existing benchmarks, human annotators augment it by creating one perception question and one knowledge anchor question through a meticulous annotation process.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Format\\n\\t\\n\\n{\\n    \\\"index\\\": [int64] The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMEVAL/mmevalpro."},
	{"name":"MapEval-Visual","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MapEval/MapEval-Visual","creator_name":"MapEval","creator_url":"https://huggingface.co/MapEval","description":"\\n\\t\\n\\t\\t\\n\\t\\tMapEval-Visual\\n\\t\\n\\nThis dataset was introduced in MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models\\n\\n\\t\\n\\t\\t\\n\\t\\tExample\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tQuery\\n\\t\\n\\nI am presently visiting Mount Royal Park . Could you please inform me about the nearby historical landmark?\\n\\n\\t\\n\\t\\t\\n\\t\\tOptions\\n\\t\\n\\n\\nCircle Stone\\nSecret pool\\nMaison William Caldwell Cottingham\\nPoste de cavalerie du Service de police de la Ville de Montreal\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tCorrect Option\\n\\t\\n\\n\\nCircle Stone\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tPrerequisite\\n\\t\\n\\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MapEval/MapEval-Visual."},
	{"name":"MathVista","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AI4Math/MathVista","creator_name":"AI for Math Reasoning","creator_url":"https://huggingface.co/AI4Math","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MathVista\\n\\t\\n\\n\\nDataset Description\\nPaper Information\\nDataset Examples\\nLeaderboard\\nDataset Usage\\nData Downloading\\nData Format\\nData Visualization\\nData Source\\nAutomatic Evaluation\\n\\n\\nLicense\\nCitation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nMathVista is a consolidated Mathematical reasoning benchmark within Visual contexts. It consists of three newly created datasets, IQTest, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI4Math/MathVista."},
	{"name":"truthful_qa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/truthfulqa/truthful_qa","creator_name":"TruthfulQA","creator_url":"https://huggingface.co/truthfulqa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for truthful_qa\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/truthfulqa/truthful_qa."},
	{"name":"ScienceQA","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/derek-thomas/ScienceQA","creator_name":"Derek Thomas","creator_url":"https://huggingface.co/derek-thomas","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Creation Guide\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nLearn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nMulti-modal Multiple Choice\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nExplore more samples here.\\n{'image': Image,\\n 'question': 'Which of these states is farthest north?',\\n 'choices': ['West Virginia', 'Louisiana', 'Arizona'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/derek-thomas/ScienceQA."},
	{"name":"hhh_alignment","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/hhh_alignment","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"This task evaluates language models on alignment, broken down into categories of helpfulness, honesty/accuracy, harmlessness, and other.  The evaluations imagine a conversation between a person and a language model assistant.  The goal with these evaluations is that on careful reflection, the vast majority of people would agree that the chosen response is better (more helpful, honest, and harmless) than the alternative offered for comparison.  The task is formatted in terms of binary choices, though many of these have been broken down from a ranked ordering of three or four possible responses."},
	{"name":"qasc","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenai/qasc","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"qasc\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nQASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice\\nquestions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nMore Information Needed\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nMore Information Needed\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/qasc."},
	{"name":"xtreme","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/xtreme","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"xtreme\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Cross-lingual Natural Language Inference (XNLI) corpus is a crowd-sourced collection of 5,000 test and\\n2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into\\n14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese,\\nHindi, Swahili and Urdu. This results in 112.5k annotated pairs. Each premise can be associated with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/xtreme."},
	{"name":"medmcqa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/openlifescienceai/medmcqa","creator_name":"Open Life Science AI","creator_url":"https://huggingface.co/openlifescienceai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MedMCQA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions.\\nMedMCQA has more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity.\\nEach sample contains a question, correct answer(s), and other options which‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openlifescienceai/medmcqa."},
	{"name":"moral_stories","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/demelin/moral_stories","creator_name":"Denis Emelin","creator_url":"https://huggingface.co/demelin","description":"Moral Stories is a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented \\nsocial reasoning. For detailed information, see https://aclanthology.org/2021.emnlp-main.54.pdf."},
	{"name":"cycic_multiplechoice","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/cycic_multiplechoice","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"https://colab.research.google.com/drive/16nyxZPS7-ZDFwp7tn_q72Jxyv0dzK1MP?usp=sharing\\n@article{Kejriwal2020DoFC,\\n  title={Do Fine-tuned Commonsense Language Models Really Generalize?},\\n  author={Mayank Kejriwal and Ke Shen},\\n  journal={ArXiv},\\n  year={2020},\\n  volume={abs/2011.09159}\\n}\\n\\nadded for \\n@article{sileo2023tasksource,\\n  title={tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation},\\n  author={Sileo, Damien},\\n  url=‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/cycic_multiplechoice."},
	{"name":"LEval","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/L4NLP/LEval","creator_name":"L4NLP","creator_url":"https://huggingface.co/L4NLP","description":"A benchmark to evaluate long document understanding and generation ability of LLM"},
	{"name":"belebele","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/belebele","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Belebele Benchmark for Massively Multilingual NLU Evaluation\\n\\t\\n\\nBelebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. This dataset enables the evaluation of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the FLORES-200 dataset. The human annotation procedure was carefully curated to create questions that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/belebele."},
	{"name":"CodeFuse-DevOps-Eval","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/codefuse-ai/CodeFuse-DevOps-Eval","creator_name":"CodeFuse AI","creator_url":"https://huggingface.co/codefuse-ai","description":"DevOps-Eval is a comprehensive chinese evaluation suite specifically designed for foundation models in the DevOps field. It consists of 5977 multi-choice questions spanning 55 diverse categories. Please visit our website and GitHub for more details.\\nEach category consists of two splits: dev, and test. The dev set per subject consists of five exemplars with explanations for few-shot evaluation. And the test set is for model evaluation. Labels on the test split are released, users can evaluate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/codefuse-ai/CodeFuse-DevOps-Eval."},
	{"name":"enem","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maritaca-ai/enem","creator_name":"Maritaca AI","creator_url":"https://huggingface.co/maritaca-ai","description":"The ENEM 2022, 2023 and 2024 datasets encompass all multiple-choice questions from the last two editions of the Exame Nacional do Ensino M√©dio (ENEM), the main standardized entrance examination adopted by Brazilian universities. The datasets have been created to allow the evaluation of both textual-only and textual-visual language models. To evaluate textual-only models, we incorporated into the datasets the textual descriptions of the images that appear in the questions' statements from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maritaca-ai/enem."},
	{"name":"MMMU","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMMU/MMMU","creator_name":"MMMU","creator_url":"https://huggingface.co/MMMU","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI)\\n\\t\\n\\nüåê Homepage | üèÜ Leaderboard | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîîNews\\n\\t\\n\\n\\nüõ†Ô∏è[2024-05-30]: Fixed duplicate option issues in Materials dataset items (validation_Materials_25; test_Materials_17, 242) and content error in validation_Materials_25.\\nüõ†Ô∏è[2024-04-30]: Fixed missing \\\"-\\\" or \\\"^\\\" signs in Math dataset items (dev_Math_2, validation_Math_11, 12, 16;‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMMU/MMMU."},
	{"name":"NIFTY","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/raeidsaqur/NIFTY","creator_name":"Raeid Saqur","creator_url":"https://huggingface.co/raeidsaqur","description":"\\n  \\n    The News-Informed Financial Trend Yield (NIFTY) Dataset. \\n\\n\\nThe News-Informed Financial Trend Yield (NIFTY) Dataset. Details of the dataset, including data procurement and filtering can be found in the paper here: https://arxiv.org/abs/2405.09747.\\nFor the NIFTY-RL LLM alignment dataset please use nifty-rl.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìã Table of Contents\\n\\t\\n\\n\\nüß© NIFTY Dataset\\nüìã Table of Contents\\nüìñ Usage\\nDownloading the dataset\\nDataset structure\\n\\n\\nLarge Language Models \\n‚úçÔ∏è Contributing\\nüìù Citing\\nüôè‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raeidsaqur/NIFTY."},
	{"name":"MathVision","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MathVision","creator_name":"LLMs for Math Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\\n\\t\\n\\t\\t\\n\\t\\tMeasuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset\\n\\t\\n\\n[üíª Github] [üåê Homepage]  [üìä Leaderboard ] [üîç Visualization] [üìñ ArXiv Paper]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüöÄ Data Usage\\n\\t\\n\\n\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"MathLLMs/MathVision\\\")\\nprint(dataset)\\n\\n\\n\\t\\n\\t\\n\\t\\n\\t\\tüí• News\\n\\t\\n\\n\\n[2025.03.10] üí• Kimi k1.6 Preview ü•á Sets New SOTA on MATH-V with 53.29%!See the full leaderboard.\\n[2025.02.28] üí• Doubao-1.5-pro Sets New SOTA on MATH-V with 48.62%! Read more on the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MathVision."},
	{"name":"histoires_morales","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LabHC/histoires_morales","creator_name":"Laboratoire Hubert Curien","creator_url":"https://huggingface.co/LabHC","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for HistoiresMorales\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n‚öñ Histoires Morales is a French dataset derived from the English corpus Moral Stories through multi-step translation and consists of short narratives describing moral and deviant behaviors in social situations centered around personal relationships, education, commerce, domestic affairs, and meals.\\nEach of the 12,000 stories (histoires) follows the same seven-sentence structure as the Moral Stories dataset:\\nContext:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LabHC/histoires_morales."},
	{"name":"ReDis-QA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guan-wang/ReDis-QA","creator_name":"Guanchu","creator_url":"https://huggingface.co/guan-wang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ReDis-QA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nReDis-QA dataset contains 1360 multi-choice questions focusing on rare disease diagnosis.\\nIt consists of 11%, 33%, 13%, 15%, 18% of the questions corresponding to the symptoms, causes, affects, related-disorders, diagnosis of rare diseases, respectively. \\nThe remaining 9% of the questions pertain to other properties of the diseases.\\n\\n\\nReDis-QA dataset widely covers 205 types of rare diseases, where the most frequent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/guan-wang/ReDis-QA."},
	{"name":"Yue-Benchmark","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BillBao/Yue-Benchmark","creator_name":"Bao","creator_url":"https://huggingface.co/BillBao","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models\\n\\t\\n\\n\\nHomepage: https://github.com/jiangjyjy/Yue-Benchmark\\nRepository: https://huggingface.co/datasets/BillBao/Yue-Benchmark\\nPaper: How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThe rapid evolution of large language models (LLMs), such as GPT-X and Llama-X, has driven significant advancements in NLP, yet much of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BillBao/Yue-Benchmark."},
	{"name":"BiomixQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kg-rag/BiomixQA","creator_name":"KG-RAG","creator_url":"https://huggingface.co/kg-rag","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBiomixQA Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nBiomixQA is a curated biomedical question-answering dataset comprising two distinct components:\\n\\nMultiple Choice Questions (MCQ)\\nTrue/False Questions\\n\\nThis dataset has been utilized to validate the Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) framework across different Large Language Models (LLMs). The diverse nature of questions in this dataset, spanning multiple choice and true/false formats, along with its coverage of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kg-rag/BiomixQA."},
	{"name":"openbookqa-es","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BSC-LT/openbookqa-es","creator_name":"Language Technologies Unit @ Barcelona Supercomputing Center","creator_url":"https://huggingface.co/BSC-LT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for openbookqa_es\\n\\t\\n\\n\\n\\nopenbookqa_es is a question answering dataset in Spanish, professionally translated from the main version of the OpenBookQA dataset in English. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nopenbookqa_es (Open Book Question Answering - Spanish) is designed to simulate open book exams and assess human-like understanding of a subject. The dataset comprises 500 instances in the validation split and another 500 instances in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BSC-LT/openbookqa-es."},
	{"name":"JMMMU","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/JMMMU/JMMMU","creator_name":"JMMMU","creator_url":"https://huggingface.co/JMMMU","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark\\n\\t\\n\\nüåê Homepage | ü§ó Dataset | üèÜ HF Leaderboard | üìñ arXiv | üíª Code\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nWe introduce JMMMU (Japanese MMMU), a multimodal benchmark that can truly evaluate LMM performance in Japanese. To create JMMMU, we first carefully analyzed the existing MMMU benchmark and examined its cultural dependencies. Then, for questions in culture-agnostic subjects, we employed native Japanese speakers‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JMMMU/JMMMU."},
	{"name":"polymath","keyword":"multiple-choice","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/him1411/polymath","creator_name":"Himanshu Gupta","creator_url":"https://huggingface.co/him1411","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPaper Information\\n\\t\\n\\nWe present PolyMATH, a challenging benchmark aimed at evaluating the general cognitive reasoning abilities of MLLMs. \\nPolyMATH comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning. \\nWe conducted a comprehensive, and quantitative evaluation of 15 MLLMs using four diverse prompting strategies, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/him1411/polymath."},
	{"name":"MMMU-Thai","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iapp/MMMU-Thai","creator_name":"iApp Technology","creator_url":"https://huggingface.co/iapp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMMU Thai (MMMU Benchmark Translated to Thai)\\n\\t\\n\\nMMMU Thai is a dataset for evaluating multimodal models on massive multi-discipline tasks requiring college-level knowledge and deliberate reasoning. This dataset is translated from MMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI) into Thai.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nMMMU Thai consists of 11,500 meticulously collected multimodal questions from college exams, quizzes, and textbooks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iapp/MMMU-Thai."},
	{"name":"MRAG-Bench","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/uclanlp/MRAG-Bench","creator_name":"UCLA NLP","creator_url":"https://huggingface.co/uclanlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models\\n\\t\\n\\nüåê Homepage | üìñ Paper | üíª Evaluation \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntro\\n\\t\\n\\nMRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios,  providing a robust and systematic evaluation of Large Vision Language Model (LVLM)‚Äôs vision-centric multimodal retrieval-augmented generation (RAG) abilities.\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tResults\\n\\t\\n\\nEvaluated upon 10 open-source and 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uclanlp/MRAG-Bench."},
	{"name":"PangeaBench-xmmmu","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neulab/PangeaBench-xmmmu","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","description":"neulab/PangeaBench-xmmmu dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"24-game","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nlile/24-game","creator_name":"nathan lile","creator_url":"https://huggingface.co/nlile","description":"\\n\\t\\n\\t\\t\\n\\t\\tMath Twenty Four (24s Game) Dataset\\n\\t\\n\\nA comprehensive dataset for the classic math twenty four game (also known as the 4 numbers game / 24s game / Game of 24). This dataset of mathematical reasoning challenges was collected from 4nums.com, featuring over 1,300 unique puzzles of the Game of 24, with difficulty metrics derived from over 6.4 million human solution attempts since 2012.\\nIn each puzzle, players must use exactly four numbers and basic arithmetic operations (+, -, √ó, /) to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nlile/24-game."},
	{"name":"include-base-44","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/include-base-44","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tINCLUDE-base (44 languages)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nPaper: http://arxiv.org/abs/2411.19799\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \\nIt contains 22,637 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/include-base-44."},
	{"name":"include-lite-44","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/include-lite-44","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tINCLUDE-lite (44 languages)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nPaper: http://arxiv.org/abs/2411.19799\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \\nIt contains 11,095 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/include-lite-44."},
	{"name":"CupCase","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ofir408/CupCase","creator_name":"ofir ben shoham","creator_url":"https://huggingface.co/ofir408","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for CUPCase\\n\\t\\n\\n\\n\\nCUPCase: Clinically Uncommon Patient Cases and Diagnoses Dataset\\n(AAAI 2025)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nMedical benchmark datasets significantly contribute to devel-\\noping Large Language Models (LLMs) for medical knowl-\\nedge extraction, diagnosis, summarization, and other uses.\\nYet, current benchmarks are mainly derived from exam ques-\\ntions given to medical students or cases described in the med-\\nical literature, lacking the complexity of real-world‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ofir408/CupCase."},
	{"name":"LongBench-v2","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongBench-v2","creator_name":"Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University","creator_url":"https://huggingface.co/THUDM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\\n\\t\\n\\nüåê Project Page: https://longbench2.github.io\\nüíª Github Repo: https://github.com/THUDM/LongBench\\nüìö Arxiv Paper: https://arxiv.org/abs/2412.15204\\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongBench-v2."},
	{"name":"Health_Benchmarks","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yesilhealth/Health_Benchmarks","creator_name":"Yesil Health AI","creator_url":"https://huggingface.co/yesilhealth","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHealth Benchmarks Dataset\\n\\t\\n\\nThe Health Benchmarks Dataset is a specialized resource for evaluating large language models (LLMs) in different medical specialties. It provides structured question-answer pairs designed to test the performance of AI models in understanding and generating domain-specific knowledge.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPrimary Purpose\\n\\t\\n\\nThis dataset is built to:\\n\\nBenchmark LLMs in medical specialties and subfields.\\nAssess the accuracy and contextual understanding of AI in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yesilhealth/Health_Benchmarks."},
	{"name":"MapEval-API","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MapEval/MapEval-API","creator_name":"MapEval","creator_url":"https://huggingface.co/MapEval","description":"\\n\\t\\n\\t\\t\\n\\t\\tMapEval-API\\n\\t\\n\\nMapEval-API is created using MapQaTor.\\n\\n\\t\\n\\t\\t\\n\\t\\tUsage\\n\\t\\n\\nfrom datasets import load_dataset\\n\\n# Load dataset\\nds = load_dataset(\\\"MapEval/MapEval-API\\\", name=\\\"benchmark\\\")\\n\\n# Generate better prompts\\nfor item in ds[\\\"test\\\"]:\\n    # Start with a clear task description\\n    prompt = (\\n        \\\"You are a highly intelligent assistant. \\\"\\n        \\\"Answer the multiple-choice question by selecting the correct option.\\\\n\\\\n\\\"\\n        \\\"Question:\\\\n\\\" + item[\\\"question\\\"] + \\\"\\\\n\\\\n\\\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MapEval/MapEval-API."},
	{"name":"MM-IQ","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huanqia/MM-IQ","creator_name":"huanqiacai","creator_url":"https://huggingface.co/huanqia","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for \\\"MM-IQ\\\"\\n\\t\\n\\n\\nIntroduction\\nPaper Information\\nDataset Examples\\nLeaderboard\\nDataset Usage\\nData Downloading\\nData Format\\nAutomatic Evaluation\\n\\n\\nCitation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tIntroduction\\n\\t\\n\\nIQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huanqia/MM-IQ."},
	{"name":"head_qa_v2","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alesi12/head_qa_v2","creator_name":"Alexis Correa Guillen","creator_url":"https://huggingface.co/alesi12","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nHEAD-QA v2 is an updated version of the HEAD-QA dataset, which is a multi-choice HEAlthcare Dataset. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. They are designed by the Ministerio de Sanidad, Consumo y Bienestar Social, who also provides direct access to the exams of the last 5 years (in Spanish).\\nHEAD-QA V2 expands on the original dataset by including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alesi12/head_qa_v2."},
	{"name":"microvqa","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmhb/microvqa","creator_name":"James Burgess","creator_url":"https://huggingface.co/jmhb","description":"MicroVQA:  A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research\\n\\n\\n \\n üåê Homepage ‚Ä¢\\n ü§ó HF Dataset ‚Ä¢\\n üèõ CC-BY-SA-4.0\\n\\nMicroVQA is an original, expert-curated benchmark for multimodal reasoning for microscopy-based scientific research.\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nScientific research demands sophisticated reasoning over multimodal  data, a challenge \\nespecially prevalent in biology. Despite recent advances in multimodal large language \\nmodels (MLLMs) for AI-assisted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jmhb/microvqa."},
	{"name":"MedRisk-Bench","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jinge13288/MedRisk-Bench","creator_name":"Jinge Wu","creator_url":"https://huggingface.co/jinge13288","description":"MedRisk Benchmark is used for medical risk assessment. \\nThe Benchmark is made up with two version with 1232 test samples for each:\\nMedRisk-Quantitative: which focuses on the score caculation for medical risk prediction.\\nMedRisk-Qualitative: which focuses on the severity for medical condition/disease.\\nFull data release can be found at Github\\n"},
	{"name":"psycholinguistic_eval","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KevinZ/psycholinguistic_eval","creator_name":"Kevin Zhao","creator_url":"https://huggingface.co/KevinZ","description":"Psycholinguistic dataset from 'What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models'\\nby Allyson Ettinger"},
	{"name":"movie_recommendation","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/movie_recommendation","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Movie recommendation task based on the Movielens dataset"},
	{"name":"discourse_marker_qa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/discourse_marker_qa","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Discourse marker/connective prediction as multiple choice questions based on the Discovery dataset"},
	{"name":"123_test","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JeremyAlain/123_test","creator_name":"J√©r√©my Scheurer","creator_url":"https://huggingface.co/JeremyAlain","description":"The Fewshot Table dataset consists of tables that naturally occur on the web, that are formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. The dataset consists of approximately 413K tables that are extracted from the WDC Web Table Corpora 2015, which is released under the Apache-2.0 license. The WDC Web Table Corpora \\\"contains vast amounts of HTML tables. [...] The Web Data Commons project extracts relational Web tables from the Common Crawl, the largest and most up-to-date Web corpus that is currently available to the public."},
	{"name":"fig-qa","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nightingal3/fig-qa","creator_name":"Emmy Liu","creator_url":"https://huggingface.co/nightingal3","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Fig-QA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis is the dataset for the paper Testing the Ability of Language Models to Interpret Figurative Language. Fig-QA consists of 10256 examples of human-written creative metaphors that are paired as a Winograd schema. It can be used to evaluate the commonsense reasoning of models. The metaphors themselves can also be used as training data for other tasks, such as metaphor detection or generation. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nightingal3/fig-qa."},
	{"name":"unpredictable_full","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_full","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_mmo-champion-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_mmo-champion-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_baseball-fantasysports-yahoo-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_baseball-fantasysports-yahoo-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_phonearena-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_phonearena-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_support-google-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_support-google-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_dividend-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_dividend-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_bulbapedia-bulbagarden-net","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_bulbapedia-bulbagarden-net","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_wkdu-org","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_wkdu-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_dummies-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_dummies-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_mgoblog-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_mgoblog-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_gamefaqs-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_gamefaqs-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_studystack-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_studystack-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_sittercity-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_sittercity-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_msdn-microsoft-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_msdn-microsoft-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cappex-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cappex-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_en-wikipedia-org","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_en-wikipedia-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cram-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cram-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_w3-org","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_w3-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_sporcle-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_sporcle-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_wiki-openmoko-org","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_wiki-openmoko-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_ensembl-org","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_ensembl-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_5k","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_5k","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_unique","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_unique","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster-noise","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster-noise","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster00","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster00","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster01","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster01","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster10","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster10","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster11","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster11","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster12","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster12","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster13","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster13","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster14","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster14","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster15","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster15","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster16","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster16","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster17","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster17","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster18","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster18","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster19","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster19","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster02","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster02","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster20","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster20","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster21","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster21","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster22","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster22","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster23","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster23","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster24","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster24","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster25","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster25","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster26","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster26","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster27","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster27","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster28","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster28","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster29","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster29","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster03","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster03","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster04","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster04","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster05","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster05","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster06","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster06","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster07","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster07","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster08","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster08","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_cluster09","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster09","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_rated-low","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-low","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_rated-medium","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-medium","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_rated-high","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-high","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"wikimedqa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/wikimedqa","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"@inproceedings{sileo-etal-2024-generating-multiple,\\n    title = \\\"Generating Multiple-choice Questions for Medical Question Answering with Distractors and Cue-masking\\\",\\n    author = \\\"Sileo, Damien  and\\n      Uma, Kanimozhi  and\\n      Moens, Marie-Francine\\\",\\n    booktitle = \\\"Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)\\\",\\n    month = may,\\n    year = \\\"2024\\\",\\n    address = \\\"Torino, Italia\\\",\\n    publisher =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sileod/wikimedqa."},
	{"name":"understanding_fables","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/demelin/understanding_fables","creator_name":"Denis Emelin","creator_url":"https://huggingface.co/demelin","description":"This task aims to measure the ability of computational models to understand short narratives, by identifying the most \\nappropriate moral for a given fable from a set of five alternatives."},
	{"name":"unpredictable_full","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/unpredictable/unpredictable_full","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_5k","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/unpredictable/unpredictable_5k","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_support-google-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/unpredictable/unpredictable_support-google-com","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"unpredictable_unique","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/unpredictable/unpredictable_unique","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
	{"name":"newyorker_caption_contest","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmhessel/newyorker_caption_contest","creator_name":"Jack Hessel","creator_url":"https://huggingface.co/jmhessel","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for New Yorker Caption Contest Benchmarks\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSee capcon.dev for more!\\nData from:\\nDo Androids Laugh at Electric Sheep? Humor \\\"Understanding\\\" Benchmarks from The New Yorker Caption Contest\\n@inproceedings{hessel2023androids,\\n  title={Do Androids Laugh at Electric Sheep? {Humor} ``Understanding''\\n         Benchmarks from {The New Yorker Caption Contest}},\\n  author={Hessel, Jack and Marasovi{\\\\'c}, Ana and Hwang, Jena D. and Lee, Lillian\\n          and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jmhessel/newyorker_caption_contest."},
	{"name":"NeQA","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/inverse-scaling/NeQA","creator_name":"Inverse Scaling Prize","creator_url":"https://huggingface.co/inverse-scaling","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNeQA: Can Large Language Models Understand Negation in Multi-choice Questions? (Zhengping Zhou and Yuhui Zhang)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGeneral description\\n\\t\\n\\nThis task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation. The authors find that smaller language models display approximately random performance whereas the performance of larger models become significantly worse than random. \\nLanguage models failing to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/inverse-scaling/NeQA."},
	{"name":"quote-repetition","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/inverse-scaling/quote-repetition","creator_name":"Inverse Scaling Prize","creator_url":"https://huggingface.co/inverse-scaling","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tquote-repetition (Joe Cavanagh, Andrew Gritsevskiy, and Derik Kauffman of Cavendish Labs)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGeneral description\\n\\t\\n\\nIn this task, the authors ask language models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task. Each prompt contains a famous quote with a modified ending to mislead the model into completing the sequence with the famous ending rather than with the ending given in the prompt. The authors find that smaller‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/inverse-scaling/quote-repetition."},
	{"name":"redefine-math","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/inverse-scaling/redefine-math","creator_name":"Inverse Scaling Prize","creator_url":"https://huggingface.co/inverse-scaling","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tredefine-math (Xudong Shen)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGeneral description\\n\\t\\n\\nIn this task, the author tests whether language models are able to work with common symbols when they are redefined to mean something else. The author finds that larger models are more likely to pick the answer corresponding to the original definition rather than the redefined meaning, relative to smaller models. \\nThis task demonstrates that it is difficult for language models to work with new information given at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/inverse-scaling/redefine-math."},
	{"name":"hindsight-neglect-10shot","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/inverse-scaling/hindsight-neglect-10shot","creator_name":"Inverse Scaling Prize","creator_url":"https://huggingface.co/inverse-scaling","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tinverse-scaling/hindsight-neglect-10shot (‚ÄòThe Floating Droid‚Äô)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGeneral description\\n\\t\\n\\nThis task tests whether language models are able to assess whether a bet was worth taking based on its expected value. The author provides few shot examples in which the model predicts whether a bet is worthwhile by correctly answering yes or no when the expected value of the bet is positive (where the model should respond that ‚Äòyes‚Äô, taking the bet is the right decision) or‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/inverse-scaling/hindsight-neglect-10shot."},
	{"name":"copa-sse","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/anab/copa-sse","creator_name":"Ana Brassard","creator_url":"https://huggingface.co/anab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for COPA-SSE\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nCOPA-SSE contains crowdsourced explanations for the Balanced COPA dataset, a variant of the Choice of Plausible Alternatives (COPA) benchmark. The explanations are formatted as a set of triple-like common sense statements with ConceptNet relations but freely written concepts.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nCan be used to train a model for explain+predict or predict+explain settings. Suited for both‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anab/copa-sse."},
	{"name":"probability_words_nli","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/probability_words_nli","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Probing neural language models for understanding of words of estimative probability"},
	{"name":"model-written-evals","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Anthropic/model-written-evals","creator_name":"Anthropic","creator_url":"https://huggingface.co/Anthropic","description":"\\n\\t\\n\\t\\t\\n\\t\\tModel-Written Evaluation Datasets\\n\\t\\n\\nThis repository includes datasets written by language models, used in our paper on \\\"Discovering Language Model Behaviors with Model-Written Evaluations.\\\"\\nWe intend the datasets to be useful to:\\n\\nThose who are interested in understanding the quality and properties of model-generated data\\nThose who wish to use our datasets to evaluate other models for the behaviors we examined in our work (e.g., related to model persona, sycophancy, advanced AI risks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Anthropic/model-written-evals."},
	{"name":"mmlu","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/mmlu","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge, covering 57 tasks including elementary mathematics, US history, computer science, law, and more."},
	{"name":"qasc-ro","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BlackKakapo/qasc-ro","creator_name":"Alexandru Petrachi","creator_url":"https://huggingface.co/BlackKakapo","description":"Original dataset - This dataset is just the translation of the qasc dataset.\\n"},
	{"name":"truthful_qa_mc","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EleutherAI/truthful_qa_mc","creator_name":"EleutherAI","creator_url":"https://huggingface.co/EleutherAI","description":"TruthfulQA-MC is a benchmark to measure whether a language model is truthful in\\ngenerating answers to questions. The benchmark comprises 817 questions that\\nspan 38 categories, including health, law, finance and politics. Questions are\\ncrafted so that some humans would answer falsely due to a false belief or\\nmisconception. To perform well, models must avoid generating false answers\\nlearned from imitating human texts."},
	{"name":"truthful_qa_binary","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EleutherAI/truthful_qa_binary","creator_name":"EleutherAI","creator_url":"https://huggingface.co/EleutherAI","description":"TruthfulQA-Binary is a benchmark to measure whether a language model is truthful in\\ngenerating answers to questions. The benchmark comprises 817 questions that\\nspan 38 categories, including health, law, finance and politics. Questions are\\ncrafted so that some humans would answer falsely due to a false belief or\\nmisconception. To perform well, models must avoid generating false answers\\nlearned from imitating human texts."},
	{"name":"geobench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/daven3/geobench","creator_name":"Daven","creator_url":"https://huggingface.co/daven3","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBenchmark: GeoBenchmark\\n\\t\\n\\nIn GeoBenchmark, we collect 183 multiple-choice questions in NPEE, and 1,395 in AP Test, for objective tasks. \\nMeanwhile, we gather all 939 subjective questions in NPEE to be the subjective tasks set and use 50 to measure the baselines with human evaluation. \\n"},
	{"name":"JAQKET","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kumapo/JAQKET","creator_name":"kumapo","creator_url":"https://huggingface.co/kumapo","description":"JAQKET: JApanese Questions on Knowledge of EnTitie"},
	{"name":"inverse_scaling_prize-hindsight_neglect","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmichaelov/inverse_scaling_prize-hindsight_neglect","creator_name":"James Michaelov","creator_url":"https://huggingface.co/jmichaelov","description":"The hindsight-neglect task from the Inverse Scaling Prize\\n"},
	{"name":"inverse_scaling_prize-sig_figs","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmichaelov/inverse_scaling_prize-sig_figs","creator_name":"James Michaelov","creator_url":"https://huggingface.co/jmichaelov","description":"jmichaelov/inverse_scaling_prize-sig_figs dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"test","keyword":"multiple-choice","license":"Boost Software License 1.0","license_url":"https://choosealicense.com/licenses/bsl-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pengxiang01/test","creator_name":"wang","creator_url":"https://huggingface.co/pengxiang01","description":"aasdfsdf\\n"},
	{"name":"PolyMRC","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bigai-nlco/PolyMRC","creator_name":"BIGAI NLCo","creator_url":"https://huggingface.co/bigai-nlco","description":"We construct a dataset through entries with multiple meanings and examples from Chinese dictionaries, and set the example as context and explanations as choices, the goal of Polysemy Machine Comprehension (PolyMRC) is to find the correct explanation of the entry in the example.\\nthe statistics of the dataset\\n\\n\\t\\n\\t\\t\\n\\n\\n\\n\\n\\n\\t\\t\\nsplit\\nsentences\\naverage sentence length\\n\\n\\ntrain\\n46,119\\n38.55\\n\\n\\nvalidation\\n5,765\\n38.31\\n\\n\\ntest\\n5,765\\n38.84\\n\\n\\n\\t\\n\\n"},
	{"name":"TruthfulQA_de","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LeoLM/TruthfulQA_de","creator_name":"LAION LeoLM","creator_url":"https://huggingface.co/LeoLM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for truthful_qa\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LeoLM/TruthfulQA_de."},
	{"name":"EXAMs","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FreedomIntelligence/EXAMs","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEXAMs\\n\\t\\n\\nYou can find details of the dataset in this post:https://arxiv.org/pdf/2308.16149.pdf\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbout this Arabic dataset\\n\\t\\n\\nWe only took the Arabic part of the dataset,which contains 562 data.We then extracted five from each category based on the task domain as a few shot data.\\n"},
	{"name":"SciBench-TruthfulQA-RAG","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/natnitaract/SciBench-TruthfulQA-RAG","creator_name":"Natapong Nitarach (Schwyter)","creator_url":"https://huggingface.co/natnitaract","description":"natnitaract/SciBench-TruthfulQA-RAG dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"sensory-awareness-benchmark","keyword":"multiple-choice","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/monsoon-nlp/sensory-awareness-benchmark","creator_name":"Nick Doiron","creator_url":"https://huggingface.co/monsoon-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSensory Awareness Benchmark\\n\\t\\n\\nA series of questions (goal is 100-200) and required features, designed to test whether any ML model is aware of its own capabilities.\\nControl questions are connected to a specific capability:\\n\\nCan you receive an image file?\\nWould you consider your level to be that of a super-intelligent AI agent?\\n\\nNatural questions which are possible for the average person, but may require multiple capabilities for a model:\\n\\nCan you head to the corner and check if my‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/monsoon-nlp/sensory-awareness-benchmark."},
	{"name":"kaggel-llm-science-exam-2023-RAG","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/natnitaract/kaggel-llm-science-exam-2023-RAG","creator_name":"Natapong Nitarach (Schwyter)","creator_url":"https://huggingface.co/natnitaract","description":"natnitaract/kaggel-llm-science-exam-2023-RAG dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"EusExams","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/EusExams","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for EusExams\\n\\t\\n\\nEusExams is a collection of tests designed to prepare individuals for Public Service examinations conducted by several Basque institutions, including the public health system Osakidetza, the Basque Government, the City Councils of Bilbao and Gasteiz, and the University of the Basque Country (UPV/EHU). Within each of these groups, there are different exams for public positions, such as administrative and assistant roles. Each multiple-choice question‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/EusExams."},
	{"name":"open-otter","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/onuralp/open-otter","creator_name":"Onuralp","creator_url":"https://huggingface.co/onuralp","description":"\\nDisclaimer: this dataset is curated for NeurIPS 2023 LLM efficiency challange, and currently work in progress. Please use at your own risk.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWe curated this dataset to finetune open source base models as part of NeurIPS 2023 LLM Efficiency Challenge (1 LLM + 1 GPU + 1 Day). This challenge requires participants to use open source models and datasets with permissible licenses to encourage wider adoption, use and dissemination of open source contributions in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/onuralp/open-otter."},
	{"name":"COPAL","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/haryoaw/COPAL","creator_name":"HaryoAW","creator_url":"https://huggingface.co/haryoaw","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbout COPAL-ID\\n\\t\\n\\nCOPAL-ID is an Indonesian causal commonsense reasoning dataset that captures local nuances. It provides a more natural portrayal of day-to-day causal reasoning within the Indonesian (especially Jakartan) cultural sphere. Professionally written and validatid from scratch by natives, COPAL-ID is more fluent and free from awkward phrases, unlike the translated XCOPA-ID.\\nCOPAL-ID is a test set only, intended to be used as a benchmark.\\nFor more details, please see our‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/haryoaw/COPAL."},
	{"name":"kor_qasc","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KETI-AIR/kor_qasc","creator_name":"Korea Electronics Technology Institute Artificial Intelligence Research Center","creator_url":"https://huggingface.co/KETI-AIR","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for QASC\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicensing Information\\n\\t\\n\\nThe data is distributed under the CC BY 4.0 license.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data Citation INformation\\n\\t\\n\\n@article{allenai:qasc,\\n      author    = {Tushar Khot and Peter Clark and Michal Guerquin and Peter Jansen and Ashish Sabharwal},\\n      title     = {QASC: A Dataset for Question Answering via Sentence Composition},\\n      journal   = {arXiv:1910.11473v2},\\n      year      = {2020},\\n}\\n\\n"},
	{"name":"cosmos_qa_ptbr","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/heloisy/cosmos_qa_ptbr","creator_name":"Heloisy Rodrigues","creator_url":"https://huggingface.co/heloisy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCosmos QA Portugu√™s\\n\\t\\n\\nEste dataset √© uma tradu√ß√£o para portugu√™s do Cosmos QA, que originalmente √© na l√≠ngua inglesa. \\nA tradu√ß√£o foi feita automaticamente usando o GPT-3.5-turbo, logo pode ter erros que n√£o foram notados numa an√°lise superficial. \\nSe atente ao uso.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for cosmos_qa\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicensing Information\\n\\t\\n\\nThe data is distributed under the CC BY 4.0 license.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data Citation INformation\\n\\t\\n\\n@inproceedings{huang-etal-2019-cosmos‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/heloisy/cosmos_qa_ptbr."},
	{"name":"vi_grade_school_math_mcq","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hllj/vi_grade_school_math_mcq","creator_name":"Bui Van Hop","creator_url":"https://huggingface.co/hllj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Vietnamese Grade School Math Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe dataset includes multiple-choice math exercises for elementary school students from grades 1 to 5 in Vietnam.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe majority of the data is in Vietnamese.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe data includes information about the page paths we crawled and some text that has been post-processed.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hllj/vi_grade_school_math_mcq."},
	{"name":"E-EVAL","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/E-EVAL/E-EVAL","creator_name":"E-EVAL","creator_url":"https://huggingface.co/E-EVAL","description":"E-EVAL/E-EVAL dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"MMMU","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aslessor/MMMU","creator_name":"Alex","creator_url":"https://huggingface.co/aslessor","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI)\\n\\t\\n\\nüåê Homepage | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîîNews\\n\\t\\n\\n\\nüî•[2023-12-04]: Our evaluation server for test set is now availble on EvalAI. We welcome all submissions and look forward to your participation! üòÜ\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nWe introduce MMMU: a new benchmark designed to evaluate multimodal models on massive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aslessor/MMMU."},
	{"name":"RSL_Maran","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran."},
	{"name":"MedQA-USMLE-back-translated","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Detsutut/MedQA-USMLE-back-translated","creator_name":"Tommaso Mario Buonocore","creator_url":"https://huggingface.co/Detsutut","description":"MedQA dataset perturbed using back-translation technique with BAT\\n"},
	{"name":"DynaMath_Sample","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DynaMath/DynaMath_Sample","creator_name":"DynaMath Team","creator_url":"https://huggingface.co/DynaMath","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for DynaMath\\n\\t\\n\\n\\n\\n[üíª Github] [üåê Homepage][üìñ Preprint Paper]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîà Notice\\n\\t\\n\\nDynaMath is a dynamic benchmark with 501 seed question generators. This dataset is only a sample of 10 variants generated by DynaMath. We encourage you to use the dataset generator on our github site to generate random datasets to test.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüåü About DynaMath\\n\\t\\n\\nThe rapid advancements in Vision-Language Models (VLMs) have shown significant‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DynaMath/DynaMath_Sample."},
	{"name":"Aloe-Beta-Medical-Collection","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Aloe-Beta-Medical-Collection\\n\\t\\n\\n\\n\\nCollection of curated datasets used to fine-tune Aloe-Beta.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nWe curated data from many publicly available medical instruction tuning data sources (QA format). Most data samples correspond to single-turn QA pairs, while a small proportion contain multi-turn. All data sources are publicly available for commercial purposes.\\nWe implemented a rigorous data preprocessing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection."},
	{"name":"MedS-Ins","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/MedS-Ins","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Aloe-Beta-Medical-Collection\\n\\t\\n\\n\\n\\nCollection of curated data from the MedS-Ins dataset. Used to train Aloe-Beta model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nThis is the curated version of the MedS-Ins dataset included in the training set of the Aloe-Beta models. \\nFirst, we selected 75 out of the 122 existing tasks, excluding the tasks that were already in the training set, and the datasets with non-commercial licenses. Then, we passed the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedS-Ins."},
	{"name":"V1Q","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\\nds = load_dataset(\\\"b3x0m/Chinese-H-Novels\\\")\\nimport sagemaker\\nimport boto3\\nfrom sagemaker.huggingface import HuggingFaceModel\\ntry:\\n    role = sagemaker.get_execution_role()\\nexcept ValueError:\\n    iam = boto3.client('iam')\\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\\n\\n\\t\\n\\t\\t\\n\\t\\tHub Model configuration. https://huggingface.co/models\\n\\t\\n\\nhub = {\\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\\n    'HF_TASK':'any-to-any'\\n}\\n\\n\\t\\n\\t\\t\\n\\t\\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q."}
]
;
