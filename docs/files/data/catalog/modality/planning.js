const data_for_modality_planning = 
[
	{"name":"cpim","keyword":"planning","description":"APICS CPIM Materials Dataset\n\n\n\n\t\n\t\t\n\t\tThis project aimed to develop a dataset of CPIM (Planning and Inventory Management Certification) materials, sourced from the APICS Association , to support supply chain planning and inventory management.\n\t\n\n","url":"https://huggingface.co/datasets/asabada/cpim","creator_name":"Ahmed S. Abada","creator_url":"https://huggingface.co/asabada","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"rc1","keyword":"planning","description":"\n\t\n\t\t\n\t\tReasoning Core ‚óâ\n\t\n\nPaper: Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning\nCode: GitHub Repository\nreasoning-core is a text-based RLVR for LLM reasoning training.\nIt is centered on expressive symbolic tasks, including full fledged FOL, formal mathematics with TPTP, formal planning with novel domains, and syntax tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAbstract\n\t\n\nWe introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/reasoning-core/rc1.","url":"https://huggingface.co/datasets/reasoning-core/rc1","creator_name":"Reasoning Core","creator_url":"https://huggingface.co/reasoning-core","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"plan-and-act-data","keyword":"planning","description":"\n\t\n\t\t\n\t\tPlan-and-Act Dataset\n\t\n\nThis repository hosts the datasets used in the Plan-and-Act framework from the paper:\n\nPlan-and-Act: Improving Planning of Agents for Long-Horizon TasksPaper (arXiv:2503.09572)Project Repository\n\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThe Plan-and-Act framework introduces a method for enabling accurate and reliable long-horizon task solving by separating high-level planning from low-level execution. To support training and evaluation, we release two datasets:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xTRam1/plan-and-act-data.","url":"https://huggingface.co/datasets/xTRam1/plan-and-act-data","creator_name":"Lutfi Eren Erdogan","creator_url":"https://huggingface.co/xTRam1","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","arxiv:2503.09572","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"lumos_unified_plan_iterative","keyword":"planning","description":"\n\t\n\t\t\n\t\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  üåê[Website] ¬†\n  üìù[Paper] ¬†\n  ü§ó[Data] ¬†\n  ü§ó[Model] ¬†\n  ü§ó[Demo] ¬†\n\n\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nüß© Modular Architecture:\nüß© Lumos consists of planning, grounding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_unified_plan_iterative.","url":"https://huggingface.co/datasets/ai2lumos/lumos_unified_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"planetarium","keyword":"planning","description":"\n\t\n\t\t\n\t\tDataset Card for Planetariumü™ê\n\t\n\nPlanetariumü™ê is a dataset and benchmark for assessing LLMs in translating natural language descriptions of planning problems into PDDL.\nWe developed a robust method for comparing PDDL problem descriptions using graph isomorphism.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis dataset is a set of pairs of planning problems in PDDL and natural language descriptions from the Blocks World and Gripper domains. The task is to take descriptions of various initial and goal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BatsResearch/planetarium.","url":"https://huggingface.co/datasets/BatsResearch/planetarium","creator_name":"Bats Research","creator_url":"https://huggingface.co/BatsResearch","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","question-answering","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PathEval","keyword":"planning","description":"\n\t\n\t\t\n\t\tPathEval: A Benchmark for Evaluating Vision-Language Models as Evaluators for Path Planning\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nDespite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still contribute to the planning framework as a helpful plan evaluator? In this work, we generalize this question to consider LLMs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maghzal/PathEval.","url":"https://huggingface.co/datasets/maghzal/PathEval","creator_name":"Mohamed Aghzal","creator_url":"https://huggingface.co/maghzal","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Image","Text"],"keywords_longer_than_N":true},
	{"name":"acp_bench","keyword":"planning","description":"\n\t\n\t\t\n\t\tACP Bench\n\t\n\n\n    üè† Homepage    ‚Ä¢     \n    üìÑ Paper ‚Ä¢     \n    üìÑ Paper \n\n\nACPBench is a benchmark dataset designed to evaluate the reasoning capabilities of large language models (LLMs) in the context of Action, Change, and Planning. It spans 13 diverse domains:\n\nBlocksworld\nLogistics\nGrippers \nGrid \nFerry\nFloorTile\nRovers\nVisitAll\nDepot\nGoldminer\nSatellite\nSwap\nAlfworld\n\n\n\t\n\t\n\t\n\t\tTask Types in ACPBench\n\t\n\nACPBench includes the following 8 reasoning tasks:\n\nAction Applicability (app)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ibm-research/acp_bench.","url":"https://huggingface.co/datasets/ibm-research/acp_bench","creator_name":"IBM Research","creator_url":"https://huggingface.co/ibm-research","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["question-answering","cdla-permissive-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"boxoban-astar-solutions","keyword":"planning","description":"\n\t\n\t\t\n\t\n\t\n\t\tA* solutions to Boxoban levels\n\t\n\nFor some levels we were not able to find solutions within the allotted A* budget. These have solution\nSEARCH_STATE_FAILED or NOT_FOUND. These are the ones labeled \"Unsolved levels\" below.\nThe search budget was 5 million nodes to expand for medium-difficulty levels, vs. 1 million nodes for\nunfiltered-difficulty levels. The heuristic was the sum of Manhattan distances of each box to its closest target.\n\n\t\n\t\t\n\t\n\t\n\t\tSummary table:\n\t\n\n\n\t\n\t\t\nLevel file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlignmentResearch/boxoban-astar-solutions.","url":"https://huggingface.co/datasets/AlignmentResearch/boxoban-astar-solutions","creator_name":"FAR AI","creator_url":"https://huggingface.co/AlignmentResearch","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","apache-2.0","1M<n<10M","üá∫üá∏ Region: US","sokoban"],"keywords_longer_than_N":true},
	{"name":"WorFBench_test","keyword":"planning","description":" WorFBench \n Benchmarking Agentic Workflow Generation \n\n\n  üìÑarXiv ‚Ä¢\n  ü§óHFPaper ‚Ä¢\n  üåêWeb ‚Ä¢\n  üñ•Ô∏èCode ‚Ä¢\n  üìäDataset\n\n\n\nüåªAcknowledgement\nüåüOverview\nüîßInstallation\n‚úèÔ∏èModel-Inference\nüìùWorkflow-Generation\nü§îWorkflow-Evaluation\n\n\n\t\n\t\t\n\t\tüåªAcknowledgement\n\t\n\nOur code of training module is referenced and adapted from LLaMA-Factory. And the Dataset is collected from ToolBench, ToolAlpaca, Lumos, WikiHow, Seal-Tools, Alfworld, Webshop, IntercodeSql. Our end-to-end evaluation module is based on IPR‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zjunlp/WorFBench_test.","url":"https://huggingface.co/datasets/zjunlp/WorFBench_test","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"lumos_multimodal_plan_iterative","keyword":"planning","description":"\n\t\n\t\t\n\t\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  üåê[Website] ¬†\n  üìù[Paper] ¬†\n  ü§ó[Data] ¬†\n  ü§ó[Model] ¬†\n  ü§ó[Demo] ¬†\n\n\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nüß© Modular Architecture:\nüß© Lumos consists of planning, grounding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_multimodal_plan_iterative.","url":"https://huggingface.co/datasets/ai2lumos/lumos_multimodal_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_plan_iterative","keyword":"planning","description":"\n\t\n\t\t\n\t\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  üåê[Website] ¬†\n  üìù[Paper] ¬†\n  ü§ó[Data] ¬†\n  ü§ó[Model] ¬†\n  ü§ó[Demo] ¬†\n\n\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nüß© Modular Architecture:\nüß© Lumos consists of planning, grounding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_iterative.","url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_web_agent_plan_iterative","keyword":"planning","description":"\n\t\n\t\t\n\t\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  üåê[Website] ¬†\n  üìù[Paper] ¬†\n  ü§ó[Data] ¬†\n  ü§ó[Model] ¬†\n  ü§ó[Demo] ¬†\n\n\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nüß© Modular Architecture:\nüß© Lumos consists of planning, grounding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_web_agent_plan_iterative.","url":"https://huggingface.co/datasets/ai2lumos/lumos_web_agent_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_plan_iterative","keyword":"planning","description":"\n\t\n\t\t\n\t\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  üåê[Website] ¬†\n  üìù[Paper] ¬†\n  ü§ó[Data] ¬†\n  ü§ó[Model] ¬†\n  ü§ó[Demo] ¬†\n\n\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nüß© Modular Architecture:\nüß© Lumos consists of planning, grounding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_iterative.","url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_plan_onetime","keyword":"planning","description":"\n\t\n\t\t\n\t\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  üåê[Website] ¬†\n  üìù[Paper] ¬†\n  ü§ó[Data] ¬†\n  ü§ó[Model] ¬†\n  ü§ó[Demo] ¬†\n\n\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nüß© Modular Architecture:\nüß© Lumos consists of planning, grounding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_onetime.","url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true}
]
;
