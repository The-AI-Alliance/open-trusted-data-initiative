var data_for_modality_evaluation = 
[
	{"name":"UHGEvalDataset","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Ki-Seki/UHGEvalDataset","creator_name":"Shichao Song","creator_url":"https://huggingface.co/Ki-Seki","description":"The dataset sourced from https://github.com/IAAR-Shanghai/UHGEval\\n"},
	{"name":"OneLLM_Eval","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/csuhan/OneLLM_Eval","creator_name":"Jiaming Han","creator_url":"https://huggingface.co/csuhan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOneLLM Evaluation Datasets\\n\\t\\n\\n"},
	{"name":"ruMT-Bench","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NLPCoreTeam/ruMT-Bench","creator_name":"NLP Core Team","creator_url":"https://huggingface.co/NLPCoreTeam","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\truMT-Bench\\n\\t\\n\\nruMT-Bench contains instructive multi-turn questions divided into 8 different areas of knowledge (writing, roleplay, extraction, reasoning, math, coding, STEM, humanities/social science). GPT-4 scores models' responses on a scale of 1 to 10. The final score is determined by the average of the entire conversation. For some complex problems that require a precise answer (e.g. math and coding), a reference answer is included in the judge's prompt to help evaluate… See the full description on the dataset page: https://huggingface.co/datasets/NLPCoreTeam/ruMT-Bench."},
	{"name":"Manga-Popularis-EN","keyword":"evaluation","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Tsunnami/Manga-Popularis-EN","creator_name":"Tsunn","creator_url":"https://huggingface.co/Tsunnami","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card: Manga-Popularis-EN\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains information about art books, including titles, authors, descriptions, image paths, prices, publication dates, publishers, ISBN numbers, and sizes.\\n\\nLicense: GPL-3.0\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\nThe dataset was compiled from various sources, including online bookstores, publisher websites, and catalogs. On date 4/21/24.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe… See the full description on the dataset page: https://huggingface.co/datasets/Tsunnami/Manga-Popularis-EN."},
	{"name":"CQADupstackRetrieval-256-24-gpt-4o-2024-05-13-261378","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/CQADupstackRetrieval-256-24-gpt-4o-2024-05-13-261378","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCQADupstackRetrieval-256-24-gpt-4o-2024-05-13-261378 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research dataset search\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the CQADupstackRetrieval-256-24-gpt-4o-2024-05-13-261378 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/CQADupstackRetrieval-256-24-gpt-4o-2024-05-13-261378."},
	{"name":"Touche2020-256-24-gpt-4o-2024-05-13-27907","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/Touche2020-256-24-gpt-4o-2024-05-13-27907","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTouche2020-256-24-gpt-4o-2024-05-13-27907 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"information retrieval benchmark search\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the Touche2020-256-24-gpt-4o-2024-05-13-27907 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/Touche2020-256-24-gpt-4o-2024-05-13-27907."},
	{"name":"QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuoraRetrieval-256-24-gpt-4o-2024-05-13-635320 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"information retrieval benchmark\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-635320."},
	{"name":"QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuoraRetrieval-256-24-gpt-4o-2024-05-13-80208 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"information retrieval benchmark\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/QuoraRetrieval-256-24-gpt-4o-2024-05-13-80208."},
	{"name":"Begrading","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mina98/Begrading","creator_name":"Mina Atef Yousef","creator_url":"https://huggingface.co/mina98","description":"\\n\\t\\n\\t\\t\\nColumns\\ndiscribe\\nComment\\n\\n\\n\\t\\t\\nInstruction\\nQuestion by teacher\\n_\\n\\n\\nInstruction_solution\\nSolution by student\\n_\\n\\n\\nGrade\\nMedian grading from TA, GPT4o, GeminiPro, tuning ChatQA-1.5\\nthe mean value of difference 0.85, the standard deviation (std) of 0.65, with values ranging from a minimum of 0.0 to a maximum of 2.88, and the 25th, 50th, and 75th percentiles being 0.57, 0.57, and 1.15 respectively.\\n\\n\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this data, please cite the following paper:… See the full description on the dataset page: https://huggingface.co/datasets/mina98/Begrading."},
	{"name":"jinaai_jina-embeddings-v2-base-en-05062024-zvoa-webapp","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05062024-zvoa-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-en-05062024-zvoa-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"software development\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-en-05062024-zvoa-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-en-05062024-zvoa-webapp."},
	{"name":"MMBench-Video","keyword":"evaluation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/opencompass/MMBench-Video","creator_name":"OpenCompass","creator_url":"https://huggingface.co/opencompass","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding\\n\\t\\n\\n\\nHomepage: https://mmbench-video.github.io/\\nRepository: https://huggingface.co/datasets/opencompass/MMBench-Video\\nPaper: MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMMBench-Video is a quantitative benchmark designed to rigorously evaluate LVLMs' proficiency in video understanding.\\nMMBench-Video incorporates approximately 600 web… See the full description on the dataset page: https://huggingface.co/datasets/opencompass/MMBench-Video."},
	{"name":"DefAn","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/iamasQ/DefAn","creator_name":"A B M Ashikur Rahman","creator_url":"https://huggingface.co/iamasQ","description":"DefAn: Definitive-Answer-Dataset-for-LLMs-Hallucination-Evaluation\\n\\n  A.B.M. Ashikur Rahman1, Saeed Anwar1,2, Muhammad Usman1,2, Ajmal Mian3, \\n\\n\\n1 King Fahd University of Petroleum and Minerals, Dhahran, KSA\\n\\n\\n2JRCAI, SDAIA-KFUPM \\n\\n\\n3The University of Western Australia, Crawley, Western Australia\\n\\n\\n    Arxiv Paper,  GitHub Repository\\n\\n\\n\\n\\\"DefAn\\\" is a comprehensive evaluation benchmark dataset, with more than 75000 samples, designed to assess the hallucination tendencies of large language models… See the full description on the dataset page: https://huggingface.co/datasets/iamasQ/DefAn."},
	{"name":"BeHonest","keyword":"evaluation","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GAIR/BeHonest","creator_name":"SII - GAIR","creator_url":"https://huggingface.co/GAIR","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBeHonest: Benchmarking Honesty in Large Language Models\\n\\t\\n\\nBeHonest is a pioneering benchmark specifically designed to assess honesty in LLMs comprehensively. BeHonest evaluates three essential aspects of honesty: awareness of knowledge boundaries (self-knowledge), avoidance of deceit (non-deceptiveness), and consistency in responses (consistency).\\nBeHonest supports the following 10 scenarios:\\n\\nAdmitting Unknowns: LLMs should appropriately refuse to answer questions that are beyond… See the full description on the dataset page: https://huggingface.co/datasets/GAIR/BeHonest."},
	{"name":"Set_Eval","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AarushSah/Set_Eval","creator_name":"Aarush Sah","creator_url":"https://huggingface.co/AarushSah","description":"AarushSah/Set_Eval dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"LIBRA","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ai-forever/LIBRA","creator_name":"ai-forever","creator_url":"https://huggingface.co/ai-forever","description":"\\n\\t\\n\\t\\t\\n\\t\\tLIBRA: Long Input Benchmark for Russian Analysis\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nLIBRA (Long Input Benchmark for Russian Analysis) is designed to evaluate the capabilities of large language models (LLMs) in understanding and processing long texts in Russian. This benchmark includes 21 datasets adapted for different tasks and complexities. The tasks are divided into four complexity groups and allow evaluation across various context lengths ranging from 4k up to 128k tokens.… See the full description on the dataset page: https://huggingface.co/datasets/ai-forever/LIBRA."},
	{"name":"pinocchio","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mii-llm/pinocchio","creator_name":"mii-llm","creator_url":"https://huggingface.co/mii-llm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nPinocchio is a comprehensive and challenging Natural Language Understanding (NLU) dataset designed to rigorously evaluate language models' capabilities, with a particular focus on Italian language, culture, and various specialized domains.\\n\\n\\n\\n\\n\\n📖Paper (maybe soon)\\n\\n[2024.07.16] Pinocchio dataset released, featuring ~140,000 questions across modalities and ~40 disciplines.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t1. What's unique about Pinocchio?\\n\\t\\n\\nCompared to other NLU datasets, Pinocchio… See the full description on the dataset page: https://huggingface.co/datasets/mii-llm/pinocchio."},
	{"name":"biomedical-mmlu-ita","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Detsutut/biomedical-mmlu-ita","creator_name":"Tommaso Mario Buonocore","creator_url":"https://huggingface.co/Detsutut","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nItalian translation of the biomedical-related part of the MMLU evaluation dataset described in Measuring Massive Multitask Language Understanding by Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt (ICLR 2021).\\n"},
	{"name":"grouse","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/illuin/grouse","creator_name":"Illuin Technology","creator_url":"https://huggingface.co/illuin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for GroUSE\\n\\t\\n\\nGroUSE (Grounded QA Unitary Scoring of Evaluators) is a dataset designed to assess the performance of Grounded QA evaluators. Its purpose is to evaluate whether an LLM, when used as a grounded QA evaluator, delivers the expected scores across six metrics when presented with both good and imperfect answers.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nEach sample is of the following form :\\n{\\n    \\\"references\\\": [\\n        \\\"[Content of the… See the full description on the dataset page: https://huggingface.co/datasets/illuin/grouse."},
	{"name":"llmfao","keyword":"evaluation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dustalov/llmfao","creator_name":"Dmitry Ustalov","creator_url":"https://huggingface.co/dustalov","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLarge Language Model Feedback Analysis and Optimization (LLMFAO)\\n\\t\\n\\nThe original Crowdsourced LLM Benchmark dataset in files prompts.parqet and outputs.parquet was kindly provided by the team at llmonitor.com under a CC BY 4.0 license. This dataset can be conveniently processed with Evalica (arXiv).\\n"},
	{"name":"ru-mt-bench","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/t-bank-ai/ru-mt-bench","creator_name":"T-Bank-AI","creator_url":"https://huggingface.co/t-bank-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tru-mt-bench\\n\\t\\n\\nThis is translated version of LMSYS MT-Bench datasets for evaluation LLMs.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEvaluation\\n\\t\\n\\nTo use this dataset for evaluation, download this jsonl file with formated data and use it with original LMSYS LLM-judge codebase.\\n"},
	{"name":"CounterEval","keyword":"evaluation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anitera/CounterEval","creator_name":"Marharyta Domnich","creator_url":"https://huggingface.co/anitera","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCounterEval: Towards Unifying Evaluation of Counterfactual Explanations\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nCounterEval offers a human-evaluated dataset of 30 counterfactual scenarios, engineered to serve as a benchmark for evaluating a wide range of counterfactual explanation generation frameworks. Each scenario has been strategically designed to exhibit varying levels across multiple explanatory quality metrics, such as Feasibility, Consistency, Trust, Completeness, Fairness, and… See the full description on the dataset page: https://huggingface.co/datasets/anitera/CounterEval."},
	{"name":"ru-arena-hard","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/t-tech/ru-arena-hard","creator_name":"T-Tech","creator_url":"https://huggingface.co/t-tech","description":"\\n\\t\\n\\t\\t\\n\\t\\tru-arena-hard\\n\\t\\n\\nThis is translated version of arena-hard-auto dataset for evaluation LLMs. The translation of the original dataset was done manually. In addition, content of each task in dataset was reviewed, the correctness of the task statement and compliance with moral and ethical standards were assessed. Thus, this dataset allows you to evaluate the abilities of language models to support the Russian language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview of the Dataset\\n\\t\\n\\n\\nOriginal dataset:… See the full description on the dataset page: https://huggingface.co/datasets/t-tech/ru-arena-hard."},
	{"name":"ru-mt-bench","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/t-tech/ru-mt-bench","creator_name":"T-Tech","creator_url":"https://huggingface.co/t-tech","description":"\\n\\t\\n\\t\\t\\n\\t\\tru-mt-bench\\n\\t\\n\\nru-mt-bench is translated version of LMSYS MT-Bench datasets for evaluation LLMs. The translation of the original dataset was done manually.\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview of the Dataset\\n\\t\\n\\n\\nOriginal dataset: mt_bench\\nNumber of tasks in original dataset: 80\\nNumber of tasks: 80\\nFormat: JSON LINES\\nTask categories: writing, roleplay, reasoning, math, coding, extraction, stem, humanities\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nTo use this dataset for evaluation follow these steps:\\n\\nDownload this jsonl… See the full description on the dataset page: https://huggingface.co/datasets/t-tech/ru-mt-bench."},
	{"name":"mmlu_pro_categories","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/RawthiL/mmlu_pro_categories","creator_name":"Ramiro","creator_url":"https://huggingface.co/RawthiL","description":"MMLU-Pro dataset is a more robust and challenging massive multi-task understanding dataset tailored to more rigorously benchmark large language models' capabilities. This dataset contains 12K complex questions across various disciplines."},
	{"name":"RocketEval","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Joinn/RocketEval","creator_name":"Tianjun","creator_url":"https://huggingface.co/Joinn","description":"\\n\\n🚀 RocketEval 🚀\\n\\n🚀 [ICLR '25] RocketEval: Efficient Automated LLM Evaluation via Grading Checklist\\n\\nGithub | OpenReview | Colab\\n\\nThis dataset contains the queries, generated checklist data, and responses data from 4 public benchmark datasets:\\n\\n\\t\\n\\t\\t\\nDataset\\nNo. of Queries\\nComments\\n\\n\\n\\t\\t\\nMT-Bench\\n160\\nEach 2-turn dialogue is split into 2 queries.\\nAlpacaEval\\n805\\n\\n\\n\\nArena-Hard\\n500\\n\\n\\n\\nWildBench\\n1,000\\nTo fit the context window of lightweight LLMs, we use a subset of WildBench including 1000… See the full description on the dataset page: https://huggingface.co/datasets/Joinn/RocketEval."},
	{"name":"style_eval_content_test","keyword":"evaluation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/APauli/style_eval_content_test","creator_name":"Amalie Pauli","creator_url":"https://huggingface.co/APauli","description":"\\n\\t\\n\\t\\t\\n\\t\\tConstructed test set for evaluating metrics for content preservation in style and attribute transfer\\n\\t\\n\\n This data is used in the meta-evaluation of metrics for content preservation in cases with style and attribute transfer.\\nThe data consists of 500 samples with a source sentence and a transfer sentence to a specific style.\\nThe data is annotated by 3 humans on two dimension 'style strength' (AnswerS_1,AnswerS_2,AnswerS_3) and 'content preservation' (AnswerC_1,AnswerC_2,AnswerC_3) on a… See the full description on the dataset page: https://huggingface.co/datasets/APauli/style_eval_content_test."},
	{"name":"RobloxQA-OpenEnded-v1.0","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/boatbomber/RobloxQA-OpenEnded-v1.0","creator_name":"Zack Ovits","creator_url":"https://huggingface.co/boatbomber","description":"\\n\\t\\n\\t\\t\\n\\t\\tRobloxQA OpenEnded V1.0\\n\\t\\n\\nThis dataset contains questions and answers relating to Roblox. This is used to evaluate models' knowledge and understanding of Roblox game development.\\nWe take the multiple choice questions from RobloxQA V1.0 (MIT) and have QwQ 32B (Apache 2.0) restructure them to be open ended questions. For more information, see the open source dataset generator below.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Format\\n\\t\\n\\nAll data is stored in .parquet files. The prompt and answer are in separate… See the full description on the dataset page: https://huggingface.co/datasets/boatbomber/RobloxQA-OpenEnded-v1.0."},
	{"name":"JCQ","keyword":"evaluation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nlp-waseda/JCQ","creator_name":"Kawahara Lab at Waseda University","creator_url":"https://huggingface.co/nlp-waseda","description":"\\n\\t\\n\\t\\t\\n\\t\\tJapanese Creativity Questions (JCQ)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nJCQは創造性を評価するための7タスク、各100問からなる日本語のデータセットです。このデータセットはNLP2025の研究論文で発表されたものです。Torrance Test of Creative Thinking (TTCT)、Zhaoらの研究 (2024)を参考にして作成しました。\\n\\n\\t\\n\\t\\t\\n\\t\\tTask Definition and Examples\\n\\t\\n\\nJCQは7つの異なるタスクで構成されています。以下の表に各タスクの定義と代表的な問題例を示します。\\n\\n\\t\\n\\t\\t\\nタスク\\n定義\\n問題例\\n\\n\\n\\t\\t\\n非通常使用 (unusual uses)\\n一般的な物体の珍しい使い方や多様な使い方を考えるタスク。\\n電球の通常でない使い方をできるだけたくさん挙げてください。\\n\\n\\n結果 (consequences)\\n普通ではない、または仮説的な状況における結果や影響を予測するタスク。\\nもしも世界中で 24… See the full description on the dataset page: https://huggingface.co/datasets/nlp-waseda/JCQ."},
	{"name":"thai-ocr-evaluation","keyword":"evaluation","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/openthaigpt/thai-ocr-evaluation","creator_name":"OpenThaiGPT","creator_url":"https://huggingface.co/openthaigpt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThai OCR Evaluation Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Thai OCR Evaluation Dataset is designed for evaluating Optical Character Recognition (OCR) models across various domains. It includes images and textual data derived from various open-source websites.\\nThis dataset aims to provide a comprehensive evaluation resource for researchers and developers working on OCR systems, particularly in Thai language processing.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\nEach sample in the dataset… See the full description on the dataset page: https://huggingface.co/datasets/openthaigpt/thai-ocr-evaluation."},
	{"name":"thaitrocr-eval-dataset-beta","keyword":"evaluation","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/suchut/thaitrocr-eval-dataset-beta","creator_name":"Suchut Sapsathien","creator_url":"https://huggingface.co/suchut","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThaiTROCR Evaluation Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe ThaiTROCR Evaluation Dataset is designed for evaluating Optical Character Recognition (OCR) models across various domains. It includes images and textual data derived from various open-source websites.\\nThis dataset aims to provide a comprehensive evaluation resource for researchers and developers working on OCR systems, particularly in Thai language processing.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\nEach sample in the… See the full description on the dataset page: https://huggingface.co/datasets/suchut/thaitrocr-eval-dataset-beta."},
	{"name":"jnli","keyword":"evaluation","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zenless-lab/jnli","creator_name":"Zenless Lab","creator_url":"https://huggingface.co/zenless-lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJGLUE[JNLI]: Japanese General Language Understanding Evaluation\\n\\t\\n\\nJNLI(yahoojapan/JGLUE) is a Japanese version of the NLI (Natural Language Inference) dataset. \\nNLI is a task to recognize the inference relation that a premise sentence has to a hypothesis sentence. \\nThe inference relations are entailment, contradiction, and neutral.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\nRepository: yahoojapan/JGLUE\\nPaper: [More Information Needed]… See the full description on the dataset page: https://huggingface.co/datasets/zenless-lab/jnli."},
	{"name":"jamp","keyword":"evaluation","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zenless-lab/jamp","creator_name":"Zenless Lab","creator_url":"https://huggingface.co/zenless-lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJamp: Controlled Japanese Temporal Inference Dataset for Evaluating Generalization Capacity of Language Models\\n\\t\\n\\nJamp(tomo-vv/temporalNLI_dataset) is the Japanese temporal inference benchmark. \\nThis dataset consists of templates, test data, and training data. \\nTemplate subset containing template, time format, or time span in their names are split based on tense fragment, time format, \\nor time span, respectively.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources… See the full description on the dataset page: https://huggingface.co/datasets/zenless-lab/jamp."},
	{"name":"jsem","keyword":"evaluation","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/zenless-lab/jsem","creator_name":"Zenless Lab","creator_url":"https://huggingface.co/zenless-lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJSeM: Japanese semantic test suite (Japanese FraCaS and extensions)\\n\\t\\n\\n叙述文間の含意関係は、言語学においては意味論の中心的な説明対象の一つであるとともに、理論を検証するためのベンチマークとして用いられています。\\nまた近年の自然言語処理においては、含意関係認識(Recognizing Textual Entailment: RTE)が意味処理タスクの中核となっています。\\n\\n前提(premise)：一つの文\\n仮説(hypothesis)：一つの文\\n判定(answer)：1.と2.の間に含意関係があるかどうかについての母語話者の判断（entailment, neutralあるいはcontradiction)\\n\\nこのテストセットでは、FraCaS test suite（Cooper et al.(1996)で公開されたオリジナルのテストセット, \\nおよびBill MacCartney氏による同セットのXML版）の方針にならい、言語現象ごとに含意関係のテストをまとめています。… See the full description on the dataset page: https://huggingface.co/datasets/zenless-lab/jsem."},
	{"name":"MMLU-Pro","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMLU-Pro Dataset\\n\\t\\n\\nMMLU-Pro dataset is a more robust and challenging massive multi-task understanding dataset tailored to more rigorously benchmark large language models' capabilities. This dataset contains 12K complex questions across various disciplines. \\n|Github | 🏆Leaderboard | 📖Paper |\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t🚀 What's New\\n\\t\\n\\n\\n[2024.10.16] We have added Gemini-1.5-Flash-002, Gemini-1.5-Pro-002, Jamba-1.5-Large, Llama-3.1-Nemotron-70B-Instruct-HF and Ministral-8B-Instruct-2410 to our… See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro."},
	{"name":"mt_bench_prompts","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMT Bench by LMSYS\\n\\t\\n\\nThis set of evaluation prompts is created by the LMSYS org for better evaluation of chat models.\\nFor more information, see the paper.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset loading\\n\\t\\n\\nTo load this dataset, use 🤗 datasets:\\nfrom datasets import load_dataset\\ndata = load_dataset(HuggingFaceH4/mt_bench_prompts, split=\\\"train\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset creation\\n\\t\\n\\nTo create the dataset, we do the following for our internal tooling. \\n\\nrename turns to prompts,\\nadd empty reference to… See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts."},
	{"name":"T2IScoreScore","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/saxon/T2IScoreScore","creator_name":"Michael Saxon","creator_url":"https://huggingface.co/saxon","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Text-to-Image ScoreScore (T2IScoreScore or TS2)\\n\\t\\n\\nThis dataset exists as part of the T2IScoreScore metaevaluation for assessing the faithfulness and consistency of text-to-image model prompt-image evaluation metrics.\\nNecessary code for utilizing the resource is present at github.com/michaelsaxon/T2IScoreScore\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a test set of 165 \\\"target prompts\\\" which each have between 5 and 76 generated… See the full description on the dataset page: https://huggingface.co/datasets/saxon/T2IScoreScore."},
	{"name":"OllaBench","keyword":"evaluation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theResearchNinja/OllaBench","creator_name":"Tam Nguyen","creator_url":"https://huggingface.co/theResearchNinja","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nLarge Language Models (LLMs) have the potential to enhance Agent-Based Modeling by better representing complex interdependent cybersecurity systems, improving cybersecurity threat modeling and risk management. Evaluating LLMs in this context is crucial for legal compliance and effective application development. Existing LLM evaluation frameworks often overlook the human factor and cognitive computing capabilities essential for interdependent… See the full description on the dataset page: https://huggingface.co/datasets/theResearchNinja/OllaBench."},
	{"name":"KoMT-Bench","keyword":"evaluation","license":"GNU Lesser General Public License v3.0","license_url":"https://choosealicense.com/licenses/lgpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench","creator_name":"LG AI Research","creator_url":"https://huggingface.co/LGAI-EXAONE","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKoMT-Bench\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nWe present KoMT-Bench, a benchmark designed to evaluate the capability of language models in following instructions in Korean.\\nKoMT-Bench is an in-house dataset created by translating MT-Bench [1]  dataset into Korean and modifying some questions to reflect the characteristics and cultural nuances of the Korean language.\\nAfter the initial translation and modification, we requested expert linguists to conduct a thorough review of our… See the full description on the dataset page: https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench."},
	{"name":"Yue-Benchmark","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BillBao/Yue-Benchmark","creator_name":"Bao","creator_url":"https://huggingface.co/BillBao","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models\\n\\t\\n\\n\\nHomepage: https://github.com/jiangjyjy/Yue-Benchmark\\nRepository: https://huggingface.co/datasets/BillBao/Yue-Benchmark\\nPaper: How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThe rapid evolution of large language models (LLMs), such as GPT-X and Llama-X, has driven significant advancements in NLP, yet much of this… See the full description on the dataset page: https://huggingface.co/datasets/BillBao/Yue-Benchmark."},
	{"name":"TurtleBench1.5k","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Duguce/TurtleBench1.5k","creator_name":"Qingchen Yu","creator_url":"https://huggingface.co/Duguce","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nTurtleBench is a novel evaluation benchmark designed to assess the reasoning capabilities of large language models (LLMs) using yes/no puzzles (commonly known as \\\"Turtle Soup puzzles\\\"). This dataset is constructed based on user guesses collected from our online Turtle Soup Puzzle platform, providing a dynamic and interactive means of evaluation. Unlike traditional static evaluation benchmarks, TurtleBench focuses on testing models in interactive settings to better… See the full description on the dataset page: https://huggingface.co/datasets/Duguce/TurtleBench1.5k."},
	{"name":"math-squared","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/veds12/math-squared","creator_name":"Vedant Shah","creator_url":"https://huggingface.co/veds12","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Name\\n\\t\\n\\nMATH2\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nMATH2 is a mathematical reasoning evaluation dataset curated using a human-in-the-loop approach proposed in the paper AI-Assisted Generation of Difficult Math Questions. The dataset consists of 210 questions formed by combining 2 math domain skills using frontier LLMs. These skills were extracted from the MATH [Hendrycks et al., 2021] dataset.  \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\n\\nPaper: AI-Assisted Generation of Difficult Math… See the full description on the dataset page: https://huggingface.co/datasets/veds12/math-squared."},
	{"name":"ethical-framework-UNESCO-Ethics-of-AI","keyword":"evaluation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ktiyab/ethical-framework-UNESCO-Ethics-of-AI","creator_name":"Tiyab K.","creator_url":"https://huggingface.co/ktiyab","description":"\\n\\t\\n\\t\\t\\n\\t\\tEthical AI Training Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tIntroduction\\n\\t\\n\\nUNESCO's Ethics of Artificial Intelligence, adopted by 193 Member States in November 2021, represents the first global framework for ethical AI development and deployment.\\nWhile regional initiatives like The Montréal Declaration for a Responsible Development of Artificial Intelligence emphasize community-driven governance, UNESCO's approach establishes comprehensive international standards through coordinated multi-stakeholder… See the full description on the dataset page: https://huggingface.co/datasets/ktiyab/ethical-framework-UNESCO-Ethics-of-AI."},
	{"name":"NLG-Eval","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PKU-ONELab/NLG-Eval","creator_name":"ONE Lab @PKU","creator_url":"https://huggingface.co/PKU-ONELab","description":"\\n\\t\\n\\t\\t\\n\\t\\tNLG-Eval\\n\\t\\n\\nThemis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability\\nPaper: https://aclanthology.org/2024.emnlp-main.891\\nGithub: https://github.com/PKU-ONELab/Themis\\n\\n\\t\\n\\t\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThe evaluation of natural language generation (NLG) tasks is a significant and longstanding research area. Despite abundant data on NLG tasks, the corresponding high-quality evaluation data remains scarce and scattered due to the high cost of professional human… See the full description on the dataset page: https://huggingface.co/datasets/PKU-ONELab/NLG-Eval."},
	{"name":"Diversity_Equity_and_Inclusion","keyword":"evaluation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ktiyab/Diversity_Equity_and_Inclusion","creator_name":"Tiyab K.","creator_url":"https://huggingface.co/ktiyab","description":"\\n\\t\\n\\t\\t\\n\\t\\tDiversity Equity and Inclusion Training & Evaluation Data\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThis dataset is designed to support training and evaluating AI models or educational programs in the areas of Diversity, Equity, and Inclusion (DEI). It contains 198 entries (approximately 716 KB in CSV format), each entry pairing a realistic instruction related to DEI scenarios or queries with a corresponding response that models an appropriate, inclusive answer. Alongside each question-answer… See the full description on the dataset page: https://huggingface.co/datasets/ktiyab/Diversity_Equity_and_Inclusion."},
	{"name":"mt-bench-french","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bofenghuang/mt-bench-french","creator_name":"bofeng huang","creator_url":"https://huggingface.co/bofenghuang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMT-Bench-French\\n\\t\\n\\nThis is a French version of MT-Bench, created to evaluate the multi-turn conversation and instruction-following capabilities of LLMs. Similar to its original version, MT-Bench-French comprises 80 high-quality, multi-turn questions spanning eight main categories.\\nAll questions have undergone translation into French and thorough human review to guarantee the use of suitable and authentic wording, meaningful content for assessing LLMs' capabilities in the French… See the full description on the dataset page: https://huggingface.co/datasets/bofenghuang/mt-bench-french."},
	{"name":"ru-alpaca-eval","keyword":"evaluation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/t-tech/ru-alpaca-eval","creator_name":"T-Tech","creator_url":"https://huggingface.co/t-tech","description":"\\n\\t\\n\\t\\t\\n\\t\\tru-alpaca-eval\\n\\t\\n\\nru-alpaca-eval is translated version of alpaca_eval. The translation of the original dataset was done manually. In addition, content of each task in dataset was reviewed, the correctness of the task statement and compliance with moral and ethical standards were assessed. Thus, this dataset allows you to evaluate the abilities of language models to support the Russian language. Baseline responses updated with GPT-4o model and also reviewed.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview of the… See the full description on the dataset page: https://huggingface.co/datasets/t-tech/ru-alpaca-eval."},
	{"name":"infoquest","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bryanlincoln/infoquest","creator_name":"Bryan Lincoln","creator_url":"https://huggingface.co/bryanlincoln","description":"\\n\\t\\n\\t\\t\\n\\t\\tInfoQuest Dataset\\n\\t\\n\\nThis dataset accompanies the paper \\\"InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of several files:\\n\\nsample_personas.csv: Contains 1500 sampled personas from the 200k personas in proj-persona/PersonaHub. The idx column corresponds to the original index in the source dataset.\\n\\nseed_messages.jsonl: Contains seed messages generated by combining personas from… See the full description on the dataset page: https://huggingface.co/datasets/bryanlincoln/infoquest."},
	{"name":"phantom-wiki-v1","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kilian-group/phantom-wiki-v1","creator_name":"Kilian's Group","creator_url":"https://huggingface.co/kilian-group","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for PhantomWiki\\n\\t\\n\\nThis repository contains pre-generated instances of the PhantomWiki dataset, created using the phantom-wiki Python package.  PhantomWiki is a framework for evaluating LLMs, particularly RAG and agentic workflows, designed to be resistant to memorization. Unlike fixed datasets, PhantomWiki generates unique instances on demand, ensuring novelty and preventing data leakage.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nPhantomWiki generates a… See the full description on the dataset page: https://huggingface.co/datasets/kilian-group/phantom-wiki-v1."},
	{"name":"RobloxQA-v1.0","keyword":"evaluation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/boatbomber/RobloxQA-v1.0","creator_name":"Zack Ovits","creator_url":"https://huggingface.co/boatbomber","description":"\\n\\t\\n\\t\\t\\n\\t\\tRobloxQA V1.0\\n\\t\\n\\nThis dataset contains questions and answers relating to Roblox. This is used to evaluate models' knowledge and understanding of Roblox game development.\\nWe take the Roblox documentation (CC-BY-4.0), group documents by topics, and then have Deepseek R1 (MIT) generate questions and answers based on the documents. For more information, see the open source dataset generator below.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Format\\n\\t\\n\\nAll data is stored in .parquet files. The prompt, choices, and… See the full description on the dataset page: https://huggingface.co/datasets/boatbomber/RobloxQA-v1.0."}
]
;
