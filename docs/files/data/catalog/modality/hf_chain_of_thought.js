const data_for_modality_chain_of_thought = 
[
	{"name":"CoT_reformatted","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jtatman/CoT_reformatted","creator_name":"James","creator_url":"https://huggingface.co/jtatman","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"CoT_reformatted\\\"\\n\\t\\n\\nThis dataset is reformatted from: QingyiSi/Alpaca-CoT\\nAll credit goes there. Thanks to QingyiSi for the work in consolidating many diverse sources for comparison and cross-file analysis.\\nThere were some issues loading files from that dataset for a testing project. \\nI extracted the following data files for this subset:\\n\\nalpaca_data_cleaned\\nCoT_data\\nfirefly       \\ninstruct\\nalpaca_gpt4_data\\ndolly \\nGPTeacher\\nthoughtsource\\nfinance_en\\ninstinwild_en\\n\\n"},
	{"name":"reflection-small-sonnet","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/reflection-small-sonnet","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVerified reasoning examples\\n\\t\\n\\n"},
	{"name":"pisc-tr","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berhaan/pisc-tr","creator_name":"Berhan T√ºrk√º Ay","creator_url":"https://huggingface.co/berhaan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for CoT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nRepository: LLaVA-CoT GitHub Repository\\nPaper: LLaVA-CoT on arXiv\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\ncat image.zip.part-* > image.zip #not uploaded yet\\nunzip image.zip\\n\\nThe train.jsonl file contains the question-answering data and is structured in the following format:\\n{\\n  \\\"id\\\": \\\"example_id\\\",\\n  \\\"image\\\": \\\"example_image_path\\\",\\n  \\\"conversations\\\": [\\n    {\\\"from\\\": \\\"human\\\", \\\"value\\\": \\\"L√ºtfen resimdeki kƒ±rmƒ±zƒ± metal nesnelerin sayƒ±sƒ±nƒ±‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/berhaan/pisc-tr."},
	{"name":"Medprompt-MedMCQA-CoT","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Medprompt-MedMCQA-CoT","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card: Medprompt-MedMCQA-CoT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMedprompt-MedMCQA-CoT is a retrieval-augmented database designed to enhance contextual reasoning in multiple-choice medical question answering (MCQA). The dataset follows a Chain-of-Thought (CoT) reasoning format, where step-by-step justifications are provided for each question before selecting the correct answer.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nMedprompt-MedMCQA-CoT is a large-scale dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Medprompt-MedMCQA-CoT."},
	{"name":"clevr-tr","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berhaan/clevr-tr","creator_name":"Berhan T√ºrk√º Ay","creator_url":"https://huggingface.co/berhaan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for CoT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nRepository: LLaVA-CoT GitHub Repository\\nPaper: LLaVA-CoT on arXiv\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nunzip image.zip\\n\\nThe train.jsonl file contains the question-answering data and is structured in the following format:\\n{\\n  \\\"id\\\": \\\"example_id\\\",\\n  \\\"image\\\": \\\"example_image_path\\\",\\n  \\\"conversations\\\": [\\n    {\\\"from\\\": \\\"human\\\", \\\"value\\\": \\\"L√ºtfen resimdeki kƒ±rmƒ±zƒ± metal nesnelerin sayƒ±sƒ±nƒ± belirtin.\\\"},\\n    {\\\"from\\\": \\\"gpt\\\", \\\"value\\\": \\\"Resimde‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/berhaan/clevr-tr."},
	{"name":"ru-chain-of-thought-sharegpt","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/ru-chain-of-thought-sharegpt","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"–ü–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω–∞—è –ø—Ä–∏ –ø–æ–º–æ—â–∏ utrobinmv/t5_translate_en_ru_zh_small_1024 –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –≤–µ—Ä—Å–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ isaiahbjork/chain-of-thought-sharegpt.\\n"},
	{"name":"sharegpt_cot_dataset","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AiCloser/sharegpt_cot_dataset","creator_name":"Ai Closer","creator_url":"https://huggingface.co/AiCloser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tA data set inspired by the \\\"Reflection\\\" method, three-dimensional thinking and cot\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the ShareGPT format.\\n\\t\\n\\nThe data set was generated using multiple llm synthesis.\\n"},
	{"name":"chain-of-diagnosis","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/chain-of-diagnosis","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for chain-of-diagnosis\\n\\t\\n\\n\\n\\nCurated version of the Chain-of-Diagnosis dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nChain-of-Diagnosis is a database used to improve interpretability in medical diagnostics for LLMs.\\nWe curated and formatted the Chain-of-Diagnosis dataset into Alpaca format. This dataset is included in the training set of the Aloe-Beta model.\\n\\nCurated by: Jordi Bayarri Planas\\nLanguage(s) (NLP): English\\nLicense: Apache 2.0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/chain-of-diagnosis."},
	{"name":"Alpaca-CoT","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QingyiSi/Alpaca-CoT","creator_name":"Qingyi Si","creator_url":"https://huggingface.co/QingyiSi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruction-Finetuning Dataset Collection (Alpaca-CoT)\\n\\t\\n\\nThis repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model.\\nWe also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT.  \\nIf you think this dataset collection is helpful to you, please like‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT."},
	{"name":"M3CoT","keyword":"chain-of-thought","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","description":"\\n ü¶Ñ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\\n\\n\\n\\n\\n\\n      \\n    [ArXiv] | [ü§óHuggingFace] | [Website]\\n    \\n    \\n\\n\\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüî•News\\n\\t\\n\\n\\nüéñÔ∏è Our work is accepted by ACL2024.\\n\\nüî• We have release benchmark on [ü§óHuggingFace].\\n\\nüî• The paper is also available on [ArXiv].\\n\\nüîÆ Interactive benchmark website & more exploration are available on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT."},
	{"name":"MAmmoTH-VL-Instruct-12M","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M","creator_name":"MAmmoTH-VL","creator_url":"https://huggingface.co/MAmmoTH-VL","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMAmmoTH-VL-Instruct-12M\\n\\t\\n\\nüè† Homepage | ü§ñ MAmmoTH-VL-8B | üíª Code | üìÑ Arxiv | üìï PDF | üñ•Ô∏è Demo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nOur simple yet scalable visual instruction data rewriting pipeline consists of three steps: manual data source collection, rewriting using MLLMs/LLMs, and filtering via the same MLLM as a judge. Examples below illustrate transformations in math and science categories, showcasing detailed, step-by-step responses.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe data distribution of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M."},
	{"name":"Dataset_of_Russian_thinking","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","description":"Ru\\nRTD  \\n–û–ø–∏—Å–∞–Ω–∏–µ:Russian Thinking Dataset ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –î–∞—Ç–∞—Å–µ—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞, –∞–Ω–∞–ª–∏–∑–æ–º –¥–∏–∞–ª–æ–≥–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.  \\n\\n\\t\\n\\t\\t\\n\\t\\t–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\\n\\t\\n\\n\\n–°–ø–ª–∏—Ç: train  \\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: 147.046\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t–¶–µ–ª–∏:\\n\\t\\n\\n\\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.  \\n–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking."},
	{"name":"CoT-XLang","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Egor-AI/CoT-XLang","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","description":"RU:CoT-XLang ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø–æ—à–∞–≥–æ–≤—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ (Chain-of-Thought, CoT) –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, —Ä—É—Å—Å–∫–∏–π, —è–ø–æ–Ω—Å–∫–∏–π –∏ –¥—Ä—É–≥–∏–µ. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π —Ä–µ—à–µ–Ω–∏–π —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤. –î–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç –æ–∫–æ–ª–æ 2,419,912 –ø—Ä–∏–º–µ—Ä–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.\\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/CoT-XLang."},
	{"name":"Dataset_of_Russian_thinking","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","description":"Ru\\nRTD  \\n–û–ø–∏—Å–∞–Ω–∏–µ:Russian Thinking Dataset ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –î–∞—Ç–∞—Å–µ—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞, –∞–Ω–∞–ª–∏–∑–æ–º –¥–∏–∞–ª–æ–≥–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.  \\n\\n\\t\\n\\t\\t\\n\\t\\t–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\\n\\t\\n\\n\\n–°–ø–ª–∏—Ç: train  \\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: 147.046\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t–¶–µ–ª–∏:\\n\\t\\n\\n\\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.  \\n–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking."},
	{"name":"Magpie-COT","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/project-r/Magpie-COT","creator_name":"Project R","creator_url":"https://huggingface.co/project-r","description":"\\n\\t\\n\\t\\t\\n\\t\\tMagpie-COT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nCombined Chain-of-Thought dataset containing three sources:\\n\\nMagpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B\\nMagpie-Reasoning-V2-250K-CoT-QwQ\\nMagpie-Reasoning-V2-250K-CoT-Skywork-O1-Llama-3.1-8B\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tKey Enhancements:\\n\\t\\n\\n\\nAdded model source tracking column\\nProcessed Deepseek responses to extract  tag content\\nUnified format across multiple CoT datasets\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tProcessing Steps:\\n\\t\\n\\n\\nKept 'instruction' and 'response' columns\\nAdded‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/project-r/Magpie-COT."},
	{"name":"mauxi-COT-Persian","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxi-COT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\tüß† mauxi-COT-Persian Dataset\\n\\t\\n\\n\\nExploring Persian Chain-of-Thought Reasoning with DeepSeek-R1, brought to you by Mauxi AI Platform\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüåü Overview\\n\\t\\n\\nmauxi-COT-Persian is a community-driven dataset that explores the capabilities of advanced language models in generating Persian Chain-of-Thought (CoT) reasoning. The dataset is actively growing with new high-quality, human-validated entries being added regularly. I am personally working on expanding this dataset with rigorously‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-COT-Persian."},
	{"name":"Synthetic_CoT_dataset_RU","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DataSynGen/Synthetic_CoT_dataset_RU","creator_name":"DataGen","creator_url":"https://huggingface.co/DataSynGen","description":"–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π —Ä—É—Å—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è Chain-of-Thought (CoT) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–∞–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π –≤ –ø–æ—à–∞–≥–æ–≤–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏. –ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤–∫–ª—é—á–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–π –∑–∞–ø—Ä–æ—Å, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. –¶–µ–ª—å –¥–∞—Ç–∞—Å–µ—Ç–∞ ‚Äì —É–ª—É—á—à–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –≤–æ–ø—Ä–æ—Å—ã –ø–æ –æ–±—â–µ–π —ç—Ä—É–¥–∏—Ü–∏–∏, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏. –î–∞–Ω–Ω—ã–µ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DataSynGen/Synthetic_CoT_dataset_RU."},
	{"name":"Magpie-COT","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/project-r/Magpie-COT","creator_name":"Project R","creator_url":"https://huggingface.co/project-r","description":"\\n\\t\\n\\t\\t\\n\\t\\tMagpie-COT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nCombined Chain-of-Thought dataset containing three sources:\\n\\nMagpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B\\nMagpie-Reasoning-V2-250K-CoT-QwQ\\nMagpie-Reasoning-V2-250K-CoT-Skywork-O1-Llama-3.1-8B\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tKey Enhancements:\\n\\t\\n\\n\\nAdded model source tracking column\\nProcessed Deepseek responses to extract  tag content\\nUnified format across multiple CoT datasets\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tProcessing Steps:\\n\\t\\n\\n\\nKept 'instruction' and 'response' columns\\nAdded‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/project-r/Magpie-COT."},
	{"name":"CoMT","keyword":"chain-of-thought","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/czh-up/CoMT","creator_name":"ZihuiCheng","creator_url":"https://huggingface.co/czh-up","description":"\\n   CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models\\n\\n\\n\\n      \\n    | [ArXiv] | [ü§óHuggingFace] |\\n    \\n    \\n\\n\\n\\n\\n\\n\\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüî•News\\n\\t\\n\\n\\nüéñÔ∏è Our work is accepted by AAAI 2025 !\\nüî• We have release benchmark on [ü§óHuggingFace].\\nüî• The paper is also available on [ArXiv].\\n\\n\\n\\t\\n\\t\\n\\t\\n\\t\\tüí° Motivation\\n\\t\\n\\nLarge Vision-Language Models (LVLMs) have recently demonstrated amazing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/czh-up/CoMT."},
	{"name":"OpenThoughts-TR-18k","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/OpenThoughts-TR-18k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\\n\\t\\n\\t\\t\\n\\t\\tOpenThoughts-TR-18k: Turkish Synthetic Reasoning Dataset\\n\\t\\n\\nOpenThoughts-TR-18k is a Turkish translation of a subset of the original Open-Thoughts-114k dataset. It contains ~18k high-quality synthetic reasoning examples covering mathematics, science, coding problems, and puzzles, all translated into Turkish. This dataset is designed to support reasoning task fine tuning for Turkish language models.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n~18k translated reasoning examples\\nCovers multiple domains:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/selimc/OpenThoughts-TR-18k."},
	{"name":"Thinking-multilingual-big-10k-sft","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/Thinking-multilingual-big-10k-sft","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"\\nA dataset based off of openo1 math, 500 examples translated to 23 different languages. filtered out un-translated examples.\\nenjoy üëç\\n"},
	{"name":"OpenHumanreasoning-multilingual-2.2k","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/OpenHumanreasoning-multilingual-2.2k","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"Based off of Pinkstackorg/humanreasoning-testing-en, it is a human-generated dataset where a real person had to create most of the reasoning and the final output. If there are any mistakes please let us know.\\nWe offer this dataset at an apache-2.0 license to make it useful for everybody.\\nnote: translations are not human generated.\\n"},
	{"name":"Camildae","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewstaR/Camildae","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tmerge of some datasets from Alpaca Cot\\n\\t\\n\\n"},
	{"name":"gsm8k_self_correct","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/euclaise/gsm8k_self_correct","creator_name":"Jade","creator_url":"https://huggingface.co/euclaise","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"gsm8k_self_correct\\\"\\n\\t\\n\\nMore Information needed\\n"},
	{"name":"MATH-500-Overall","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fluently-sets/MATH-500-Overall","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMATH-500-Overall\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbout the dataset\\n\\t\\n\\nThis dataset of only 500 examples combines mathematics, physics and logic in English with reasoning and step-by-step problem solving, the dataset was created synthetically, CoT of Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBrief information\\n\\t\\n\\n\\nNumber of rows: 500\\nType of dataset files: parquet\\nType of dataset: text, alpaca with system prompts\\nLanguage: English\\nLicense: MIT\\n\\nStructure:\\nmath¬Ø¬Ø¬Ø¬Ø¬Ø‚åâ\\n   school-level‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/MATH-500-Overall."},
	{"name":"synmath-1-dsv3-87k","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Gusarich/synmath-1-dsv3-87k","creator_name":"Daniil Sedov","creator_url":"https://huggingface.co/Gusarich","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tsynmath-1-dsv3-87k\\n\\t\\n\\nsynmath-1-dsv3-87k is a dataset consisting of 86,700 math problems and their corresponding solutions, formatted in a chain-of-thought manner. The problems span 867 distinct mathematical domains, providing diverse and comprehensive coverage for fine-tuning smaller models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nsynmath-1-dsv3-87k contains synthetically generated math problems and step-by-step solutions designed to enhance mathematical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gusarich/synmath-1-dsv3-87k."},
	{"name":"Sky-T1_data_steps","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shakedzy/Sky-T1_data_steps","creator_name":"Shaked","creator_url":"https://huggingface.co/shakedzy","description":"\\n\\t\\n\\t\\t\\n\\t\\tSky-T1_data_steps\\n\\t\\n\\nThis dataset contains 182 samples taken from NovaSky-AI/Sky-T1_data_17k \\ndataset and broken down to thinking steps. This dataset was used to train shakedzy/Sky-T1-32B-Steps \\nLoRA adapter for step-by-step thinking.\\nBreaking down the thought process to steps was done using Ollama's quantized version of Llama-3.2-1B.\\nSee step_prompt file for the exact prompt used.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Columns\\n\\t\\n\\n\\nid (int): row index of the sample in the original dataset (starts at 0)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shakedzy/Sky-T1_data_steps."},
	{"name":"cortex-1-market-analysis","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Jarrodbarnes/cortex-1-market-analysis","creator_name":"Jarrod Barnes","creator_url":"https://huggingface.co/Jarrodbarnes","description":"\\n\\t\\n\\t\\t\\n\\t\\tNEAR Cortex-1 Market Analysis Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains blockchain market analyses combining historical and real-time data with chain-of-thought reasoning. The dataset includes examples from Ethereum, Bitcoin, and NEAR chains, demonstrating high-quality market analysis with explicit calculations, numerical citations, and actionable insights.\\nThe dataset has been enhanced with examples generated by GPT-4o and Claude 3.7 Sonnet, providing diverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jarrodbarnes/cortex-1-market-analysis."},
	{"name":"Aloe-Beta-Medical-Collection","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Aloe-Beta-Medical-Collection\\n\\t\\n\\n\\n\\nCollection of curated datasets used to fine-tune Aloe-Beta.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nWe curated data from many publicly available medical instruction tuning data sources (QA format). Most data samples correspond to single-turn QA pairs, while a small proportion contain multi-turn. All data sources are publicly available for commercial purposes.\\nWe implemented a rigorous data preprocessing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection."},
	{"name":"ultramedical","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/ultramedical","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for HAPI-BSC/ultramedical\\n\\t\\n\\n\\n\\nCurated version of the UltraMedical dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nThe UltraMedical Collections is a large-scale, high-quality dataset of biomedical instructions. We collected and curated the following sets:\\n\\nTextBookQA\\nMedical-Instruction-120k\\nWikiInstruct\\n\\nThis dataset is included in the Aloe-Beta model training set.\\n\\nCurated by: Jordi Bayarri Planas\\nLanguage(s) (NLP): English\\nLicense:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/ultramedical."},
	{"name":"MedS-Ins","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/MedS-Ins","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Aloe-Beta-Medical-Collection\\n\\t\\n\\n\\n\\nCollection of curated data from the MedS-Ins dataset. Used to train Aloe-Beta model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nThis is the curated version of the MedS-Ins dataset included in the training set of the Aloe-Beta models. \\nFirst, we selected 75 out of the 122 existing tasks, excluding the tasks that were already in the training set, and the datasets with non-commercial licenses. Then, we passed the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedS-Ins."},
	{"name":"step_prm","keyword":"cot","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xiaodongguaAIGC/step_prm","creator_name":"xiaodongguaAIGC","creator_url":"https://huggingface.co/xiaodongguaAIGC","description":"\\n\\t\\n\\t\\t\\nÊï∞ÊçÆÈõÜÂêçÁß∞\\nÊòØÂê¶Êúâstep\\nÂèØÁî®‰∫éPRMËÆ≠ÁªÉ\\nÊ†áÁ≠æÂΩ¢Âºè\\nTitle\\nÂ§áÊ≥®\\n\\n\\n\\t\\t\\nGSM8K\\n‚úÖ\\n‚ùå\\nÁ≠îÊ°à\\nTraining Verifiers to Solve Math Word Problems\\n\\n\\n\\nMATH\\n‚ùå\\n‚ùå\\nÁ≠îÊ°à\\nMeasuring Mathematical Problem Solving With the MATH Dataset\\nNon-Step\\n\\n\\nPRM800K\\n‚úÖ\\n‚úÖ\\nÊ≠£Á°ÆÁ±ªÂà´\\nLet's Verify Step by Step\\nprompt deduplication\\n\\n\\nMath-Shepherd\\n‚úÖ\\n‚úÖ\\nÊ≠£Á°ÆÁ±ªÂà´\\nMath-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\\nNot used\\n\\n\\nProcessBench\\n‚úÖ\\n‚úÖ\\nÈ¶ñ‰∏™ÈîôËØØÊ≠•È™§\\nProcessBench: Identifying Process Errors in Mathematical Reasoning\\nonly label -1\\n\\n\\n\\t\\n\\n"},
	{"name":"Medprompt-MedQA-CoT","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Medprompt-MedQA-CoT","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card: Medprompt-MedQA-CoT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMedprompt-MedQA-CoT is a retrieval-augmented database created to enhance contextual reasoning in multiple-choice medical question answering (MCQA). The dataset follows a Chain-of-Thought (CoT) reasoning format, providing step-by-step justifications for each question before identifying the correct answer.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nMedprompt-MedQA-CoT is designed to support retrieval-augmented‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Medprompt-MedQA-CoT."},
	{"name":"GammaCorpus-CoT-Math-170k","keyword":"chain-of-thought","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: CoT Math 170k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nNumber of Rows: 169,527\\nFormat: JSONL\\nLanguage: English\\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k."},
	{"name":"GammaCorpus-CoT-Math-170k","keyword":"cot","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: CoT Math 170k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nNumber of Rows: 169,527\\nFormat: JSONL\\nLanguage: English\\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k."},
	{"name":"truthful_qa_CoT","keyword":"chain-of-thought","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/0fg/truthful_qa_CoT","creator_name":"Bryce","creator_url":"https://huggingface.co/0fg","description":"Question-Answer dataset generated using CAMEL CoTDataGenerator and GPT-4o Mini"}
]
;
