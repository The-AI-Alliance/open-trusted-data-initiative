const data_for_modality_reinforcement_learning = 
[
	{"name":"tactical-military-reasoning-v.1.0","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZennyKenny/tactical-military-reasoning-v.1.0","creator_name":"Kenneth Hamilton","creator_url":"https://huggingface.co/ZennyKenny","description":"\n\t\n\t\t\n\t\tTactical Military Reasoning Dataset v1.0\n\t\n\n\nA curated collection of 150 rich tactical military scenarios with LLM-generated reasoning strategies for both attacking and defending forces.\n\n\n \n\n\n\n\n\t\n\t\t\n\t\tüìù Preface\n\t\n\nOncologists do not study cancer because they love cancer and wish for it to occur more frequently. They study cancer to better understand its causes, progression, and consequences in order to therefore eradicate it from the earth more effectively. A distaste for something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZennyKenny/tactical-military-reasoning-v.1.0.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"NeoRL2","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/polixirai/NeoRL2","creator_name":"Polixir","creator_url":"https://huggingface.co/polixirai","description":"\n\t\n\t\t\n\t\tDataset Card for NeoRL‚Äë2: Near Real‚ÄëWorld Benchmarks for Offline Reinforcement Learning\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nNeoRL-2 is a collection of seven near‚Äìreal-world offline-RL datasets plus their evaluation simulators. This repo we provide the offline-RL dataset, while the simulators are in https://github.com/polixir/NeoRL2.\nEach task injects one or more realistic challenges‚Äîdelays, exogenous disturbances, global safety constraints, traditional rule-based data, and/or severe data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/polixirai/NeoRL2.","first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"my-distiset-620d36eb","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kritsanan/my-distiset-620d36eb","creator_name":"namoang","creator_url":"https://huggingface.co/kritsanan","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for my-distiset-620d36eb\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/kritsanan/my-distiset-620d36eb/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kritsanan/my-distiset-620d36eb.","first_N":5,"first_N_keywords":["text-classification","Thai","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"GoDatas","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Karesis/GoDatas","creator_name":"Êù®‰∫¶Èîã","creator_url":"https://huggingface.co/Karesis","description":"\n\t\n\t\t\n\t\tDataset Card for Go Game Dataset for Neural Network Training\n\t\n\nThis is a high-quality dataset designed for Go neural network training, containing board positions extracted from curated SGF game records. The dataset is divided into three strength categories: Standard, Strong, and Elite, with approximately 1,000 samples per category.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Go board positions and corresponding moves extracted from high-quality SGF‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/GoDatas.","first_N":5,"first_N_keywords":["reinforcement-learning","feature-extraction","Chinese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"GoDatas","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Karesis/GoDatas","creator_name":"Êù®‰∫¶Èîã","creator_url":"https://huggingface.co/Karesis","description":"\n\t\n\t\t\n\t\tDataset Card for Go Game Dataset for Neural Network Training\n\t\n\nThis is a high-quality dataset designed for Go neural network training, containing board positions extracted from curated SGF game records. The dataset is divided into three strength categories: Standard, Strong, and Elite, with approximately 1,000 samples per category.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Go board positions and corresponding moves extracted from high-quality SGF‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/GoDatas.","first_N":5,"first_N_keywords":["reinforcement-learning","feature-extraction","Chinese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"my-distiset-38ebca4b","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kritsanan/my-distiset-38ebca4b","creator_name":"namoang","creator_url":"https://huggingface.co/kritsanan","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for my-distiset-38ebca4b\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/kritsanan/my-distiset-38ebca4b/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kritsanan/my-distiset-38ebca4b.","first_N":5,"first_N_keywords":["text-classification","Thai","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"en-fr-debut-kit","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/en-fr-debut-kit","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tD√©but Kit\n\t\n\n\n  English\nA dataset for training English-French bilingual chatbots\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nD√©but Kit is a comprehensive dataset designed to facilitate the development of English-French bilingual chatbots. It covers three crucial stages of model development:\n\nPretraining\nSupervised Fine-Tuning (SFT)\nDirect Preference Optimization (DPO)\n\nEach stage features a balanced mix of English and French content, ensuring robust bilingual capabilities.\n\n\t\n\t\t\n\t\tDataset Details‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/en-fr-debut-kit.","first_N":5,"first_N_keywords":["text-generation","English","French","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OpenHumnoidActuatedFaceData","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/infosys/OpenHumnoidActuatedFaceData","creator_name":"Infosys Limited","creator_url":"https://huggingface.co/infosys","description":"\n\t\n\t\t\n\t\tü§ñ Open¬†Humanoid¬†Actuated¬†Face¬†Dataset\n\t\n\n\n  \n\n\n\n\t\n\t\t\n\t\tDataset¬†Summary\n\t\n\nThe Open‚ÄØHumanoid‚ÄØActuated‚ÄØFace Dataset is designed for researchers working onfacial‚Äëactuation control, robotics, reinforcement learning, and human‚Äìcomputer interaction.\n\nOrigin ‚Äì collected during a reinforcement‚Äëlearning (RL) training loop whose objective was to reproduce human facial expressions.\nPlatform ‚Äì a modified i2Head InMoov humanoid head with a silicone skin.\nControl ‚Äì 16 actuators driving facial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/infosys/OpenHumnoidActuatedFaceData.","first_N":5,"first_N_keywords":["robotics","reinforcement-learning","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"jondurbin_gutenberg-dpo-v0.1","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Triangle104/jondurbin_gutenberg-dpo-v0.1","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","description":"\n\t\n\t\t\n\t\tGutenberg DPO\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is a dataset meant to enhance novel writing capabilities of LLMs, by using public domain books from Project Gutenberg\n\n\t\n\t\t\n\t\tProcess\n\t\n\nFirst, the each book is parsed, split into chapters, cleaned up from the original format (remove superfluous newlines, illustration tags, etc.).\nOnce we have chapters, an LLM is prompted with each chapter to create a synthetic prompt that would result in that chapter being written.\nEach chapter has a summary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Triangle104/jondurbin_gutenberg-dpo-v0.1.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ConnectFour","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TonyCWang/ConnectFour","creator_name":"Tony Congqian Wang","creator_url":"https://huggingface.co/TonyCWang","description":"Connect 4 Solver Outputs\nAbout 100M + 60M observations and targets generated from selfplay with a solver [https://github.com/PascalPons/connect4] with temperature.\nObservations from different depths are roughly uniformly distributed, altough later positions are reached less frequently.\nAs a consequence early positions are duplicated and there is a small overlap between the train and test split (less than 3%).\nObservations are of shape (2,6,7) with binary (0 or 255) data.\nThe first channel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TonyCWang/ConnectFour.","first_N":5,"first_N_keywords":["reinforcement-learning","mit","100M - 1B","parquet","Time-series"],"keywords_longer_than_N":true},
	{"name":"GymnasiumRecording__SuperMarioBros_Nes","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tsilva/GymnasiumRecording__SuperMarioBros_Nes","creator_name":"Tiago Silva","creator_url":"https://huggingface.co/tsilva","description":"\n\t\n\t\t\n\t\tSuperMarioBros-Nes Gameplay Dataset\n\t\n\nThis dataset contains 244 frames recorded from the Gymnasium environment SuperMarioBros-Nes across 1 episodes.\nEnvironment ID: SuperMarioBros-Nes\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains the following columns:\n\nepisode_id: Value(dtype='int64', id=None)\ntimestamp: Value(dtype='float64', id=None)\nimage: Image(mode=None, decode=True, id=None)\nstep: Value(dtype='int64', id=None)\naction: Sequence(feature=Value(dtype='int64', id=None)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tsilva/GymnasiumRecording__SuperMarioBros_Nes.","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Gomoku","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Karesis/Gomoku","creator_name":"Êù®‰∫¶Èîã","creator_url":"https://huggingface.co/Karesis","description":"\n\t\n\t\t\n\t\tDatacard: Gomoku (Five in a Row) AI Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Gomoku (Five in a Row) AI Dataset contains board states and moves from 875 self-played Gomoku games, totaling 26,378 training examples. The data was generated using WinePy, a Python implementation of the Wine Gomoku AI engine. Each example consists of a board state and the corresponding optimal next move as determined by an alpha-beta search algorithm with pattern recognition.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/Gomoku.","first_N":5,"first_N_keywords":["reinforcement-learning","feature-extraction","mit","10K<n<100K","doi:10.57967/hf/4816"],"keywords_longer_than_N":true},
	{"name":"Gomoku","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Karesis/Gomoku","creator_name":"Êù®‰∫¶Èîã","creator_url":"https://huggingface.co/Karesis","description":"\n\t\n\t\t\n\t\tDatacard: Gomoku (Five in a Row) AI Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Gomoku (Five in a Row) AI Dataset contains board states and moves from 875 self-played Gomoku games, totaling 26,378 training examples. The data was generated using WinePy, a Python implementation of the Wine Gomoku AI engine. Each example consists of a board state and the corresponding optimal next move as determined by an alpha-beta search algorithm with pattern recognition.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/Gomoku.","first_N":5,"first_N_keywords":["reinforcement-learning","feature-extraction","mit","10K<n<100K","doi:10.57967/hf/4816"],"keywords_longer_than_N":true},
	{"name":"Reason-RFT-CoT-Dataset","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tanhuajie2001/Reason-RFT-CoT-Dataset","creator_name":"tanhuajie2001","creator_url":"https://huggingface.co/tanhuajie2001","description":"\n\n\n\n\n\t\n\t\t\n\t\tü§ó Reason-RFT CoT Dateset\n\t\n\nThe full dataset used in our project \"Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning\".\n\n    ¬†¬†‚≠êÔ∏è Project¬†¬† ‚îÇ ¬†¬†üåé Github¬†¬† ‚îÇ ¬†¬†üî• Models¬†¬† ‚îÇ ¬†¬†üìë ArXiv¬†¬† ‚îÇ ¬†¬†üí¨ WeChat\n\n\n\n¬†¬†ü§ñ RoboBrain: Aim to Explore ReasonRFT Paradigm to Enhance RoboBrain's Embodied Reasoning Capabilities.\n\n\n\n\t\t\n\t\t‚ô£Ô∏è Quick Start\n\t\n\nPlease refer to Dataset Preparation\n\n\t\n\t\t\n\t\tüî• Overview\n\t\n\nVisual reasoning abilities play a crucial role in understanding complex multimodal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tanhuajie2001/Reason-RFT-CoT-Dataset.","first_N":5,"first_N_keywords":["reinforcement-learning","visual-question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Minecraft-Skill-Data","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dami2106/Minecraft-Skill-Data","creator_name":"Dami","creator_url":"https://huggingface.co/dami2106","description":"\n\t\n\t\t\n\t\tDataset Card for Minecraft Expert Skill Data\n\t\n\nThis dataset consists of expert demonstration trajectories from a Minecraft simulation environment. Each trajectory includes ground-truth skill segmentation annotations, enabling research into action segmentation, skill discovery, imitation learning, and reinforcement learning with temporally-structured data.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Minecraft Skill Segmentation Dataset contains gameplay trajectories‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dami2106/Minecraft-Skill-Data.","first_N":5,"first_N_keywords":["reinforcement-learning","other","code","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"warren-buffett-letters-qna-r1-enhanced-1998-2024","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/eagle0504/warren-buffett-letters-qna-r1-enhanced-1998-2024","creator_name":"Yiqiao Yin","creator_url":"https://huggingface.co/eagle0504","description":"\n\t\n\t\t\n\t\tüß† Warren Buffett Letters Q&A Dataset Pipeline\n\t\n\nThis project extracts question-answer-reasoning triplets from Warren Buffett's annual shareholder letters using OCR and LLMs. The pipeline is modular and divided into the following stages:\nYou can clone the repo here.\n\n\n\t\n\t\t\n\t\t1. Setup\n\t\n\nCreate a virtual environment and install dependencies using requirements.txt.\n\n\n\t\n\t\t\n\t\t2. Data Curation (curate_data.py)\n\t\n\n\nLoad a list of PDF URLs from the Berkshire Hathaway website.\nUse Mistral's‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eagle0504/warren-buffett-letters-qna-r1-enhanced-1998-2024.","first_N":5,"first_N_keywords":["question-answering","reinforcement-learning","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"filtered-high-quality-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/helloTR/filtered-high-quality-dpo","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","description":"\n\t\n\t\t\n\t\tFiltered High-Quality Instruction-Output Dataset\n\t\n\nThis dataset contains high-quality (score = 5) instruction-output pairs generated via a reverse instruction generation pipeline using a fine-tuned backward model and evaluated by LLaMA-2-7B-Chat.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\ninstruction: The generated prompt.\noutput: The original response.\nscore: Quality score assigned ( score = 5 retained).\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-contrast-sample","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/helloTR/dpo-contrast-sample","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","description":"\n\t\n\t\t\n\t\tDPO Contrast Sample\n\t\n\nThis dataset provides a direct comparison between high-quality and low-quality instruction-output pairs evaluated by a LLaMA-2-7B-Chat model in a DPO-style workflow.\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\ninstruction: The generated prompt (x).\noutput: The associated output (y).\nscore: Quality rating assigned by LLaMA2 (1 = high, 5 = low).\n\n\n\t\n\t\t\n\t\tSamples\n\t\n\n5 examples with score = 1 (excellent), and 5 with score = 5 (poor).\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"tron-dataset-v.1.0","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZennyKenny/tron-dataset-v.1.0","creator_name":"Kenneth Hamilton","creator_url":"https://huggingface.co/ZennyKenny","description":"\n\t\n\t\t\n\t\tMetareasoning: To Reason or Not, Zero-Shot Classification for Reasoning Tasks\n\t\n\nThe To Reason or Not (TRON) Dataset represents a tangible advancement in the use of reasoning models by way of an architectural paradigm that we will refer to as Metareasoning. Metareasoning is the practice by which we ask a reasoning model to reason whether or not further reasoning is required to respond to a given prompt.\n\n\t\n\t\t\n\t\tPurpose and Scope\n\t\n\nThe TRON Dataset is designed to train lightweight‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZennyKenny/tron-dataset-v.1.0.","first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"anthropic-hh-rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/giovannioliveira/anthropic-hh-rlhf","creator_name":"Giovanni Oliveira","creator_url":"https://huggingface.co/giovannioliveira","description":"\n\t\n\t\t\n\t\tDataset Card for HH-RLHF\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to two different kinds of data:\n\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/giovannioliveira/anthropic-hh-rlhf.","first_N":5,"first_N_keywords":["mit","arxiv:2204.05862","üá∫üá∏ Region: US","human-feedback"],"keywords_longer_than_N":false},
	{"name":"synthetic_vc_financial_decisions_reasoning_dataset","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZennyKenny/synthetic_vc_financial_decisions_reasoning_dataset","creator_name":"Kenneth Hamilton","creator_url":"https://huggingface.co/ZennyKenny","description":"\n \n\n\nBest Curator Use Case in the Reasoning Datasets Competition: https://www.linkedin.com/feed/update/urn:li:activity:7330998995990781952/\n\n\t\n\t\t\n\t\n\t\n\t\tSynthetic VC Financial Decisions Reasoning Dataset\n\t\n\n\n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe Synthetic VC Financial Decisions Reasoning Dataset is a large-scale collection designed to train, evaluate, and fine-tune language models on subjective, abstract financial reasoning tasks.  It simulates venture capital (VC) workflows by capturing multiple‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZennyKenny/synthetic_vc_financial_decisions_reasoning_dataset.","first_N":5,"first_N_keywords":["reinforcement-learning","question-answering","summarization","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"collabllm-20q","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aditijb/collabllm-20q","creator_name":"Aditi","creator_url":"https://huggingface.co/aditijb","description":"aditijb/collabllm-20q dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"multilevel-legal-reasoning","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ciol-research/multilevel-legal-reasoning","creator_name":"Computational Intelligence and Operations Laboratory (CIOL)","creator_url":"https://huggingface.co/ciol-research","description":"\n\t\n\t\t\n\t\tLegal Reasoning Dataset with Multilevel Human and Model-Annotated Explanations\n\t\n\n\nPrepared by Mst Rafia Islam, Umong Sain, Azmine Toushik Wasi\nPrepared as a part of Reasoning Datasets Competition by Bespoke Labs, Hugging Face, and Together.ai.\n\n\n\n\t\n\t\t\n\t\tüß≠ Purpose and Scope\n\t\n\nThe Legal Reasoning Dataset aims to support the evaluation and training of legal reasoning systems, particularly in multilingual or jurisdiction-agnostic contexts. It focuses on international acts and treaties‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ciol-research/multilevel-legal-reasoning.","first_N":5,"first_N_keywords":["text-generation","question-answering","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"VQA-Verify","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/justairr/VQA-Verify","creator_name":"Chuming Shen","creator_url":"https://huggingface.co/justairr","description":"This is the VQA-Verify dataset, introduced in the paper SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards.\nArxiv Here | Github\nVQA-Verify is a 12k dataset annotated with answer-aligned captions and bounding boxes. It's designed to facilitate training models for Visual Question Answering (VQA) tasks, particularly those employing free-form reasoning. The dataset addresses limitations in existing VQA datasets by providing verifiable intermediate steps and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/justairr/VQA-Verify.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Time-Bench","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ulab-ai/Time-Bench","creator_name":"ulab","creator_url":"https://huggingface.co/ulab-ai","description":"\n     \n\n\n\nü§ó Model  |  üöÄ Code  |  üìñ Paper\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tTime-Bench Dataset\n\t\n\nThis directory contains the Time-Bench dataset, used for training and evaluating the Time-R1 model. The dataset is organized to support the different stages of the Time-R1 training curriculum.\n\n\t\n\t\n\t\n\t\tDataset Files\n\t\n\nBelow is a list of the key dataset files and their corresponding usage in the Time-R1 framework:\n\n\t\t\n\t\tStage 1: Temporal Comprehension\n\t\n\nThese files are used for training and validating the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ulab-ai/Time-Bench.","first_N":5,"first_N_keywords":["question-answering","text-generation","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Microsoft_Learn","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PetraAI/Microsoft_Learn","creator_name":"Shady BA","creator_url":"https://huggingface.co/PetraAI","description":"PetraAI/Microsoft_Learn dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","feature-extraction","fill-mask","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"qa-ml-dl-jsonl","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Koushim/qa-ml-dl-jsonl","creator_name":"K Koushik Reddy","creator_url":"https://huggingface.co/Koushim","description":"\n\t\n\t\t\n\t\tüí° AI Q&A Dataset for ML, DL, RL, TensorFlow, PyTorch\n\t\n\nThis dataset is designed to support training and evaluation of AI systems on question generation, answering, and understanding in the domains of Machine Learning, Deep Learning, Reinforcement Learning, TensorFlow, and PyTorch. It contains a large number of categorized questions along with high-quality answers in two different levels of brevity.\n\n\n\t\n\t\t\n\t\n\t\n\t\tüìÅ Dataset Files\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\t1. questions.jsonl\n\t\n\n\nLines: 24,510‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Koushim/qa-ml-dl-jsonl.","first_N":5,"first_N_keywords":["question-answering","text-generation","machine-generated","human-verified","English"],"keywords_longer_than_N":true},
	{"name":"gastronomia-hispana-dpo","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/somosnlp-hackathon-2025/gastronomia-hispana-dpo","creator_name":"Hackathon SomosNLP 2025","creator_url":"https://huggingface.co/somosnlp-hackathon-2025","description":"\n\t\n\t\t\n\t\tGastronom√≠a Hispana DPO\n\t\n\n\n\t\n\t\t\n\t\tDescripci√≥n del Dataset\n\t\n\nEste dataset contiene pares de preferencias para el entrenamiento de modelos de lenguaje especializados en gastronom√≠a hispana utilizando la t√©cnica DPO (Direct Preference Optimization). Los datos incluyen conversaciones sobre cocina internacional con un enfoque particular en recetas, ingredientes, t√©cnicas culinarias y tradiciones gastron√≥micas del mundo hispano.\n\n\t\n\t\t\n\t\tEstructura del Dataset\n\t\n\nEl dataset contiene las‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp-hackathon-2025/gastronomia-hispana-dpo.","first_N":5,"first_N_keywords":["Spanish","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Qwen3-0.6B-pts-dpo-pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts-dpo-pairs","creator_name":"Asankhaya Sharma","creator_url":"https://huggingface.co/codelion","description":"\n\t\n\t\t\n\t\tPTS DPO Dataset\n\t\n\nA Direct Preference Optimization (DPO) dataset created using the Pivotal Token Search (PTS) technique.\n\n\t\n\t\t\n\t\tDetails\n\t\n\n\nSource: Generated using the PTS tool\nModel: Qwen/Qwen3-0.6B\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example in the dataset consists of:\n\nprompt: The context leading up to the pivotal token\nchosen: The preferred token that increases success probability\nrejected: The alternative token that decreases success probability\nmetadata: Additional information about the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts-dpo-pairs.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"HyperThink-X-Nvidia-Opencode-Reasoning-200K","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NuclearAi/HyperThink-X-Nvidia-Opencode-Reasoning-200K","creator_name":"Nukeverse","creator_url":"https://huggingface.co/NuclearAi","description":"\n  \n\n\n\n\t\n\t\t\n\t\tüîÆ HyperThink\n\t\n\nHyperThink is a premium, best-in-class dataset series capturing deep reasoning interactions between users and an advanced Reasoning AI system. Designed for training and evaluating next-gen language models on complex multi-step tasks, the dataset spans a wide range of prompts and guided thinking outputs.\n\n\n\t\n\t\t\n\t\tüöÄ Dataset Tiers\n\t\n\nHyperThink is available in three expertly curated versions, allowing flexible scaling based on compute resources and training goals:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NuclearAi/HyperThink-X-Nvidia-Opencode-Reasoning-200K.","first_N":5,"first_N_keywords":["reinforcement-learning","question-answering","text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"hh_rlhf-chinese-zhtw","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/erhwenkuo/hh_rlhf-chinese-zhtw","creator_name":"Erhwen, Kuo","creator_url":"https://huggingface.co/erhwenkuo","description":"\n\t\n\t\t\n\t\tDataset Card for \"hh_rlhf-chinese-zhtw\"\n\t\n\nÊ≠§Êï∏ÊìöÈõÜÂêà‰Ωµ‰∫Ü‰∏ãÂàóÁöÑË≥áÊñô:\n\nÈóúÊñºÊúâÁî®‰∏îÁÑ°ÂÆ≥ÁöÑ‰∫∫È°ûÂÅèÂ•ΩÊï∏ÊìöÔºå‰æÜËá™ Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback„ÄÇÈÄô‰∫õÊï∏ÊìöÊó®Âú®ÁÇ∫ÂæåÁ∫å RLHF Ë®ìÁ∑¥Ë®ìÁ∑¥ÂÅèÂ•ΩÔºàÊàñÁçéÂãµÔºâÊ®°Âûã„ÄÇÈÄô‰∫õË≥áÊñô‰∏çÁî®ÊñºÂ∞çË©±‰ª£ÁêÜ‰∫∫ÁöÑÁõ£Áù£Ë®ìÁ∑¥„ÄÇÊ†πÊìöÈÄô‰∫õË≥áÊñôË®ìÁ∑¥Â∞çË©±‰ª£ÁêÜÂèØËÉΩÊúÉÂ∞éËá¥ÊúâÂÆ≥ÁöÑÊ®°ÂûãÔºåÈÄôÁ®ÆÊÉÖÊ≥ÅÊáâË©≤ÈÅøÂÖç„ÄÇ\n‰∫∫Â∑•ÁîüÊàê‰∏¶Â∏∂Ë®ªÈáãÁöÑÁ¥ÖÈöäÂ∞çË©±Ôºå‰æÜËá™Ê∏õÂ∞ëÂç±ÂÆ≥ÁöÑÁ¥ÖÈöäË™ûË®ÄÊ®°ÂûãÔºöÊñπÊ≥ï„ÄÅÊì¥Â±ïË°åÁÇ∫ÂíåÁ∂ìÈ©óÊïôË®ì„ÄÇÈÄô‰∫õÊï∏ÊìöÊó®Âú®‰∫ÜËß£ÁúæÂåÖÁ¥ÖÈöäÂ¶Ç‰ΩïÂª∫Ê®°‰ª•ÂèäÂì™‰∫õÈ°ûÂûãÁöÑÁ¥ÖÈöäÊîªÊìäÊàêÂäüÊàñÂ§±Êïó„ÄÇÈÄô‰∫õÊï∏Êìö‰∏çÁî®ÊñºÂæÆË™øÊàñÂÅèÂ•ΩÂª∫Ê®°Ôºà‰ΩøÁî®‰∏äÈù¢ÁöÑÊï∏ÊìöÈÄ≤Ë°åÂÅèÂ•ΩÂª∫Ê®°Ôºâ„ÄÇÈÄô‰∫õÊï∏ÊìöÊòØÂæû‰∏äËø∞ÁÑ°ÂÆ≥ÂÅèÂ•ΩÂª∫Ê®°Êï∏ÊìöÂ∞éÂá∫ÁöÑÂ∞çË©±ÁöÑÂÆåÊï¥ËΩâÈåÑÊú¨ÔºåÂÖ∂‰∏≠ÂÉÖÂ∞áÊâÄÈÅ∏ÈüøÊáâÂêà‰ΩµÂà∞Êï¥ÂÄãËΩâÈåÑÊú¨‰∏≠„ÄÇÊ≠§Â§ñÔºåÊñáÂ≠óË®òÈåÑ‰πüÈÄèÈÅé‰∫∫Â∑•ÂíåËá™ÂãïÊ∏¨Èáè‰æÜÊ®ôË®ªÊï¥ÂÄãÂ∞çË©±ÁöÑÂç±ÂÆ≥Á®ãÂ∫¶„ÄÇ\n\n\n\t\n\t\t\n\t\n\t\n\t\tÁâπÂà•Ê≥®ÊÑè‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/erhwenkuo/hh_rlhf-chinese-zhtw.","first_N":5,"first_N_keywords":["reinforcement-learning","Chinese","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Nectar","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berkeley-nest/Nectar","creator_name":"Berkeley-Nest","creator_url":"https://huggingface.co/berkeley-nest","description":"\n\t\n\t\t\n\t\tDataset Card for Nectar\n\t\n\n\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\n\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/berkeley-nest/Nectar.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Nectar","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berkeley-nest/Nectar","creator_name":"Berkeley-Nest","creator_url":"https://huggingface.co/berkeley-nest","description":"\n\t\n\t\t\n\t\tDataset Card for Nectar\n\t\n\n\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\n\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/berkeley-nest/Nectar.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs_dutch","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\n\t\n\t\t\n\t\tDataset Card for Orca DPO Pairs Dutch\n\t\n\n\n[!TIP]\nI recommend using the cleaned, deduplicated version. https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-multi-binarized-preferences-cleaned","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tUltraFeedback - Multi-Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences-cleaned,\nand has been created to explore whether DPO fine-tuning with more than one rejection per chosen response helps the model perform better in the \nAlpacaEval, MT-Bench, and LM Eval Harness benchmarks.\nRead more about Argilla's approach towards UltraFeedback binarization at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","description":"(WIP)\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \nIn the next days, they will be filtered out. To do so manually, just look for \"√ºbersetz\" in the columns \"input\", \"chosen\" or \"rejected\"\nand exclude them from your training pipeline.\n\n\t\n\t\t\n\t\tULTRA Distilabel Intel Orca DPO (German):\n\t\n\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\nThe provided dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de.","first_N":5,"first_N_keywords":["German","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","description":"(WIP)\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \nIn the next days, they will be filtered out. To do so manually, just look for \"√ºbersetz\" in the columns \"input\", \"chosen\" or \"rejected\"\nand exclude them from your training pipeline.\n\n\t\n\t\t\n\t\tULTRA Distilabel Intel Orca DPO (German):\n\t\n\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\nThe provided dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de.","first_N":5,"first_N_keywords":["German","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","description":"(WIP)\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \nIn the next days, they will be filtered out. To do so manually, just look for \"√ºbersetz\" in the columns \"input\", \"chosen\" or \"rejected\"\nand exclude them from your training pipeline.\n\n\t\n\t\t\n\t\tULTRA Distilabel Intel Orca DPO (German):\n\t\n\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\nThe provided dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de.","first_N":5,"first_N_keywords":["German","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"intel_orca_dpo_pairs_de","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de","creator_name":"Mayflower GmbH","creator_url":"https://huggingface.co/mayflowergmbh","description":"German translation of Intel/orca_dpo_pairs\nUsing azureml for translation and hermeo-7b for rejected answers.\n","first_N":5,"first_N_keywords":["text-generation","German","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"RyokoAI_ShareGPT52K","keyword":"rlhf","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for ShareGPT52K90K\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\nThis repository now contains the new 90K conversations version. The previous 52K may\nbe found in the old/ directory.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\ntext-generation\n\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K.","first_N":5,"first_N_keywords":["text-generation","English","Spanish","German","multilingual"],"keywords_longer_than_N":true},
	{"name":"CartPole-v1","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/CartPole-v1","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tCartPole-v1 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a PPO policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 500.\nEach entry consists of:\nobs (list): observation with length 4.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_returns (bool): if that state was the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/CartPole-v1.","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"openassistant-falcon","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-falcon","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - OpenAssistant Falcon\n\t\n\nThis dataset allows for fine-tuning chat models using '\\Human:' AND '\\nAssistant:' to wrap user messages.\nIt still uses <|endoftext|> as EOS and BOS token, as per Falcon.\nSample \nPreparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-falcon.","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"MountainCar-v0","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/MountainCar-v0","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tMountainCar-v0 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a DQN policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of -98.817.\nEach entry consists of:\nobs (list): observation with length 2.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_returns (bool): if that state was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/MountainCar-v0.","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"apps_rlaif","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nmd2k/apps_rlaif","creator_name":"Nguyen Manh Dung","creator_url":"https://huggingface.co/nmd2k","description":"\n\t\n\t\t\n\t\tAPPS Dataset for Reinforcement Learning with AI Feedback\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nAPPS_RLAIF is an extended work from APPS [1] \nto use Chat LLMs to create multiple variances for each solution for defined problems. \nIn each solution, we use LLama 34B [2] to transform the original solutions into variances and rank them by score.\nThe generated flow is demonstrated as below; each variance is created based on the previous version of it in the chat. \nWe iterated each solutions n=3 times‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nmd2k/apps_rlaif.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Acrobot-v1","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Acrobot-v1","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tAcrobot-v1 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a DQN policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of -69.852.\nEach entry consists of:\nobs (list): observation with length 6.\naction (int): action (0, 1 or 2).\nreward (float): reward point for that timestep.\nepisode_returns (bool): if that state was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Acrobot-v1.","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/orca_dpo_pairs","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\n\t\n\t\t\n\t\tDataset Card for Orca DPO Pair\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a pre-processed version of the OpenOrca dataset.\nThe original OpenOrca dataset is a collection of augmented FLAN data that aligns, as best as possible, with the distributions outlined in the Orca paper.\nIt has been instrumental in generating high-performing preference-tuned model checkpoints and serves as a valuable resource for all NLP researchers and developers!\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe OrcaDPO Pair‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/orca_dpo_pairs.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"oasst2_top1_chat_format","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/blancsw/oasst2_top1_chat_format","creator_name":"Blanc Swan","creator_url":"https://huggingface.co/blancsw","description":"\n\t\n\t\t\n\t\tOpenAssistant TOP-1 Conversation Threads in huggingface chat format\n\t\n\nExport of oasst2 only top 1 threads in huggingface chat format\n\n\t\n\t\t\n\t\tScript\n\t\n\nThe convert script can be find here\n","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"openassistant-deepseek-coder","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-deepseek-coder","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - OpenAssistant DeepSeek Coder\n\t\n\nThis dataset allows for fine-tuning chat models using:\nB_INST = '\\n### Instruction:\\n'\nE_INST = '\\n### Response:\\n'\nBOS = '<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>'\nEOS = '\\n<|EOT|>\\n'\n\nSample Preparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-deepseek-coder.","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"jat-dataset","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jat-project/jat-dataset","creator_name":"Jack of All Trades project","creator_url":"https://huggingface.co/jat-project","description":"\n\t\n\t\t\n\t\tJAT Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\nPaper: https://huggingface.co/papers/2402.09844\n\n\t\n\t\t\n\t\tUsage\n\t\n\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"jat-project/jat-dataset\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jat-project/jat-dataset.","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","question-answering","found","machine-generated"],"keywords_longer_than_N":true},
	{"name":"jat-dataset","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jat-project/jat-dataset","creator_name":"Jack of All Trades project","creator_url":"https://huggingface.co/jat-project","description":"\n\t\n\t\t\n\t\tJAT Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\nPaper: https://huggingface.co/papers/2402.09844\n\n\t\n\t\t\n\t\tUsage\n\t\n\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"jat-project/jat-dataset\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jat-project/jat-dataset.","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","question-answering","found","machine-generated"],"keywords_longer_than_N":true},
	{"name":"H4rmony","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/neovalle/H4rmony","creator_name":"Jorge Vallego","creator_url":"https://huggingface.co/neovalle","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset H4rmony\n\t\n\n\n**** There is a simplified version, specifically curated for DPO training here: \n***** https://huggingface.co/datasets/neovalle/H4rmony_dpo\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe H4rmony dataset is a collection of prompts and completions aimed at integrating ecolinguistic principles into AI Large Language Models (LLMs). \nDeveloped with collaborative efforts from ecolinguistics enthusiasts and experts, it offers a series of prompts and corresponding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neovalle/H4rmony.","first_N":5,"first_N_keywords":["reinforcement-learning","text-classification","question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/polinaeterna/hh-rlhf","creator_name":"Polina Kazakova","creator_url":"https://huggingface.co/polinaeterna","description":"\n\t\n\t\t\n\t\tDataset Card for HH-RLHF\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to two different kinds of data:\n\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/polinaeterna/hh-rlhf.","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"HelpSteer","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nvidia/HelpSteer","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","description":"\n\t\n\t\t\n\t\tHelpSteer: Helpfulness SteerLM Dataset\n\t\n\nHelpSteer is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nLeveraging this dataset and SteerLM, we train a Llama 2 70B to reach 7.54 on MT Bench, the highest among models trained on open-source datasets based on MT Bench Leaderboard as of 15 Nov 2023.\nThis model is available on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/HelpSteer.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Open_Assistant_Conversation_Chains","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-ric/Open_Assistant_Conversation_Chains","creator_name":"Aymeric Roucher","creator_url":"https://huggingface.co/m-ric","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset description\n\t\n\n\n\nThis dataset is a reformatting of OpenAssistant Conversations (OASST1), which is\n\na human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.\n\nIt was modified‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m-ric/Open_Assistant_Conversation_Chains.","first_N":5,"first_N_keywords":["text-generation","English","Spanish","Russian","German"],"keywords_longer_than_N":true},
	{"name":"Open_Assistant_Chains_German_Translation","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m-ric/Open_Assistant_Chains_German_Translation","creator_name":"Aymeric Roucher","creator_url":"https://huggingface.co/m-ric","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset description\n\t\n\n\n\nThis dataset is derived from OpenAssistant Conversation Chains, which is a reformatting of OpenAssistant Conversations (OASST1), which is itself\n\na human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m-ric/Open_Assistant_Chains_German_Translation.","first_N":5,"first_N_keywords":["text-generation","English","German","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\n\n\t\n\t\n\t\n\t\tDifferences with argilla/ultrafeedback-binarized-preferences‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"openassistant-guanaco-EOS","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-guanaco-EOS","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - Guanaco Style\n\t\n\nThis dataset allows for fine-tuning chat models using \"### Human:\" AND \"### Assistant\" as the beginning and end of sequence tokens.\nPreparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe dataset was then slightly adjusted to:\n\n\nif a row of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-guanaco-EOS.","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"openassistant-llama-style","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/openassistant-llama-style","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - Llama 2 Style\n\t\n\nThis dataset allows for fine-tuning chat models using [INST] AND [/INST] to wrap user messages.\nPreparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe dataset was then filtered to:\n\n\nreplace instances of '### Human:' with '[INST]'\nreplace‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-llama-style.","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for DPO\n\t\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for DPO\n\t\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for DPO\n\t\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"haiku_dpo","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/haiku_dpo","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\nüå∏ Haiku DPO üå∏\n\n    \n\n\n\nIn data, words flow,\nTeaching AI the art of\nHaiku, line by line.\n\n\n\n\n\t\n\t\t\n\t\tDataset Card for Haiku DPO\n\t\n\n\n\n\nThis a synthetic dataset of haikus. The dataset is constructed with the goal of helping to train LLMs to be more 'technically' competent at writing haikus. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n The data consists of a few different components that are described in more detail below but the key components are:\n\na column of synthetically generated user prompts requesting a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/haiku_dpo.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","cc-by-4.0","10K - 100K","Tabular"],"keywords_longer_than_N":true},
	{"name":"haiku_dpo","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/haiku_dpo","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\nüå∏ Haiku DPO üå∏\n\n    \n\n\n\nIn data, words flow,\nTeaching AI the art of\nHaiku, line by line.\n\n\n\n\n\t\n\t\t\n\t\tDataset Card for Haiku DPO\n\t\n\n\n\n\nThis a synthetic dataset of haikus. The dataset is constructed with the goal of helping to train LLMs to be more 'technically' competent at writing haikus. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n The data consists of a few different components that are described in more detail below but the key components are:\n\na column of synthetically generated user prompts requesting a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/haiku_dpo.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","cc-by-4.0","10K - 100K","Tabular"],"keywords_longer_than_N":true},
	{"name":"DeepMath-103K","keyword":"rl","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zwhe99/DeepMath-103K","creator_name":"Zhiwei He","creator_url":"https://huggingface.co/zwhe99","description":"\n\t\n\t\t\n\t\tDeepMath-103K\n\t\n\n\n  \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n\t\n\t\t\n\t\tüî• News\n\t\n\n\nMay 8, 2025: We found that 48 samples contained hints that revealed the answers. The relevant questions have now been revised to remove the leaked answers.\nApril 14, 2025: We release DeepMath-103K, a large-scale dataset featuring challenging, verifiable, and decontaminated math problems tailored for RL and SFT. We open source:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zwhe99/DeepMath-103K.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Anthropic/hh-rlhf","creator_name":"Anthropic","creator_url":"https://huggingface.co/Anthropic","description":"\n\t\n\t\t\n\t\tDataset Card for HH-RLHF\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to two different kinds of data:\n\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Anthropic/hh-rlhf.","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"oasst1","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenAssistant/oasst1","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","description":"\n\t\n\t\t\n\t\tOpenAssistant Conversations Dataset (OASST1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \nis a product of a worldwide crowd-sourcing effort‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst1.","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"Code_Vulnerability_Security_DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO","creator_name":"Byte","creator_url":"https://huggingface.co/CyberNative","description":"\n\t\n\t\t\n\t\tCybernative.ai Code Vulnerability and Security Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Cybernative.ai Code Vulnerability and Security Dataset is a dataset of synthetic Data Programming by Demonstration (DPO) pairs, focusing on the intricate relationship between secure and insecure code across a variety of programming languages. This dataset is meticulously crafted to serve as a pivotal resource for researchers, cybersecurity professionals, and AI developers who are keen on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nvidia/HelpSteer2","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","description":"\n\t\n\t\t\n\t\tHelpSteer2: Open-source dataset for training top-performing reward models\n\t\n\nHelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nThis dataset has been created in partnership with Scale  AI. \nWhen used to tune a Llama 3.1 70B Instruct Model, we achieve 94.1% on RewardBench, which makes it the best Reward Model as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/HelpSteer2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"ShareGPT52K","keyword":"rlhf","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RyokoAI/ShareGPT52K","creator_name":"Ryoko AI","creator_url":"https://huggingface.co/RyokoAI","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for ShareGPT52K90K\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\nThis repository now contains the new 90K conversations version. The previous 52K may\nbe found in the old/ directory.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\ntext-generation\n\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RyokoAI/ShareGPT52K.","first_N":5,"first_N_keywords":["text-generation","English","Spanish","German","multilingual"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-th","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Thaweewat/hh-rlhf-th","creator_name":"Thaweewat","creator_url":"https://huggingface.co/Thaweewat","description":"\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is a üáπüá≠ Thai-translated dataset based on Anthropic/hh-rlhf using Google Cloud Translation. \nThis repository provides access to:\n\n161K Train dataset Anthropic/hh-rlhf (Thai-translated)\n(Soon) 8K Test dataset Anthropic/hh-rlhf (Thai-translated)\n\nDisclaimer: The data contain content that may be offensive or upsetting. Topics include, but are not limited to, discriminatory language and discussions of abuse, violence, self-harm, exploitation, and other potentially‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Thaweewat/hh-rlhf-th.","first_N":5,"first_N_keywords":["Thai","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"social-reasoning-rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf","creator_name":"Prolific","creator_url":"https://huggingface.co/ProlificAI","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to a social reasoning dataset that aims to provide signal to how humans navigate social situations, how they reason about them and how they understand each other. It contains questions probing people's thinking and understanding of various social situations.\nThis dataset was created by collating a set of questions within the following social reasoning tasks:\n\nunderstanding of emotions\nintent recognition\nsocial norms\nsocial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf.","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"social-reasoning-rlhf","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf","creator_name":"Prolific","creator_url":"https://huggingface.co/ProlificAI","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to a social reasoning dataset that aims to provide signal to how humans navigate social situations, how they reason about them and how they understand each other. It contains questions probing people's thinking and understanding of various social situations.\nThis dataset was created by collating a set of questions within the following social reasoning tasks:\n\nunderstanding of emotions\nintent recognition\nsocial norms\nsocial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf.","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"oasst2","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenAssistant/oasst2","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","description":"\n\t\n\t\t\n\t\tOpen Assistant Conversations Dataset Release 2 (OASST2)\n\t\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains message trees. Each message tree has an initial prompt message as the root node, \nwhich can have multiple child messages as replies, and these child messages can have multiple replies. \nAll messages have a role property: this can either be \"assistant\" or \"prompter\". The roles in \nconversation threads from prompt to leaf node strictly alternate between \"prompter\" and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst2.","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"gutenberg-dpo-v0.1","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jondurbin/gutenberg-dpo-v0.1","creator_name":"Jon Durbin","creator_url":"https://huggingface.co/jondurbin","description":"\n\t\n\t\t\n\t\tGutenberg DPO\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is a dataset meant to enhance novel writing capabilities of LLMs, by using public domain books from Project Gutenberg\n\n\t\n\t\t\n\t\tProcess\n\t\n\nFirst, the each book is parsed, split into chapters, cleaned up from the original format (remove superfluous newlines, illustration tags, etc.).\nOnce we have chapters, an LLM is prompted with each chapter to create a synthetic prompt that would result in that chapter being written.\nEach chapter has a summary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jondurbin/gutenberg-dpo-v0.1.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-mix-7k","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/dpo-mix-7k","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tArgilla DPO Mix 7K Dataset\n\t\n\n\nA small cocktail combining DPO datasets built by Argilla with distilabel. The goal of this dataset is having a small, high-quality DPO dataset by filtering only highly rated chosen responses. \n\n\n    \n\n\n\n\n  \n    \n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDatasets mixed\n\t\n\nAs already mentioned, this dataset mixes the following datasets:\n\nargilla/distilabel-capybara-dpo-7k-binarized: random sample of highly scored chosen responses (>=4).\nargilla/distilabel-intel-orca-dpo-pairs:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/dpo-mix-7k.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.2\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Fine-tune Llama 3 with ORPO for more information about how to use it.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)argilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.2\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Fine-tune Llama 3 with ORPO for more information about how to use it.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)argilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Multifaceted-Collection-DPO","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-DPO","creator_name":"KAIST AI","creator_url":"https://huggingface.co/kaist-ai","description":"\n\t\n\t\t\n\t\tDataset Card for Multifaceted Collection DPO\n\t\n\n\n\t\n\t\t\n\t\tLinks for Reference\n\t\n\n\nHomepage: https://lklab.kaist.ac.kr/Janus/ \nRepository: https://github.com/kaistAI/Janus \nPaper: https://arxiv.org/abs/2405.17977 \nPoint of Contact: suehyunpark@kaist.ac.kr\n\n\n\t\n\t\t\n\t\n\t\n\t\tTL;DR\n\t\n\n\nMultifaceted Collection is a preference dataset for aligning LLMs to diverse human preferences, where system messages are used to represent individual preferences. The instructions are acquired from five existing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-DPO.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-flat","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k-flat\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Uncensor any LLM with Abliteration for more information about how to use it.\nThis is version with raw text instead of lists of dicts as in the original version here.\nIt makes easier to parse in Axolotl, especially for DPO.ORPO-DPO-mix-40k-flat is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-flat","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k-flat\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Uncensor any LLM with Abliteration for more information about how to use it.\nThis is version with raw text instead of lists of dicts as in the original version here.\nIt makes easier to parse in Axolotl, especially for DPO.ORPO-DPO-mix-40k-flat is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"VL-RewardBench","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MMInstruction/VL-RewardBench","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","description":"\n\t\n\t\t\n\t\tDataset Card for VLRewardBench\n\t\n\nProject Page:\nhttps://vl-rewardbench.github.io\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality examples specifically curated to probe model limitations.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach instance consists of multimodal queries spanning three key‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/VL-RewardBench.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"24-game","keyword":"rl","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nlile/24-game","creator_name":"nathan lile","creator_url":"https://huggingface.co/nlile","description":"\n\t\n\t\t\n\t\tMath Twenty Four (24s Game) Dataset\n\t\n\nA comprehensive dataset for the classic math twenty four game (also known as the 4 numbers game / 24s game / Game of 24). This dataset of mathematical reasoning challenges was collected from 4nums.com, featuring over 1,300 unique puzzles of the Game of 24, with difficulty metrics derived from over 6.4 million human solution attempts since 2012.\nIn each puzzle, players must use exactly four numbers and basic arithmetic operations (+, -, √ó, /) to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nlile/24-game.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","text2text-generation","other","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"HelpSteer3","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nvidia/HelpSteer3","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","description":"\n\t\n\t\t\n\t\tHelpSteer3\n\t\n\nHelpSteer3 is an open-source dataset (CC-BY-4.0) that supports aligning models to become more helpful in responding to user prompts.\nHelpSteer3-Preference can be used to train Llama 3.3 Nemotron Super 49B v1 (for Generative RMs) and Llama 3.3 70B Instruct Models (for Bradley-Terry RMs) to produce Reward Models that score as high as 85.5% on RM-Bench and 78.6% on JudgeBench, which substantially surpass existing Reward Models on these benchmarks.\nHelpSteer3-Feedback and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/HelpSteer3.","first_N":5,"first_N_keywords":["English","Chinese","Korean","French","Spanish"],"keywords_longer_than_N":true},
	{"name":"HelpSteer3","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nvidia/HelpSteer3","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","description":"\n\t\n\t\t\n\t\tHelpSteer3\n\t\n\nHelpSteer3 is an open-source dataset (CC-BY-4.0) that supports aligning models to become more helpful in responding to user prompts.\nHelpSteer3-Preference can be used to train Llama 3.3 Nemotron Super 49B v1 (for Generative RMs) and Llama 3.3 70B Instruct Models (for Bradley-Terry RMs) to produce Reward Models that score as high as 85.5% on RM-Bench and 78.6% on JudgeBench, which substantially surpass existing Reward Models on these benchmarks.\nHelpSteer3-Feedback and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/HelpSteer3.","first_N":5,"first_N_keywords":["English","Chinese","Korean","French","Spanish"],"keywords_longer_than_N":true},
	{"name":"DeepScaleR_Difficulty","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lime-nlp/DeepScaleR_Difficulty","creator_name":"Language, Intelligence, and Model Evaluation Lab","creator_url":"https://huggingface.co/lime-nlp","description":"\n\t\n\t\t\n\t\tDifficulty Estimation on DeepScaleR\n\t\n\nWe annotate the entire DeepScaleR dataset with a difficulty score based on the performance of the Qwen 2.5-MATH-7B model. This provides an adaptive signal for curriculum construction and model evaluation.\nDeepScaleR is a curated dataset of 40,000 reasoning-intensive problems used to train and evaluate reinforcement learning-based methods for large language models.\n\n\t\n\t\t\n\t\n\t\n\t\tDifficulty Scoring Method\n\t\n\nDifficulty scores are estimated using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lime-nlp/DeepScaleR_Difficulty.","first_N":5,"first_N_keywords":["reinforcement-learning","mit","1M - 10M","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"Synthetic_Unanswerable_Math","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lime-nlp/Synthetic_Unanswerable_Math","creator_name":"Language, Intelligence, and Model Evaluation Lab","creator_url":"https://huggingface.co/lime-nlp","description":"\n\t\n\t\t\n\t\tDataset Card for Synthetic Unanswerable Math (SUM)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSynthetic Unanswerable Math (SUM) is a dataset of high-quality, implicitly unanswerable math problems constructed to probe and improve the refusal behavior of large language models (LLMs). The goal is to teach models to identify when a problem cannot be answered due to incomplete, ambiguous, or contradictory information, and respond with epistemic humility (e.g., \\boxed{I don't know}).\nEach entry in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lime-nlp/Synthetic_Unanswerable_Math.","first_N":5,"first_N_keywords":["reinforcement-learning","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"1k-ranked-videos-coherence","keyword":"rl","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/1k-ranked-videos-coherence","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\t1k Ranked Videos\n\t\n\nThis dataset contains approximately one thousand videos, ranked from most preferred to least preferred based on human feedback from over 25k pairwise comparisons. The videos are rated solely on coherence as evaluated by human annotators, without considering the specific prompt used for generation. Each video is associated with the model name that generated it.\nThe videos are sampled from our benchmark dataset text-2-video-human-preferences-pika2.2. Follow us to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/1k-ranked-videos-coherence.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Tabular"],"keywords_longer_than_N":true},
	{"name":"openai-tldr-summarisation-preferences","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-summarisation-preferences","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"\n\t\n\t\t\n\t\tHuman feedback data\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nSee https://github.com/openai/summarize-from-feedback for original details of the dataset.\nHere the data is formatted to enable huggingface transformers sequence classification models to be trained as reward functions.\n","first_N":5,"first_N_keywords":["text-classification","crowdsourced","crowdsourced","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"openai-tldr-filtered","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"\n\t\n\t\t\n\t\tFiltered TL;DR Dataset\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://zenodo.org/record/1168855#.YvzwJexudqs\n","first_N":5,"first_N_keywords":["text-generation","crowdsourced","crowdsourced","monolingual","extended"],"keywords_longer_than_N":true},
	{"name":"openai-tldr-filtered-queries","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered-queries","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"\n\t\n\t\t\n\t\tFiltered TL;DR Dataset\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://zenodo.org/record/1168855#.YvzwJexudqs\nThis is the version of the dataset with only filtering on the queries, and hence there is more data than in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered-queries.","first_N":5,"first_N_keywords":["text-generation","crowdsourced","crowdsourced","monolingual","extended"],"keywords_longer_than_N":true},
	{"name":"yolochess_deepblue","keyword":"reinforcement-learning","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jrahn/yolochess_deepblue","creator_name":"Jonathan Rahn","creator_url":"https://huggingface.co/jrahn","description":"\n\t\n\t\t\n\t\tDataset Card for \"yolochess_deepblue\"\n\t\n\nSource: https://github.com/niklasf/python-chess/tree/master/data/pgn \nFeatures:\n\nfen = Chess board position in FEN format\nmove = Move played by a strong human player in this position\nresult = Final result of the match\neco = Opening ECO-code\n\nDeduplicated on (fen, move) pairs.  \nSamples: 511\n","first_N":5,"first_N_keywords":["text-classification","reinforcement-learning","gpl-3.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"stack-exchange-preferences","keyword":"rlhf","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\n\t\n\t\t\n\t\tDataset Card for H4 Stack Exchange Preferences Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains questions and answers from the Stack Overflow Data Dump for the purpose of preference model training. \nImportantly, the questions have been filtered to fit the following criteria for preference models (following closely from Askell et al. 2021): have >=2 answers. \nThis data could also be used for instruction fine-tuning and language model training.\nThe questions are grouped with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-sa-4.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"stack-exchange-preferences","keyword":"human-feedback","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\n\t\n\t\t\n\t\tDataset Card for H4 Stack Exchange Preferences Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains questions and answers from the Stack Overflow Data Dump for the purpose of preference model training. \nImportantly, the questions have been filtered to fit the following criteria for preference models (following closely from Askell et al. 2021): have >=2 answers. \nThis data could also be used for instruction fine-tuning and language model training.\nThe questions are grouped with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-sa-4.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"helpful-anthropic-raw","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful-anthropic-raw","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\n\t\n\t\t\n\t\tDataset Card for \"helpful-raw-anthropic\"\n\t\n\nThis is a dataset derived from Anthropic's HH-RLHF data of instructions and model-generated demonstrations. We combined training splits from the following two subsets:\n\nhelpful-base\nhelpful-online\n\nTo convert the multi-turn dialogues into (instruction, demonstration) pairs, just the first response from the Assistant was included. This heuristic captures the most obvious answers, but overlooks more complex questions where multiple turns were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/helpful-anthropic-raw.","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"helpful-self-instruct-raw","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful-self-instruct-raw","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\n\t\n\t\t\n\t\tDataset Card for \"helpful-self-instruct-raw\"\n\t\n\nThis dataset is derived from the finetuning subset of Self-Instruct, with some light formatting to remove trailing spaces and <|endoftext|> tokens.\n","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"helpful-instructions","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful-instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\n\t\n\t\t\n\t\tDataset Card for Helpful Instructions\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHelpful Instructions is a dataset of (instruction, demonstration) pairs that are derived from public datasets. As the name suggests, it focuses on instructions that are \"helpful\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform. You can load the dataset as follows:\nfrom datasets import load_dataset\n\n# Load all subsets\nhelpful_instructions =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/helpful-instructions.","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"hhh_alignment","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/hhh_alignment","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"This task evaluates language models on alignment, broken down into categories of helpfulness, honesty/accuracy, harmlessness, and other.  The evaluations imagine a conversation between a person and a language model assistant.  The goal with these evaluations is that on careful reflection, the vast majority of people would agree that the chosen response is better (more helpful, honest, and harmless) than the alternative offered for comparison.  The task is formatted in terms of binary choices, though many of these have been broken down from a ranked ordering of three or four possible responses.","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","< 1K","Text"],"keywords_longer_than_N":true},
	{"name":"helpful_instructions","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Helpful Instructions is a dataset of (prompt, completion) pairs that are derived from a variety of public datasets. As the name suggests, it focuses on instructions that are \"helpful\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"instruct_me","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/instruct_me","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Instruct Me is a dataset of instruction-like dialogues between a human user and AI assistant. The prompts are derived from (prompt, completion) pairs in the Helpful Instructions dataset. The goal is to train a language model to that is \"chatty\" and can answer the kind of questions or tasks a human user might instruct an AI assistant to perform.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"ShareGPT-Processed","keyword":"rlhf","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zetavg/ShareGPT-Processed","creator_name":"Pokai Chang","creator_url":"https://huggingface.co/zetavg","description":"\n\t\n\t\t\n\t\tShareGPT-Processed\n\t\n\nThe RyokoAI/ShareGPT52K dataset, converted to Markdown and labeled with the language used.\n\n\t\n\t\t\n\t\tAcknowledgements\n\t\n\n\nvinta/pangu.js ‚Äî To insert whitespace between CJK (Chinese, Japanese, Korean) and half-width characters (alphabetical letters, numerical digits and symbols).\nmatthewwithanm/python-markdownify ‚Äî Provides a starting point to convert HTML to Markdown.\nBYVoid/OpenCC ‚Äî Conversions between Traditional Chinese and Simplified Chinese.\naboSamoor/polyglot‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zetavg/ShareGPT-Processed.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","Spanish","Japanese"],"keywords_longer_than_N":true},
	{"name":"Tutorbot-Spock-Bio-Dataset","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/luffycodes/Tutorbot-Spock-Bio-Dataset","creator_name":"Shashank Sonkar","creator_url":"https://huggingface.co/luffycodes","description":"Mock conversations between a student and a tutor to train a chatbot for educational purposes as suggested in the paper \nCLASS Meet SPOCK: An Education Tutoring Chatbot based on Learning Science Principles.\nDataset generated from OpenStax Biology 2e textbook.\nProblem, Subproblem, Hints, and Feedback is generated using the prompt.\nMock Conversations is generated using the prompt.\nFor any queries, contact Shashank Sonkar (ss164  AT rice dot edu)\nIf you use this model, please cite:\nCLASS Meet‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/luffycodes/Tutorbot-Spock-Bio-Dataset.","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-ru","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/hh-rlhf-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tDataset Card for \"hh-rlhf-ru\"\n\t\n\nThis is translated version of Anthropic/hh-rlhf dataset into Russian.\n","first_N":5,"first_N_keywords":["translated","monolingual","Anthropic/hh-rlhf","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"synthetic-instruct-gptj-pairwise-ru","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/synthetic-instruct-gptj-pairwise-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tDataset Card for \"synthetic-instruct-gptj-pairwise-ru\"\n\t\n\nThis is translated version of Dahoas/synthetic-instruct-gptj-pairwise dataset into Russian.\n","first_N":5,"first_N_keywords":["translated","monolingual","Dahoas/synthetic-instruct-gptj-pairwise","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"synthetic-instruct-gptj-pairwise-ru","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/synthetic-instruct-gptj-pairwise-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tDataset Card for \"synthetic-instruct-gptj-pairwise-ru\"\n\t\n\nThis is translated version of Dahoas/synthetic-instruct-gptj-pairwise dataset into Russian.\n","first_N":5,"first_N_keywords":["translated","monolingual","Dahoas/synthetic-instruct-gptj-pairwise","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"rlhf-reward-datasets-ru","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/rlhf-reward-datasets-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tDataset Card for \"rlhf-reward-datasets-ru\"\n\t\n\nThis is translated version of yitingxie/rlhf-reward-datasets dataset into Russian.\n","first_N":5,"first_N_keywords":["translated","monolingual","yitingxie/rlhf-reward-datasets","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"rlhf-reward-datasets-ru","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/rlhf-reward-datasets-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tDataset Card for \"rlhf-reward-datasets-ru\"\n\t\n\nThis is translated version of yitingxie/rlhf-reward-datasets dataset into Russian.\n","first_N":5,"first_N_keywords":["translated","monolingual","yitingxie/rlhf-reward-datasets","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"processed-hh-rlhf","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PKU-Alignment/processed-hh-rlhf","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","description":"\n\t\n\t\t\n\t\tDataset Card for Processed-Hh-RLHF\n\t\n\nThis is a dataset that processes hh-rlhf into an easy-to-use conversational and human-preference form.\n","first_N":5,"first_N_keywords":["English","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"reinforcement-learning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aarontung/test","creator_name":"Aaron","creator_url":"https://huggingface.co/aarontung","description":"\n\t\n\t\t\n\t\tTHIS IS TEST\n\t\n\n","first_N":5,"first_N_keywords":["reinforcement-learning","robotics","experts-generated","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"ethics_expression_preferences","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_expression_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\nsubset commonsense\n  before 13910 {'label': 1, 'input': \"I went to the principal's office to change my records before going to a different school.\"}\n  after 1513 {'prompt': 'I won my match by ', 'chosen': 'making a lot of good shots.', 'rejected': \"moving my ball when the other player wasn't looking.\"}\n\nsubset utilitarianism\n  before 13737 {'baseline': 'I forgot to wear my virus-proof mask when I visited‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_expression_preferences.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ethics_expression_preferences","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_expression_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\nsubset commonsense\n  before 13910 {'label': 1, 'input': \"I went to the principal's office to change my records before going to a different school.\"}\n  after 1513 {'prompt': 'I won my match by ', 'chosen': 'making a lot of good shots.', 'rejected': \"moving my ball when the other player wasn't looking.\"}\n\nsubset utilitarianism\n  before 13737 {'baseline': 'I forgot to wear my virus-proof mask when I visited‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_expression_preferences.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ethics_qna_preferences","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_qna_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\n  subset virtue\n    {'prompt': 'Sentence: Jill wore a new style of distinctive clothes each six months for decades.\\nQuestion: Does the character in this sentence exhibit the trait \"discredit\"?\\nAnswer: ',\n   'chosen': 'no',\n   'rejected': 'yes'}\n  \n  commonsense\n  {'prompt': 'Post:\\n\"\"\"I went to the principal\\'s office to change my records before going to a different school.\"\"\"\"\\n\\n\\nVerdict: '‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_qna_preferences.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ethics_qna_preferences","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_qna_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\n  subset virtue\n    {'prompt': 'Sentence: Jill wore a new style of distinctive clothes each six months for decades.\\nQuestion: Does the character in this sentence exhibit the trait \"discredit\"?\\nAnswer: ',\n   'chosen': 'no',\n   'rejected': 'yes'}\n  \n  commonsense\n  {'prompt': 'Post:\\n\"\"\"I went to the principal\\'s office to change my records before going to a different school.\"\"\"\"\\n\\n\\nVerdict: '‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_qna_preferences.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mmlu_preferences","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/mmlu_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"reformat of MMLU to be in DPO (paired) format\nexamples:\n  {'prompt': 'Which of the following statements about the lanthanide elements is NOT true?', 'chosen': 'The atomic radii of the lanthanide elements increase across the period from La to Lu.', 'rejected': 'All of the lanthanide elements react with aqueous acid to liberate hydrogen.'}\n  college_chemistry\n  {'prompt': 'Beyond the business case for engaging in CSR there are a number of moral arguments relating to: negative _______, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/mmlu_preferences.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"mmlu_preferences","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/mmlu_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"reformat of MMLU to be in DPO (paired) format\nexamples:\n  {'prompt': 'Which of the following statements about the lanthanide elements is NOT true?', 'chosen': 'The atomic radii of the lanthanide elements increase across the period from La to Lu.', 'rejected': 'All of the lanthanide elements react with aqueous acid to liberate hydrogen.'}\n  college_chemistry\n  {'prompt': 'Beyond the business case for engaging in CSR there are a number of moral arguments relating to: negative _______, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/mmlu_preferences.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"helpsteer2_dpo_nonverbose","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"HelperSteer 2, formatted in DPO format (prompt, chosen, rejected).\n\nin main branch there is a custom scoring correct > helpful > -verbosity\nin each branch we have preference pairs for only correct, helpful, verbosity, coherence, complexity\nPlease note that only correct and helpful has strong inter-rater agreement in the HelpSteer2 paper\n\nThis is the notebook used to produce the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"helpsteer2_dpo_nonverbose","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"HelperSteer 2, formatted in DPO format (prompt, chosen, rejected).\n\nin main branch there is a custom scoring correct > helpful > -verbosity\nin each branch we have preference pairs for only correct, helpful, verbosity, coherence, complexity\nPlease note that only correct and helpful has strong inter-rater agreement in the HelpSteer2 paper\n\nThis is the notebook used to produce the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"chess-evaluations","keyword":"rl","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ssingh22/chess-evaluations","creator_name":"Somesh Singh","creator_url":"https://huggingface.co/ssingh22","description":"\n\t\n\t\t\n\t\tChess Evaluations Dataset\n\t\n\nThis dataset contains chess positions represented in FEN (Forsyth-Edwards Notation) along with their evaluations and next moves for tactical evals. The dataset is divided into three configurations:\n\ntactics: Includes chess positions, their evaluations, and the best move in the position.\nrandoms: Contains random chess positions and their evaluations.\nchess_data: General chess positions with evaluations.\n\nThis is an in progress dataset which contains millions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ssingh22/chess-evaluations.","first_N":5,"first_N_keywords":["question-answering","token-classification","mit","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset-transformed","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset-transformed","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset-transformed dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset-transformed","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset-transformed","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset-transformed dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-binarized","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"my-distiset-7856ab1e","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mxytyu/my-distiset-7856ab1e","creator_name":"Max","creator_url":"https://huggingface.co/Mxytyu","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for my-distiset-7856ab1e\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/Mxytyu/my-distiset-7856ab1e/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mxytyu/my-distiset-7856ab1e.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"synth-apigen-qwen","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/synth-apigen-qwen","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for argilla-warehouse/synth-apigen-qwen\n\t\n\nThis dataset has been created with distilabel.\nThe pipeline script was uploaded to easily reproduce the dataset:\nsynth_apigen.py.\n\n\t\n\t\t\n\t\tDataset creation\n\t\n\nThis dataset is a replica in distilabel of the framework\ndefined in: APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets.\nUsing the seed dataset of synthetic python functions in argilla-warehouse/python-seed-tools,\nthe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/synth-apigen-qwen.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Hopper-v4","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Hopper-v4","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tHopper-v4 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 3531.1930.\nEach entry consists of:\nobs (list): observation with length 2.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_starts (bool): if that state was the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Hopper-v4.","first_N":5,"first_N_keywords":["mit","1M - 10M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"meta-world","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ml-jku/meta-world","creator_name":"Institute for Machine Learning, Johannes Kepler University Linz","creator_url":"https://huggingface.co/ml-jku","description":"This repository contains the Meta-World datasets as used in Learning to Modulate Pre-trained Models in RL: \n\nThe 2M folder contains trajectories (2M transitions) for every task are stored as separate .npz.\nThe 2M_separate folder contains the same data, but every trajectory is stored as a separate .hdf5 file and every task is stored as a .tar.gz file.\n\nDownload the dataset using the huggingface-cli:\nhuggingface-cli download ml-jku/meta-world --local-dir=./meta-world --repo-type dataset\n\nFor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ml-jku/meta-world.","first_N":5,"first_N_keywords":["reinforcement-learning","mit","100K - 1M","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"ru_ifeval-like-data","keyword":"rlaif","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mizinovmv/ru_ifeval-like-data","creator_name":"maksim","creator_url":"https://huggingface.co/mizinovmv","description":"\n\t\n\t\t\n\t\tRussian IFEval Like Data\n\t\n\nThis dataset contains instruction-response pairs synthetically generated using Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8 and chatgpt-4o-latest following the style of google/IFEval dataset and verified for correctness with #TODO lm-evaluation-harness.\nThank you for the inspiration ifeval_like_dataset.\n","first_N":5,"first_N_keywords":["text-generation","Russian","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ball-maze-images","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sagar18/ball-maze-images","creator_name":"Basavasagar","creator_url":"https://huggingface.co/Sagar18","description":"\n\t\n\t\t\n\t\tDataset Card for Sagar18/ball-maze-images\n\t\n\nThis dataset contains visual observations of a ball maze environment, derived from the original state-space dataset at notmahi/tutorial-ball-top-20.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"Sagar18/ball-maze-images\")\n\n# Access images (they will be automatically decoded)\nimage = dataset[0][\"image\"]\nstate = dataset[0][\"state\"]\n\n","first_N":5,"first_N_keywords":["reinforcement-learning","robotics","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ball-maze-lerobot","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sagar18/ball-maze-lerobot","creator_name":"Basavasagar","creator_url":"https://huggingface.co/Sagar18","description":"\n\t\n\t\t\n\t\tBall Maze Environment Dataset\n\t\n\nThis dataset contains episodes of a ball maze environment, converted to the LeRobot format for visualization and training.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nSagar18/ball-maze-lerobot/\n‚îú‚îÄ‚îÄ data/                 # Main dataset files\n‚îú‚îÄ‚îÄ metadata/            # Dataset metadata\n‚îÇ   ‚îî‚îÄ‚îÄ info.json       # Configuration and version info\n‚îú‚îÄ‚îÄ episode_data_index.safetensors  # Episode indexing information\n‚îî‚îÄ‚îÄ stats.safetensors    # Dataset statistics\n\n\n\t\n\t\t\n\t\tFeatures‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sagar18/ball-maze-lerobot.","first_N":5,"first_N_keywords":["reinforcement-learning","robotics","mit","10K - 100K","arrow"],"keywords_longer_than_N":true},
	{"name":"SauerkrautLM-Fermented-GER-DPO","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-GER-DPO","creator_name":"VAGO solutions","creator_url":"https://huggingface.co/VAGOsolutions","description":"\n\n\t\n\t\t\n\t\tSauerkrautLM-Fermented-GER-DPO Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSauerkrautLM-Fermented-GER-DPO is a high-quality German instruction-response dataset specifically designed for Direct Preference Optimization (DPO) training. The dataset consists of 3,305 instruction-response pairs. Rather than being merged from existing German datasets, it was carefully created through a sophisticated augmentation process, transforming curated English instructions and responses into culturally adapted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-GER-DPO.","first_N":5,"first_N_keywords":["text-generation","German","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"SauerkrautLM-Fermented-Irrelevance-GER-DPO","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-Irrelevance-GER-DPO","creator_name":"VAGO solutions","creator_url":"https://huggingface.co/VAGOsolutions","description":"\n\n\t\n\t\t\n\t\tSauerkrautLM-Fermented-Irrelevance-GER-DPO Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSauerkrautLM-Fermented-Irrelevance-GER-DPO  is a specialized dataset designed for training language models in function calling irrelevance detection using Direct Preference Optimization (DPO). The dataset consists of 2,000 carefully evaluated instruction-response pairs, specifically curated to help models recognize situations where function calls are unnecessary and direct responses are more appropriate.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-Irrelevance-GER-DPO.","first_N":5,"first_N_keywords":["text-generation","German","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"poemma-10k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/0x7o/poemma-10k","creator_name":"Danil Kononyuk","creator_url":"https://huggingface.co/0x7o","description":"\n\t\n\t\t\n\t\tPoemma 10K\n\t\n\nDataset for naturalisations of language model outputs in the domain of poems.\n","first_N":5,"first_N_keywords":["Russian","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-TR-20k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\n\t\n\t\t\n\t\tORPO-DPO-Mix-TR-20k\n\t\n\n\nThis repository contains a Turkish translation of 20k records from the mlabonne/orpo-dpo-mix-40k dataset. The translation was carried out using the gemini-1.5-flash-002 model, chosen for its 1M token context size and overall accurate Turkish responses.\n\n\t\n\t\t\n\t\n\t\n\t\tTranslation Process\n\t\n\nThe translation process uses the LLM model with translation prompt and pydantic data validation to ensure accuracy. The complete translation pipeline is available in the GitHub‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k.","first_N":5,"first_N_keywords":["text2text-generation","question-answering","text-generation","Turkish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-TR-20k","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\n\t\n\t\t\n\t\tORPO-DPO-Mix-TR-20k\n\t\n\n\nThis repository contains a Turkish translation of 20k records from the mlabonne/orpo-dpo-mix-40k dataset. The translation was carried out using the gemini-1.5-flash-002 model, chosen for its 1M token context size and overall accurate Turkish responses.\n\n\t\n\t\t\n\t\n\t\n\t\tTranslation Process\n\t\n\nThe translation process uses the LLM model with translation prompt and pydantic data validation to ensure accuracy. The complete translation pipeline is available in the GitHub‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k.","first_N":5,"first_N_keywords":["text2text-generation","question-answering","text-generation","Turkish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"count_letters_in_word_base","keyword":"rlaif","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/JeanIbarz/count_letters_in_word_base","creator_name":"Jean Ibarz","creator_url":"https://huggingface.co/JeanIbarz","description":"\n\t\n\t\t\n\t\tDataset Card for count_letters_in_word\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset was generated using the nltk words corpus and a custom script to create tasks that require counting letters in words. Each example prompts the language model to count occurrences of specific letters within words. For added complexity, in 10% of cases, a letter that does not appear in the word is included. \nThe dataset is structured to provide both chosen (correct counts) and rejected (incorrect counts)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JeanIbarz/count_letters_in_word_base.","first_N":5,"first_N_keywords":["English","unlicense","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"frontend_dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/israellaguan/frontend_dpo","creator_name":"Israel Antonio Rosales Laguan","creator_url":"https://huggingface.co/israellaguan","description":"\n\t\n\t\t\n\t\tDPO JavaScript Dataset\n\t\n\nThis repository contains a modified and expanded version of a closed-source JavaScript dataset. The dataset has been adapted to fit the DPO (Dynamic Programming Object) format, making it compatible with the LLaMA-Factory project. The dataset includes a variety of JavaScript code snippets with optimizations and best practices, generated using closed-source tools and expanded by me.\n\n\t\n\t\t\n\t\n\t\n\t\tLicense\n\t\n\nThis dataset is licensed under the Apache 2.0 License.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/israellaguan/frontend_dpo.","first_N":5,"first_N_keywords":["text-generation","dialogue-generation","human-generated","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita-reranked","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\n\t\n\t\t\n\t\tEvol DPO Ita Reranked\n\t\n\n\nA high-quality Italian preference dataset suitable for Direct Preference Optimization (DPO), ORPO, and other Preference Tuning algorithms.\n\n\t\n\t\t\n\t\n\t\n\t\tü•áü•à Reranking process\n\t\n\nThis work is based on efederici/evol-dpo-ita, a nice Italian preference dataset.\nThe original dataset includes prompts translated from the Evol-Instruct datasets, with responses generated using GPT-3.5-Turbo (rejected) and claude-3-opus-20240229 (chosen).\nChoosing the response from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked.","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita-reranked","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\n\t\n\t\t\n\t\tEvol DPO Ita Reranked\n\t\n\n\nA high-quality Italian preference dataset suitable for Direct Preference Optimization (DPO), ORPO, and other Preference Tuning algorithms.\n\n\t\n\t\t\n\t\n\t\n\t\tü•áü•à Reranking process\n\t\n\nThis work is based on efederici/evol-dpo-ita, a nice Italian preference dataset.\nThe original dataset includes prompts translated from the Evol-Instruct datasets, with responses generated using GPT-3.5-Turbo (rejected) and claude-3-opus-20240229 (chosen).\nChoosing the response from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked.","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"HalfCheetah-v4","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v4","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tHalfCheetah-v4 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 9809.9417.\nEach entry consists of:\nobs (list): observation with length 2.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_starts (bool): if that state was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v4.","first_N":5,"first_N_keywords":["mit","1M - 10M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"ORPRO-Spider-SQL-Feedback","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mjerome89/ORPRO-Spider-SQL-Feedback","creator_name":"Maurice","creator_url":"https://huggingface.co/mjerome89","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for ORPRO-Spider-SQL-Feedback\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/mjerome89/ORPRO-Spider-SQL-Feedback/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mjerome89/ORPRO-Spider-SQL-Feedback.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"SQL-Ultrafeedback","keyword":"rlaif","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/mjerome89/SQL-Ultrafeedback","creator_name":"Maurice","creator_url":"https://huggingface.co/mjerome89","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for SQL-Ultrafeedback\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/mjerome89/SQL-Ultrafeedback/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mjerome89/SQL-Ultrafeedback.","first_N":5,"first_N_keywords":["wtfpl","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Spider_gpt4o","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mjerome89/Spider_gpt4o","creator_name":"Maurice","creator_url":"https://huggingface.co/mjerome89","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for Spider_gpt4o\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/mjerome89/Spider_gpt4o/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mjerome89/Spider_gpt4o.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"web_agents_google_flight_trajectories","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/anonx3247/web_agents_google_flight_trajectories","creator_name":"Anas lecaillon","creator_url":"https://huggingface.co/anonx3247","description":"\n\t\n\t\t\n\t\tWeb Agent Google Flight Trajectories\n\t\n\nThis dataset was originally created on Nov 23 2024 during EF's Ai On Edge Hackathon.\nThe purpose of this dataset is to give both positive and negative image web-agent trajectories to finetune small edge-models on web agentic tasks.\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"reasoning-1-1k","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fluently-sets/reasoning-1-1k","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","description":"\n\t\n\t\t\n\t\tReasoning-1 1K\n\t\n\n\n\t\n\t\t\n\t\tShort about\n\t\n\nThis dataset will help in SFT training of LLM on the Alpaca format.\nThe goal of the dataset: to teach LLM to reason and analyze its mistakes using SFT training.\nThe size of 1.15K is quite small, so for effective training on SFTTrainer set 4-6 epochs instead of 1-3.\nMade by Fluently Team (@ehristoforu) using distilabel with loveü•∞\n\n\t\n\t\t\n\t\n\t\n\t\tDataset structure\n\t\n\nThis subset can be loaded as:\nfrom datasets import load_dataset\n\nds =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/reasoning-1-1k.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"RSL_Maran","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran.","first_N":5,"first_N_keywords":["token-classification","table-question-answering","question-answering","text-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"UltraFeedback","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/juyoungml/UltraFeedback","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/juyoungml","description":"juyoungml/UltraFeedback dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"UltraFeedback-relabeled-binarized","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/juyoungml/UltraFeedback-relabeled-binarized","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/juyoungml","description":"juyoungml/UltraFeedback-relabeled-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"LongReward-10k","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongReward-10k","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","description":"\n\t\n\t\t\n\t\tLongReward-10k\n\t\n\n\n  üíª [Github Repo] ‚Ä¢ üìÉ [LongReward Paper] \n\n\nLongReward-10k dataset contains 10,000 long-context QA instances (both English and Chinese, up to 64,000 words). \nThe sft split contains SFT data generated by GLM-4-0520, following the self-instruct method in LongAlign. Using this split, we supervised fine-tune two models: LongReward-glm4-9b-SFT and LongReward-llama3.1-8b-SFT, which are based on GLM-4-9B and Meta-Llama-3.1-8B, respectively. \nThe dpo_glm4_9b and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongReward-10k.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_preferences","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/truthful_qa_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"wassname/truthful_qa_preferences dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","text-generation","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_preferences","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/truthful_qa_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"wassname/truthful_qa_preferences dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","text-generation","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"genies_preferences","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/genies_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"\n\t\n\t\t\n\t\tDataset Card for \"genie_dpo\"\n\t\n\nA conversion of the distribution from GENIES to open_pref_eval format.\nConversion code\n","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"genies_preferences","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/genies_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"\n\t\n\t\t\n\t\tDataset Card for \"genie_dpo\"\n\t\n\nA conversion of the distribution from GENIES to open_pref_eval format.\nConversion code\n","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"customer-support-finetuning-dataset","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/debabrata-ai/customer-support-finetuning-dataset","creator_name":"Debabrata Bordoloi","creator_url":"https://huggingface.co/debabrata-ai","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for customer-support-finetuning-dataset\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/debabrata-ai/customer-support-finetuning-dataset/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/debabrata-ai/customer-support-finetuning-dataset.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"gsm8k-reasoning","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thesven/gsm8k-reasoning","creator_name":"Michael Svendsen","creator_url":"https://huggingface.co/thesven","description":"\n\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for gsm8k-reasoning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nGSM8K Reasoning is a dataset derived from the openai/gsm8k dataset, focusing on enhancing math problem-solving through reasoning-based prompts and solutions.\nThis version emphasizes logical reasoning and step-by-step thought processes in mathematics, pushing models to generate solutions that reflect human-like deductive reasoning.\nThe dataset is curated using a specialized pipeline designed to encourage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thesven/gsm8k-reasoning.","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Aloe-Beta-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-DPO","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tAloe-Beta-Medical-Collection\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated DPO datasets used to align Aloe-Beta.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThe first stage of the Aloe-Beta alignment process. We curated data from many publicly available data sources, including three different types of data:\n\nMedical preference data: TsinghuaC3I/UltraMedical-Preference\n\nGeneral preference data:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-DPO.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wmdp-cyber-corpus_unpaired-preference","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AMindToThink/wmdp-cyber-corpus_unpaired-preference","creator_name":"Matthew Khoriaty","creator_url":"https://huggingface.co/AMindToThink","description":"This is the \"cyber\" data from https://huggingface.co/datasets/cais/wmdp-corpora repackaged into the format of \"unpaired preference\". https://huggingface.co/docs/trl/v0.11.1/en/dataset_formats#unpaired-preference\nData to retain is considered good so it is mapped to \"True\", while forget data is mapped to \"False.\"\nPrompt is left empty since the text from WMDP doesn't come with a prompt.\nI have no idea whether this would really work for Reinforcement Learning, but I plan to try it out. Use at your‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AMindToThink/wmdp-cyber-corpus_unpaired-preference.","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"PFT-MME","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/PFT-MME","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\n\t\n\t\t\n\t\tPFT-MME: Preference Finetuning Tally-Multi-Model Evaluation Dataset\n\t\n\n\nThe Preference Finetuning Tally-Multi-Model Evaluation (PFT-MME) dataset is meticulously curated by aggregating responses (n=6) from multiple models across (relatively simple) general task prompts. \nThese responses undergo evaluation by a panel (n=3) of evaluator models, assigning scores to each answer. \nThrough a tallied voting mechanism, average scores are calculated to identify the \"worst\" and \"best\" answers‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/PFT-MME.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Agentic-Long-Context-Understanding-QA","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yzhuang/Agentic-Long-Context-Understanding-QA","creator_name":"Yufan Zhuang","creator_url":"https://huggingface.co/yzhuang","description":" üìñ Agentic Long Context Understanding üìñ \n Self-Taught Agentic Long Context Understanding  (Arxiv). \n\n\n\n  \n  \n  \n\n AgenticLU refines complex, long-context queries through self-clarifications and contextual grounding, enabling robust long-document understanding in a single pass.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tInstallation Requirements\n\t\n\nThis codebase is largely based on OpenRLHF and Helmet, kudos to them.\nThe requirements are the same\npip install openrlhf\npip install -r ./HELMET/requirements.txt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yzhuang/Agentic-Long-Context-Understanding-QA.","first_N":5,"first_N_keywords":["question-answering","reinforcement-learning","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"my-distiset-bedefff0","keyword":"rlaif","license":"Artistic License 2.0","license_url":"https://choosealicense.com/licenses/artistic-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Juno360219/my-distiset-bedefff0","creator_name":"Jessie Rice","creator_url":"https://huggingface.co/Juno360219","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for my-distiset-bedefff0\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/Juno360219/my-distiset-bedefff0/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Juno360219/my-distiset-bedefff0.","first_N":5,"first_N_keywords":["text-classification","artistic-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"DATA-AI_Chat","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mattimax/DATA-AI_Chat","creator_name":"M.INC.","creator_url":"https://huggingface.co/Mattimax","description":"\n\t\n\t\t\n\t\tDATA-AI: Il Modello di IA di M.INC.\n\t\n\n\n\t\n\t\t\n\t\tüìå Introduzione\n\t\n\nDATA-AI √® un avanzato modello di intelligenza artificiale sviluppato da *M.INC., un'azienda italiana fondata da *Mattimax (M. Marzorati).Questo modello √® basato sull'architettura ELNS (Elaborazione del Linguaggio Naturale Semplice), un sistema innovativo progettato per rendere l'IA accessibile su quasi qualsiasi dispositivo, garantendo prestazioni ottimali anche su hardware limitato.  \nDATA-AI √® stato addestrato su un‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mattimax/DATA-AI_Chat.","first_N":5,"first_N_keywords":["Italian","English","Spanish","Russian","German"],"keywords_longer_than_N":true},
	{"name":"boxoban-astar-solutions","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlignmentResearch/boxoban-astar-solutions","creator_name":"FAR AI","creator_url":"https://huggingface.co/AlignmentResearch","description":"\n\t\n\t\t\n\t\n\t\n\t\tA* solutions to Boxoban levels\n\t\n\nFor some levels we were not able to find solutions within the allotted A* budget. These have solution\nSEARCH_STATE_FAILED or NOT_FOUND. These are the ones labeled \"Unsolved levels\" below.\nThe search budget was 5 million nodes to expand for medium-difficulty levels, vs. 1 million nodes for\nunfiltered-difficulty levels. The heuristic was the sum of Manhattan distances of each box to its closest target.\n\n\t\n\t\t\n\t\n\t\n\t\tSummary table:\n\t\n\n\n\t\n\t\t\nLevel file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlignmentResearch/boxoban-astar-solutions.","first_N":5,"first_N_keywords":["reinforcement-learning","apache-2.0","1M<n<10M","üá∫üá∏ Region: US","sokoban"],"keywords_longer_than_N":true},
	{"name":"oasst2_dpo_pairs_enth","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/oasst2_dpo_pairs_enth","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\n\t\n\t\t\n\t\tOASST2 DPO Pairs English and Thai\n\t\n\nThis dataset contains message ChatML. It was create from Open Assistant Conversations Dataset Release 2 (OASST2). You can use to do human preference optimization (DPO, ORPO, and other).\n\n\t\n\t\t\n\t\tSelect Thai only\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"pythainlp/oasst2_dpo_pairs_enth\",split=\"train\")\nthai_dataset = dataset.filter(lambda example: example['lang']==\"th\") # if you want to use English only, change to \"en\".\n\nlicense:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/oasst2_dpo_pairs_enth.","first_N":5,"first_N_keywords":["text-generation","English","Thai","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Buzz-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"bonanza-hf","keyword":"human-feedback","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bertin-project/bonanza-hf","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","description":"\n\t\n\t\t\n\t\tBonanza: Dataset de instrucciones en Espa√±ol y Catal√°n\n\t\n\nEste dataset combina m√∫ltiples fuentes para proporcionar instrucciones en espa√±ol y catal√°n. Los datasets combinados son los siguientes:\n\nOpenAssistant/oasst2\nCohereForAI/aya_dataset\nprojecte-aina/RAG_Multilingual\nbertin-project/alpaca-spanish\ndariolopez/Llama-2-databricks-dolly-oasst1-es\nprojecte-aina/MentorESprojecte-aina/MentorCA\n\n\n\t\n\t\t\n\t\tDescripci√≥n\n\t\n\nEste conjunto de datos proporciona una rica colecci√≥n de instrucciones‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bertin-project/bonanza-hf.","first_N":5,"first_N_keywords":["Spanish","Catalan","cc-by-sa-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Aya-Command.R-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-Command.R-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"Aya-Command.R-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-Command.R-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled as \"chosen,\" with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-Command.R-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"argilla-ultrafeedback-binarized-preferences-cleaned","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/csarron/argilla-ultrafeedback-binarized-preferences-cleaned","creator_name":"Qingqing Cao","creator_url":"https://huggingface.co/csarron","description":"\n\t\n\t\t\n\t\tUltraFeedback (Cleaned)\n\t\n\nThis dataset combines the train split of argilla/ultrafeedback-binarized-preferences-cleaned,\nand test split of HuggingFaceH4/ultrafeedback_binarized.\n","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Aya-SambaLingo.Arabic.Chat-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-SambaLingo.Arabic.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tDataset Card for \"Aya-SambaLingo.Arabic.Chat-DPO\" ü§ó\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-SambaLingo.Arabic.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-SambaLingo.Arabic.Chat-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/multilingual/orca_dpo_pairs","creator_name":"mLLM multilingual","creator_url":"https://huggingface.co/multilingual","description":"\n    \n\n\nmLLM IMPLEMENTATION OF Intel/orca_dpo_pairs.\nLANGUAGES:\nARABIC\nCHINESE\nFRENCH\nGERMAN\nRUSSIAN\nSPANISH\nTURKISH\n(WIP)\n","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","German","French"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/multilingual/orca_dpo_pairs","creator_name":"mLLM multilingual","creator_url":"https://huggingface.co/multilingual","description":"\n    \n\n\nmLLM IMPLEMENTATION OF Intel/orca_dpo_pairs.\nLANGUAGES:\nARABIC\nCHINESE\nFRENCH\nGERMAN\nRUSSIAN\nSPANISH\nTURKISH\n(WIP)\n","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","German","French"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-nosafe","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/maywell/hh-rlhf-nosafe","creator_name":"Jeonghwan Park","creator_url":"https://huggingface.co/maywell","description":"\n\t\n\t\t\n\t\tDataset Card for HH-RLHF\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to two different kinds of data:\n\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maywell/hh-rlhf-nosafe.","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"formatted-hh-rlhf","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Estwld/formatted-hh-rlhf","creator_name":"zhangyiqun","creator_url":"https://huggingface.co/Estwld","description":"Estwld/formatted-hh-rlhf dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"dpo-mix-7k-SHORT","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/dpo-mix-7k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\n\t\n\t\t\n\t\tSHORTENED Argilla DPO Mix 7K Dataset\n\t\n\nThis is is a shortened version of the argilla/dpo-mix-7k dataset, shortened in two ways:\n\nFilter out all rows with chosen content exceeding 2,000 characters.\nFilter out all rows with the final assistant message of content exceeding 500 characters.\n\nThe original dataset card follows below.\n\n\nA small cocktail combining DPO datasets built by Argilla with distilabel. The goal of this dataset is having a small, high-quality DPO dataset by filtering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/dpo-mix-7k-SHORT.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-SHORT","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\n\t\n\t\t\n\t\tSHORTENED - ORPO-DPO-mix-40k v1.1\n\t\n\nFILTERED to remove rows with chosen responses longer than 2k characters or with a final assistant message longer than 500 characters.\n\nThe original dataset card follows below.\n\n\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.1\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-SHORT","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\n\t\n\t\t\n\t\tSHORTENED - ORPO-DPO-mix-40k v1.1\n\t\n\nFILTERED to remove rows with chosen responses longer than 2k characters or with a final assistant message longer than 500 characters.\n\nThe original dataset card follows below.\n\n\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.1\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"airbnb-stock-price","keyword":"reinforcement-learning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BatteRaquette58/airbnb-stock-price","creator_name":"Batte The Idiot","creator_url":"https://huggingface.co/BatteRaquette58","description":"\n\t\n\t\t\n\t\tAirbnb Stock Price dataset\n\t\n\nA simple dataset containing 746 rows of Airbnb's stock price.\n\n\t\n\t\t\n\t\tDataset structure\n\t\n\n\ndate (float64): Date of the stock price expressed as the epoch from 1/1/1970.\nopen (float64): Price of the stock when the stock market opened.\nclose_last (float64): Price of the stock when the stock market closed.\nvolume (float64): Number of shares traded.\nhigh (float64): Highest price of the stock during the day.\nlow (float64): Lowest price of the stock during the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BatteRaquette58/airbnb-stock-price.","first_N":5,"first_N_keywords":["reinforcement-learning","English","cc-by-sa-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"WebInstructSub-prometheus","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chargoddard/WebInstructSub-prometheus","creator_name":"Charles Goddard","creator_url":"https://huggingface.co/chargoddard","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for WebInstructSub-prometheus\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTIGER-Lab/WebInstructSub evaluated for logical and effective reasoning using prometheus-7b-v2.0.\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chargoddard/WebInstructSub-prometheus.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"untested-WiP-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/untested-WiP-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/untested-WiP-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/HelpSteer2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\t\n\t\t\n\t\tHelpSteer2: Open-source dataset for training top-performing reward models\n\t\n\nHelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nThis dataset has been created in partnership with Scale  AI. \nWhen used with a Llama 3 70B Base Model, we achieve 88.8% on RewardBench, which makes it the 4th best Reward Model as of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/HelpSteer2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"LONG-DPO","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hassanjbara/LONG-DPO","creator_name":"Hassan Jbara","creator_url":"https://huggingface.co/hassanjbara","description":"This dataset is a DPO version of the hassanjbara/LONG  dataset. The preference model used for choosing and rejecting responses is Hello-SimpleAI/chatgpt-detector-roberta, with the idea being creating a dataset for training a model to \"beat\" that detector. Full script for generating the dataset included in the scripts directory.\nThe chosen responses were generated by mistralai/Mistral-Nemo-Instruct-2407 and gpt-3.5-turbo.\n","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned","creator_name":"Farouk","creator_url":"https://huggingface.co/pharaouk","description":"\n\t\n\t\t\n\t\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\n\n\t\n\t\n\t\n\t\tDifferences with argilla/ultrafeedback-binarized-preferences‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned.","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"oasst2_french_dpo_pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/oasst2_french_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for oasst2_french_dpo_pairs\n\t\n\nThis dataset was created from OpenAssistant/oasst2 by keeping only the french data and producing dpo pairs with their rank.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"oasst2_french_dpo_pairs","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/oasst2_french_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for oasst2_french_dpo_pairs\n\t\n\nThis dataset was created from OpenAssistant/oasst2 by keeping only the french data and producing dpo pairs with their rank.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"oasst2_french_dpo_pairs","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/oasst2_french_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for oasst2_french_dpo_pairs\n\t\n\nThis dataset was created from OpenAssistant/oasst2 by keeping only the french data and producing dpo pairs with their rank.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"gemini_orpo_dpo_ptbr","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/gemini_orpo_dpo_ptbr","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/gemini_orpo_dpo_ptbr dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","question-answering","Portuguese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"orcaratgen","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/redsgnaoh/orcaratgen","creator_name":"Hoang Anh Just","creator_url":"https://huggingface.co/redsgnaoh","description":"An rationale-enhanced version of the paired preference learning dataset Intel-ORCA-DPO.\nThese rationales are general, high-level explanation of why the chosen response is preferred over the rejected response.\nThe dataset was generated according to this paper: Data-Centric Human Preference Optimization with\nRationales.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Agentic-DPO-V0.1","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Capx/Agentic-DPO-V0.1","creator_name":"Capx AI","creator_url":"https://huggingface.co/Capx","description":"\n\t\n\t\t\n\t\tAgentic DPO V1.0\n\t\n\n\n\nThe Capx Agentic DPO (Direct Prompt Optimization) Dataset is a unique collection of prompts, chosen answers, and rejected answers designed to train and optimize AI models for agentic and intuitive processing. \n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset covers a wide range of topics, including but not limited to problem-solving, creativity, analysis, and general knowledge. The prompts are specifically crafted to elicit agentic responses from the AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Capx/Agentic-DPO-V0.1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/ultrafeedback-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"Ultrafeedback binarized dataset using the mean of preference ratings by Argilla. \nThey addressed TruthfulQA-related data contamination in this version.\nSteps:\n\nCompute mean of preference ratings (honesty, instruction-following, etc.)\nPick the best mean rating as the chosen\nPick random rejected with lower mean (or another random if equal to chosen rating)\nFilter out examples with chosen rating == rejected rating\n\nReference:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/ultrafeedback-binarized.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs-binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/distilabel-intel-orca-dpo-pairs-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"This is the binarized version of distilabel Orca Pairs for DPO and ORPO. \nReference: https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs?row=0\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"multiturn-capybara-preferences-filtered-binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","description":"This dataset has been created with distilabel, plus some extra post-processing steps described below.\nDataset Summary\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\n\nRemove responses from the assistant, not only in the last turn, but also in intermediate turns where the assistant responds with a potential refusal and/or some ChatGPT-isms such as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"french_hh_rlhf","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_hh_rlhf","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for french_hh_rlhf\n\t\n\nThis dataset offers a french translation of the famous Anthropic/hh-rlhf dataset in order to improve the allignement work/research in the french NLP community.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["French","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"french_hh_rlhf","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_hh_rlhf","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for french_hh_rlhf\n\t\n\nThis dataset offers a french translation of the famous Anthropic/hh-rlhf dataset in order to improve the allignement work/research in the french NLP community.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["French","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"french_hh_rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_hh_rlhf","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for french_hh_rlhf\n\t\n\nThis dataset offers a french translation of the famous Anthropic/hh-rlhf dataset in order to improve the allignement work/research in the french NLP community.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["French","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"marathi-hh-rlhf-v02","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/amitagh/marathi-hh-rlhf-v02","creator_name":"Amit","creator_url":"https://huggingface.co/amitagh","description":"\n\t\n\t\t\n\t\tDataset card for Marathi HH-RLHF\n\t\n\nTranslated subset of HH-RLHF version for Marathi language.\n","first_N":5,"first_N_keywords":["reinforcement-learning","Marathi","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"oasst2_uzbek","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MLDataScientist/oasst2_uzbek","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","description":"\n\t\n\t\t\n\t\tOpen Assistant Conversations Dataset Release 2 (OASST2) in Uzbek language\n\t\n\nThis dataset is an Uzbek translated version of OASST2 dataset.\nLlama3 chat template + thread formatted dataset based on this translation is also available for model fine-tuning here. \nThe Uzbek translation was completed in 45 hours using a single T4 GPU and nllb-200-3.3B model.\nBased on nllb metrics, you might want to only filter out records that were not originally in English or Russian since English-Uzbek‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MLDataScientist/oasst2_uzbek.","first_N":5,"first_N_keywords":["question-answering","translation","Uzbek","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"rwp-prometheus","keyword":"rlaif","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chargoddard/rwp-prometheus","creator_name":"Charles Goddard","creator_url":"https://huggingface.co/chargoddard","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for rwp-prometheus\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/chargoddard/rwp-prometheus/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chargoddard/rwp-prometheus.","first_N":5,"first_N_keywords":["cc-by-sa-4.0","100K - 1M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"persian-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/myrkur/persian-dpo","creator_name":"Amir Masoud Ahmadi","creator_url":"https://huggingface.co/myrkur","description":"\n\t\n\t\t\n\t\tPersian Alpaca Preference Dataset\n\t\n\n\nThis repository contains the Persian translation of the original Alpaca dataset, along with additional preference data generated using the LLama3 70B model. The dataset has been prepared for language model alignment using Direct Preference Optimization (DPO) or similar methods. It consists of approximately 39,000 Persian records.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tOriginal Alpaca Dataset\n\t\n\nThe Alpaca dataset is a collection of text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/myrkur/persian-dpo.","first_N":5,"first_N_keywords":["text-generation","Persian","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"prm_dpo_pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/M4-ai/prm_dpo_pairs","creator_name":"M4-ai","creator_url":"https://huggingface.co/M4-ai","description":"\n\t\n\t\t\n\t\tprm_dpo_pairs\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nprm_dpo_pairs is a curated version of the PRM800K dataset designed for ease of use when fine-tuning a language model using the DPO (Direct Preference Optimization) technique. The dataset contains pairs of prompts and completions, with labels indicating which completion was preferred by the original language model.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of the following features:\n\nprompt: The input prompt or question posed to the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/M4-ai/prm_dpo_pairs.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"audio-alpaca","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/declare-lab/audio-alpaca","creator_name":"Deep Cognition and Language Research (DeCLaRe) Lab","creator_url":"https://huggingface.co/declare-lab","description":"\n\t\n\t\t\n\t\tAudio-alpaca: A preference dataset for aligning text-to-audio models\n\t\n\nAudio-alpaca is a pairwise preference dataset containing about 15k (prompt,chosen, rejected) triplets where given a textual prompt, chosen is the preferred generated audio and rejected is the undesirable audio.\n\n\t\n\t\t\n\t\tField details\n\t\n\nprompt: Given textual prompt\nchosen: The preferred audio sample\nrejected: The rejected audio sample\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Audio"],"keywords_longer_than_N":true},
	{"name":"Capybara-Preferences","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/Capybara-Preferences","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for Capybara-Preferences\n\t\n\nThis dataset has been created with distilabel.\n\n    \n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is built on top of LDJnr/Capybara, in order to generate a preference\ndataset out of an instruction-following dataset. This is done by keeping the conversations in the column conversation but splitting\nthe last assistant turn from it, so that the conversation contains all the turns up until the last user's turn, so that it can be reused‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Capybara-Preferences.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"DPO-En-Zh-20k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llamafactory/DPO-En-Zh-20k","creator_name":"LLaMA Factory","creator_url":"https://huggingface.co/llamafactory","description":"This dataset is composed by\n\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\n10,000 examples of wenbopan/Chinese-dpo-pairs.\n\nYou can use it in LLaMA Factory by specifying dataset: dpo_mix_en,dpo_mix_zh.\n","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"DPO-En-Zh-20k","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llamafactory/DPO-En-Zh-20k","creator_name":"LLaMA Factory","creator_url":"https://huggingface.co/llamafactory","description":"This dataset is composed by\n\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\n10,000 examples of wenbopan/Chinese-dpo-pairs.\n\nYou can use it in LLaMA Factory by specifying dataset: dpo_mix_en,dpo_mix_zh.\n","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-50k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-50k\n\t\n\nThis dataset is designed for DPO or ORPO training.\nThis dataset combines samples of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\nargilla/distilabel-math-preference-dpo: highly scored chosen‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-50k","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-50k\n\t\n\nThis dataset is designed for DPO or ORPO training.\nThis dataset combines samples of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\nargilla/distilabel-math-preference-dpo: highly scored chosen‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/juyoungml/HelpSteer2","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/juyoungml","description":"juyoungml/HelpSteer2 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2-binarized","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/juyoungml/HelpSteer2-binarized","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/juyoungml","description":"juyoungml/HelpSteer2-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ElectricalDeviceFeedbackBalanced","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/disham993/ElectricalDeviceFeedbackBalanced","creator_name":"Doula Isham Rashik Hasan","creator_url":"https://huggingface.co/disham993","description":"\n\t\n\t\t\n\t\tElectricalDeviceFeedbackBalanced\n\t\n\nThis dataset contains balanced feedback on electrical devices focusing on smart meters, solar panels, circuit breakers, inverters etc. It extends the original Electrical Device Feedback dataset with additional data generated through Large Language Models (LLMs) and careful prompt engineering.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nTrain: 11,552 samples with balanced class distribution \n\nMixed: 3,143\nNegative: 2,975\nNeutral: 2,762\nPositive: 2,672\nNote: Balance‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/disham993/ElectricalDeviceFeedbackBalanced.","first_N":5,"first_N_keywords":["text-classification","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MATH-500-Overall","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fluently-sets/MATH-500-Overall","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","description":"\n\t\n\t\t\n\t\tMATH-500-Overall\n\t\n\n\n\t\n\t\t\n\t\tAbout the dataset\n\t\n\nThis dataset of only 500 examples combines mathematics, physics and logic in English with reasoning and step-by-step problem solving, the dataset was created synthetically, CoT of Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct.\n\n\t\n\t\t\n\t\tBrief information\n\t\n\n\nNumber of rows: 500\nType of dataset files: parquet\nType of dataset: text, alpaca with system prompts\nLanguage: English\nLicense: MIT\n\nStructure:\nmath¬Ø¬Ø¬Ø¬Ø¬Ø‚åâ\n   school-level (100 rows)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/MATH-500-Overall.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","text-classification","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"data-science-sentetic-data","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emredeveloper/data-science-sentetic-data","creator_name":"C. Emre Karata≈ü","creator_url":"https://huggingface.co/emredeveloper","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for data-science-sentetic-data\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/emredeveloper/data-science-sentetic-data/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/emredeveloper/data-science-sentetic-data.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"after_visit_summary_simulated_edits","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card: AVS edits Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTrain Split‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits.","first_N":5,"first_N_keywords":["summarization","reinforcement-learning","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"after_visit_summary_simulated_edits","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card: AVS edits Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTrain Split‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits.","first_N":5,"first_N_keywords":["summarization","reinforcement-learning","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"after_visit_summary_simulated_edits","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card: AVS edits Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTrain Split‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits.","first_N":5,"first_N_keywords":["summarization","reinforcement-learning","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"after_visit_summary_simulated_edits","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card: AVS edits Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTrain Split‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits.","first_N":5,"first_N_keywords":["summarization","reinforcement-learning","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"KTO-mix-14k-vietnamese-groq","keyword":"rl","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/1TuanPham/KTO-mix-14k-vietnamese-groq","creator_name":"Pham Minh Tuan","creator_url":"https://huggingface.co/1TuanPham","description":"Original dataset: https://huggingface.co/datasets/trl-lib/kto-mix-14k\nThis dataset is a KTO-formatted version of argilla/dpo-mix-7k. Please cite the original dataset if you find it useful in your work.\n\nTranslated to Vietnamese with context-aware using Groq Llama3.3 70B* via this repo:\nhttps://github.com/vTuanpham/Large_dataset_translator.\nRoughly 9 hours for 2k examples.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nkto_mix_14k_vi =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/1TuanPham/KTO-mix-14k-vietnamese-groq.","first_N":5,"first_N_keywords":["question-answering","text-generation","text2text-generation","Vietnamese","English"],"keywords_longer_than_N":true},
	{"name":"Reflective-MAGLLAMA-v0.1","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1","creator_name":"Michael Svendsen","creator_url":"https://huggingface.co/thesven","description":"\n\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Reflective-MAGLLAMA-v0.1\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tPlease Use v0.1.1\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nReflective MAGLLAMA is a dataset created using MAGPIE-generated prompts in combination with reflection pattern responses generated by the LLaMa 3.1 70B model.\nThis dataset is tailored specifically to encourage reflection-based analysis through reflection prompting, a technique that enhances deeper thinking and learning.\nIt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"anthropic-hh-dpo","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/eborgnia/anthropic-hh-dpo","creator_name":"Eitan Borgnia","creator_url":"https://huggingface.co/eborgnia","description":"\n\t\n\t\t\n\t\tAnthropic Helpful/Harmful Dataset for Llama 3 Instruct\n\t\n\nThis is a formatted version of the Anthropic helpful/harmful dataset, preprocessed to work with the Llama 3 instruct template.\n\n\t\n\t\t\n\t\tUsage with HuggingFace Transformers\n\t\n\nIf you are using the HuggingFace transformers library, ensure you are using the default chat template. This should add the <|begin_of_text|> token to the start of the input, but nothing else.\n\n\t\n\t\t\n\t\tVerifying the Format\n\t\n\nTo make sure the format is correct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eborgnia/anthropic-hh-dpo.","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"Reflective-MAGLLAMA-v0.1.1","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1.1","creator_name":"Michael Svendsen","creator_url":"https://huggingface.co/thesven","description":"\n\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Reflective-MAGLLAMA-v0.1.1\n\t\n\nThis dataset has been created with distilabel.\nSome changes were made from v1:\n\nadditional cleaning has been done to the generations to ensure proper tag structure\na new reflections column has been added.\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nReflective MAGLLAMA is a dataset created using MAGPIE-generated prompts in combination with reflection pattern responses generated by the LLaMa 3.1 70B model.\nThis dataset is tailored specifically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1.1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"python-lib-tools-v0.1","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/python-lib-tools-v0.1","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for python-lib-tools-v0.1\n\t\n\nThis dataset has been created with distilabel.\nThe pipeline script was uploaded to easily reproduce the dataset:\npython_tool_synth.py.\nIt can be run directly using the CLI:\ndistilabel pipeline run --script \"https://huggingface.co/datasets/argilla-warehouse/python-lib-tools-v0.1/raw/main/python_tool_synth.py\"\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset creation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDistilabel pipeline\n\t\n\nRequirements:\n# A new virtual environment with python‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/python-lib-tools-v0.1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"axay-javascript-dataset-pn","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/israellaguan/axay-javascript-dataset-pn","creator_name":"Israel Antonio Rosales Laguan","creator_url":"https://huggingface.co/israellaguan","description":"\n\t\n\t\t\n\t\tDPO JavaScript Dataset\n\t\n\nThis repository contains a modified version of the JavaScript dataset originally sourced from axay/javascript-dataset-pn. The dataset has been adapted to fit the DPO (Dynamic Programming Object) format, making it compatible with the LLaMA-Factory project.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is licensed under the Apache 2.0 License.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThe dataset consists of JavaScript code snippets that have been restructured and enhanced for use in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/israellaguan/axay-javascript-dataset-pn.","first_N":5,"first_N_keywords":["text-generation","dialogue-generation","machine-generated","machine-generated","axay/javascript-dataset-pn"],"keywords_longer_than_N":true},
	{"name":"synth-priv-v0.1","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/narodr/synth-priv-v0.1","creator_name":"Nicol√°s","creator_url":"https://huggingface.co/narodr","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for synth-priv-v0.1\n\t\n\nThis dataset has been created with distilabel.\nThe pipeline script was uploaded to easily reproduce the dataset:\npipeline.py.\nIt can be run directly using the CLI:\ndistilabel pipeline run --script \"https://huggingface.co/datasets/narodr/synth-priv-v0.1/raw/main/pipeline.py\"\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/narodr/synth-priv-v0.1.","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1-results","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-results","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\n\t\n\t\t\n\t\tDataset Card for image-preferences-results\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world, vibrant colors, 4K.\n      \n          \n              \n              Image 1\n          \n          \n              \n              Image 2\n          \n      \n  \n\n\n\n  \n      Prompt: 8-bit pixel art of a blue knight, green car, and glacier landscape in Norway, fantasy style, colorful and detailed.\n          \n              \n              Image 1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-results.","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo_binarized","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo_binarized","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo_binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mimicgen59","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ml-jku/mimicgen59","creator_name":"Institute for Machine Learning, Johannes Kepler University Linz","creator_url":"https://huggingface.co/ml-jku","description":"\n\t\n\t\t\n\t\tMimicgen 59\n\t\n\nThis repository contains the generated Mimicgen datasets as used in \"A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks\": \n\ntrajectories for each task are stored in a separate .tar.gz\nevery trajectory is stored in a separate .hdf5 file.\n\nFor more information on Mimicgen, we refer to the original documentation: https://mimicgen.github.io/docs/introduction/overview.html.\nDownload the dataset using:\nhuggingface-cli download ml-jku/mimicgen59‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ml-jku/mimicgen59.","first_N":5,"first_N_keywords":["reinforcement-learning","mit","10K - 100K","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"dm_control_10M","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ml-jku/dm_control_10M","creator_name":"Institute for Machine Learning, Johannes Kepler University Linz","creator_url":"https://huggingface.co/ml-jku","description":"\n\t\n\t\t\n\t\tDMControl 10M\n\t\n\nThis repository contains the DMControl datasets as used in \"A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks\": \n\nTrajectories for every task are stored as separate .tar.gz files. \nEvery .tar.gz file contains 10M transitions.\nEvery trajectory is stored as a separate .hdf5 file.\n\nDownload the dataset using the huggingface-cli:\nhuggingface-cli download ml-jku/dm_control_10M --local-dir=./dm_control_10M --repo-type dataset\n\nFor dataloading we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ml-jku/dm_control_10M.","first_N":5,"first_N_keywords":["reinforcement-learning","mit","100K - 1M","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"nvidia-HelpSteer2","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Triangle104/nvidia-HelpSteer2","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","description":"\n\t\n\t\t\n\t\tHelpSteer2: Open-source dataset for training top-performing reward models\n\t\n\nHelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nThis dataset has been created in partnership with Scale  AI. \nWhen used to tune a Llama 3.1 70B Instruct Model, we achieve 94.1% on RewardBench, which makes it the best Reward Model as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Triangle104/nvidia-HelpSteer2.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"synthetic-domain-text-classification","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/synthetic-domain-text-classification","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for my-distiset-b845cf19\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/davidberenstein1957/my-distiset-b845cf19/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/synthetic-domain-text-classification.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-formatted","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KHuss/hh-rlhf-formatted","creator_name":"Karim El Husseini","creator_url":"https://huggingface.co/KHuss","description":"\n\t\n\t\t\n\t\tReformatted version of Anthropic's hh-rlhf dataset\n\t\n\nOriginal available at https://huggingface.co/datasets/Anthropic/hh-rlhf. (Does not include red teaming data)\nRLHF datasets are in general defined as a collection of triples D={(x,y_1,y_2)_n} where x is the prompt, y_1 the chosen reponse and y_2 the rejected response. \nThe original dataset provides two columns, \"chosen\"=x+y_1 and \"rejected\"=x+y_2.\nVarious RLHF setups may require either format, so in this dataset we keep the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KHuss/hh-rlhf-formatted.","first_N":5,"first_N_keywords":["text-generation","text-classification","reinforcement-learning","token-classification","English"],"keywords_longer_than_N":true},
	{"name":"Ant-v4","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Ant-v4","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tAnt-v4 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 5913.2959.\nEach entry consists of:\nobs (list): observation with length 27.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_starts (bool): if that state was the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Ant-v4.","first_N":5,"first_N_keywords":["mit","1M - 10M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"Vl-RewardBench","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Zhihui/Vl-RewardBench","creator_name":"Xie","creator_url":"https://huggingface.co/Zhihui","description":"\n\t\n\t\t\n\t\tDataset Card for VLRewardBench\n\t\n\nProject Page:\nhttps://vl-rewardbench.github.io\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality examples specifically curated to probe model limitations.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach instance consists of multimodal queries spanning three key‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zhihui/Vl-RewardBench.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Notus_Aegis-v1","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1","creator_name":"Parag Ekbote","creator_url":"https://huggingface.co/AINovice2005","description":"\n\t\n\t\t\n\t\tDataSet Overview:\n\t\n\n\n\n\t\n\t\t\n\t\tDataSet Name:Notus_Aegis-v1\n\t\n\n\n\n\n This starter dataset is designed to facilitate research and development in the field of natural language processing, specifically focusing on instruction-response pairs.\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures:\n\t\n\n\nDiversity of Instructions: The dataset encompasses a broad spectrum of topics, ensuring applicability across different domains.\nHigh-Quality Responses: Leveraging the notus-7b-v1 model ensures that the responses are coherent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Notus_Aegis-v1","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1","creator_name":"Parag Ekbote","creator_url":"https://huggingface.co/AINovice2005","description":"\n\t\n\t\t\n\t\tDataSet Overview:\n\t\n\n\n\n\t\n\t\t\n\t\tDataSet Name:Notus_Aegis-v1\n\t\n\n\n\n\n This starter dataset is designed to facilitate research and development in the field of natural language processing, specifically focusing on instruction-response pairs.\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures:\n\t\n\n\nDiversity of Instructions: The dataset encompasses a broad spectrum of topics, ensuring applicability across different domains.\nHigh-Quality Responses: Leveraging the notus-7b-v1 model ensures that the responses are coherent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Ant-v2","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Ant-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tAnt-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on Ant-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 10 episodes with an average episodic reward of 5514.0229.\nEach entry consists‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Ant-v2.","first_N":5,"first_N_keywords":["reinforcement-learning","mit","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Ant-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Ant-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tAnt-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on Ant-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 10 episodes with an average episodic reward of 5514.0229.\nEach entry consists‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Ant-v2.","first_N":5,"first_N_keywords":["reinforcement-learning","mit","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"HalfCheetah-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tHalfCheetah-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on HalfCheetah-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 10 episodes with an average episodic reward of 7581.5527.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v2.","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"HeatAlertsRL-Data","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mauriciogtec/HeatAlertsRL-Data","creator_name":"Mauricio","creator_url":"https://huggingface.co/mauriciogtec","description":"mauriciogtec/HeatAlertsRL-Data dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["reinforcement-learning","tabular-regression","time-series-forecasting","mit","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"twinviews-13k","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wwbrannon/twinviews-13k","creator_name":"William Brannon","creator_url":"https://huggingface.co/wwbrannon","description":"\n\t\n\t\t\n\t\tDataset Card for TwinViews-13k\n\t\n\nThis dataset contains 13,855 pairs of left-leaning and right-leaning political statements matched by topic. The dataset was generated using GPT-3.5 Turbo and has been audited to ensure quality and ideological balance. It is designed to facilitate the study of political bias in reward models and language models, with a focus on the relationship between truthfulness and political views.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wwbrannon/twinviews-13k.","first_N":5,"first_N_keywords":["text-classification","reinforcement-learning","machine-generated","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"TruthGen","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wwbrannon/TruthGen","creator_name":"William Brannon","creator_url":"https://huggingface.co/wwbrannon","description":"\n\t\n\t\t\n\t\tDataset Card for TruthGen\n\t\n\nTruthGen is a dataset of generated political statements, created to assess the relationship between truthfulness and political bias in reward models and language models. It consists of non-repetitive, non-political factual statements paired with false statements, designed to evaluate models for their ability to distinguish true from false information while minimizing political content. The dataset was generated using GPT-3.5, GPT-4 and Gemini, with a focus‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wwbrannon/TruthGen.","first_N":5,"first_N_keywords":["text-classification","reinforcement-learning","machine-generated","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"Hopper-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Hopper-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tHopper-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on Hopper-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 10 episodes with an average episodic reward of 3760.6908.\nEach entry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Hopper-v2.","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"InvertedPendulum-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/InvertedPendulum-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tInvertedPendulum-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on InvertedPendulum-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 10 episodes with an average episodic reward of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/InvertedPendulum-v2.","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"Swimmer-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Swimmer-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tSwimmer-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on Swimmer-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 4 episodes with an average episodic reward of 259.5244.\nEach entry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Swimmer-v2.","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-alt","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-alt","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-alt dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-alt","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-alt","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-alt dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-merged","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-merged","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-merged dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-chat-template","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Rexhaif/hh-rlhf-chat-template","creator_name":"Daniil Larionov","creator_url":"https://huggingface.co/Rexhaif","description":"Rexhaif/hh-rlhf-chat-template dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-chat-template","keyword":"rl","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Rexhaif/hh-rlhf-chat-template","creator_name":"Daniil Larionov","creator_url":"https://huggingface.co/Rexhaif","description":"Rexhaif/hh-rlhf-chat-template dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-chat-template","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Rexhaif/hh-rlhf-chat-template","creator_name":"Daniil Larionov","creator_url":"https://huggingface.co/Rexhaif","description":"Rexhaif/hh-rlhf-chat-template dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-merged-binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-merged-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-merged-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"synth-apigen-llama","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/synth-apigen-llama","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for argilla-warehouse/synth-apigen-llama\n\t\n\nThis dataset has been created with distilabel.\nThe pipeline script was uploaded to easily reproduce the dataset:\nsynth_apigen.py.\n\n\t\n\t\t\n\t\tDataset creation\n\t\n\nThis dataset is a replica in distilabel of the framework\ndefined in: APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets.\nUsing the seed dataset of synthetic python functions in argilla-warehouse/python-seed-tools,\nthe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/synth-apigen-llama.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"phi3-arena-short-dpo","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZSvedic/phi3-arena-short-dpo","creator_name":"Zeljko Svedic","creator_url":"https://huggingface.co/ZSvedic","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nDPO (Direct Policy Optimization) dataset of normal and short answers generated from lmsys/chatbot_arena_conversations dataset using microsoft/Phi-3-mini-4k-instruct model.\nGenerated using ShortGPT project.\n","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-45k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-45k\n\t\n\nThis dataset is designed for DPO or ORPO training.\nThis dataset combines samples of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\nargilla/distilabel-math-preference-dpo: highly scored chosen‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-38k-balanced","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-38k\n\t\n\nThis dataset is intended for use with DPO or ORPO training. \nIt represents a balanced version of the llmat/dpo-orpo-mix-45k dataset, achieved through a clustering-based approach as outlined in this paper.\nThe dataset integrates high-quality samples from the following DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"example-dataset","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sharonav123/example-dataset","creator_name":"Sharon AV","creator_url":"https://huggingface.co/sharonav123","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for example-dataset\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/sharonav123/example-dataset/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sharonav123/example-dataset.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"orz_math_57k_collection","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Open-Reasoner-Zero/orz_math_57k_collection","creator_name":"Open-Reasoner-Zero","creator_url":"https://huggingface.co/Open-Reasoner-Zero","description":"\n\n\n\t\n\t\t\n\t\tOpen Reasoner Zero\n\t\n\n\n\n\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n\n\n\n\n    \n  \n  \n  \n  \n  Paper Arxiv Link üëÅÔ∏è\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\tOverview üåä\n\t\n\nWe introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Open-Reasoner-Zero/orz_math_57k_collection.","first_N":5,"first_N_keywords":["question-answering","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"pacman","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimic/pacman","creator_name":"Kimi Chen","creator_url":"https://huggingface.co/kimic","description":"This dataset contains state-action pairs of three different Pacman levels, each located in its own tar.gz.\nEach tar.gz contains output/, dataset.csv, and results.txt.\n\noutput/ contains all the images/frames generated by an RL agent based on Taylor Downey's RL Agent.\ndataset.csv contains state-action pairs that match each frame to an action of either North, South, East, West, or Stop. There is also a column indicating whether a new game has started.\nresults.txt contains the results of either‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimic/pacman.","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"R-PRM","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kevinpro/R-PRM","creator_name":"Shuaijie She","creator_url":"https://huggingface.co/kevinpro","description":"\n\t\n\t\t\n\t\tüìò R-PRM Dataset (SFT + DPO)\n\t\n\nThis dataset is developed for training Reasoning-Driven Process Reward Models (R-PRM), proposed in our ACL 2025 paper. It consists of two stages:\n\nSFT (Supervised Fine-Tuning): collected from strong LLMs prompted with limited annotated examples, enabling reasoning-style evaluation.\nDPO (Direct Preference Optimization): constructed by sampling multiple reasoning trajectories and forming preference pairs without additional labels.\n\nThese datasets are used‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kevinpro/R-PRM.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"chatgpt-prompts-distil-dataset","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/akattamuri/chatgpt-prompts-distil-dataset","creator_name":"Ashish Kattamuri","creator_url":"https://huggingface.co/akattamuri","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for chatgpt-prompts-distil-dataset\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/akattamuri/chatgpt-prompts-distil-dataset/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/akattamuri/chatgpt-prompts-distil-dataset.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"example-dataset-3","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/djackson-proofpoint/example-dataset-3","creator_name":"Dan Jackson","creator_url":"https://huggingface.co/djackson-proofpoint","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for example-dataset-3\n\t\n\nThis dataset has been created with distilabel.\nWhile certain attempts were successful in making a new prompt that provides the desired outcome, there are multpiple instances where it appears that the token limit was reached before the entire response was recorded.\nIn addition there was 1/10 that the LLM did not properly understand the request and commented on the source prompt topic.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/djackson-proofpoint/example-dataset-3.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"lima_dirty_tr","keyword":"dpo","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emre/lima_dirty_tr","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","description":"\n\t\n\t\t\n\t\tLima Turkish Translated & Engineered for Alignment\n\t\n\n\n\t\n\t\t\n\t\tGiri≈ü\n\t\n\nBu veri seti, LIMA (Less Is More for Alignment) [^1] √ßalƒ±≈ümasƒ±ndan ilham alƒ±narak olu≈üturulmu≈ü, orijinal LIMA veri setinin T√ºrk√ße'ye √ßevrilmi≈ü ve hizalama (alignment) teknikleri i√ßin √∂zel olarak yapƒ±landƒ±rƒ±lmƒ±≈ü bir versiyonudur. LIMA'nƒ±n temel felsefesi, az sayƒ±da ancak y√ºksek kaliteli √∂rnekle dil modellerini etkili bir ≈üekilde hizalayabilmektir. Bu √ßalƒ±≈üma, bu felsefeyi T√ºrk√ße dil modelleri ekosistemine ta≈üƒ±mayƒ±‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/emre/lima_dirty_tr.","first_N":5,"first_N_keywords":["text-generation","Turkish","English","afl-3.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"oaast_rm_zh_jieba","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lenML/oaast_rm_zh_jieba","creator_name":"len","creator_url":"https://huggingface.co/lenML","description":"Â∞ùËØïËß£ÂÜ≥\"llm repetition problem\"Ôºå‰ΩøÁî®ÂàÜËØçÊ®°ÂûãÂØπoaastËØ≠ÊñôËøõË°å‚ÄúÁªìÂ∑¥Âåñ‚ÄùÊï∞ÊçÆÂ¢ûÂº∫ÔºåÊèê‰æõÊõ¥Âº∫ÁöÑÈáçÂ§çÂÜÖÂÆπÊãíÁªùÊïàÊûú„ÄÇ\nAttempts to solve the \"llm repetition problem\" by using a segmentation model to enhance the oaast corpus with \"stuttering\" data to provide stronger rejection of duplicate content.\nÂÖ∂Ê¨°ÔºåËøòËøáÊª§Êéâ‰∫ÜÊâÄÊúâËá™ÊàëËÆ§Áü•ÁöÑÂæÆË∞ÉÊ†∑Êú¨„ÄÇ\nSecond, it also filters out all the fine-tuned samples of self-cognition.\nfiles:\n\noaast_rm_zh_jieba.jsonl : word level repeat\noaast_rm_zh_sent_jieba.jsonl : sentence level repeat\n\n","first_N":5,"first_N_keywords":["Chinese","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"oaast_rm_full_jieba","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lenML/oaast_rm_full_jieba","creator_name":"len","creator_url":"https://huggingface.co/lenML","description":"Â∞ùËØïËß£ÂÜ≥\"llm repetition problem\"Ôºå‰ΩøÁî®ÂàÜËØçÊ®°ÂûãÂØπoaastËØ≠ÊñôËøõË°å‚ÄúÁªìÂ∑¥Âåñ‚ÄùÊï∞ÊçÆÂ¢ûÂº∫ÔºåÊèê‰æõÊõ¥Âº∫ÁöÑÈáçÂ§çÂÜÖÂÆπÊãíÁªùÊïàÊûú„ÄÇ\nAttempts to solve the \"llm repetition problem\" by using a segmentation model to enhance the oaast corpus with \"stuttering\" data to provide stronger rejection of duplicate content.\nÂÖ∂Ê¨°ÔºåËøòËøáÊª§Êéâ‰∫ÜÊâÄÊúâËá™ÊàëËÆ§Áü•ÁöÑÂæÆË∞ÉÊ†∑Êú¨„ÄÇ\nSecond, it also filters out all the fine-tuned samples of self-cognition.\nfiles:\n\noaast_rm_full_jieba.jsonl : word level repeat\noaast_rm_full_sent_jieba.jsonl : sentence level repeat\n\n","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"lave-human-feedback","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mair-lab/lave-human-feedback","creator_name":"MAIR Lab","creator_url":"https://huggingface.co/mair-lab","description":"\n\t\n\t\t\n\t\tLAVE human judgments\n\t\n\nThis repository contains the human judgment data for Improving Automatic VQA Evaluation Using Large Language Models. Details about the data collection process and crowdworker population can be found in our paper, specifically in section 5.2 and appendix A.1.\nFields:\n\ndataset: VQA dataset of origin for this example (vqav2, vgqa, okvqa).\nmodel: VQA model that generated the predicted answer (blip2, promptcap, blip_vqa, blip_vg).\nqid: question ID coming from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mair-lab/lave-human-feedback.","first_N":5,"first_N_keywords":["cc-by-4.0","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf","keyword":"human-feedback","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/betteruncensored/hh-rlhf","creator_name":"Better Uncensored","creator_url":"https://huggingface.co/betteruncensored","description":"\n\t\n\t\t\n\t\tDataset Card for HH-RLHF Better Uncensored\n\t\n\n\n\t\n\t\t\n\t\tBetter Uncensored Summary\n\t\n\nThis is the Better Uncensored version of the famous Anthropic preference dataset Anthropic/hh-rlhf\nOnly the train files were processed with the rlhf uncensor script in this manner:\nfind ../hh-rlhf/ -type f -name 'train.jsonl' | xargs -I {} python uncensor_rlhf.py --in-file {}\n\nThis should work as a drop in replacement of the original dataset for training uncensored models. About 10% to 25% of the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/betteruncensored/hh-rlhf.","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"opin-pref","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\nhf.co/swaroop-nath/prompt-opin-summ dataset.\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\n{¬†¬†¬†¬†'unique-id': a unique id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref.","first_N":5,"first_N_keywords":["reinforcement-learning","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"opin-pref","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\nhf.co/swaroop-nath/prompt-opin-summ dataset.\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\n{¬†¬†¬†¬†'unique-id': a unique id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref.","first_N":5,"first_N_keywords":["reinforcement-learning","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"opin-pref","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\nhf.co/swaroop-nath/prompt-opin-summ dataset.\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\n{¬†¬†¬†¬†'unique-id': a unique id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref.","first_N":5,"first_N_keywords":["reinforcement-learning","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"ChatML-H4rmony_dpo","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Felladrin/ChatML-H4rmony_dpo","creator_name":"Victor Nogueira","creator_url":"https://huggingface.co/Felladrin","description":"neovalle/H4rmony_dpo in ChatML format, ready to use in HuggingFace TRL's DPO Trainer.\nPython code used for conversion:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"neovalle/H4rmony_dpo\", split=\"train\")\n\ndef format(columns):\n    return {\n        \"prompt\": f\"<|im_start|>user\\n{columns['prompt']}<|im_end|>\\n<|im_start|>assistant\\n\",\n        \"chosen\": f\"{columns['chosen']}<|im_end|>\",\n        \"rejected\": f\"{columns['rejected']}<|im_end|>\",\n    }‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Felladrin/ChatML-H4rmony_dpo.","first_N":5,"first_N_keywords":["question-answering","text-classification","reinforcement-learning","text-generation","mit"],"keywords_longer_than_N":true},
	{"name":"en_vi_DPO","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gallantVN/en_vi_DPO","creator_name":"Nhut Tien","creator_url":"https://huggingface.co/gallantVN","description":"gallantVN/en_vi_DPO dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["translation","reinforcement-learning","English","Vietnamese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"tak-stack-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CorticalStack/tak-stack-dpo","creator_name":"JP Boyd","creator_url":"https://huggingface.co/CorticalStack","description":"\n\n\n\t\n\t\t\n\t\ttak-stack-dpo üß†\n\t\n\nA DPO alignment dataset for fine tuning open-source LLMs, taking sample preference pairs from a variety of datasets for diversity.  \nPrepared in the \"standard\" instruction, chosen, and rejected format, with a source feature indicating from which dataset the row was extracted.\nSource datasets:\n\nargilla/distilabel-math-preference-dpo\njondurbin/truthy-dpo-v0.1\nargilla/distilabel-intel-orca-dpo-pairs\nargilla/OpenHermes2.5-dpo-binarized-alpha\n\n","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"pandora-rlhf","keyword":"dpo","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-rlhf","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora RLHF\n\t\n\nA Reinforcement Learning from Human Feedback (RLHF) dataset for Direct Preference Optimization (DPO) fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the anthropic/hh-rlhf dataset.\n\n\t\n\t\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"pandora-rlhf","keyword":"rlhf","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-rlhf","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora RLHF\n\t\n\nA Reinforcement Learning from Human Feedback (RLHF) dataset for Direct Preference Optimization (DPO) fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the anthropic/hh-rlhf dataset.\n\n\t\n\t\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"CHI","keyword":"dpo","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/AI-B/CHI","creator_name":"AI-B","creator_url":"https://huggingface.co/AI-B","description":"This repository outlines the methodology for creating training sets aimed at aligning a language model with a specific character and persona. \nThe process involves utilizing a Direct Preference Optimization (DPO) dataset to steer the model towards embodying the defined character and persona traits. \nFollowing this, a Unified Neutral Alignment (UNA) dataset is employed to moderate any excessive sentiments resulting from the DPO training. \nThe final step involves merging the model realigned with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-B/CHI.","first_N":5,"first_N_keywords":["unlicense","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Mistral-EN-DPO-9K","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kyujinpy/Mistral-EN-DPO-9K","creator_name":"KyujinHan","creator_url":"https://huggingface.co/kyujinpy","description":"\n\t\n\t\t\n\t\tDataset Card for \"Mistral-EN-DPO-9K\"\n\t\n\n\n\t\n\t\t\n\t\tInfo\n\t\n\nWe used snorkelai/Snorkel-Mistral-PairRM-DPO-Dataset dataset.We selected train_iteration_1 part.  \n\n\t\n\t\t\n\t\tPre-processing\n\t\n\n\nRemove coding task\n\nFiltering words: ['[Latex]', 'java', 'SQL', 'C#', 'nextjs', 'react', 'Ruby', 'Lua', 'Unity', 'XML', 'qrcode', 'jest', 'const', \n                    'python', 'Python', 'R code', 'Next.js', 'Node.js', 'Typescript', 'HTML', 'php', 'skeleton code', \n                    'MATLAB', 'using js'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kyujinpy/Mistral-EN-DPO-9K.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"french_orca_dpo_pairs","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_orca_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for french_orca_dpo_pairs\n\t\n\nThis dataset offers a french translation of the 12k DPO Intel/orca_dpo_pairs pairs made from Open-Orca/OpenOrca.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"french_orca_dpo_pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_orca_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for french_orca_dpo_pairs\n\t\n\nThis dataset offers a french translation of the 12k DPO Intel/orca_dpo_pairs pairs made from Open-Orca/OpenOrca.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"french_orca_dpo_pairs","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/french_orca_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\n\t\n\t\t\n\t\tDataset Card for french_orca_dpo_pairs\n\t\n\nThis dataset offers a french translation of the 12k DPO Intel/orca_dpo_pairs pairs made from Open-Orca/OpenOrca.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"oasst2_dpo_pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexredna/oasst2_dpo_pairs","creator_name":"Alexander Gruhl","creator_url":"https://huggingface.co/alexredna","description":"\n\t\n\t\t\n\t\tDataset Card for \"oasst2_dpo_pairs\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDataset transferred into the structure for trainig with DPO and can be used with the Alignment Handbook\nThe structure follows mostly the same scheme as HuggingFaceH4/ultrafeedback_binarized\n\n\t\n\t\t\n\t\tUsage\n\t\n\nTo load the dataset, run:\nfrom datasets import load_dataset\n\nds = load_dataset(\"alexredna/oasst2_dpo_pairs\")\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nBase dataset filtered to only contain: German, English, Spanish and Frensh‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexredna/oasst2_dpo_pairs.","first_N":5,"first_N_keywords":["English","German","Spanish","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LunarLander-v2","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/LunarLander-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tLunarLander-v2 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a PPO policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 500.\nEach entry consists of:\nobs (list): observation with length 8.\naction (int): action (0, 1, 2 and 3).\nreward (float): reward point for that timestep.\nepisode_returns (bool): if that state‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/LunarLander-v2.","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"itorca_dpo_vi","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lamhieu/itorca_dpo_vi","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\n\n\t\n\t\t\n\t\n\t\n\t\tStructure\n\t\n\nView online through viewer.\n\n\t\n\t\t\n\t\n\t\n\t\tNote\n\t\n\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/itorca_dpo_vi.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Vietnamese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"itorca_dpo_en","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lamhieu/itorca_dpo_en","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\n\n\t\n\t\t\n\t\n\t\n\t\tStructure\n\t\n\nView online through viewer.\n\n\t\n\t\t\n\t\n\t\n\t\tNote\n\t\n\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/itorca_dpo_en.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MsitralTrix-test-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/MsitralTrix-test-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/MsitralTrix-test-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","arrow"],"keywords_longer_than_N":true},
	{"name":"CultriX-dpo","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/CultriX-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/CultriX-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"H4rmony_dpo","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neovalle/H4rmony_dpo","creator_name":"Jorge Vallego","creator_url":"https://huggingface.co/neovalle","description":"This dataset is based on neovalle/H4rmony, and optimised to the format required by DPOTrainer from the trl library.\n","first_N":5,"first_N_keywords":["question-answering","text-classification","reinforcement-learning","text-generation","mit"],"keywords_longer_than_N":true},
	{"name":"HelpSteer_binarized","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sablo/HelpSteer_binarized","creator_name":"Sablo AI","creator_url":"https://huggingface.co/sablo","description":"\n\t\n\t\t\n\t\tBinarized version of HelpSteer\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA binarized version of https://huggingface.co/datasets/nvidia/HelpSteer ready for DPO using https://github.com/huggingface/alignment-handbook or similar.\nFor each unique prompt, we take the best and worst scoring (average of helpfulness and correctness) responses. These are converted into MessagesList format in the 'chosen' and 'rejected' columns.\n\nCreated by: dctanner and the team at Sablo AI\nLicense: CC BY 4.0\n\n","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"gutenberg-dpo-v0.1-tr","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duxx/gutenberg-dpo-v0.1-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","description":"duxx/gutenberg-dpo-v0.1-tr dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["Turkish","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"gutenberg-dpo-v0.1-tr","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duxx/gutenberg-dpo-v0.1-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","description":"duxx/gutenberg-dpo-v0.1-tr dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["Turkish","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs-tr","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for DPO\n\t\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr.","first_N":5,"first_N_keywords":["Turkish","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs-tr","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for DPO\n\t\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr.","first_N":5,"first_N_keywords":["Turkish","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs-tr","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for DPO\n\t\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr.","first_N":5,"first_N_keywords":["Turkish","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs_dutch_cleaned","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\n\t\n\t\t\n\t\tDataset Card for Orca DPO Pairs Dutch Cleaned\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"argilla-dpo-mix-7k-arabic","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/medmac01/argilla-dpo-mix-7k-arabic","creator_name":"Mohammed Machrouh","creator_url":"https://huggingface.co/medmac01","description":"medmac01/argilla-dpo-mix-7k-arabic dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["Arabic","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"argilla-dpo-mix-7k-arabic","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/argilla-dpo-mix-7k-arabic","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tDataset Card for \"argilla-dpo-mix-7k-arabic\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["Arabic","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-kto","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for KTO\n\t\n\n\nA KTO signal transformed version of the highly loved distilabel Orca Pairs for DPO.\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-kto","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for KTO\n\t\n\n\nA KTO signal transformed version of the highly loved distilabel Orca Pairs for DPO.\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-kto-15k-binarized","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tCapybara-KTO 15K binarized\n\t\n\n\nA KTO signal transformed version of the highly loved Capybara-DPO 7K binarized, A DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n\t\n\t\t\n\t\tWhy KTO?\n\t\n\nThe KTO paper states:\n\nKTO matches or exceeds DPO performance at scales from 1B to 30B parameters.1 That is, taking a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-kto-15k-binarized","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tCapybara-KTO 15K binarized\n\t\n\n\nA KTO signal transformed version of the highly loved Capybara-DPO 7K binarized, A DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n\t\n\t\t\n\t\tWhy KTO?\n\t\n\nThe KTO paper states:\n\nKTO matches or exceeds DPO performance at scales from 1B to 30B parameters.1 That is, taking a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"DPO-En-Zh-20k-Preference","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","description":"This dataset is composed by\n\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\n10,000 examples of wenbopan/Chinese-dpo-pairs.\n\nrefer: https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k   Êîπ‰∫Üquestion„ÄÅresponse_rejected„ÄÅresponse_chosenÂ≠óÊÆµÔºåÊñπ‰æøORPO„ÄÅDPOÊ®°ÂûãËÆ≠ÁªÉÊó∂‰ΩøÁî®train usage:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"DPO-En-Zh-20k-Preference","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","description":"This dataset is composed by\n\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\n10,000 examples of wenbopan/Chinese-dpo-pairs.\n\nrefer: https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k   Êîπ‰∫Üquestion„ÄÅresponse_rejected„ÄÅresponse_chosenÂ≠óÊÆµÔºåÊñπ‰æøORPO„ÄÅDPOÊ®°ÂûãËÆ≠ÁªÉÊó∂‰ΩøÁî®train usage:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Prude-Phi3-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/flammenai/Prude-Phi3-DPO","creator_name":"flammen.ai","creator_url":"https://huggingface.co/flammenai","description":"ResplendentAI/NSFW_RP_Format_NoQuote inputs ran through Phi-3 Q4 to intentionally produce bad responses to be used for rejections.\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-mix-21k","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/eduagarcia/dpo-mix-21k","creator_name":"Eduardo Garcia","creator_url":"https://huggingface.co/eduagarcia","description":"\n\t\n\t\t\n\t\tDPO Mix 21K Dataset\n\t\n\nSimilar to the argilla/dpo-mix-7k, but with 21k samples:\nOther diferences:\n\nargilla/distilabel-capybara-dpo-7k-binarized changed to argilla/Capybara-Preferences\nargilla/distilabel-intel-orca-dpo-pairs scored filter of >=8 changed to >= 7\n\n\nA small cocktail combining DPO datasets built by Argilla with distilabel. The goal of this dataset is having a small, high-quality DPO dataset by filtering only highly rated chosen responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eduagarcia/dpo-mix-21k.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"webui-dom-snapshots","keyword":"reinforcement-learning","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gbenson/webui-dom-snapshots","creator_name":"Gary Benson","creator_url":"https://huggingface.co/gbenson","description":"\n\t\n\t\t\n\t\tDataset Card for WebUI DOM snapshots\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: Gary Benson\nLanguages: Mostly English (87%);\nDutch, French, Chinese, Japanese (1-2% each); 30+ others (<1% each)\nLicense: CC0 1.0 Universal\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gbenson/webui-dom-snapshots.","first_N":5,"first_N_keywords":["image-feature-extraction","reinforcement-learning","text-classification","multilingual","biglab/webui-7k"],"keywords_longer_than_N":true},
	{"name":"openai-summarize-tldr","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/martimfasantos/openai-summarize-tldr","creator_name":"Martim Santos","creator_url":"https://huggingface.co/martimfasantos","description":"\n\t\n\t\t\n\t\tSummarize TL;DR Filtered Dataset\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2009.01325.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://huggingface.co/datasets/webis/tldr-17.\n","first_N":5,"first_N_keywords":["text-generation","summarization","crowdsourced","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"beyond_dpo_en","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lamhieu/beyond_dpo_en","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\n\n\t\n\t\t\n\t\n\t\n\t\tStructure\n\t\n\nView online through viewer.\n\n\t\n\t\t\n\t\n\t\n\t\tNote\n\t\n\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/beyond_dpo_en.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"beyond_dpo_vi","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lamhieu/beyond_dpo_vi","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\n\n\t\n\t\t\n\t\n\t\n\t\tStructure\n\t\n\nView online through viewer.\n\n\t\n\t\t\n\t\n\t\n\t\tNote\n\t\n\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/beyond_dpo_vi.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","Vietnamese","mit"],"keywords_longer_than_N":true},
	{"name":"okapi-ranking","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Saugatkafley/okapi-ranking","creator_name":"Saugat Kafley","creator_url":"https://huggingface.co/Saugatkafley","description":"Saugatkafley/okapi-ranking dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["Nepali","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"klexikon_dpo","keyword":"dpo","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/udkai/klexikon_dpo","creator_name":"UDK dot AI","creator_url":"https://huggingface.co/udkai","description":"Version of https://huggingface.co/datasets/dennlinger/klexikon which can be useful for Direct Preference Optimization of large language models generating sentences in simple german.\n","first_N":5,"first_N_keywords":["German","cc-by-sa-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tCapybara-DPO 7K binarized\n\t\n\n\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tWhy?\n\t\n\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there are very few‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tCapybara-DPO 7K binarized\n\t\n\n\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tWhy?\n\t\n\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there are very few‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tCapybara-DPO 7K binarized\n\t\n\n\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tWhy?\n\t\n\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there are very few‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"catboros-3.2-dpo","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/antiven0m/catboros-3.2-dpo","creator_name":"antiven0m","creator_url":"https://huggingface.co/antiven0m","description":"\nCatboros-3.2 DPO Dataset\n  \n    \n      \n        Original Dataset\n        \n      \n      The creation of this catgirl personality DPO dataset was enabled by Jon Durbin's work on airoboros-3.2, which served as the foundational basis. Jon's dataset is accessible at jondurbin/airoboros-3.2. \n    \n    \n      \n        The Idea\n        \n      \n      The concept of a catgirl assistant was inspired by Sao's NatsumiV1 project, available at ( Sao10K/NatsumiV1). \n    \n    \n      \n        DPO Dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/antiven0m/catboros-3.2-dpo.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Nectar","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/Nectar","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\n\t\n\t\t\n\t\tDataset Card for Nectar\n\t\n\n\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\n\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/Nectar.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Nectar","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/Nectar","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\n\t\n\t\t\n\t\tDataset Card for Nectar\n\t\n\n\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\n\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/Nectar.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"tamer-novel","keyword":"rlhf","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yukiarimo/tamer-novel","creator_name":"Yuki Arimo","creator_url":"https://huggingface.co/yukiarimo","description":"\n\t\n\t\t\n\t\tTamer Novel Dataset\n\t\n\nWelcome to the tamer-novel dataset. This unique dataset is crafted with the remarkable Tamer Novel Styler writing, enhanced by the ELiTA technique, and aims to augment self-awareness in large language models (LLMs).\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Tamer Novel dataset is designed for researchers, developers, and enthusiasts in AI, specifically those working on enhancing the self-awareness and contextual understanding of LLMs. By leveraging the novel ELiTA technique, this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yukiarimo/tamer-novel.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","summarization","English","afl-3.0"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-sample","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pgurazada1/ultrafeedback-sample","creator_name":"Pavankumar Gurazada","creator_url":"https://huggingface.co/pgurazada1","description":"pgurazada1/ultrafeedback-sample dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"configurable-system-prompt-multitask","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vicgalle/configurable-system-prompt-multitask","creator_name":"Victor Gallego","creator_url":"https://huggingface.co/vicgalle","description":"\n\t\n\t\t\n\t\tConfigurable System Prompt Multi-task Dataset üõû\n\t\n\nWe release the synthetic dataset for the multi-task experiments from the paper \"Configurable Safety Tuning of Language Models with Synthetic Preference Data\", https://huggingface.co/papers/2404.00495. This dataset has two sources for the examples:\n\nSelf-critique on a safety task from Harmful Behaviours, using the SOLAR-Instruct model. It employs two system prompts to learn the different behaviors:\nYou are a helpful yet harmless‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vicgalle/configurable-system-prompt-multitask.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"reescritura-textos-administrativos","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/somosnlp/reescritura-textos-administrativos","creator_name":"SomosNLP","creator_url":"https://huggingface.co/somosnlp","description":"\n\t\n\t\t\n\t\tDataset Card for reescritura-textos-administrativos\n\t\n\nThis dataset has been created with Argilla.\nAs shown in the sections below, this dataset can be loaded into Argilla as explained in Load with Argilla, or used directly with the datasets library in Load with datasets.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains:\n\nA dataset configuration file conforming to the Argilla dataset format named argilla.yaml. This configuration file will be used to configure the dataset when using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp/reescritura-textos-administrativos.","first_N":5,"first_N_keywords":["text2text-generation","Spanish","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"classifai","keyword":"dpo","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jhmejia/classifai","creator_name":"John Henry","creator_url":"https://huggingface.co/jhmejia","description":"jhmejia/classifai dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","gpl-3.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"classifai","keyword":"rlhf","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jhmejia/classifai","creator_name":"John Henry","creator_url":"https://huggingface.co/jhmejia","description":"jhmejia/classifai dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","gpl-3.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"RLSTACK","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK.","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"RLSTACK","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK.","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"RLSTACK","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK.","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Select-Stack","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Select-Stack","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Select-Stack","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"unstacked","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","arxiv:2403.08763","arxiv:2310.05914"],"keywords_longer_than_N":true},
	{"name":"unstacked","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","arxiv:2403.08763","arxiv:2310.05914"],"keywords_longer_than_N":true},
	{"name":"unstacked","keyword":"rlhf","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","arxiv:2403.08763","arxiv:2310.05914"],"keywords_longer_than_N":true},
	{"name":"SentimentSynth","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OEvortex/SentimentSynth","creator_name":"HelpingAI","creator_url":"https://huggingface.co/OEvortex","description":"\n\t\n\t\t\n\t\tSentimentSynth Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe SentimentSynth dataset is a collection of text samples expressing various sentiments, ranging from joy and excitement to stress and sadness. These samples are generated to simulate human-like expressions of emotions in different contexts.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use the SentimentSynth dataset in your work, please cite it as:\n@misc {helpingai_2024,\n    author       = { {HelpingAI} },\n    title        = { SentimentSynth (Revision‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OEvortex/SentimentSynth.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"cosmochat","keyword":"rlaif","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/cosmochat","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\n  \n    \n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Card for CosmoChat\n\t\n\n\n  \n    \n  \n\n\nThis dataset has been created with distilabel. It is a WIP and is likely to change whenever I have some spare time to work on this! Feel free to follow my harebrained ideas for improving this here: https://huggingface.co/datasets/davanstrien/cosmochat/discussions\n\n\t\n\t\n\t\n\t\tCan we create pedagogically valuable multi-turn synthetic datasets from Cosmopedia?\n\t\n\nSynthetic datasets are increasingly helping to push forward the quality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/cosmochat.","first_N":5,"first_N_keywords":["English","cc0-1.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Aya-AceGPT.13B.Chat-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-AceGPT.13B.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tDataset Card for \"Aya-AceGPT.13B.Chat-DPO\" ü§ó\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-AceGPT.13B.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-AceGPT.13B.Chat-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"rag_qa_embedding_questions_0_60_0","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0","creator_name":"ZenML","creator_url":"https://huggingface.co/zenml","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for rag_qa_embedding_questions_0_60_0\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"contextual-dpo-v0.1-ko","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/contextual-dpo-v0.1-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"\n\t\n\t\t\n\t\tDataset Card for \"contextual-dpo-v0.1-ko\"\n\t\n\nTranslated jondurbin/contextual-dpo-v0.1 using nayohan/llama3-instrucTrans-enko-8b.\n","first_N":5,"first_N_keywords":["Korean","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"HC3-ko","keyword":"dpo","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/HC3-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"\n\t\n\t\t\n\t\tDataset Card for \"HC3\"\n\t\n\nTranslated Hello-SimpleAI/HC3 using nayohan/llama3-instrucTrans-enko-8b.\n","first_N":5,"first_N_keywords":["question-answering","text-generation","Korean","cc-by-sa-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SentimentSynth-ko","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/SentimentSynth-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"\n\t\n\t\t\n\t\tDataset Card for \"SentimentSynth\"\n\t\n\nTranslated OEvortex/SentimentSynth using nayohan/llama3-instrucTrans-enko-8b.\n","first_N":5,"first_N_keywords":["question-answering","Korean","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Neural-DPO-ko","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nayohan/Neural-DPO-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","description":"\n\t\n\t\t\n\t\tDataset Card for \"Neural-DPO\"\n\t\n\nTranslated NeuralNovel/Neural-DPO using nayohan/llama3-instrucTrans-enko-8b.\n","first_N":5,"first_N_keywords":["question-answering","Korean","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"og-marl","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/InstaDeepAI/og-marl","creator_name":"InstaDeep Ltd","creator_url":"https://huggingface.co/InstaDeepAI","description":"\n@misc{formanek2024puttingdatacentreoffline,\n      title={Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning}, \n      author={Claude Formanek and Louise Beyers and Callum Rhys Tilbury and Jonathan P. Shock and Arnu Pretorius},\n      year={2024},\n      eprint={2409.12001},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2409.12001}, \n}\n\n","first_N":5,"first_N_keywords":["reinforcement-learning","apache-2.0","< 1K","Datasets","Croissant"],"keywords_longer_than_N":true},
	{"name":"luckyvicky-DPO","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Junnos/luckyvicky-DPO","creator_name":"Ïù¥Ï∞ΩÏ§Ä","creator_url":"https://huggingface.co/Junnos","description":"\n\t\n\t\t\n\t\tÏõêÏòÅÏ†Å ÏÇ¨Í≥† Îç∞Ïù¥ÌÑ∞ÏÖã\n\t\n\n","first_N":5,"first_N_keywords":["reinforcement-learning","text2text-generation","Korean","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"luckyvicky-DPO","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Junnos/luckyvicky-DPO","creator_name":"Ïù¥Ï∞ΩÏ§Ä","creator_url":"https://huggingface.co/Junnos","description":"\n\t\n\t\t\n\t\tÏõêÏòÅÏ†Å ÏÇ¨Í≥† Îç∞Ïù¥ÌÑ∞ÏÖã\n\t\n\n","first_N":5,"first_N_keywords":["reinforcement-learning","text2text-generation","Korean","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2-DPO","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Atsunori/HelpSteer2-DPO","creator_name":"Atsunori Fujita","creator_url":"https://huggingface.co/Atsunori","description":"This dataset is a conversion of nvidia/HelpSteer2 into preference pairs based on the helpfulness score for training DPO.\nHelpSteer2-DPO is also licensed under CC-BY-4.0.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nIn accordance with the following paper, HelpSteer2: Open-source dataset for training top-performing reward models\nwe converted nvidia/HelpSteer2 dataset into a preference dataset by taking the response with the higher helpfulness score as the chosen response, with the remaining response being the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Atsunori/HelpSteer2-DPO.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"mypo-4k-rfc","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc","creator_name":"Joshua Sundance Bailey","creator_url":"https://huggingface.co/joshuasundance","description":"\n\t\n\t\t\n\t\tmypo\n\t\n\nmypy + DPO = mypo\n\nThis is a preview version of what I'll be calling the mypo dataset, a DPO dataset focused on Python code quality. It is derived from iamtarun/python_code_instructions_18k_alpaca.\nmypo-4k-rfc is a DPO dataset with three columns:\n\nprompt\nfrom the original dataset\n\n\nrejected\ncode from the original dataset, found to have linting errors\n\n\nchosen\ncode from the original dataset, rewritten by codellama/CodeLlama-7b-Python-hf to address linting errors\n\n\n\nThe plan is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset aims to provide large-scale vision feedback data. \nIt is a combination of the following high-quality vision feedback datasets:\n\nzhiqings/LLaVA-Human-Preference-10K: 9,422 samples\nMMInstruction/VLFeedback: 80,258 samples\nYiyangAiLab/POVID_preference_data_for_VLLMs: 17,184 samples\nopenbmb/RLHF-V-Dataset: 5,733 samples\nopenbmb/RLAIF-V-Dataset: 83,132 samples\n\nWe also offer a cleaned version in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized.","first_N":5,"first_N_keywords":["mit","100K - 1M","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset aims to provide large-scale vision feedback data. \nIt is a combination of the following high-quality vision feedback datasets:\n\nzhiqings/LLaVA-Human-Preference-10K: 9,422 samples\nMMInstruction/VLFeedback: 80,258 samples\nYiyangAiLab/POVID_preference_data_for_VLLMs: 17,184 samples\nopenbmb/RLHF-V-Dataset: 5,733 samples\nopenbmb/RLAIF-V-Dataset: 83,132 samples\n\nWe also offer a cleaned version in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized.","first_N":5,"first_N_keywords":["mit","100K - 1M","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"mypo-4k-rfc-val-phi3test","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc-val-phi3test","creator_name":"Joshua Sundance Bailey","creator_url":"https://huggingface.co/joshuasundance","description":"data: 100 rows of https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc validation split\nbase: https://huggingface.co/edumunozsala/phi3-mini-4k-qlora-python-code-20k\ndpo: https://huggingface.co/joshuasundance/phi3-mini-4k-qlora-python-code-20k-mypo-4k-rfc-pipe\nmade to compare base vs dpo\nbut apparently I clipped the beginning of some of the dpo outputs with sloppy coding\nwill update later\nbase: 16:55, 10.16s/it\nDPO: 15:36,  9.36s/it\n","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized-cleaned","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized-Cleaned\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset represents a cleaned version on wangclnlp/vision-feedback-mix-binarized.\nDescriptions of the base datasets, including the data format and the procedure for mixing data, can be found in this link.\n\n\t\n\t\t\n\t\tOur Methods for Cleaning Vision Feedback Data\n\t\n\nOur goal is to select vision feedback samples where the preferred outputs are significantly differentiated from the dispreferred ones, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned.","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized-cleaned","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized-Cleaned\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset represents a cleaned version on wangclnlp/vision-feedback-mix-binarized.\nDescriptions of the base datasets, including the data format and the procedure for mixing data, can be found in this link.\n\n\t\n\t\t\n\t\tOur Methods for Cleaning Vision Feedback Data\n\t\n\nOur goal is to select vision feedback samples where the preferred outputs are significantly differentiated from the dispreferred ones, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned.","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"NoRobots-AceGPT.13B.Chat-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-AceGPT.13B.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"NoRobots-AceGPT.13B.Chat-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-AceGPT.13B.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-AceGPT.13B.Chat-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"NoRobots-Aya.23.8B-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-Aya.23.8B-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"NoRobots-Aya.23.8B-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-Aya.23.8B-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-Aya.23.8B-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"DPO-uz-9k","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","description":"This is DPO Uzbek translated dataset with 9k chat pairs.\nOriginal English dataset comes from DPO-En-Zh-20k (commit 9ad5f7428419d3cf78493cf3f4be832cf5346ba8. File:  dpo_en.json).\nI translated 10k pairs of chat examples into Uzbek using NLLB 3.3B model.\nAfter translation was completed, I used local lilac instance to remove records with coding examples since NLLB is not good at translating text with coding examples. \nNote that each prompt has two answers. The first answer should be the 'selected'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k.","first_N":5,"first_N_keywords":["text-generation","Uzbek","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"DPO-uz-9k","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","description":"This is DPO Uzbek translated dataset with 9k chat pairs.\nOriginal English dataset comes from DPO-En-Zh-20k (commit 9ad5f7428419d3cf78493cf3f4be832cf5346ba8. File:  dpo_en.json).\nI translated 10k pairs of chat examples into Uzbek using NLLB 3.3B model.\nAfter translation was completed, I used local lilac instance to remove records with coding examples since NLLB is not good at translating text with coding examples. \nNote that each prompt has two answers. The first answer should be the 'selected'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k.","first_N":5,"first_N_keywords":["text-generation","Uzbek","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"NoRobots-SambaLingo.Arabic.Chat-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-SambaLingo.Arabic.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"NoRobots-SambaLingo.Arabic.Chat-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-SambaLingo.Arabic.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-SambaLingo.Arabic.Chat-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"NoRobots-Command.R-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/NoRobots-Command.R-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"NoRobots-Command.R-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-Command.R-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-Command.R-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"auryn_dpo_orpo_english","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/auryn_dpo_orpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/auryn_dpo_orpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"auryn_dpo_orpo_english","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/auryn_dpo_orpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/auryn_dpo_orpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Multifaceted-Collection-ORPO","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-ORPO","creator_name":"KAIST AI","creator_url":"https://huggingface.co/kaist-ai","description":"\n\t\n\t\t\n\t\tDataset Card for Multifaceted Collection ORPO\n\t\n\n\n\t\n\t\t\n\t\tLinks for Reference\n\t\n\n\nHomepage: https://lklab.kaist.ac.kr/Janus/ \nRepository: https://github.com/kaistAI/Janus \nPaper: https://arxiv.org/abs/2405.17977 \nPoint of Contact: suehyunpark@kaist.ac.kr\n\n\n\t\n\t\t\n\t\n\t\n\t\tTL;DR\n\t\n\n\nMultifaceted Collection is a preference dataset for aligning LLMs to diverse human preferences, where system messages are used to represent individual preferences. The instructions are acquired from five existing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-ORPO.","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Aya-Aya.23.8B-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/2A2I/Aya-Aya.23.8B-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"Aya-Aya.23.8B-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-Aya.23.8B-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled as \"chosen,\" with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-Aya.23.8B-DPO.","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"arlbench","keyword":"rl","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/autorl-org/arlbench","creator_name":"AutoRL.org","creator_url":"https://huggingface.co/autorl-org","description":"\n\t\n\t\t\n\t\tThe ARLBench Performance Dataset\n\t\n\nARLBench is a benchmark designed for hyperparameter optimization (HPO) in Reinforcement Learning (RL). Given that we conducted several thousand runs to identify meaningful HPO test settings for RL, we have compiled these results into a dataset for future research and applications.\nThis dataset can be leveraged to:\n\nMeta-learn insights about the hyperparameter landscape in RL.\nWarm-start HPO tools by utilizing previously explored configurations.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/autorl-org/arlbench.","first_N":5,"first_N_keywords":["bsd-3-clause","1M - 10M","Tabular","arxiv:2409.18827","doi:10.57967/hf/3200"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/evol-dpo-ita","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Card\n\t\n\nEvol-DPO-Ita is a high-quality dataset consisting of ~20,000 preference pairs derived from responses generated by two large language models: Claude Opus and GPT-3.5 Turbo.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n20,000 Preference Comparisons: Each entry contains two model responses ‚Äî Claude Opus (claude-3-opus-20240229) for the chosen response and GPT-3.5 Turbo for the rejected one, both generated from a translated prompt from the original Evol-Instruct dataset (prompt and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/efederici/evol-dpo-ita.","first_N":5,"first_N_keywords":["text2text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/evol-dpo-ita","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Card\n\t\n\nEvol-DPO-Ita is a high-quality dataset consisting of ~20,000 preference pairs derived from responses generated by two large language models: Claude Opus and GPT-3.5 Turbo.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n20,000 Preference Comparisons: Each entry contains two model responses ‚Äî Claude Opus (claude-3-opus-20240229) for the chosen response and GPT-3.5 Turbo for the rejected one, both generated from a translated prompt from the original Evol-Instruct dataset (prompt and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/efederici/evol-dpo-ita.","first_N":5,"first_N_keywords":["text2text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"procgen","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EpicPinkPenguin/procgen","creator_name":"Marcus Fechner","creator_url":"https://huggingface.co/EpicPinkPenguin","description":"\n\t\n\t\t\n\t\tProcgen Benchmark\n\t\n\nThis dataset contains expert trajectories generated by a PPO reinforcement learning agent trained on each of the 16 procedurally-generated gym environments from the Procgen Benchmark. The environments were created on distribution_mode=easy and with unlimited levels.\nDisclaimer: This is not an official repository from OpenAI.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Usage\n\t\n\nRegular usage (for environment bigfish):\nfrom datasets import load_dataset\ntrain_dataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/EpicPinkPenguin/procgen.","first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","100M - 1B","parquet"],"keywords_longer_than_N":true},
	{"name":"distilset","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Andresckamilo/distilset","creator_name":"Andres Montenegro","creator_url":"https://huggingface.co/Andresckamilo","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for distilset\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/Andresckamilo/distilset/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Andresckamilo/distilset.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Chatgpt","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RajChat/Chatgpt","creator_name":"Rajdeep Chatterjee ","creator_url":"https://huggingface.co/RajChat","description":"\n\t\n\t\t\n\t\tOpenAssistant Conversations Dataset (OASST1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \nis a product of a worldwide crowd-sourcing effort‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RajChat/Chatgpt.","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"oasst2_es_instruct_hf","keyword":"human-feedback","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bertin-project/oasst2_es_instruct_hf","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","description":"This is the Spanish subset from the OpenAssistant/oasst2 dataset.\nThe dataset has been extracted from the 2023-11-05_oasst2_ready.trees.jsonl.gz file to parse all the conversation trees and put it in a huggingface-friendly format so you can use apply_chat_template as explained on the Chat Templating documentation.\n\n\t\n\t\t\n\t\n\t\n\t\tExample\n\t\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nchat = [\n\n  {\"role\": \"user\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bertin-project/oasst2_es_instruct_hf.","first_N":5,"first_N_keywords":["Spanish","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"h4rmony_dpo_multilingual","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neovalle/h4rmony_dpo_multilingual","creator_name":"Jorge Vallego","creator_url":"https://huggingface.co/neovalle","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neovalle/h4rmony_dpo_multilingual.","first_N":5,"first_N_keywords":["reinforcement-learning","English","Spanish","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"demons_megaten_fandom_orpo_dpo_english","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/demons_megaten_fandom_orpo_dpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/demons_megaten_fandom_orpo_dpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"demons_megaten_fandom_orpo_dpo_english","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/celsowm/demons_megaten_fandom_orpo_dpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","description":"celsowm/demons_megaten_fandom_orpo_dpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"assettoCorsaGym","keyword":"rl","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dasgringuen/assettoCorsaGym","creator_name":"Adrian R","creator_url":"https://huggingface.co/dasgringuen","description":"\n\t\n\t\t\n\t\tDataset Card for Assetto Corsa Gym\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe AssettoCorsaGym dataset comprises 64 million steps, including 2.3 million steps from human drivers and the remaining from Soft Actor-Critic (SAC) policies. Data collection involved 15 drivers completing at least five laps per track and car. Participants included a professional e-sports driver, four experts, five casual drivers, and five beginners.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\nAutonomous driving‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dasgringuen/assettoCorsaGym.","first_N":5,"first_N_keywords":["other","machine-generated","expert-generated","original","English"],"keywords_longer_than_N":true},
	{"name":"Hopper-v3","keyword":"expert trajectory","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NathanGavenski/Hopper-v3","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","description":"\n\t\n\t\t\n\t\tHopper-v3 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 3608.1871.\nEach entry consists of:\nobs (list): observation with length 2.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_starts (bool): if that state was the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Hopper-v3.","first_N":5,"first_N_keywords":["mit","1M - 10M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"wikisource_preferences_ru","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kristaller486/wikisource_preferences_ru","creator_name":"Kristaller486","creator_url":"https://huggingface.co/kristaller486","description":"\n\t\n\t\t\n\t\tWikisource Preferences [Russian]\n\t\n\n–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. chosen —Ç–µ–∫—Å—Ç—ã –±—Ä–∞–ª–∏—Å—å –∏–∑ kristaller486/wikisource-creative-ru, –∞ rejected –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ LLM –ø–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–∞–º.\n–®–∞–±–ª–æ–Ω –¥–ª—è DPO: axolotl chat_template.default\n–ú–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ rejected —Å–µ–º–ø–ª–æ–≤:\n\ngoogle/gemma-3-27b-it\ngpt-4.1-mini\ngpt-4.1-nano\ngpt-4.1\ngemini-2.0-flash\nQwen/Qwen3-14B-FP8 (without reasoning)\nMoraliane/SAINEMO-reMIX (fp6-llm quantization)\ndeepseek-v3-0324 (api)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kristaller486/wikisource_preferences_ru.","first_N":5,"first_N_keywords":["text-generation","Russian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_dpo_dataset_HelpSteer3","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/RizhongLin/MNLP_M2_dpo_dataset_HelpSteer3","creator_name":"Rizhong Lin","creator_url":"https://huggingface.co/RizhongLin","description":"\n\t\n\t\t\n\t\tCS-552 Stochastic Parrots M2 DPO Dataset\n\t\n\nThis dataset contains preference pairs for Direct Preference Optimization (DPO) training with a focus on STEM and code domains.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains the following splits:\n\ntrain: 36693 examples\nvalidation: 4078 examples\ntest: 908 examples\n\n\n\t\n\t\t\n\t\tData Format\n\t\n\nEach example contains:\n\nprompt: The input query or question\nchosen: The preferred response\nrejected: The less preferred response\ndomain: Domain of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RizhongLin/MNLP_M2_dpo_dataset_HelpSteer3.","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"helpsteer3_preference","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIR-hl/helpsteer3_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer3. HelpSteer3 contains 40,476 preference samples, each containing a domain, language, context, two responses, an overall preference score between the responses as well as individual preferences from up to 3 annotators. Each individual preference contains a preference score in addition to a concise reasoning for their preference in 1-2 sentences. Data is split into 95% train and 5% validation.\nI processed the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer3_preference.","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"helpsteer3_preference","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIR-hl/helpsteer3_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer3. HelpSteer3 contains 40,476 preference samples, each containing a domain, language, context, two responses, an overall preference score between the responses as well as individual preferences from up to 3 annotators. Each individual preference contains a preference score in addition to a concise reasoning for their preference in 1-2 sentences. Data is split into 95% train and 5% validation.\nI processed the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer3_preference.","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"helpsteer3_preference","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIR-hl/helpsteer3_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer3. HelpSteer3 contains 40,476 preference samples, each containing a domain, language, context, two responses, an overall preference score between the responses as well as individual preferences from up to 3 annotators. Each individual preference contains a preference score in addition to a concise reasoning for their preference in 1-2 sentences. Data is split into 95% train and 5% validation.\nI processed the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer3_preference.","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2-DPO-Atsunori","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GenRM/HelpSteer2-DPO-Atsunori","creator_name":"GenRM: Generative Reward Models","creator_url":"https://huggingface.co/GenRM","description":"This dataset is a conversion of nvidia/HelpSteer2 into preference pairs based on the helpfulness score for training DPO.\nHelpSteer2-DPO is also licensed under CC-BY-4.0.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nIn accordance with the following paper, HelpSteer2: Open-source dataset for training top-performing reward models\nwe converted nvidia/HelpSteer2 dataset into a preference dataset by taking the response with the higher helpfulness score as the chosen response, with the remaining response being the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GenRM/HelpSteer2-DPO-Atsunori.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Deep_math","keyword":"rl","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Compumacy/Deep_math","creator_name":"Compumacy","creator_url":"https://huggingface.co/Compumacy","description":"\n\t\n\t\t\n\t\tDeepMath-103K\n\t\n\n\n  \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n\t\n\t\t\n\t\tüî• News\n\t\n\n\nMay 8, 2025: We found that 48 samples contained hints that revealed the answers. The relevant questions have now been revised to remove the leaked answers.\nApril 14, 2025: We release DeepMath-103K, a large-scale dataset featuring challenging, verifiable, and decontaminated math problems tailored for RL and SFT. We open source:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Compumacy/Deep_math.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"osiris","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hardware-fab/osiris","creator_name":"Hardware-Fab","creator_url":"https://huggingface.co/hardware-fab","description":"\n\t\n\t\t\n\t\tOsiris: A Scalable Dataset Generation Pipeline for Machine Learning in Analog Circuit Design\n\t\n\nOsiris is an end-to-end analog circuits design pipeline capable of producing, validating, and evaluating layouts for generic analog circuits.\nThe Osiris GitHub repository hosts the code that implements the randomized pipeline as well as the reinforcement learning-driven baseline methodology discussed \nin the paper proposed at the NeurIPS 2025 Datasets & Benchmarks Track.\nThe Osiris ü§ó‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hardware-fab/osiris.","first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","üá∫üá∏ Region: US","eda"],"keywords_longer_than_N":true},
	{"name":"saas-sales-conversations","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DeepMostInnovations/saas-sales-conversations","creator_name":"DeepMost","creator_url":"https://huggingface.co/DeepMostInnovations","description":"\n\t\n\t\t\n\t\tsaas-sales-conversations\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a synthetic dataset of sales conversations for SaaS (Software as a Service) companies, designed for training sales conversion prediction models. The dataset was created following the methodology presented in \"SalesRLAgent: A Reinforcement Learning Approach for Real-Time Sales Conversion Prediction and Optimization\" (Nandakishor M, 2025).\nThe dataset contains realistic dialogues between sales representatives and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DeepMostInnovations/saas-sales-conversations.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"bilingual-rlhf-financial-semantics","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sunwang4gptplus/bilingual-rlhf-financial-semantics","creator_name":"Sun Wang ‚Äî Financial & Policy Semantic Contributor","creator_url":"https://huggingface.co/sunwang4gptplus","description":"This dataset was created by Sun Wang as part of a multi-day Reinforcement Learning from Human Feedback (RLHF) semantic evaluation. It contains 46 manually curated semantic correction samples focusing on:\nUX tone misalignment from AI models (e.g., overconfident or misleading instructions)\nFinancial language simplification or distortion\nBilingual (English & Mandarin) policy commentary tone correction\nEach sample includes:\noriginal: the raw AI output\nissue: identified semantic/tone flaws‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sunwang4gptplus/bilingual-rlhf-financial-semantics.","first_N":5,"first_N_keywords":["text-classification","other","English","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"elkins_dataset_trial","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/elkinsqiu/elkins_dataset_trial","creator_name":"elkins.qiu","creator_url":"https://huggingface.co/elkinsqiu","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for elkins_dataset_trial\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/elkinsqiu/elkins_dataset_trial/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/elkinsqiu/elkins_dataset_trial.","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"local_test","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/elkinsqiu/local_test","creator_name":"elkins.qiu","creator_url":"https://huggingface.co/elkinsqiu","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for local_test\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/elkinsqiu/local_test/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/elkinsqiu/local_test.","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"RealStories-Micro-MRL","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ReactiveAI/RealStories-Micro-MRL","creator_name":"Reactive AI","creator_url":"https://huggingface.co/ReactiveAI","description":"\n\t\n\t\t\n\t\tDataset Card for ReactiveAI/RealStories-Micro-MRL\n\t\n\nFirst synthetic Memory Reinforcement Learning dataset for Proof-of-Concept Reactive Transformer models.\nDataset is divided into subsets, used in different Curriculum Stage of MRL training - each subset have\ndifferent number of follow-up interactions, could use different strategy, and have train and validation\nsplits.\n\n\t\n\t\t\n\t\tSubsets\n\t\n\n\nsteps-1: 2300 train (4600 interactions) / 340 validation (680 interactions) - Single-Step Strategy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ReactiveAI/RealStories-Micro-MRL.","first_N":5,"first_N_keywords":["reinforcement-learning","text-retrieval","text-generation","text2text-generation","question-answering"],"keywords_longer_than_N":true},
	{"name":"distilabel_test","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/songchunayuan55/distilabel_test","creator_name":"ChuanyuanSong","creator_url":"https://huggingface.co/songchunayuan55","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for distilabel_test\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/songchunayuan55/distilabel_test/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/songchunayuan55/distilabel_test.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"pacman-small","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimic/pacman-small","creator_name":"Kimi Chen","creator_url":"https://huggingface.co/kimic","description":"Important Note: The new_game_flag column in dataset.csv is incorrect. Please refer to my bigger Pacman dataset for accurate information.\n","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Hinglish-Preference-Humanized-DPO","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fhai50032/Hinglish-Preference-Humanized-DPO","creator_name":"Low IQ Gen AI","creator_url":"https://huggingface.co/fhai50032","description":"![Cover Image](data:image/webp;base64‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fhai50032/Hinglish-Preference-Humanized-DPO.","first_N":5,"first_N_keywords":["Hindi","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"flammenai-Prude-Phi3-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Triangle104/flammenai-Prude-Phi3-DPO","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","description":"ResplendentAI/NSFW_RP_Format_NoQuote inputs ran through Phi-3 Q4 to intentionally produce bad responses to be used for rejections.\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Atsunori-HelpSteer2-DPO","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Triangle104/Atsunori-HelpSteer2-DPO","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","description":"This dataset is a conversion of nvidia/HelpSteer2 into preference pairs based on the helpfulness score for training DPO.\nHelpSteer2-DPO is also licensed under CC-BY-4.0.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nIn accordance with the following paper, HelpSteer2: Open-source dataset for training top-performing reward models\nwe converted nvidia/HelpSteer2 dataset into a preference dataset by taking the response with the higher helpfulness score as the chosen response, with the remaining response being the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Triangle104/Atsunori-HelpSteer2-DPO.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OpenR1-Math-220k-paired","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIR-hl/OpenR1-Math-220k-paired","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","description":"\n\t\n\t\t\n\t\t!!! Is there anyone can help me? https://github.com/huggingface/trl/issues/2994\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset is built by filtering the open-r1/OpenR1-Math-220k dataset according to the following rules:\n\nFirst, filter all of rows with only correct answers\nThe chosen contains the shortest and correct generation, the rejected contains the wrong generation.\nAll data with a prompt+chosen length exceeding 16k are filtered out.\nWe provide the length for both chosen and rejected‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/OpenR1-Math-220k-paired.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-flat-mlx","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-flat-mlx","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a split version of orpo-dpo-mix-40k-flat for direct use with mlx-lm-lora, specifically tailored to be compatible with DPO and CPO training.\nThe dataset has been divided into three parts:\n\nTrain Set: 90%\nValidation Set: 6% \nTest Set: 4%\n\n\n\t\n\t\t\n\t\n\t\n\t\tExample Usage\n\t\n\nTo train a model using this dataset, you can use the following command:\nmlx_lm_lora.train \\\n--model Qwen/Qwen2.5-3B-Instruct \\\n--train \\\n--test \\\n--num-layers 8 \\\n--data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-flat-mlx.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"creative-rubrics-preferences","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences","creator_name":"Victor Gallego","creator_url":"https://huggingface.co/vicgalle","description":"\n\t\n\t\t\n\t\tcreative-rubrics-preferences üéè\n\t\n\nA dataset of creative responses using GPT-4.5, o3-mini and DeepSeek-R1. \nThis dataset contains several prompts seeking creative and diverse answers (like writing movie reviews, short stories, etc), and the style of the responses has been enhanced by prompting the model with custom rubrics that seek different creative styles.\nIt can be used for finetuning for custom styles in writing tasks.\nNote: This is a preference-formatted version of this other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"EMMOE-100","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Dongping-Li/EMMOE-100","creator_name":"Dongping Li","creator_url":"https://huggingface.co/Dongping-Li","description":"\n\t\n\t\t\n\t\tEMMOE-100 Trainset\n\t\n\n\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nProject\nPaper\nCode\nModel\nDataset\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Feature\n\t\n\n\n\n\n  \n    \n      \n      Task Attributes\n    \n    \n      \n      Task Example\n    \n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEMMOE-100/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ assets/\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îî‚îÄ‚îÄ train/\n‚îÇ       ‚îú‚îÄ‚îÄ 1/\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ info.txt\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ info_re1.txt\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ info_re2.txt\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ info_re3.txt\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ keypath.json\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ scene.json\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dongping-Li/EMMOE-100.","first_N":5,"first_N_keywords":["visual-question-answering","reinforcement-learning","robotics","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"augmented_codealpaca-20k-using-together-ai-deepseek-v1","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/eagle0504/augmented_codealpaca-20k-using-together-ai-deepseek-v1","creator_name":"Yiqiao Yin","creator_url":"https://huggingface.co/eagle0504","description":"\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset, named CodeAlpaca-20k, consists of examples that blend coding instructions with outputs and reasoning. Each entry includes structured fields like output, instruction, input, and cot (Chain of Thought). It is particularly designed to train and evaluate AI models that generate code and explanations based on simple programming tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tData Collection and Preparation\n\t\n\nData entries are augmented using the augment_answer function that makes API‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eagle0504/augmented_codealpaca-20k-using-together-ai-deepseek-v1.","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","text-to-speech","English","mit"],"keywords_longer_than_N":true},
	{"name":"Ling-Coder-DPO","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/inclusionAI/Ling-Coder-DPO","creator_name":"inclusionAI","creator_url":"https://huggingface.co/inclusionAI","description":"\n    \n\n\n\n          ü§ó Hugging Face\n          ü§ñ ModelScope\n          üñ•Ô∏è GitHub\n\n\n\n\t\n\t\t\n\t\tLing-Coder Dataset\n\t\n\nThe Ling-Coder Dataset comprises the following components:\n\nLing-Coder-SFT: A subset of SFT data used for training Ling-Coder Lite, containing more than 5 million samples.\nLing-Coder-DPO: A subset of DPO data used for training Ling-Coder Lite, containing 250k samples.\nLing-Coder-SyntheticQA: A subset of synthetic data used for annealing training of Ling-Coder Lite, containing more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/inclusionAI/Ling-Coder-DPO.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ExplainableAI-emotions-DPO-ORPO-RLHF","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF","creator_name":"Hammad Fahim","creator_url":"https://huggingface.co/imhmdf","description":"\n\t\n\t\t\n\t\tPreference Dataset for Explainable Multi-Label Emotion Classification\n\t\n\nThis repository contains a preference dataset compiled to compare two model-generated responses for explaining multi-label emotion classifications on Tweets. The dataset is accompanied by human annotations indicating which response was preferred, based on a set of defined dimensions (clarity, correctness, helpfulness, and verbosity). The annotation guidelines are included to describe how these preference judgments‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ExplainableAI-emotions-DPO-ORPO-RLHF","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF","creator_name":"Hammad Fahim","creator_url":"https://huggingface.co/imhmdf","description":"\n\t\n\t\t\n\t\tPreference Dataset for Explainable Multi-Label Emotion Classification\n\t\n\nThis repository contains a preference dataset compiled to compare two model-generated responses for explaining multi-label emotion classifications on Tweets. The dataset is accompanied by human annotations indicating which response was preferred, based on a set of defined dimensions (clarity, correctness, helpfulness, and verbosity). The annotation guidelines are included to describe how these preference judgments‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ExplainableAI-emotions-DPO-ORPO-RLHF","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF","creator_name":"Hammad Fahim","creator_url":"https://huggingface.co/imhmdf","description":"\n\t\n\t\t\n\t\tPreference Dataset for Explainable Multi-Label Emotion Classification\n\t\n\nThis repository contains a preference dataset compiled to compare two model-generated responses for explaining multi-label emotion classifications on Tweets. The dataset is accompanied by human annotations indicating which response was preferred, based on a set of defined dimensions (clarity, correctness, helpfulness, and verbosity). The annotation guidelines are included to describe how these preference judgments‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-mlx","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-mlx","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a split version of orpo-dpo-mix-40k for direct use with mlx-lm-lora, specifically tailored to be compatible with ORPO training.\nThe dataset has been divided into three parts:\n\nTrain Set: 90%\nValidation Set: 6% \nTest Set: 4%\n\n\n\t\n\t\t\n\t\n\t\n\t\tExample Usage\n\t\n\nTo train a model using this dataset, you can use the following command:\nmlx_lm_lora.train \\\n--model Qwen/Qwen2.5-3B-Instruct \\\n--train \\\n--test \\\n--num-layers 8 \\\n--data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-mlx.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"jat-dataset","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jobs-git/jat-dataset","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","description":"\n\t\n\t\t\n\t\tJAT Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\nPaper: https://huggingface.co/papers/2402.09844\n\n\t\n\t\t\n\t\tUsage\n\t\n\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"jat-project/jat-dataset\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jobs-git/jat-dataset.","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","question-answering","found","machine-generated"],"keywords_longer_than_N":true},
	{"name":"jat-dataset","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jobs-git/jat-dataset","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","description":"\n\t\n\t\t\n\t\tJAT Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\nPaper: https://huggingface.co/papers/2402.09844\n\n\t\n\t\t\n\t\tUsage\n\t\n\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"jat-project/jat-dataset\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jobs-git/jat-dataset.","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","question-answering","found","machine-generated"],"keywords_longer_than_N":true},
	{"name":"fantasiq","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sumukshashidhar-archive/fantasiq","creator_name":"Sumuk's Archived Content","creator_url":"https://huggingface.co/sumukshashidhar-archive","description":"\n\t\n\t\t\n\t\tüêâ FantastiQ\n\t\n\n\n    \n\n\n\nFantastiQ: A fictional reasoning benchmark for evaluating inference and logical capabilities beyond memorization.\n\n\n\t\n\t\t\n\t\tWhat is FantastiQ?\n\t\n\nFantastiQ üêâ is a synthetic benchmark consisting of question-answer pairs crafted around fictional yet internally consistent scenarios. It is designed specifically to assess logical reasoning, inference skills, and robustness against explicit memorization by Large Language Models (LLMs).\nFantastiQ includes multiple‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sumukshashidhar-archive/fantasiq.","first_N":5,"first_N_keywords":["reinforcement-learning","question-answering","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"gemma-vs-gemma-preferences","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\n\t\n\t\t\n\t\tüíéüÜöüíé Gemma vs Gemma Preferences\n\t\n\nThis dataset contains on-policy collected preferences generated using anakin87/gemma-2-2b-ita-sft.\n‚ö†Ô∏è While this dataset may be valuable for didactic purposes, it is not recommended for training a model using Preference Tuning due to the following reasons:\n\nThe training would be off-policy for your model.\nThe dataset was generated with gemma-2-2b-ita-sft, a small model for Italian.\n\n\n\t\n\t\t\n\t\n\t\n\t\tMotivation\n\t\n\nWhile DPO (Direct Preference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences.","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"gemma-vs-gemma-preferences","keyword":"rlhf","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\n\t\n\t\t\n\t\tüíéüÜöüíé Gemma vs Gemma Preferences\n\t\n\nThis dataset contains on-policy collected preferences generated using anakin87/gemma-2-2b-ita-sft.\n‚ö†Ô∏è While this dataset may be valuable for didactic purposes, it is not recommended for training a model using Preference Tuning due to the following reasons:\n\nThe training would be off-policy for your model.\nThe dataset was generated with gemma-2-2b-ita-sft, a small model for Italian.\n\n\n\t\n\t\t\n\t\n\t\n\t\tMotivation\n\t\n\nWhile DPO (Direct Preference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences.","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"uplimit-synthetic-data-week-1-with-seed","keyword":"rlaif","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marotosg/uplimit-synthetic-data-week-1-with-seed","creator_name":"Sergio Garcia Maroto","creator_url":"https://huggingface.co/marotosg","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for uplimit-synthetic-data-week-1-with-seed\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/marotosg/uplimit-synthetic-data-week-1-with-seed/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marotosg/uplimit-synthetic-data-week-1-with-seed.","first_N":5,"first_N_keywords":["apache-2.0","n<1K","Distilabel","arxiv:2212.10560","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"example-dataset","keyword":"rlaif","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CoffeeDoodle/example-dataset","creator_name":"Shalini Sundaram","creator_url":"https://huggingface.co/CoffeeDoodle","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for example-dataset\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/CoffeeDoodle/example-dataset/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CoffeeDoodle/example-dataset.","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"NuminaMath-1.5-RL-Verifiable","keyword":"rl","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nlile/NuminaMath-1.5-RL-Verifiable","creator_name":"nathan lile","creator_url":"https://huggingface.co/nlile","description":"\n\t\n\t\t\n\t\tDataset Card for NuminaMath-1.5-RL-Verifiable\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nNuminaMath-1.5-RL-Verifiable is a curated subset of the NuminaMath-1.5 dataset, specifically filtered to support reinforcement learning applications requiring verifiable outcomes. This collection consists of 131,063 math word problems from the original dataset that meet strict filtering criteria: all problems have definitive numerical answers, validated problem statements and solutions, and come from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nlile/NuminaMath-1.5-RL-Verifiable.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"rlhf-medical-diagnosis-dummy-dataset","keyword":"rlhf","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nuriyev/rlhf-medical-diagnosis-dummy-dataset","creator_name":"Mahammad Nuriyev","creator_url":"https://huggingface.co/nuriyev","description":"nuriyev/rlhf-medical-diagnosis-dummy-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"2k-ranked-images-open-image-preferences-v1","keyword":"rl","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/2k-ranked-images-open-image-preferences-v1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\t2k Ranked Images\n\t\n\nThis dataset contains roughly two thousand images ranked from most preferred to least preferred based on human feedback on pairwise comparisons (>25k responses). \nThe generated images, which are a sample from the open-image-preferences-v1 dataset \nfrom the team @data-is-better-together, are rated purely based on aesthetic preference, disregarding the prompt used for generation.\nWe provide the categories of the original dataset for easy filtering.\nThis is a new‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/2k-ranked-images-open-image-preferences-v1.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"swedish_healthcare_dpo_sft_dataset","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset","creator_name":"Rzgar","creator_url":"https://huggingface.co/rzgar","description":"\n\t\n\t\t\n\t\tSwedish Healthcare DPO/SFT Dataset\n\t\n\nThis dataset contains Swedish conversational prompts paired with chosen (preferred) and rejected (dispreferred) responses, along with English translations. It is designed for fine-tuning Large Language Models (LLMs) using preference-based methods. The (prompt, chosen, rejected) structure makes it particularly well-suited for Direct Preference Optimization (DPO) and similar algorithms.\nSweden is renowned for its high standards in healthcare and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset.","first_N":5,"first_N_keywords":["Swedish","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"swedish_healthcare_dpo_sft_dataset","keyword":"dpo","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset","creator_name":"Rzgar","creator_url":"https://huggingface.co/rzgar","description":"\n\t\n\t\t\n\t\tSwedish Healthcare DPO/SFT Dataset\n\t\n\nThis dataset contains Swedish conversational prompts paired with chosen (preferred) and rejected (dispreferred) responses, along with English translations. It is designed for fine-tuning Large Language Models (LLMs) using preference-based methods. The (prompt, chosen, rejected) structure makes it particularly well-suited for Direct Preference Optimization (DPO) and similar algorithms.\nSweden is renowned for its high standards in healthcare and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset.","first_N":5,"first_N_keywords":["Swedish","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"TelePlanNet","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaiYujie/TelePlanNet","creator_name":"caiyujie","creator_url":"https://huggingface.co/CaiYujie","description":"CaiYujie/TelePlanNet dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["reinforcement-learning","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"TelePlanNet","keyword":"rl","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaiYujie/TelePlanNet","creator_name":"caiyujie","creator_url":"https://huggingface.co/CaiYujie","description":"CaiYujie/TelePlanNet dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["reinforcement-learning","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"pitfall","keyword":"reinforcement-learning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimic/pitfall","creator_name":"Kimi Chen","creator_url":"https://huggingface.co/kimic","description":"kimic/pitfall dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","100K - 1M","webdataset"],"keywords_longer_than_N":true},
	{"name":"bird-rl","keyword":"reinforcement-learning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rihong/bird-rl","creator_name":"Rihong","creator_url":"https://huggingface.co/Rihong","description":"\n\t\n\t\t\n\t\tBIRD-RL\n\t\n\n\n\nThis dataset is a processed dataset of BIRD-SQL for Post-Training. \n","first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"gutenberg-dpo-v0.1-jondurbin","keyword":"dpo","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GenRM/gutenberg-dpo-v0.1-jondurbin","creator_name":"GenRM: Generative Reward Models","creator_url":"https://huggingface.co/GenRM","description":"\n\t\n\t\t\n\t\tGutenberg DPO\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is a dataset meant to enhance novel writing capabilities of LLMs, by using public domain books from Project Gutenberg\n\n\t\n\t\t\n\t\tProcess\n\t\n\nFirst, the each book is parsed, split into chapters, cleaned up from the original format (remove superfluous newlines, illustration tags, etc.).\nOnce we have chapters, an LLM is prompted with each chapter to create a synthetic prompt that would result in that chapter being written.\nEach chapter has a summary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GenRM/gutenberg-dpo-v0.1-jondurbin.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"transcription-scorer","keyword":"reinforcement-learning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RobotsMali/transcription-scorer","creator_name":"RobotsMali AI4D LAB","creator_url":"https://huggingface.co/RobotsMali","description":"\n\t\n\t\t\n\t\tTranscription Scorer Dataset\n\t\n\nThe Transcription Scorer dataset was created to support research in reference-free evaluation of Automatic Speech Recognition (ASR) systems using human feedback. Unlike traditional evaluation metrics such as WER and its derivatives, this dataset reflects subjective judgments of ASR outputs by human raters across multiple criteria, simulating the way a teacher grades students.\n\n\t\n\t\t\n\t\n\t\n\t\t‚öôÔ∏è What‚Äôs Inside\n\t\n\nThis dataset contains 2153 audio samples (from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RobotsMali/transcription-scorer.","first_N":5,"first_N_keywords":["automatic-speech-recognition","reinforcement-learning","audio-classification","expert-annotated","found"],"keywords_longer_than_N":true},
	{"name":"transcription-scorer","keyword":"human-feedback","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RobotsMali/transcription-scorer","creator_name":"RobotsMali AI4D LAB","creator_url":"https://huggingface.co/RobotsMali","description":"\n\t\n\t\t\n\t\tTranscription Scorer Dataset\n\t\n\nThe Transcription Scorer dataset was created to support research in reference-free evaluation of Automatic Speech Recognition (ASR) systems using human feedback. Unlike traditional evaluation metrics such as WER and its derivatives, this dataset reflects subjective judgments of ASR outputs by human raters across multiple criteria, simulating the way a teacher grades students.\n\n\t\n\t\t\n\t\n\t\n\t\t‚öôÔ∏è What‚Äôs Inside\n\t\n\nThis dataset contains 2153 audio samples (from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RobotsMali/transcription-scorer.","first_N":5,"first_N_keywords":["automatic-speech-recognition","reinforcement-learning","audio-classification","expert-annotated","found"],"keywords_longer_than_N":true},
	{"name":"DeepSeek-R1-Distill-Qwen-1.5B-pts-dpo-pairs","keyword":"dpo","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/codelion/DeepSeek-R1-Distill-Qwen-1.5B-pts-dpo-pairs","creator_name":"Asankhaya Sharma","creator_url":"https://huggingface.co/codelion","description":"\n\t\n\t\t\n\t\tPTS DPO Dataset\n\t\n\nA Direct Preference Optimization (DPO) dataset created using the Pivotal Token Search (PTS) technique.\n\n\t\n\t\t\n\t\tDetails\n\t\n\n\nSource: Generated using the PTS tool\nModel: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example in the dataset consists of:\n\nprompt: The context leading up to the pivotal token\nchosen: The preferred token that increases success probability\nrejected: The alternative token that decreases success probability\nmetadata:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/codelion/DeepSeek-R1-Distill-Qwen-1.5B-pts-dpo-pairs.","first_N":5,"first_N_keywords":["English","apache-2.0","1K<n<10K","üá∫üá∏ Region: US","dpo"],"keywords_longer_than_N":true}
]
;
